{"primary": "Physical Sciences and Engineering",
"domain": "Chemical Engineering",
"subdomain": "Chemical Engineering (General)",
"journal name": "Computers & Chemical Engineering",
"articles": [
    {"article name": "Receding horizon scheduling of processes with shared resources",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.03.005",
     "publication date": "06-2019",
     "abstract": "The paper deals with the integration of shared resources in the formulation of process industry scheduling problems, with reference to the optimal operation of those processes that are composed by continuous and batch units tightly integrated by means of mass or energy flows. It uses as a reference a benchmark developed in the EU project HYCON2: a simplified crystallization section of a sugar factory. The aim is to present a novel formulation that allows the coordination of the continuous and discrete processes with shared resources generating feasible schedules that can be easily integrated in a wider optimization scheme. The approach uses formal scheduling methods, mixing three different time frames: one in continuous time for scheduling, another to describe the time evolution of the shared resources and a discrete one for synchronization of the shared resources and the integration with the continuous dynamics.",
     "keywords": ["Real-time optimization", "Process scheduling", "Shared resources", "Continuous-batch process optimization", "Sugar crystallizers"]},
    {"article name": "An MINLP formulation for the optimization of multicomponent distillation configurations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.02.013",
     "publication date": "06-2019",
     "abstract": "Designing configurations for multicomponent distillation, a ubiquitous process in chemical and petrochemical industries, is often challenging. This is because, as the number of components increases, the number of admissible distillation configurations grows rapidly and these configurations vary substantially in their energy needs. Consequently, if a method could identify a few energy-efficient choices from this large set of alternatives, it would be extremely attractive to process designers. This paper develops such a method by solving a Mixed Integer Nonlinear Program (MINLP) that is formulated to pick, among the regular-column configurations of Shah and Agrawal (2010b), those configurations that have a low vapor-duty requirement. To compute the minimum vapor-duty requirement for each column within the configuration, we use techniques that rely on the Underwood\u2019s method. The combined difficulty arising from the nonlinearity of Underwood equations and the combinatorial explosion of the choice-set of alternatives poses unmistakable challenges for the branch-and-bound algorithm, the current method of choice to globally solve MINLPs. To address this difficulty, we exploit the structure of Underwood equations and derive valid cuts that expedite the convergence of branch-and-bound by enabling global solvers, such as BARON, infer tighter bounds on Underwood roots. This provides a quick way to identify a few lucrative alternative configurations for separation of a given non-azeotropic mixture. We illustrate the practicality of our approach on a case-study concerning heavy-crude distillation and on various other examples from the literature.",
     "keywords": ["Multicomponent distillation", "Mixed integer nonlinear program", "Fractional program", "Global optimization"]},
    {"article name": "Kaibel column: Modeling, optimization, and conceptual design of multi-product dividing wall columns",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.03.006",
     "publication date": "06-2019",
     "abstract": "In this work, we present the modeling, optimization, and conceptual design of a dividing wall column for the separation of four products, commonly referred to in the literature as a Kaibel column, by implementing three different formulations: an NLP, an MINLP, and a GDP formulation. For its solution, we propose a rigorous tray-by-tray model and compared it to results from a commercial software, followed by its reformulation to include a mixed-integer nonlinear programming and a general disjunctive programming formulation to respond to the conceptual design problem attached to these complex configurations. Considering the proposed rigorous model and the two formulations, the Kaibel column is solved, obtaining four high-purity products and new optimal tray locations for the feed and two side product streams, when the mixed-integer nonlinear programming formulation is applied. The use of these optimally located side streams showed reductions in the energy consumption when compared to cases were non-optimal fixed tray locations are used. When the general disjunctive programming problem was solved, the minimum number of trays needed in the main column and dividing wall are obtained, showing a great reduction of the remixing effects in the Kaibel column, and with that, a more energy efficient configuration. The models were coded in Pyomo using the solver IPOPT for the solution of the nonlinear programming problem, the solver Bonmin for the solution of the mixed-integer nonlinear programming problem, and GDPopt for the solution of the general disjunctive programming optimization problem.",
     "keywords": ["Kaibel column", "Dividing wall columns", "Nonlinear programming optimization", "Mixed integer nonlinear programming optimization", "General disjunctive programming", "CDC Continuous Distillation Column", "Continuous Distillation Column", "DWC Dividing Wall Column", "Dividing Wall Column", "GDP General Disjunctive Programming", "General Disjunctive Programming", "KC Kaibel Column", "Kaibel Column", "MINLP Mixed Integer Nonlinear Programming", "Mixed Integer Nonlinear Programming", "ne Dividing Wall Ending Tray", "Dividing Wall Ending Tray", "ns Dividing Wall Starting Tray", "Dividing Wall Starting Tray", "NLP Nonlinear Programming", "Nonlinear Programming", "TAC Total Annual Cost", "Total Annual Cost"]},
    {"article name": "MILP model for a packed bed sensible thermal energy storage",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.03.007",
     "publication date": "06-2019",
     "abstract": "In this work, MILP formulations for a packed bed sensible thermal energy storage for the integration in a UC-framework are presented, taking into account the storage temperature. First, a model for power and temperature from previous work is extended to describe the maximum available power and the storage temperature more precisely. Second, a formulation is presented that satisfies a minimum temperature requirement at the process demand side by mixing the storage mass flow with hot mass flow from a generating unit, if necessary. Third, an approximation of the storage saturation losses during the charging process is presented. All proposed formulations are compared by simulations with receding horizon with feedback of an accurate simulation model. The results show that the minimum temperature requirement can be adhered with the corresponding formulation and the consideration of saturation losses has a significant impact on the system performance and should not be neglected.",
     "keywords": ["Packed bed regenerator", "Mixed integer linear programming", "Optimization", "Sensible thermal energy storage", "Unit commitment"]},
    {"article name": "GPdoemd: A Python package for design of experiments for model discrimination",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.03.010",
     "publication date": "06-2019",
     "abstract": "Model discrimination identifies a mathematical model that usefully explains and predicts a given system\u2019s behaviour. Researchers will often have several models, i.e. hypotheses, about an underlying system mechanism, but insufficient experimental data to discriminate between the models, i.e. discard inaccurate models. Given rival mathematical models and an initial experimental data set, optimal design of experiments suggests maximally informative experimental observations that maximise a design criterion weighted by prediction uncertainty. The model uncertainty requires gradients, which may not be readily available for black-box models. This paper (i) proposes a new design criterion using the Jensen-R\u00e9nyi divergence, and (ii) develops a novel method replacing black-box models with Gaussian process surrogates. Using the surrogates, we marginalise out the model parameters with approximate inference. Results show these contributions working well for both classical and new test instances. We also (iii) introduce and discuss GPdoemd, the open-source implementation of the Gaussian process surrogate method.",
     "keywords": ["Design of experiments", "Model discrimination", "Jensen\u2013R\u00e9nyi divergence", "Gaussian processes", "Open-source software"]},
    {"article name": "Dynamic scheduling of multi-product continuous biopharmaceutical facilities: A hyper-heuristic framework",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.03.002",
     "publication date": "06-2019",
     "abstract": "The biopharmaceutical industry is increasingly interested in moving from batch to semi-continuous manufacturing processes. These continuous bioprocesses are more failure-prone and process failure is more consequential. In addition, the probability of failure is dependent on process run time which generally is determined independent of scheduling considerations. This work presents a discrete-event simulation of continuous bioprocesses in a scheduling environment. Dynamic scheduling policies are investigated to make operational decisions in a multi-product manufacturing facility and react to process failure events and uncertain demand. First, different scheduling policies are adapted from the stochastic lot sizing literature and a novel look-ahead scheduling policy is proposed. Then, policy parameters (including process run time) are tuned using evolutionary algorithms. Our results demonstrate that the tuned policies perform much better than a policy that estimates policy parameters based on service level considerations and a policy based on a fixed cyclical sequence.",
     "keywords": ["Stochastic economic lot scheduling problem", "Hyper-heuristics", "Biopharmaceutical manufacture", "Perfusion", "Simulation optimisation", "Machine failure"]},
    {"article name": "Model reformulations for Work and Heat Exchange Network (WHEN) synthesis problems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.02.018",
     "publication date": "06-2019",
     "abstract": "The Duran-Grossmann model can deal with heat integration problems with variable process streams. Work and Heat Exchange Networks (WHENs) represent an extension of Heat Exchange Networks. In WHEN problems, the identities of streams (hot/cold) are regarded as variables. The original Duran-Grossmann model has been extended and applied to WHENs without knowing the identity of streams a priori. In the original Duran-Grossmann model, the max operator is a challenge for solving the model. This paper analyzes four ways to reformulate the Duran-Grossmann model. Smooth Approximation, Explicit Disjunctions, Direct Disjunctions and Intermediate Temperature strategy are reviewed and compared. The Extended Duran-Grossmann model for WHEN problems consists of both binary variables and non-smooth functions. The Extended Duran-Grossmann model can be reformulated in similar ways. In this study, the performance of different reformulations of the Extended Duran-Grossmann model for WHEN problems are compared based on a small case study in this paper.",
     "keywords": ["Work and heat exchange networks", "Duran-Grossmann model", "Reformulations", "disjunctive programming", "MINLP"]},
    {"article name": "A Multi-Parametric optimization approach for bilevel mixed-integer linear and quadratic programming problems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.01.021",
     "publication date": "06-2019",
     "abstract": "Optimization problems involving two decision makers at two different decision levels are referred to as bi-level programming problems. In this work, we present novel algorithms for the exact and global solution of two classes of bi-level programming problems, namely (i) bi-level mixed-integer linear programming problems (B-MILP) and (ii) bi-level mixed-integer convex quadratic programming problems (B-MIQP) containing both integer and bounded continuous variables at both optimization levels. Based on multi-parametric programming theory, the main idea is to recast the lower level problem as a multi-parametric programming problem, in which the optimization variables of the upper level problem are considered as bounded parameters for the lower level. The resulting exact multi-parametric mixed-integer linear or quadratic solutions are then substituted into the upper level problem, which can be solved as a set of single-level, independent, deterministic mixed-integer optimization problems. Extensions to problems including right-hand-side uncertainty on both lower and upper levels are also discussed. Finally, computational implementation and studies are presented through test problems.",
     "keywords": ["Bilevel programming", "Multi parametric programming", "Mixed-integer programming"]},
    {"article name": "Parallel column model for Dividing Wall Column simulations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.02.008",
     "publication date": "06-2019",
     "abstract": "Dividing Wall Columns (DWC) have attracted considerable attention in recent years. It is often stated that no commercial flowsheet simulation package has yet offered a DWC as a standard model. As a result, nearly all simulations of DWCs employ interlinked multi-column equilibrium stage models in a sequential-modular process simulator. It is claimed that equation-based simulators offer advantages over sequential modular simulators when modelling DWCs but, to date, no empirical evidence in support of such a contention has been published.In this paper we describe an equation-based parallel column model (PCM) designed to enable the straightforward simulation of an entire DWC as though it were a single column. With the aid of simulations of several different DWCs with one, two, or three walls, and with one or two condensers, we demonstrate that an equation-oriented PCM can have good numerical performance, arguably superior to sequential-modular multi-column models.",
     "keywords": ["Dividing Wall Column", "DWC", "Parallel column model", "Equation-based"]},
    {"article name": "Graph-based modeling and simulation of complex systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.03.009",
     "publication date": "06-2019",
     "abstract": "We present graph-based modeling abstractions to represent cyber-physical dependencies arising in complex systems. Specifically, we propose an algebraic graph abstraction to capture physical connectivity in complex optimization models and a computing graph abstraction to capture communication connectivity in computing architectures. The proposed abstractions are scalable and are used as the backbone of a Julia -based software package that we call Plasmo . jl . We show how the algebraic graph abstraction facilitates the implementation, analysis, and decomposition of optimization problems and we show how the computing graph abstraction facilitates the implementation of optimization and control algorithms and their simulation in virtual environments that involve distributed, centralized, and hierarchical computing architectures.",
     "keywords": ["Graphs", "Cyber-physical", "Connectivity", "Algebraic", "Computing"]},
    {"article name": "Integrated approach for the bucking and production planning problems in forest industry",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.03.008",
     "publication date": "06-2019",
     "abstract": "The appropriate integration among production activities is a key factor for the development of the forest industry in the northeast of Argentina. Sawmills are one of the main logs consumers. However, production planning of sawmills has been managed independently of the forest production. In this work, a mixed integer linear programming formulation is proposed in order to achieve an adequate logs supply that allows carrying out an efficient lumber production plan. Bucking decisions, as the number of stems to be harvested and the employed bucking patterns, are jointly solved with sawmill planning decisions, like the number and type of processed logs and the used cutting patterns, in order to fulfill the boards demands maximizing the net profit along the time horizon composed by several time periods (days). Through examples, the capabilities of the proposed approach are highlighted and the computational results are analyzed.",
     "keywords": ["Bucking", "Log procurement policy", "Sawmill production planning", "Mixed integer linear programming model"]},
    {"article name": "Integrated process design, scheduling, and control using multiparametric programming",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.03.004",
     "publication date": "06-2019",
     "abstract": "A unified theory and framework for the integration of process design, control, and scheduling based on a single high fidelity model is presented. The framework features (i) a mixed-integer dynamic optimization (MIDO) formulation with design, scheduling, and control considerations, and (ii) a multiparametric optimization strategy for the derivation of offline/explicit maps of optimal receding horizon policies. Explicit model predictive control schemes are developed as a function of design and scheduling decisions, and similarly design dependent scheduling policies are derived accounting for the closed-loop dynamics. Inherent multi-scale gap issues are addressed by an offline design dependent surrogate model. The proposed framwork is illustrated by two example problems, a system of two continuous stirred tank reactor, and a small residential combined heat and power (CHP) network.",
     "keywords": ["Enterprise-wide optimization", "Integration", "Multi-parametric programming", "Process scheduling", "Model predictive control", "Process design"]},
    {"article name": "Improved method to converge pressure equalization steps when simulating a cyclic adsorption process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.03.021",
     "publication date": "06-2019",
     "abstract": "Two new techniques for single-vessel Pressure or Vacuum Swing Adsorption (P-VSA) mathematical models are presented that improve convergence stability to Cyclic Steady State (CSS) when Pressure Equalization (PE) steps are involved.The first method, the Reflected Equalization Pressure (REP) method uses simplifying assumptions to derive a simple and efficient analytical solution for the unknown pressure profile boundary condition (BC) to use during PE steps on the first simulated cycle of a P-VSA simulation without the need for user-defined functions or additional ODEs.The second method, the Weighted Store-and-Retrieve (WSR) method is a variation on the Traditional Store-and-Retrieve (TSR) method for simulating PE steps where gases flow from one vessel to another in a single-vessel mathematical model. The WSR method retains history across several cycles, passing this back into the mathematical model via weighting factors to average the data and dampen the oscillatory response of a mathematical model to PE steps.Several examples of this technique are applied to the O2 VSA and H2 PSA systems.",
     "keywords": ["Pressure Swing Adsorption", "Vacuum Swing Adsorption", "Pressure Equalization", "V-PSA Mathematical Model", "O2 VSA", "H2 PSA"]},
    {"article name": "Operational safety of chemical processes via Safeness-Index based MPC: Two large-scale case studies",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.03.003",
     "publication date": "06-2019",
     "abstract": "This work presents two applications of Safeness Index-based model predictive control to improve process operational safety in safety critical chemical processes. In the first case study, a high-pressure flash drum separator together with pressure relief valve as safety system is used to analyze the benefits of integrating Safeness Index-based considerations in model predictive control (MPC). In the second case study, four units in an ammonia process are simulated to demonstrate the application of Safeness Index-based MPC to handle significant disturbances.",
     "keywords": ["Process safety", "Operational safety", "Process control", "Model predictive control", "Nonlinear processes"]},
    {"article name": "A perspective on Quality-by-Control (QbC) in pharmaceutical continuous manufacturing",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.03.001",
     "publication date": "06-2019",
     "abstract": "The Quality-by-Design (QbD) guidance issued by the US Food and Drug Administration (FDA) has catalyzed the modernization of pharmaceutical manufacturing practices including the adoption of continuous manufacturing. Active process control was highlighted recently as a means to improve the QbD implementation. This advance has since been evolving into the concept of Quality-by-Control (QbC). In this study, the concept of QbC is discussed, including a definition of QbC, a review of the recent developments towards the QbC, and a perspective on the challenges of QbC implementation in continuous manufacturing. The QbC concept is demonstrated using a rotary tablet press, integrated into a pilot scale continuous direct compaction process. The results conclusively showed that active process control, based on product and process knowledge and advanced model-based techniques, including data reconciliation, model predictive control (MPC), and risk analysis, is indispensable to comprehensive QbC implementation, and ensures robustness and efficiency.",
     "keywords": ["Pharmaceutical continuous manufacturing", "Process control", "Quality-by-Design", "Quality-by-Control", "Process automation", "Systems integration"]},
    {"article name": "Assisting continuous biomanufacturing through advanced control in downstream purification",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.03.013",
     "publication date": "06-2019",
     "abstract": "Aiming to significantly improve their processes and secure market share, monoclonal antibody (mAb) manufacturers seek innovative solutions that will yield improved production profiles. In that space, continuous manufacturing has been gaining increasing interest, promising more stable processes with lower operating costs. However, challenges in the operation and control of such processes arise mainly from the lack of appropriate process analytics tools that will provide the required measurements to guarantee product quality. Here we demonstrate a Process Systems Engineering approach for the design a novel control scheme for a semi-continuous purification process. The controllers are designed employing multi-parametric Model Predictive Control (mp-MPC) strategies and the successfully manage to: (a) follow the system periodicity, (b) respond to measured disturbances and (c) result in satisfactory yield and product purity. The proposed strategy is also compared to experimentally optimized profiles, yielding a satisfactory agreement.",
     "keywords": ["Control", "Monoclonal antibodies", "Manufacturing", "Continuous processes"]},
    {"article name": "CFD\u2013DEM\u2013PBM coupled model development and validation of a 3D top-spray fluidized bed wet granulation process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.01.023",
     "publication date": "06-2019",
     "abstract": "In pharmaceutical manufacturing, fluidized bed granulation is one of the common processing options available to achieve better flowability of powders through size enlargement of primary particles. In fact over the last 50 years, various fluidized bed operations including freezing, drying, impregnation, coating, etc. have become a common place in the chemical processing industry due to the high level of contacts between fluids and solids attainable in a fluid bed system. These complex interactions between the fluid and particles also mean that simulating fluidized beds are still a challenging endeavor. Generally, Computational Fluid Dynamics (CFD) packages are employed to model the pressure drops in fluids; however, the presence of high concentration of solids and the complexity of granulation behavior require more advanced particle models than are available with CFD software. As a result, coupled frameworks that utilize the strength of particulate simulations such as Discrete Element Method (DEM) and bulk granulation modeling such as Population Balance Model (PBM) in conjunction with CFD information are the next steps to developing practical fluid bed granulation models.This paper aims to provide a comprehensive description of the development and validation of a coupled CFD\u2013DEM\u2013PBM framework for a fluidized bed wet granulation operation. A two-way coupled CFD\u2013DEM model is developed for a 3-dimensional, lab-scale, top spray fluid bed granulator to study the effects of process parameters such as inlet air flow rate and inlet air temperature on the particle flow dynamics and the residence time in the spray zone. A one-way transfer of data from CFD\u2013DEM to PBM is then applied to relate the effects of particle-fluid interactions to granulation behavior occurring within the fluidized bed system. Mechanistic rate expressions were developed in the PBM to create links between CFD\u2013DEM results and PBM rate kernels which can express the effect of critical process parameters (CPPs) such as air flow rate, inlet air temperature, binder spray rate, etc. to experimentally measured critical quality attributes (CQAs) including granule size distribution and liquid content values. From comparison with experimental results, the framework presented shows good accuracy at capturing the dynamics of the system. The presented framework demonstrates a practical process model development methodology by efficiently coupling multi-phase simulation techniques which can be used for effective process design, development and scale-up purposes.",
     "keywords": ["Fluidized bed granulation", "Hybrid particulate modeling", "Granulation", "Population balance model", "Discrete element model", "Computational fluid dynamics", "FBG Fluidized Bed Granulation", "Fluidized Bed Granulation", "CFD Computational Fluid Dynamics", "Computational Fluid Dynamics", "DEM Discrete Element Method", "Discrete Element Method", "PBM Population Balance Method", "Population Balance Method", "CPP Critical Processing Parameters", "Critical Processing Parameters", "CQA Critical Quality Attributes", "Critical Quality Attributes"]},
    {"article name": "Design space determination and process optimization in at-scale continuous twin screw wet granulation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.03.026",
     "publication date": "06-2019",
     "abstract": "At-scale industrial continuous twin screw wet granulation process was analyzed and optimized via design of experiment (DoE). The Box-Behnken method was selected to investigate the influence of key process variables on granule properties and tablet properties. The effects of the key process variables were analyzed. The experimental results have demonstrated that both the granule properties and tablet properties are influenced by the throughput, the screw speed and the screw element. Particularly, the screw element type and the milling process were found having significant effects on tablet tensile strength and disintegration time. The design space (DS) based on volume average granule diameter (Y4\u202f<\u202f800\u202f\u00b5m) and tablet disintegration time (Y8\u202f<\u202f4\u202fmin) was determined using Monte Carlo simulations. Validation experiments have shown the robustness of the DS. A case study of the DS application is presented. It was demonstrated how to select optimum working conditions of key process parameters based on the DS.",
     "keywords": null},
    {"article name": "Production scheduling and linear MPC: Complete integration via complementarity conditions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.01.024",
     "publication date": "06-2019",
     "abstract": "Competitive global market conditions and the availability of real-time pricing data call for agile and flexible operations in the chemical process industries. As the operating paradigm shifts, process scheduling decisions may consider a shorter time scale, and interactions with the process control layer become significant. The integration of these two decision-making layers is an active area of research. In this work, we propose a novel framework for the integration of production scheduling and linear model predictive control. Our approach explicitly represents the closed-loop response of the system by embedding the KKT conditions of the controller in a nonlinear programming formulation of the optimal scheduling problem. We present two case studies which demonstrate that control-informed scheduling leads to superior realized performance compared with a hierarchical, sequential decision-making structure.",
     "keywords": ["Optimal scheduling", "Process control", "MPC", "Integrated scheduling and control", "Mathematical program with complementarity constraints"]},
    {"article name": "Strategic planning of supply chains considering extreme events: Novel heuristic and application to the petrochemical industry",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.03.020",
     "publication date": "06-2019",
     "abstract": "Chemical supply chains are a crucial component in the ongoing supply of large population centres. Unfortunately, episodes of extreme weather have, in recent years, revealed vulnerabilities in global supply networks to high-impact events. With a possible increase in both frequency and intensity of these events due to climate change, supply chains are at risk of disruption now more than ever, with potential dire economic, societal, and environmental consequences. Acknowledging that the direct application of stochastic programming in this context can quickly lead to very large CPU times, we propose an algorithm that combines the sample average approximation method with a selection heuristic for extreme event scenarios. Our method allows to analyse the tradeoff between economic performance and disruption risk, identifying supply chain configurations which are more resilient against extreme events. We demonstrate the effectiveness of this methodology in multiple case studies, showing how it identifies near optimal solutions in short CPU times.",
     "keywords": ["Supply chains", "Optimization", "Stochastic programming", "Climate change", "Extreme events", "Risk management"]},
    {"article name": "The Monte Carlo driven and machine learning enhanced process simulator",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.03.016",
     "publication date": "06-2019",
     "abstract": "This study presents a methodology with tools integration to apply advanced uncertainty propagation and sensitivity analysis in connection with commercial process simulation software. The methodology was applied to two processes: a heat pump system and a molecular distillation process. The input parameters of the selected thermodynamic model, namely critical temperature, critical pressure and acentric factor, were considered as a source of uncertainty and analysed using Monte Carlo sampling techniques. This enabled the process model output uncertainty to be described as an empirical distribution function with a 95% confidence interval. Variance-based decomposition such as the Sobol method or standard regression were used to analyse the sensitivity of the respective properties. We also show that machine learning methods such as polynomial chaos expansion (PCE) can be applied to reduce the number of necessary process simulations and obtained equivalent results in comparison with the more costly full Monte Carlo based procedure.",
     "keywords": ["Process simulation", "Uncertainty", "Sensitivity", "Polynomial chaos"]},
    {"article name": "Optimal site selection for modular nuclear power plants",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.03.024",
     "publication date": "06-2019",
     "abstract": "Small Modular Reactor (SMR) is a small, compact version of a conventional Nuclear Power Plant (NPP) and holds much promise for the future. Installing SMRs in a region requires a series of carefully planned steps out of which site selection is a critical one. This paper proposes a novel mathematical model for evaluating potential land sites for their suitability of hosting a modular NPP. Most existing decision making tools for NPP site selection rely on qualitative information from the experts. These tools require significant resources and therefore can only be applied to a limited number of selected sites. The proposed model is a Mixed Integer Non-linear Programming (MINLP) formulation which considers a variety of factors like cost, cooling water availability, earthquake risk, etc. to identify best locations for the SMRs in a distributed power system. A case study based on a virtual EIP simulator is conducted to demonstrate the capabilities of the model by finding the optimal locations of modular NPPs. The model offers a preliminary platform for carrying out further extensive studies.",
     "keywords": ["Site selection", "Small Modular Reactors", "MINLP", "Risk assessment", "NPP Nuclear Power Plant", "Nuclear Power Plant", "EIP Eco Industrial Park", "Eco Industrial Park", "JPS J Park Simulator", "J Park Simulator", "MINLP Mixed Integer Nonlinear Programming", "Mixed Integer Nonlinear Programming", "SMR Small Modular Reactor", "Small Modular Reactor", "PWR Pressurized Water Reactor", "Pressurized Water Reactor", "GAMS General Algebraic Modeling System", "General Algebraic Modeling System"]},
    {"article name": "Optimal scheduling of a by-product gas supply system in the iron- and steel-making process under uncertainties",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.01.025",
     "publication date": "06-2019",
     "abstract": "This paper addresses the real-time by-product gas scheduling in an integrated iron- and steel-making industry with uncertainty in by-product gas flows from a rolling horizon algorithm. Adaptive time-series models determined from real data performs forecast for each producer and consumer of by-product gases in main units of the steel-making plant. The individual consumptions of the blast furnace and coke oven gases are modelled using the seasonal Holt-Winters method with smoothing constants estimated via genetic algorithm, whereas the individual productions of the blast furnace and coke oven are identified from autoregressive and integrated moving-average. LDG gas production is forecasted using a heuristic method that leverages the operational information. The model\u2019s parameters are updated periodically due to the nonlinearities present in the time series. After the forecasting phase, the algorithm performs short-term decisions using a MILP optimization model, that minimizes the imbalance between the random dynamics of by-product fuel generation and consumption and maximizes the energy efficiency. Through computational simulations, we show that the operational stability of gas holders and the electrical energy production increase, whereas the waste of gas in flare stack decreases, when the control horizon of the rolling horizon algorithm is reduced.",
     "keywords": ["Rolling horizon algorithm", "Scheduling", "By-product gas forecasting", "Iron- and steel-making process", "Gas holder level control", "Energy saving"]},
    {"article name": "Nonlinear model predictive control of a climatization system using rigorous nonlinear model",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.03.014",
     "publication date": "06-2019",
     "abstract": "Buildings are known for being great consumers of energy and for operating under On/Off and linear control strategies in common commercial packages. In order to reduce their energy footprints and to improve thermal comfort, new methodologies are needed to obtain better input profiles. This paper presents a detailed study of the implementation of a nonlinear model predictive control (NMPC) approach for a heating, ventilation, and air conditioning (HVAC) system. The HVAC system is modeled by an index-1 differential-algebraic system of equations, obtained from rigorous material and energy balances. The proposed NMPC algorithm was implemented in GAMS and compared with another approach from the literature. Similarly, the objective function is defined using one or more of the following criteria: (1) tracking of temperature and relative humidity set points; (2) maximization of thermal comfort; (3) minimization of energy consumption. To better represent the situation in a real application, the simulations include model-plant mismatch in parameters such as air infiltration into the building, the coefficient of the thermal exchange between the building and the exterior, and occupancy level. Additionally, simulations with random measurement noise have been performed. The results have shown that the proposed approach, in which the model is not linearized at any step, is able to reduce the energy consumption and maintain the Predicted Mean Vote (PMV) close to the desired set point, while the computation burden is only increased by one second per iteration, which is negligible in comparison with a sampling time of 10\u00a0min.",
     "keywords": ["Model predictive control", "Energy efficiency", "Building climatization", "NMPC", "HVAC", "DAE", "Thermal comfort"]},
    {"article name": "Heat exchanger networks retrofit with an extended superstructure model and a meta-heuristic solution approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.03.029",
     "publication date": "06-2019",
     "abstract": "Retrofit of heat exchanger networks (HEN) is an important subject in the heat integration field. Several studies on the matter use Pinch-based techniques and mathematical models solved with deterministic methods. This work aims at providing an alternative method based on a model derived from a broad superstructure solved with a meta-heuristic approach. The referred superstructure comprises features not present in most HEN studies based on mathematical programming. Structural options such as the possibility of streams sub-splitting, partial mixing, serial units at a single stream branch and the allocation of heaters/coolers at intermediate positions are possible. The methodology was applied to three large-scale industrial case studies from the literature, based on oil refineries. Two of them contained streams with variable heat capacity. In all examples, the present methodology was able to outperform previous studies regarding total annual costs, proving an interesting alternative to methods that are currently applied in the literature.",
     "keywords": ["Optimization", "Heat exchanger networks", "Retrofit", "Meta-heuristics", "Mathematical modeling", "Process synthesis"]},
    {"article name": "Process monitoring and fault detection on a hot-melt extrusion process using in-line Raman spectroscopy and a hybrid soft sensor",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.03.019",
     "publication date": "06-2019",
     "abstract": "We propose a real-time process monitoring and fault detection scheme for a pharmaceutical hot-melt extrusion process producing Paracetamol-Affinisol extrudate. The scheme involves prediction of Paracetamol concentration from two independent sources: a hybrid soft sensor and a Raman-based Partial Least Squares (PLS) calibration model. Both these predictions are used by the developed PCA (Principal Component Analysis) and SPC (Statistical Process Control) monitors to detect process faults and raise alarms. Through real-time extrusion results, it is shown that this two-sensor approach enables the detection of various common process faults which would otherwise remain undetected with a single-sensor monitoring scheme.",
     "keywords": ["Process monitoring", "Fault detection", "Hot-Melt extrusion", "PCA", "PLS", "Calibration model", "Hybrid soft sensor", "Affinisol", "HPMC", "API Active Pharmaceutical Ingredient", "Active Pharmaceutical Ingredient", "CSTR Continuous Stirred-Tank Reactor", "Continuous Stirred-Tank Reactor", "DoE Design of Experiment", "Design of Experiment", "FOPTD First Order Plus Time Delay", "First Order Plus Time Delay", "HME Hot-Melt Extrusion", "Hot-Melt Extrusion", "NIR Near-Infrared", "Near-Infrared", "OPC OLE for Process Control", "OLE for Process Control", "PAT Process Analytical Technology", "Process Analytical Technology", "PCA Principal Component Analysis", "Principal Component Analysis", "PLC Programmable Logic Controller", "Programmable Logic Controller", "PLS Partial Least Squares", "Partial Least Squares", "PV Process Value", "Process Value", "QbD Quality by Design", "Quality by Design", "RMSEE Root Mean Squared Error of Estimation", "Root Mean Squared Error of Estimation", "SP Setpoint Value", "Setpoint Value", "SNV Standard Normal Variate", "Standard Normal Variate", "SPC Statistical Process Control", "Statistical Process Control", "SPE Squared Prediction Error", "Squared Prediction Error", "LCL Lower Control Limit", "Lower Control Limit", "LWL Lower Warning Limit", "Lower Warning Limit", "LV Latent Variable", "Latent Variable", "UCL Upper Control Limit", "Upper Control Limit", "UWL Upper Warning Limit", "Upper Warning Limit"]},
    {"article name": "Data-driven methods for batch data analysis \u2013 A critical overview and mapping on the complexity scale",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.01.014",
     "publication date": "05-2019",
     "abstract": "More than two decades have passed since the first holistic data-driven approaches for batch data analysis (BDA) were published. The emphasis was on multivariate statistical process monitoring and quality prediction. In the subsequent years, more methods were proposed, with varying degrees of success and acceptance by practitioners. A detailed and comprehensive analysis of these contributions reveals different complexity levels, both in terms of the degrees of freedom used for setting up the techniques (modeling complexity) and the expertise/training required by practitioners to autonomously apply them in concrete real world applications (implementation complexity). Both dimensions decisively contribute to the impact of a given proposal in industry. In this paper, we present a structured overview of BDA methodologies and analyze them from both perspectives. As a corollary, we map the BDA methods into the complexity scale, and elaborate how it can be used for selecting a suitable method for a given task.",
     "keywords": ["Batch data analysis", "Complexity", "Parsimony", "Process monitoring", "Quality prediction"]},
    {"article name": "Smoothed particle hydrodynamics \u2013 A new approach for modeling flow in oscillatory baffled reactors",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.02.003",
     "publication date": "05-2019",
     "abstract": "Existing numerical models of mixing processes in oscillatory baffled reactors (OBR) are mainly Eulerian-based. An alternative Lagrangian based methodology, Smoothed-Particle Hydrodynamics (SPH), for predicting flow patterns and assessing mixing performance is presented in this paper. A bespoke SPH solver is developed for single phase modeling, as it is, for the first time, applied to OBR, and the results are compared with those from Eulerian modeling, i.e. Finite Volume (FV) method. SPH has successfully captured the expected flow characteristics in OBR as clearly and equally as its Eulerian counterpart, validating the SPH method. Since SPH provides historical information of individually tracked fluid packets/particles in the domain of interest, it allows for readily quantitative assessments of mixing without additional models. Two new indexes to assess mixing and plug flow efficiency have been proposed by making full use of SPH's capabilities.",
     "keywords": ["Smoothed Particle Hydrodynamics (SPH)", "Computational Fluid Dynamics (CFD)", "Lagrangian based modeling", "Oscillatory baffled reactor", "Flow patterns", "Mixing efficiency"]},
    {"article name": "Process modelling, design and technoeconomic Liquid\u2013Liquid Extraction (LLE) optimisation for comparative evaluation of batch vs. continuous pharmaceutical manufacturing of atropine",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.12.028",
     "publication date": "05-2019",
     "abstract": "Continuous Pharmaceutical Manufacturing (CPM) can revolutionise industrial efficiency via potential operational and economic benefits over currently dominant batch methods. Process modelling and optimisation are valuable towards rapid design space evaluation and elucidation of optimal process design configurations, without expensive and time-consuming experimental campaigns. This paper pursues total cost minimisation via nonlinear optimisation of different separation design options for atropine, a product of great societal importance. The study considers a demonstrated continuous flow synthesis, presents reaction kinetic parameter estimation towards reactor design, and illustrates a comparative analysis of the subsequent batch vs. continuous liquid\u2013liquid extraction (LLE) for product purification, using published partition coefficient data and UNIFAC-modelled ternary liquid\u2013liquid equilibria. Original optimisation results show that toluene is the best continuous LLE solvent, attaining the lowest total costs at both plant capacities considered and the greatest total cost savings with respect to the batch LLE design for varying solvent recovery, at acceptable material efficiencies.",
     "keywords": ["Continuous pharmaceutical manufacturing (CPM)", "Atropine", "Process design", "Process optimisation", "Reactor design", "Liquid\u2013liquid extraction (LLE)"]},
    {"article name": "An investigation of initialisation strategies for dynamic temperature optimisation in beer fermentation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.12.020",
     "publication date": "05-2019",
     "abstract": "A wide range of optimisation methodologies exist for solving optimal control trajectory problems. With most approaches it is necessary to solve iteratively, starting from an initialising solution (often referred to as an initial guess). In this paper we investigate the performance of two different dynamic optimisation strategies for batch beer fermentation temperature control, using different initialisation profiles. A sequential method, Control Vector Parameterisation (CVP), is demonstrated as unable to produce solution profiles satisfying the industrially imposed undesirable by-product species concentration constraints. An alternative simultaneous method, Complete Parameterisation (CP), is employed in order to determine solutions satisfying these essential industrial production constraints. Blind initialisation guesses (isothermal profiles) have been shown to produce solution profiles not suitable for implementation on real fermentors; more promising candidate profile initialisations yield superior solutions. The use of a state-of-art NLP solver (IPOPT) with analytical first derivatives achieves remarkable solution robustness, eliminating initialisation and discretisation level dependence.",
     "keywords": ["Dynamic optimisation", "Multi-objective optimisation", "Consistent initialisation", "Control Vector Parameterisation (CVP)", "Complete Parameterisation (CP)", "Beer fermentation"]},
    {"article name": "Dynamic modeling and optimization of a coal-fired utility boiler to forecast and minimize NOx and CO emissions simultaneously",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.02.001",
     "publication date": "05-2019",
     "abstract": "Increasing penetration of renewable energy sources to the power grid has prompted new ramping scenarios to dispatchable thermal power plants to balance the variability caused by intermittent renewable supplies. With many thermal power plants designed to be base-loaded, ramping of the power output results in increased emission of pollutants. This study develops a dynamic data-driven model of a coal-fired utility boiler that estimates NOx and CO emissions simultaneously. Given a production schedule of a power plant, estimation of NOx and CO emissions for 3\u00a0h into the future is performed that can be further utilized in a dynamic optimization algorithm to minimize the emissions over a horizon. It is observed that a dynamic model always has a higher prediction accuracy than a static model, when training and forecasting of the models are concerned. Application of dynamic and steady-state optimization also results in reduced emissions as compared to historical plant emissions.",
     "keywords": ["Power generation", "Dynamic NARX model", "Static model", "Emission forecast", "Optimization"]},
    {"article name": "Stochastic back-off-based robust process design for continuous crystallization of ibuprofen",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.02.009",
     "publication date": "05-2019",
     "abstract": "Robust model-based process design in continuous pharmaceutical manufacturing aims to implement quality by design principles under uncertainty. Notably, various studies have discussed the back-off concept to solve the underlying robust optimization problem; however, for the concept to have practical value, its efficiency and convergence must be improved. In this work, we introduce a novel, highly efficient stochastic back-off strategy. Instead of using statistical moments of limited validity, we incorporate the full statistical information of the constraints to solve the robust process design problem. To ensure manageable computational costs, we make use of polynomial chaos expansion for uncertainty quantification and propagation. The proposed concept is demonstrated with the design of a tubular crystallizer for ibuprofen crystallization. The results show that the novel stochastic back-off strategy is considerably faster compared with the standard back-off concept and provides more reliable quality by design results in general.",
     "keywords": ["Back-off", "Robust optimization", "Polynomial chaos expansion", "Uncertainty", "Quality by design", "Crystallization", "Ibuprofen"]},
    {"article name": "Flexibility analysis of a distillation column: Indexes comparison and economic assessment",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.02.004",
     "publication date": "05-2019",
     "abstract": "The worldwide shared definition of \u201coptimal design\u201d refers to the cheapest and simplest design able to perform the required job; most of the time this definition is strictly related to given operating conditions, i.e. the input variables are seldom subjected to considerable variations. However, in process engineering, plenty of cases don\u2019t fit this definition due to the uncertain nature of the feedstock needed to be processed. Therefore, if a system is likely to undergo several and substantial perturbations, an a priori flexibility assessment can be crucial for the good performance of the operation. In chemical engineering the leading separation process is distillation. Hence the first purpose of this paper is to define a procedure and compare the different flexibility indexes found in literature in order to perform a simple distillation column flexibility assessment. The second goal of this paper is to couple the flexibility and economic aspects related to the distillation column investment costs and again to compare the different indexes economic behaviours.",
     "keywords": ["Flexibility", "Index", "Distillation", "Design", "Economics"]},
    {"article name": "Variographic analysis: A new methodology for quality assurance of pharmaceutical blending processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.02.010",
     "publication date": "05-2019",
     "abstract": "Analytical methods for real time monitoring of pharmaceutical blending processes are thoroughly validated but not the sampling methods. Variographic analysis was investigated as a method to determine the sampling and analytical errors when the drug concentration of pharmaceutical powder blends is determined by near infrared spectroscopy. Variograms provide an estimate of the true process variance and the minimum possible error (MPE) defined as the sum of the total sampling error and the total analytical error under the specific sampling scenario. In this study, the MPE was 50\u2013186 times greater for the 97.00 [%w/w] blends and 75\u2013342 times higher for the 95.00 [% w/w] blends than the short-term precision of the analytical NIR method. This study shows that variographic analysis may be used along with concentration vs. time profiles to evaluate a blending process, and also to discriminate between the sources of sampling, analytical and process errors.",
     "keywords": ["Theory of sampling (TOS)", "Near infrared spectroscopy (NIRS)", "Multivariate data analysis", "Partial least-squares regression (PLS-R)", "Variographic analysis", "MPE (minimum possible error)", "True process variance"]},
    {"article name": "Beware of symmetry breaking and periodic flow regimes in axisymmetric CVD reactor setups",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.02.005",
     "publication date": "05-2019",
     "abstract": "We compute the solution space in the laminar flow regime of a vertical, stagnation point, realistic chemical vapor deposition reactor. Compared to the solution space acquired in our previous work [N. Cheimarios, E.D. Koronaki, A.G. Boudouvis, Enabling a commercial computational fluid dynamics code to perform certain nonlinear analysis tasks, Comput. Chem. Eng. 35 (2011) 2632-2645] for a 2D axisymmetric case, 3D computations reveal a much richer solution space which if not investigated thoroughly crucial information for the process maybe lost. In particular, axisymmetric solutions can co-exist with non-axisymmetric and periodic solutions, due to buoyancy. It is found that non-axisymmetric flows can appear only in the region of mixed convection flows, which argues the use of only 2D axisymmetric models for this type of flows.",
     "keywords": ["Buoyancy flows", "Solution multiplicity", "Symmetry breaking", "Periodic flows", "Computational fluid dynamics", "CVD"]},
    {"article name": "Optimal operation of dynamic (energy) systems: When are quasi-steady models adequate?",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.02.011",
     "publication date": "05-2019",
     "abstract": "Since design optimization faces the challenge of solving inherently large optimization problems, the complexity of underlying dynamic systems is often reduced by applying quasi-steady state assumptions. It is thereby indispensable to identify the components whose transient behavior is essential to ensure meaningful results. In this study, we discuss a dynamic model for an illustrative hybrid energy system, which extends the quasi-steady models of Voll et\u00a0al. (2013). Based on optimal operation with fixed design, we underline the importance of the relationship between the dynamics of the model and of the input data for the adequateness of quasi-steady operation. Our results emphasize the need for suitable ramp constraints in quasi-steady models of dynamic systems within operational optimization. Moreover, the existence of a storage unit is no sufficient justification for quasi-steady state assumptions.",
     "keywords": ["Design optimization", "Optimal control", "Dynamic systems", "Quasi-steady operation", "Time constants", "Hybrid energy system"]},
    {"article name": "Optimal design of heat and water recovery system utilizing waste flue gases for refinery CO2 reduction",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.02.015",
     "publication date": "05-2019",
     "abstract": "Energy efficiency improvement is an effective strategy for CO2 reduction as well as cost savings in the industrial sector. Petroleum refineries generate significant amounts of hot and wet flue gas containing CO2 given their large fuel consumption due to energy intensive processes. In addition, for those locations where the climate is arid, sustainable water supply turns out to be a serious problem to meet the large demand for steam. This paper proposes to synthesize an optimal heat and water recovery system (HWRS) using a superstructure based method to achieve CO2 reduction, cost savings, and water recovery all simultaneously. The water recovery rate is obtained using Aspen Plus\u00ae and the HEN is designed by formulating an MINLP problem and solving it using GAMS. As a result, the HWRS could achieve 4.348% CO2 reduction and 28\u00a0k$/day of cost savings with 29% lower water demand from the desalination plant.",
     "keywords": ["CO2 reduction", "HEN optimization", "Heat and water recovery", "Petroleum refinery", "MINLP"]},
    {"article name": "A novel algorithm for dead time estimation between soft sensor inputs and output",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.02.012",
     "publication date": "05-2019",
     "abstract": "This paper introduces a new procedure for extracting signals and their time delays that contain the most information about the desired soft sensor output using information-theoretic subset selection and a genetic algorithm. This procedure can be used before the creation of a soft sensor, as it is only based on historic values of the signals. The algorithm is tested on real problems from the cement industry, i.e., how the input\u2019s time delays affect the quality of the soft sensor estimation of cement fineness in a cement mill.",
     "keywords": ["Soft sensors", "Dead time", "Cement fineness estimation", "Input selection", "Information theory"]},
    {"article name": "Energy-water nexus design and operation towards the sustainable development goals",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.02.007",
     "publication date": "05-2019",
     "abstract": "Urbanization is taking place rapidly, while the planning of energy, water and resource capacities in many cities especially in developing countries is lagging behind. Motivated by the lack of transparent data-driven decision-making support, a systematic methodology to support sustainable development emphasizing the energy-water nexus and multiple resource systems is developed. It serves as an open-source integrated tool to advice the planning, operation and decision-making considering social, environmental and economic sustainability globally. Several applications of the platform are demonstrated based on a sub-Saharan African metropolitan area. The outputs depict energy, water, and other resource demand, supply and transport on multiple spatiotemporal scales, which are used to indicate cost effective and environmentally friendly development strategies. The total Greenhouse Gas (GHG) emissions associated with the nexus are minimized to 0.56 tonnes CO2 equivalent per capita in 2030, resulting to a 51.4% reduction compared with the business-as-usual scenario, while providing sufficient resources to address sustainable development goals.",
     "keywords": ["Energy-water nexus", "decision making", "supply and demand", "environment", "design and operation"]},
    {"article name": "A stabilized nodal spectral solver for liquid chromatography models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.02.017",
     "publication date": "05-2019",
     "abstract": "Numerical methods for simulation of linear and non-linear liquid chromatography processes are compared in terms of computational cost and quality of the solution using as few as possible degrees of freedom (discrete problem size). We apply, for the first time, the p-type (order enrichment) continuous Galerkin spectral method (p-CG) for spatial discretization with exponential decay of spatial errors (spectral convergence) for smooth problems. To stabilize the method for marginally resolved or very steep shock-waves typically encountered, we use a spectral filtering approach. This effectively removes aliasing driven instabilities while retaining the high-order accuracy of the numerical method. We benchmark the stabilized p-CG method against two state-of-the-art finite element methods, namely the second-order accurate h-type (element refinement) discontinuous Galerkin method using a total-variation-diminishing limiter (h-DG-TVD) and the arbitrary-order hp-type discontinuous Galerkin spectral element method (hp-DG). Not surprisingly, the three methods provide predictions of column elution profiles that converge to the same values. However, the computational costs differ considerably. Although second-order accurate shock-capturing methods are often adopted as the default implementation, the stable arbitrary high-order methods considered in this paper are found to significantly outperform these in terms of both computational cost and discrete problem size. The p-CG method performs as well as the hp-DG method in terms of computational cost, but it has superior convergence properties, even in the presence of strong shock-waves. Further, its implementation is much simpler.",
     "keywords": ["High-order", "Nodal continuous Galerkin spectral method", "Nodal discontinuous Galerkin spectral element method", "Liquid chromatography", "Linear and nonlinear isotherm", "Equilibrium-dispersive model"]},
    {"article name": "Vapor recompressed batch distillation: Optimizing reflux ratio at variable mode",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.02.014",
     "publication date": "05-2019",
     "abstract": "This work aims at optimizing a vapor recompressed batch distillation that runs at variable reflux mode by employing a multi-objective optimization (MOO) strategy. This involves the formulation of optimization problem using factorial analysis for identifying dominating variables followed by solving the optimization problem using the elitist non-dominated sorting genetic algorithm. The selection of an optimal point is made by employing the technique for order of preference by similarity to ideal solution (TOPSIS) method with entropy information for weighting of objective functions. Here, two conflicting performance indicators, i.e., total annual cost (TAC) and total annual production (TAP) are considered as objective functions. At first, for the existing plant scenario, the conventional batch distillation column operated at variable reflux ratio mode is optimized and then its retrofit is proposed by employing an external thermal arrangement under vapor recompression framework. Subsequently, the optimal vapor recompressed batch distillation is separately developed for setting up a new plant. Finally, the proposed vapor recompressed schemes are demonstrated with an example system and their performances are quantified in terms of energy savings, TAC and TAP.",
     "keywords": ["Batch distillation with variable reflux ratio", "Vapor recompression", "Factorial design methodology", "Multi-objective optimization", "Elitist non-dominated sorting genetic algorithm", "Energy and cost savings"]},
    {"article name": "Identification of natural rubber samples for high-voltage insulation applications",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.01.016",
     "publication date": "05-2019",
     "abstract": "Latex presents high variability due to inherent differences among varieties from different countries, producers or crop seasonality. Natural rubber formulations from natural latex, to be used in insulating materials intended for high-voltage applications, require a wide variety of compounding and multitude of industrial processes. These aspects make it very difficult ensuring the same dielectric properties of the final product. At manufacturing level, it is very important to apply strict control processes to ensure that the final product fulfills all quality specifications. In this paper, a promising approach was applied to automatically identify natural rubber samples with suitable dielectric behavior from those with unsuitable dielectric behavior. This approach is based on the study of FTIR spectral data by applying suitable multivariable methods, such as principal component analysis, canonical variate analysis and k-nearest neighbors. The accurate and fast results reported in this work prove the suitability and potential of the proposed approach.",
     "keywords": ["Natural rubber", "Multivariable methods", "Infrared spectroscopy", "Identification", "Chemometrics", "High-voltage"]},
    {"article name": "Bivariate extension of the moment projection method for the particle population balance dynamics",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.12.011",
     "publication date": "05-2019",
     "abstract": "This work presents a bivariate extension of the moment projection method (BVMPM) for solving the two-dimensional population balance equations involving particle inception, growth, shrinkage, coagulation and fragmentation. A two\u2013dimensional Blumstein and Wheeler algorithm is proposed to generate a set of weighted particles that approximate the number density function. With this algorithm, the number of the smallest particles can be directly tracked, closing the shrinkage and fragmentation moment source terms. The performance of BVMPM has been tested against the hybrid method of moments (HMOM) and the stochastic method. Results suggest that BVMPM can achieve higher accuracy than HMOM in treating shrinkage and fragmentation processes where the number of the smallest particles plays an important role.",
     "keywords": ["Bivariate", "Moment projection method", "Population balance dynamics"]},
    {"article name": "Optimal batch scheduling of a multiproduct dairy process using a combined optimization/constraint programming approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.08.040",
     "publication date": "05-2019",
     "abstract": "This paper presents the optimal batch scheduling of a multi-product dairy process using an approach that combines optimization and constraint programming techniques. A suitable model describing the subprocesses and production rules is developed allowing to obtain scheduling constraints relating the production process and the machines available together with their relative efficiencies. After the scheduling problem has been formulated, the batch scheduling of a real powder milk/yogurt process is obtained in an optimal manner using the proposed approach with the objective of meeting customers\u2019 deadlines considering the efficiencies/costs of available alternative machines. Results using real consumer orders on some representative scenarios corresponding to the dairy production plant used as a case study are provided. This application shows a formulation closer to the engineering problem description thanks to the constraint-based language that facilitates the adaptation of the optimization objectives and constraints to real applications.",
     "keywords": ["Production batch-scheduling", "Constraint programming", "Food processing", "Multi-objective optimization", "Cost analysis"]},
    {"article name": "Improving the sensitivity of safety and health index assessment in optimal molecular design framework",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.12.012",
     "publication date": "05-2019",
     "abstract": "Due to the increasing public and media concerns over environmental and safety issues, many computer-aided molecular design (CAMD) works aimed to design safer chemicals that do not pose much harm to the surrounding community. The molecules can be examined using safety and health sub-indexes/parameters, which adopt a conventional index-based scoring profile that measures hazard level using molecular properties. However, the main shortcoming of this profile is the discontinuity of score at sharp property boundary, which distorts the comparison of hazard level among molecules. The main highlight of this paper is to smoothen the sub-index scoring profile in order to enhance the sensitivity of scores towards property values. A case study on solvent design for carotenoid extraction has been carried out with the presented methodology. From the results, the smoothing of sub-indexes prompts the CAMD programming to be more sensitive in selecting molecule that offers inherently safer and healthier performance.",
     "keywords": ["Computer-aided molecular design", "Safety and health index", "Index sensitivity", "Fuzzy optimization", "Carotenoid extraction", "Solvent design", "\u03b4 Hildebrand solubility parameter", "Hildebrand solubility parameter", "\u03b4car Hildebrand solubility parameter of carotenoids", "Hildebrand solubility parameter of carotenoids", "\u03b4d Dispersion solubility parameter of HSP", "Dispersion solubility parameter of HSP", "\u03b4h Hydrogen bonding solubility parameter of HSP", "Hydrogen bonding solubility parameter of HSP", "\u03b4p Polar solubility parameter of HSP", "Polar solubility parameter of HSP", "\u03b7 Viscosity", "Viscosity", "\u03bb Least satisfied degree of satisfaction", "Least satisfied degree of satisfaction", "\u03bbp Degree of satisfaction for property p", "Degree of satisfaction for property p", "\u03a9p Property operator for target property p", "Property operator for target property p", "2-MeTHF 2- Methyltetrahydrofuran", "2- Methyltetrahydrofuran", "Ci Contribution of the first-order group of type-i", "Contribution of the first-order group of type-i", "CAMD Computer-aided molecular design", "Computer-aided molecular design", "CoMT-CAMD Continuous-molecular targeting computer-aided molecular design", "Continuous-molecular targeting computer-aided molecular design", "CPME Cyclopentyl methyl ether", "Cyclopentyl methyl ether", "CPO Crude palm oil", "Crude palm oil", "Dj Contribution of the second-order group of type-j", "Contribution of the second-order group of type-j", "DMC Dimethyl carbonate", "Dimethyl carbonate", "Ek Contribution of the third-order group of type-k", "Contribution of the third-order group of type-k", "EF Effect factor", "Effect factor", "Fp Flash point", "Flash point", "FAHP Fuzzy Analytic Hierarchy Process", "Fuzzy Analytic Hierarchy Process", "FF Fate factor", "Fate factor", "FFB Fresh fruit bunches", "Fresh fruit bunches", "g Parameter in Eq.\u00a0(9) to represent the type of compounds", "Parameter in Eq.\u00a0(9) to represent the type of compounds", "Gc,T Total number of cyclic molecular groups present in a molecule", "Total number of cyclic molecular groups present in a molecule", "Gnc,T Total number of non-cyclic molecular groups present in a molecule", "Total number of non-cyclic molecular groups present in a molecule", "GO,T Total number of O-containing molecular groups present in a molecule", "Total number of O-containing molecular groups present in a molecule", "GT Total number of molecular groups present in a molecule", "Total number of molecular groups present in a molecule", "GC Group contribution", "Group contribution", "Hv Heat of vaporization", "Heat of vaporization", "HMAO Homogenous multi-agent optimization", "Homogenous multi-agent optimization", "HSP Hansen Solubility Parameters", "Hansen Solubility Parameters", "I\u03b7 Viscosity sub-index", "Viscosity sub-index", "IAH Acute health hazard sub-index", "Acute health hazard sub-index", "Icyclic Binary variable to indicate the presence of a cyclic compound", "Binary variable to indicate the presence of a cyclic compound", "IEL Exposure limit sub-index", "Exposure limit sub-index", "IEX Explosiveness sub-index", "Explosiveness sub-index", "IFL Flammability sub-index", "Flammability sub-index", "IFL,i IFL score of the molecule obtained from Fig.\u00a03b(i)", "IFL score of the molecule obtained from Fig.\u00a03b(i)", "IFL,ii IFL score of the molecule obtained from Fig.\u00a03b(ii)", "IFL score of the molecule obtained from Fig.\u00a03b(ii)", "Ij Binary variable used in disjunctive programming", "Binary variable used in disjunctive programming", "IMS Material phase sub-index", "Material phase sub-index", "Inc Binary variable to indicate the presence of non-cyclic group(s) in a molecule", "Binary variable to indicate the presence of non-cyclic group(s) in a molecule", "ISHI Total index score", "Total index score", "IV Volatility sub-index", "Volatility sub-index", "IBN Inherent Benign-ness Indicator", "Inherent Benign-ness Indicator", "IL Ionic liquid", "Ionic liquid", "IOHI Inherent Occupational Health Index", "Inherent Occupational Health Index", "IPA Isopropyl alcohol", "Isopropyl alcohol", "ISI Inherent Safety Index", "Inherent Safety Index", "Kow Octanol-water partition coefficient", "Octanol-water partition coefficient", "LB Lower bound", "Lower bound", "LC50 Acute toxicity", "Acute toxicity", "LCA Life cycle assessment", "Life cycle assessment", "LD50 Acute oral toxicity", "Acute oral toxicity", "LEL Lower explosion limit", "Lower explosion limit", "M Constant used in objective function [Eq.\u00a0(12)]", "Constant used in objective function [Eq.\u00a0(12)]", "Mj Number of second-order group of type-j in a molecule", "Number of second-order group of type-j in a molecule", "MSA Mass separating agent", "Mass separating agent", "N c U Upper bound for the total number of cyclic molecular groups present in a molecule", "Upper bound for the total number of cyclic molecular groups present in a molecule", "Nc,i Number of cyclic molecular group of type-i", "Number of cyclic molecular group of type-i", "N C H C U Upper bound for the total number of CH (cyclic) and C (cyclic) groups present in a molecule", "Upper bound for the total number of CH (cyclic) and C (cyclic) groups present in a molecule", "Ni Number of first-order group of type-i in a molecule", "Number of first-order group of type-i in a molecule", "N i * Number of molecular group of type-i of the generated optimal solution(s)", "Number of molecular group of type-i of the generated optimal solution(s)", "N n c U Upper bound for the total number of non-cyclic molecular groups present in a molecule", "Upper bound for the total number of non-cyclic molecular groups present in a molecule", "Nnc,i Number of non-cyclic molecular group of type-i", "Number of non-cyclic molecular group of type-i", "NO,i Number of O-containing molecular group of type-i", "Number of O-containing molecular group of type-i", "NuDIST Numerical Descriptive Inherent Safety Technique", "Numerical Descriptive Inherent Safety Technique", "Ok Number of third-order group of type-k in a molecule", "Number of third-order group of type-k in a molecule", "ORC Organic Rankine Cycle", "Organic Rankine Cycle", "p Property", "Property", "PCA Principal Component Analysis", "Principal Component Analysis", "PEL Permissible exposure limit", "Permissible exposure limit", "PIIS Prototype Index for Inherent Safety", "Prototype Index for Inherent Safety", "PPF Palm pressed fiber", "Palm pressed fiber", "PRHI Process Route Healthiness Index", "Process Route Healthiness Index", "Tb Normal boiling point", "Normal boiling point", "Tbound,j Tb boundary present in IV sub-index", "Tb boundary present in IV sub-index", "Tm Normal melting point", "Normal melting point", "UB Upper bound", "Upper bound", "UEL Upper explosion limit", "Upper explosion limit", "vi Valence of molecular group i", "Valence of molecular group i", "Vp Property value for target property p", "Property value for target property p", "V p L Lower bound for target property p", "Lower bound for target property p", "V p U Upper bound for target property p", "Upper bound for target property p", "w Binary variable used in property prediction model [Eq.\u00a0(7)]", "Binary variable used in property prediction model [Eq.\u00a0(7)]", "XF Exposure factor", "Exposure factor", "z Binary variable used in property prediction model [Eq.\u00a0(7)]", "Binary variable used in property prediction model [Eq.\u00a0(7)]"]},
    {"article name": "Data mining algorithm for pre-processing biopharmaceutical drug product manufacturing records",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.12.001",
     "publication date": "05-2019",
     "abstract": "The quality of data plays a crucial role in providing a reliable decision-making process when improving processes and operations under uncertainty. We present a data mining-based algorithm for robustly pre-processing the manufacturing records of biopharmaceutical batch processes. The algorithm can identify the time intervals in which the process is in commercial operation, and can characterize process failures automatically. An approximate string-matching algorithm, a decision tree classifier and a constrained clustering is applied to sequence the raw data, to classify the noise and identify each single batches; finally process failure are characterized. The algorithm was applied to the records of the process named as \u201ccleaning- and sterilizing-in-place\u201d, which is an essential process in manufacturing environment, in a case study. The algorithm was training on state of the art manual pre-processing outcome and was applied reducing the execution time of the activity down to 11.7% while maintaining high data quality and integrity.",
     "keywords": ["GMP", "Noise Filtering", "Language recognition", "Supervised machine learning", "Semi-supervised machine learning", "Ishikawa fishbone diagram", "cGMP current Good Manufacturing Practice", "current Good Manufacturing Practice", "CIP/SIP Cleaning-In-Place/Sterilizing-In-Place", "Cleaning-In-Place/Sterilizing-In-Place", "DP Drug Product", "Drug Product", "DT Decision Tree", "Decision Tree", "eETS end Extremity Task Sequence", "end Extremity Task Sequence", "ETS Extremity Task Sequence", "Extremity Task Sequence", "FDA Food and Drugs Administration", "Food and Drugs Administration", "GMP Good Manufacturing Practice", "Good Manufacturing Practice", "MSO Modelling-Simulation-Optimization", "Modelling-Simulation-Optimization", "PN Process-Noise", "Process-Noise", "QbD Quality\u2013by-Design", "Quality\u2013by-Design", "RCA Root Cause Analysis", "Root Cause Analysis", "sETS start Extremity Task Sequence", "start Extremity Task Sequence", "WF Wagner-Fischer", "Wagner-Fischer"]},
    {"article name": "An online reparametrisation approach for robust parameter estimation in automated model identification platforms",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.01.010",
     "publication date": "05-2019",
     "abstract": "Automated model identification platforms were recently employed to identify parametric models online in the course of unmanned experimental campaigns. The algorithms controlling these platforms include two computational elements: i) a tool for parameter estimation; ii) a tool for model-based experimental design. Both tools require the solution of complex optimisation problems and their effective outcome relies on their respective objective functions being well-conditioned. Ill-conditioned objective functions may arise when the model is characterised by a weak parametrisation, i.e. the model parameters are practically non-identifiable and/or extremely correlated. In this work, a robust reparametrisation technique is proposed and tested both in-silico and in an automated model identification platform. The benefit of reparametrisation is demonstrated on a case study for the identification of a kinetic model of catalytic esterification of benzoic acid with ethanol in a flow microreactor.",
     "keywords": ["Online", "Identification", "Information", "Parametrization", "Design", "Experiment"]},
    {"article name": "OptCAMD: An optimization-based framework and tool for molecular and mixture product design",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.01.006",
     "publication date": "05-2019",
     "abstract": "Chemical product design determines the structure and constitution of products that satisfies all desired properties and functions. Molecular products are usually employed as the main active ingredient, or manipulated to obtain a specific function for chemical-based products, while mixtures are one of the most widely used chemical products. Therefore, the design of molecular and mixture products is the foundation of all chemical product design problems. In this paper, the development of an optimization-based framework for molecular and mixture product design is presented. The design work-flow consists of three steps involving preliminary design, CAMD (Computer Aided Molecular-Mixture Design), as well as product evaluation and verification. In the preliminary design step, the product attributes are collected and converted into a set of desired physico-chemical properties with associated targets to formulate the CAMD problem. In the CAMD step, an optimization-based mathematical programming model is established and solved to generate feasible molecules and/or mixtures together with optimal product candidates. In the product evaluation and verification step, final selection of the optimal chemical product is made based on evaluation of in-use product performance attributes and additional properties not included in the CAMD step. The three steps have been implemented within a molecular-mixture design toolbox called \u201cOptCAMD\u201d, which is integrated in ProCAPD, a versatile tool for chemical product design and evaluation. Case studies highlighting different aspects of OptCAMD involving the design of various types of chemical products are presented.",
     "keywords": ["Computer-aided molecular-mixture design", "Product design simulator", "Mathematical programming", "Group contribution methods", "Chemical products"]},
    {"article name": "Optimising cascaded utilisation of wood resources considering economic and environmental aspects",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.01.004",
     "publication date": "05-2019",
     "abstract": "Cascaded wood utilisation could help to bridge the gap between the rising wood demand and fresh wood availability as well as contributing to a circular economy. However, the economic and environmental implications of cascading wood-based products are not fully known yet and are hence explored in this paper, considering both aspects simultaneously for the first time. The study focuses on the production of the following five products in an integrated system: medium-density fibre, oriented-strand board, particleboard, coated paper and wood pellets. Firstly, a multi-objective optimisation model has been developed to minimise the costs and greenhouse gas emissions of cascaded utilisation of wood. The \u03b5-constraint method has been used to solve the model and derive Pareto optimal solutions. The latter have been used to select two cascaded-utilisation scenarios and compare their environmental performance with two other scenarios: current situation and the use of fresh wood only. The environmental impacts have been estimated using life cycle assessment. The results reveal that cascaded utilisation is more environmentally and economically sustainable than the current situation or the use of fresh wood. One of the scenario (Scenario 2) reduces the impacts by 1%-23% on the current situation; the global warming potential (GWP) is lower by 15%. However, the costs in this scenario are only 4% lower. In another (Scenario 1), the costs are lower by 24% but the reductions in impacts are more limited, ranging from 1%-8% relative to the Reference scenario with the GWP being only 1% lower. The cascaded use of wood also offers the potential to save up to 35% of fresh wood resources, thus contributing to a circular economy. Using only fresh wood (Scenario 3) is the worst option, increasing the costs by 13% while offering small or no environmental benefits in most of the impacts. These results will be of interest to the wood industry, forestry authorities and policy makers.",
     "keywords": ["Circular economy", "Cascaded use of resources", "Life cycle assessment", "Multi-objective optimisation", "Wood"]},
    {"article name": "In-line monitoring of the thickness distribution of adhesive layers in black textile laminates by hyperspectral imaging",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.01.015",
     "publication date": "05-2019",
     "abstract": "Near-infrared (NIR) hyperspectral imaging was used for in-line monitoring of thickness and homogeneity of hot melt adhesive layers inside laminates made up of black polyester textiles. Reflection spectra were found to correlate with the thickness of the adhesive layers inside the laminates. Quantitative values were obtained from the spectra using data processing tools based on multivariate techniques such as partial least squares (PLS) regression. The prediction error (RMSEP) was found to be about 6\u202fgm\u22122. This precision is regarded to be sufficient for applications in process control. Calibration models were used for spectral imaging of textile laminates in order to demonstrate the power of the method for quantitative in-line monitoring of the application weight and its spatial distribution across the samples. The developed measuring approaches can be used for continuous large-area analysis in technical lamination processes, which qualifies them for quality and process control in field-scale.",
     "keywords": ["Near-infrared spectroscopy", "Spectral imaging", "Process control", "Application weight", "Homogeneity", "Lamination"]},
    {"article name": "Optimized data exploration applied to the simulation of a chemical process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.01.007",
     "publication date": "05-2019",
     "abstract": "In complex simulation environments, certain parameter space regions may result in non-convergent or unphysical outcomes. All parameters can therefore be labeled with a binary class describing whether or not they lead to valid results. In general, it can be very difficult to determine feasible parameter regions, especially without previous knowledge. We propose a novel algorithm to explore such an unknown parameter space and improve its feasibility classification in an iterative way. Moreover, we include an additional optimization target in the algorithm to guide the exploration towards regions of interest and to improve the classification therein. In our method we make use of well-established concepts from the field of machine learning like kernel support vector machines and kernel ridge regression. From a comparison with a Kriging-based exploration approach based on recently published results we can show the advantages of our algorithm in a binary feasibility classification scenario with a discrete feasibility constraint violation. In this context, we also propose an improvement of the Kriging-based exploration approach. We apply our novel method to a fully realistic, industrially relevant chemical process simulation to demonstrate its practical usability and find a comparably good approximation of the data space topology from relatively few data points.",
     "keywords": ["Data exploration", "Machine learning", "Feasibility classification", "Supervised learning", "Simulation"]},
    {"article name": "The synthesis problem of decentralized energy systems is strongly NP-hard",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.02.002",
     "publication date": "05-2019",
     "abstract": "We analyze the computational complexity of the synthesis problem of decentralized energy systems. This synthesis problem consists of combining various types of energy conversion units and determining their sizing as well as operations in order to meet time-varying energy demands while maximizing an objective function, e.g., the net present value. In this paper, we prove that the synthesis problem of decentralized energy systems is strongly NP-hard. Furthermore, we prove a strong inapproximability result. This paper provides the first complexity findings in the long scientific history of the synthesis problem of decentralized energy systems.",
     "keywords": ["Computational complexity", "NP-hardness", "Inapproximability", "Structural optimization", "Optimal design and operation", "Superstructure"]},
    {"article name": "Simulations of an ASA flow crystallizer with a coupled stochastic-deterministic approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.01.012",
     "publication date": "05-2019",
     "abstract": "A coupled solver for population balance systems is presented, where the flow, temperature, and concentration equations are solved with finite element methods, and the particle size distribution is simulated with a stochastic simulation algorithm, a so-called kinetic Monte-Carlo method. This novel approach is applied for the simulation of an axisymmetric model of a tubular flow crystallizer. The numerical results are compared with experimental data.",
     "keywords": ["Tubular flow crystallizer", "Population balance systems", "Stochastic particle method", "Markov jump processes", "Deterministic CFD methods"]},
    {"article name": "A benders-local branching algorithm for second-generation biodiesel supply chain network design under epistemic uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.01.013",
     "publication date": "05-2019",
     "abstract": "This paper proposes a possibilistic programming model in order to design a second-generation biodiesel supply chain network under epistemic uncertainty of input data. The developed model minimizes the total cost of the supply chain from supply centers to the biodiesel and glycerin consumer centers. Waste cooking oil and Jatropha plants, as non-edible feedstocks, are considered for biodiesel production. To cope with the epistemic uncertainty of the parameters, a credibility-based possibilistic programming approach is employed to convert the original possibilistic programming model into a crisp counterpart. An accelerated benders decomposition algorithm using efficient acceleration mechanisms is devised to deal with the computational complexity of solving the proposed model in an efficient manner. The performance of the proposed possibilistic programming model and the efficiency of the developed accelerated benders decomposition algorithm are validated by performing a computational analysis using a real case study in Iran.",
     "keywords": ["Bioenergy", "Biofuel supply chain", "Optimization", "Benders Decomposition", "Uncertainty"]},
    {"article name": "Modeling for reliability optimization of system design and maintenance based on Markov chain theory",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.02.016",
     "publication date": "05-2019",
     "abstract": "This paper proposes an MINLP model that represents the stochastic process of system failures and repairs as a continuous-time Markov chain, based on which it optimizes the selection of redundancy and the frequency of inspection and maintenance tasks for maximum profit. The model explicitly accounts for every possible state of the system. Effective decomposition and scenario reduction methods are also proposed. A small example with two processing stage is solved to demonstrate the impact of incorporating maintenance considerations. A decomposition method and a scenario reduction method are applied to this example and are shown to drastically reduce the computational effort. A larger example with four stages, which is not directly solvable, is also successfully solved using the proposed algorithm. Lastly, we show that the proposed model and algorithms are capable of solving a practical problem based on the air separation process example that motivated our work, which features multiple stages, potential units and failure modes.",
     "keywords": ["Reliability design", "Maintenance", "Optimization", "Markov chain", "MINLP"]},
    {"article name": "Control of complex sociotechnical systems: Importance of causal models and game theory",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.12.010",
     "publication date": "04-2019",
     "abstract": "Recent systemic failures in different domains have reminded us, once again, of the fragility of complex sociotechnical systems. Although the failures occurred in very different domains, there are, however, certain common underlying mechanisms driving these disasters. Understanding these mechanisms is essential to avoid such disasters in the future. To understand them, one needs to go beyond analyzing them as independent one-off accidents, and examine them in the broader perspective of the potential fragility of sociotechnical systems. It is their scale, nonlinearity, inter-connectedness, and interactions with humans and the environment that can make these systems fragile. Here we present an overview of the challenges and opportunities in the modeling and analysis of sociotechnical systems. We highlight a control-theoretic modeling framework that unifies the social and the technical components. We discuss how certain problems can be addressed by using concepts and techniques from causal modeling, game theory, and behavioral economics.",
     "keywords": ["Systemic risk", "Complex systems", "Process systems engineering"]},
    {"article name": "Well control optimization using derivative-free algorithms and a multiscale approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.12.004",
     "publication date": "04-2019",
     "abstract": "Smart well technologies, which allow remote control of well and production processes, make the problem of determining optimal control strategies a timely and valuable pursuit. The large number of well rates for each control step make the optimization problem difficult and present a high risk of achieving a suboptimal solution. Moreover, the optimal number of adjustments is not known a priori. Adjusting well controls too frequently will increase unnecessary well management and operation cost, and an excessively low number of control adjustments may not be enough to obtain a good yield. In this paper, we explore the capability of three derivative-free algorithms and a multiscale regularization framework for well control optimization over the life of an oil reservoir. The derivative-free algorithms chosen include generalized pattern search (GPS), particle swarm optimization (PSO) and covariance matrix adaptation evolution strategy (CMA-ES). These algorithms, which cover a variety of search strategies (global/local search, stochastic/deterministic search), are chosen due to their robustness and easy parallelization. Although these algorithms have been used extensively in the reservoir development optimization literature, for the first time we thoroughly explore how these algorithms perform when hybridized within a multiscale regularization framework. Starting with a reasonably small number of control steps, the control intervals are subsequently refined during the optimization. Results for the experiments studied indicate that CMA-ES performs best among the three algorithms in solving both small and large scale problems. When hybridized with a multiscale regularization approach, the ability to find the optimal solution is further enhanced, with the performance of GPS improving the most. Topics affecting the performance of the multiscale approach are discussed in this paper, including the effect of control frequency on the well control problem. The parameter settings for GPS, PSO, and CMA-ES, within the multiscale approach, are considered.",
     "keywords": ["Well control", "Production optimization", "Derivative-free algorithms", "Multiscale approach"]},
    {"article name": "Sensitivity analysis techniques for the optimal system design of forward osmosis in organic acid recovery",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.12.024",
     "publication date": "04-2019",
     "abstract": "As the critical parameter, the operating conditions are able to define the system efficiency of forward osmosis (FO) process. In other words, to reach the optimal performance, the effective operation of FO process can be achieved by working on these optimal design parameters. In this work, to simulate the influence of each initial condition on FO process performance, sensitivity analysis technique was introduced to developed dynamic FO model, focused on both single and a mixture of two organic acid feed solutions, by NH4Cl as draw solution. As functions of operating variables, the FO membrane rejection and concentration performance of the system could precisely be defined via Levenberg\u2013Marquardt algorithm. The simulation results demonstrated that sensitivity analysis techniques can be practically applied as a guideline to design and modify any comparable FO processes by optimizing operating parameters to achieve the desirable performance in terms of both minimizing cost and maximizing efficiency.",
     "keywords": ["Forward osmosis", "Sensitivity analysis", "Modeling", "Organic acid"]},
    {"article name": "Stochastic modeling of fuel procurement for electricity generation with contractual terms and logistics constraints",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.12.021",
     "publication date": "04-2019",
     "abstract": "This work addresses the procurement of fuel to supply an uncertain demand for electricity generation. Fuel purchase decisions are made with considerable advance with respect to reception. Changes in demand determine corrective decisions and associated costs, such as advance decisions on delay or cancellation of contracts in order to meet storage and logistics constraints. We present a multi-stage stochastic programming model for the problem. Uncertainty is modeled as a discrete event tree of scenarios. Three heuristic strategies, relax and fix, rolling horizon and scenario aggregation, are proposed and applied to three case studies. Results show that when delay or cancellation decisions are taken in advance of the reception period, the relax and fix strategy performs better in terms of quality of the solution and processing time. The methodology allows to model relevant aspects of the problem and proposes a robust procurement decision that hedges uncertainty and establishes corrective actions.",
     "keywords": ["Procurement planning", "Logistics", "Stochastic programming", "Mixed integer programming", "Heuristics"]},
    {"article name": "Dual network extraction algorithm to investigate multiple transport processes in porous materials: Image-based modeling of pore and grain scale processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.12.025",
     "publication date": "04-2019",
     "abstract": "Image processing of 3D tomographic images to extract structural information of porous materials has become extremely important in porous media research with the commoditization of x-ray tomography equipment to the lab scale. Extracted pore networks from images using image analysis techniques enable transport properties calculation for bigger domains at a low computational cost, allowing pore-scale investigation of porous media over meaningful macroscopic length scales. The present study reports a pore network extraction algorithm to simultaneously extract void and solid networks from tomographic images of porous materials using simple image analysis techniques. Crucially, it includes connectivity and geometrical information of both void and solid phases as well as the interlinking of these phases with each other. Validation was obtained on networks extracted from simple cubic and random sphere packings over a range of porosities. The effective diffusivity in the void phase and thermal conductivity in the solid phase was then calculated and found to agree well with direct numerical simulation results on the images, as well as a range of experimental data. One important outcome of this work was a novel and accurate means of calculating interfacial areas between grains and voids directly from digital images, which is critical to many phenomena where phase interactions occur. The efficient \u2018dual network\u2019 algorithm is written in PYTHON using open source tools and provides a new way to study critical processes that depend on transport in both void and solid phase such as catalytic reactors and electrochemical systems.",
     "keywords": ["Porous media", "Transport phenomena", "Heterogeneous catalysis", "Tomography", "Pore network modeling"]},
    {"article name": "Concentration and temperature profiles in a fixed bed column based on an analytical solution of the axial dispersion model for binary and multicomponent non-isothermal adsorption processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.12.026",
     "publication date": "04-2019",
     "abstract": "An analytical solution to model dynamic adsorption of multicomponent mixtures in packed beds is presented. The analytical solution describes transient concentration profiles together with gas and solid phase temperature profiles during adsorption and desorption processes. The breakthrough curves predicted by the analytical solution are in satisfactory agreement with numerical simulations and experimental data reported in the literature.",
     "keywords": ["Fixed bed", "Modeling", "Binary adsorption", "Multicomponent adsorption", "Breakthrough", "IAST Ideal Adsorbed Solution Theory", "Ideal Adsorbed Solution Theory", "PSA Pressure swing adsorption", "Pressure swing adsorption", "TSA Temperature swing adsorption", "Temperature swing adsorption", "AADM Analytical solution of the axial dispersion model", "Analytical solution of the axial dispersion model", "NADM Numerical solution of the axial dispersion model", "Numerical solution of the axial dispersion model", "ADM Axial dispersion model", "Axial dispersion model", "LDF Linear Driving Force", "Linear Driving Force"]},
    {"article name": "Optimal sustainable water-Energy storage strategies for off-grid systems in low-income communities",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.12.023",
     "publication date": "04-2019",
     "abstract": "Energy-water supply in low-income rural communities is addressed using a multi-objective model for defining the configuration and size of the system. The considered objective functions are minimizing the total annual cost, water and land usage, CO2 emissions and maximizing local jobs and the social opportunity associated to technological change. The multi-objective strategy meets a trade-off solution between economic, environmental and social dimensions which are integrated by subsets of objective functions. Strategies for energy-water surplus management based on batteries and pumping systems are evaluated. Conflicts between social and economic objectives are identified. As Case Study, profiles of energy and water consumption of a rural community from Mexico are presented.",
     "keywords": ["Off-grid systems", "Water-Energy storage", "Multi-objective optimization", "Social impact", "Sustainable dimensions", "Technological change"]},
    {"article name": "An MINLP formulation for integrating the operational management of crude oil supply",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.12.014",
     "publication date": "04-2019",
     "abstract": "In this paper we integrate the management of crude oil supply at the operational level (i.e., from FPSOs to CDUs), taking into account the scheduling of vessels and operations in a terminal, while handling the non-convexities associated to the blending of crudes. The problem incorporates elements of known problems in the literature: maritime inventory routing and crude oil scheduling. To tackle this problem, we propose an MINLP formulation, which is solved by an iterative MILP-NLP decomposition scheme with domain reduction. The solution strategy is illustrated with a set of instances, that yielded small solution gaps.",
     "keywords": ["Crude oil supply", "Bilinear terms", "Blending", "MINLP", "Maritime inventory routing", "Crude oil scheduling"]},
    {"article name": "Design of a self-tuning adaptive model predictive controller using recursive model parameter estimation for real-time plasma variable control",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.01.002",
     "publication date": "04-2019",
     "abstract": "The semiconductor etching process, which is the most important part of the semiconductor manufacturing process, requires higher sophistication as 10\u202fnm semiconductors are mass produced. Etching methods utilizing plasma are getting increasingly popular with the miniaturization of the etching process. As the process performance depends on the state of the plasma variables, such as electron density, it is essential to measure and control these variables in real time. Moreover, to control the plasma-based system, the sensitive and time-varying characteristics of plasma should be considered. This paper proposes a self-tuning adaptive model predictive controller that can effectively perform electron density control. As a first step, an integral squared error-based Bayesian optimization is used to tune the model predictive controller, and its performance is verified on a drift-free Ar plasma system. The self-tuning adaptive model predictive controller is constructed by combining a recursive model parameter estimator with a model predictive controller. The recursive model parameter estimator is designed using a recursive least squares algorithm with Kalman filter interpretation. The effectiveness of the proposed controller is verified through control simulations and a set-point tracking experiment on the electron density with artificial drift in real time. The experimental results show that the performance of the proposed controller is 21% better than that of the conventional model predictive controller. We expect that this result will make a significant contribution to control processes utilizing plasma.",
     "keywords": ["Plasma variable control", "Real-time control", "Adaptive model predictive control", "Self-tuning adaptive control", "Recursive model parameter estimation"]},
    {"article name": "Mixed kernel canonical variate dissimilarity analysis for incipient fault monitoring in nonlinear dynamic processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.12.027",
     "publication date": "04-2019",
     "abstract": "Incipient fault monitoring is becoming very important in large industrial plants, as the early detection of incipient faults can help avoid major plant failures. Recently, Canonical Variate Dissimilarity Analysis (CVDA) has been shown to be an efficient technique for incipient fault detection, especially under dynamic process conditions. CVDA can be extended to nonlinear processes by introducing kernel-based learning. Incipient fault monitoring requires kernels with both good interpolation and extrapolation abilities. However, conventional single kernels only exhibit one ability or the other, but not both. To overcome this drawback, this study presents a Mixed Kernel CVDA method for incipient fault monitoring in nonlinear dynamic processes. Due to the use of mixed kernels, both enhanced detection sensitivity and a better depiction of the growing fault severity in the monitoring charts are achieved. Looking ahead, this work takes a step towards understanding the impact of kernel behavior in process monitoring performance.",
     "keywords": ["Fault detection", "Canonical variate analysis", "Global kernel", "Local kernel", "Kernel density estimation"]},
    {"article name": "Optimal planning of municipal solid waste management systems in an integrated supply chain network",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.12.022",
     "publication date": "04-2019",
     "abstract": "Waste management can be considered as a strategic supply chain problem as it involves the waste generation, collection, separation, transportation, treatment, distribution, and disposal. This paper presents a mixed integer linear programming model for the coordination of tactical and operational decisions in waste supply chain networks including the logistics, production, and distribution. The model aims at maximizing the profit of the entire supply chain while satisfying the demand, production, transportation, and inventory constraints imposed by different entities of the network. A case study is selected to test the effectiveness and efficiency of the proposed model. Such an integrated network contributes to the maximum utilization of recyclable waste including paper, plastic, glass, and metal processed by recycling plants, and non-recyclable waste treated in waste to energy plants. Sensitivity analyses are performed to investigate how changing parameters including the time periods and products\u2019 prices affect the supply chain performance.",
     "keywords": ["Municipal solid waste", "Waste management", "Recycling", "Waste to energy", "Integrated supply chain", "Mixed integer linear programming"]},
    {"article name": "A hierarchical approach for causal modeling of process systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.12.017",
     "publication date": "04-2019",
     "abstract": "Cause-and-effect reasoning is at the core of fault diagnosis and hazards analysis in process systems, thereby requiring the development and use of causal models for automated approaches. Furthermore, causal models are also required to explain the decisions and recommendations of artificial intelligence-based systems, lack of which is a serious drawback of purely data-driven approaches. Here, we demonstrate an approach for building multi-level causal models. A hierarchical approach is proposed to capture both cyclic and non-cyclic features of a process plant. Decoupling these features of the plant by constructing two tiers of digraphs, one tier representing overall plant and the other representing individual subsystems, helps in better inference of causal relations present in the system. An algorithm that subsides the effects of indirect causal interactions using reachability matrix and adjacency matrix ideas is also proposed. The algorithm is tested on the Tennessee Eastman benchmark process and the resulting causal model is found to represent the true causal interactions present in the system.",
     "keywords": ["Causal models", "Digraph", "Hierarchical approach", "Process industries", "Transfer entropy"]},
    {"article name": "Demonstration of non-linear model predictive control of post-combustion CO2 capture processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.12.018",
     "publication date": "04-2019",
     "abstract": "Nonlinear model predictive control applications have been deployed on two large pilot plants for post combustion CO2 capture. The control objective is formulated in such a way that the CO2 capture ratio is controlled at a desired value, while the reboiler duty is formulated as an unreachable maximum constraint. With a correct tuning, it is demonstrated that the controllers automatically compensate for disturbances in flue gas rates and compositions to obtain the desired capture ratio while the reboiler duty is minimized. The applications are able to minimize the transient periods between two different capture rates with the use of minimum reboiler duty.",
     "keywords": ["Post-combustion CO2 capture", "Nonlinear model predictive control", "Load changes", "Pilot plants"]},
    {"article name": "Selection of equipment modules for a flexible modular production plant by a multi-objective evolutionary algorithm",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.12.009",
     "publication date": "04-2019",
     "abstract": "In this work an approach to select equipment modules for a flexible modular production plant at low investment costs using a multi-objective evolutionary algorithm is introduced. Thereby, a good determination of the plant`s overall operating window is of utmost importance. A styrene production plant serves as case study to evaluate the equipment module selection approach, to analyze the resulting compromise solutions as well as the flexible modular equipment sets with low investment costs. It is for example possible to achieve an 11-fold enlarged operating window for 50% higher investment costs compared to the conventional design.Thus, this work presents an important step towards the realization of modular production plants.",
     "keywords": ["Module-based plant design", "Equipment module selection", "Operating window", "Flexibility"]},
    {"article name": "Benders decomposition with integer sub-problem applied to pipeline scheduling problem under flow rate uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.01.003",
     "publication date": "04-2019",
     "abstract": "Important issues in a pipeline system are energy efficiency, reliability and throughput flexibility. Practically conventional pumps are not capable of operating at the highest attainable efficiency for long running time. This deficiency has prevented a pipeline system to operate close to its predefined program. A possible remedy is to take into account uncertainty due to pumps operations. In this paper, a stochastic two-stage mixed integer programming (MILP) model is developed for the multiproduct pipeline-scheduling problem under flow rate uncertainty. The problem arises in a number of settings, and the real-world applicability discussed and demonstrated. The stochastic MILP model involves many discrete variables that make it intractable for real-life cases. As a solution method, the sample average approximation is combined with a three-step solution approach based on Benders decomposition. The modeling and solving approach is evaluated in some case studies including a real-life problem from NIOPTC.",
     "keywords": ["Pipeline scheduling", "Uncertain flow rate", "Two-stage stochastic model", "Disruption", "Benders decomposition method", "Sample average approximation"]},
    {"article name": "A CFD based automatic method for compartment model development",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.12.015",
     "publication date": "04-2019",
     "abstract": "Computational fluid dynamics (CFD) is a powerful tool for quantitative prediction of fluid dependent properties in a finite volume. However, the complexity of solving the momentum balances and the continuity equations at each element of the discretized geometry can easily lead to an expensive computational task. Compartment modelling is a potential alternative to speed up the calculation, which is however reached at the expense of the level of accuracy. The most important factor in optimizing a compartment model (CM) concerning the accuracy and the computational time is the quality of the chosen compartments to represent the critical gradients. This work presents a new automated compartmentalization method to characterize an improved network of compartments derived from initial detailed CFD results, with a focus on cylindrical-shaped systems. This method was evaluated with a case study of a 700\u202fL stirred tank bioreactor by estimating the mixing performance and demonstrating its high efficiency.",
     "keywords": ["Fluid dynamics", "CFD based compartment model", "Automated compartmentalization", "Mixing"]},
    {"article name": "Coupling of the population balance equation into a two-phase model for the simulation of combined cooling and antisolvent crystallization using OpenFOAM",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.01.009",
     "publication date": "04-2019",
     "abstract": "This article proposes a two-phase Eulerian-Eulerian model coupled with granular kinetic theory, semi-discrete population balance equations, energy balance, and scalar transport equations for the simulation of the full crystal size distribution and the effects of particle settling in continuous-flow crystallizers. The model capabilities are demonstrated by application of an OpenFOAM\u00ae implementation to the combined cooling/antisolvent crystallization of Lovastatin in a coaxial mixer. The simulations show that (1) the spatial fields for the antisolvent mass fraction and crystal nucleation and growth rates can be highly asymmetric for small continuous-flow crystallizers, (2) continuous-flow crystallizers of small dimension can generate bimodal crystal size distributions, and (3) a relatively small change in the inlet feed velocity can change the crystal size distribution from being unimodal to bimodal. These results demonstrate the potential of the proposed model for gaining insights into continuous-flow crystallization that can be useful for the design of equipment or operations.",
     "keywords": ["Two-phase model", "Population balance models", "Pharmaceutical manufacturing", "Computational fluid dynamics", "Pharmaceutical crystallization"]},
    {"article name": "Design of bio-oil additives via computer-aided molecular design tools and phase stability analysis on final blends",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.01.008",
     "publication date": "04-2019",
     "abstract": "Direct application of bio-oil as fuel is limited by its undesirable properties such as low heating value, high viscosity, and non-homogenous aqueous and organic phases. Direct addition of solvent to bio-oil is one of the most practical approaches to improve bio-oil properties because of its simplicity and low processing cost. This inspired the design of solvents to enhance bio-oil properties upon physical blending. One of the major challenges faced in solvent design is the immiscibility of final blend due to the difference in polarity of solvent and bio-oil molecules. This work presents a computer-aided molecular design (CAMD) framework to identify potential solvent candidates that allow bio-oil to satisfy targeted properties with minimal solvent addition. Moreover, a model for phase stability analysis is developed based on Gibbs tangent plane distance, which covers miscibility check on solvent-oil blend, generation of phase diagram, and addition of binding agents to homogenise the blend.",
     "keywords": null},
    {"article name": "Data rectification for multiple operating modes: A MAP framework",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.01.001",
     "publication date": "04-2019",
     "abstract": "Process measurements are susceptible to random and gross errors that may affect the reliability and decision making in process control and automation applications. In this work, we propose a Maximum a Posteriori (MAP) framework to deal with gross errors having equivalent sets, while simultaneously performing data reconciliation. Furthermore, in reality, the process may work under multiple operating regions. Hence, another contribution of this study is to solve a data rectification problem on a data set containing different steady-state operating regions. To address this case, a MAP formulation with two hidden variables is introduced - one for signifying the operating mode, and the other for characterizing the noise mode. Owing to the presence of hidden variables, an Expectation Maximization (EM) algorithm is presented to solve the resulting optimization problem. Several examples are presented to demonstrate the effectiveness of our proposed framework.",
     "keywords": ["Data reconciliation", "Gaussian mixture model", "Expectation maximization (EM)", "Multiple operating regions", "Equivalent sets of gross errors"]},
    {"article name": "Alternative generation and multiobjective evaluation using a design framework: Case study on sterile filling processes of biopharmaceuticals",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.12.019",
     "publication date": "04-2019",
     "abstract": "In biopharmaceutical manufacturing, single-use equipment is becoming popular for small-scale production schemes instead of stainless steel equipment. In this work, a decision support framework is presented to systematically generate alternatives at multiple decision layers. The framework includes decision layers for product specifications, the process flowsheet, equipment material, and operating conditions. Generated alternatives are evaluated on their economic and environmental impacts in addition to the risks regarding product quality and supply robustness. A case study was performed, where a comprehensive set of alternatives for equipment material was investigated with representative elements from other layers for the sterile filling of biopharmaceuticals. The results confirmed the preference for single-use equipment for small-scale and short-term production. Results also showed a preference for using fixed equipment for large-scale and long-term production. Generated hybrid alternatives outperformed empirically generated alternatives and provided a compromise between the competing environmental and economic indicators.",
     "keywords": ["Process design", "Single-use technology", "Parenterals", "Leachables", "Sensitivity analysis", "Decision-making"]},
    {"article name": "Optimal design and scheduling for offshore oil-field development",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.01.005",
     "publication date": "04-2019",
     "abstract": "Oil-and-gas field development projects are capital-intensive, and optimizing profitability has always been a critical subject for the industry. This paper presents a mixed-integer linear programming (MILP) model to facilitate scenario comparison and selection at the design stage for green offshore oil-field development. The model solves an allocation-scheduling problem with the objective of maximizing the project\u2019s net present value (NPV). Decision variables include the drilling schedule for both production and injection wells, well assignment to FPSO (floating, production, storage and offloading unit), FPSO oil and water production capacity, and water injection capacity. The model represents the waterflood scheme through an injection-production relationship matrix, leading to consistent scheduling of producer-injector pairs. Also, the model allows the assignment of development priorities to blocks or well groups, which corresponds to the \u201cphased-development\u201d concept in real-world application. Methods to estimate production rates are proposed, and a production prediction model is established. Production rates considering scheduling effect are generated by a linear superposition of base production curves from reservoir simulations. Two synthetic reservoirs with properties resembling deepwater offshore Brazil illustrated the performance of the modeling approach. It is observed that in early development years, drilling capacity and oil production capacity are active constraints to the system, while in late development years, active constraints change to water capacity, which propels the \u201ccapacity expansion\u201d concept in the late production years, such as subsea water separation. Higher recovery is obtained with expedited production, but NPV does not necessarily increase with increased oil recovery when there is considerable investment on extra FPSOs.",
     "keywords": ["Offshore development optimization", "MILP", "Production prediction", "Phased development"]},
    {"article name": "Evaluating and ranking patents with multiple criteria: How many criteria are required to find the most promising patents?",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.01.011",
     "publication date": "04-2019",
     "abstract": "Patents contain a wealth of information about technological progress and market trends. Many existing techniques for patent assessment rely on citation analysis. Despite its importance, citation analysis alone is not adequate to identify all important patents for a given topic. We propose the simultaneous use of eight criteria for patent ranking and evaluation. Additionally, we investigate computationally the effect on ranking quality when fewer than eight criteria are utilized. Contrary to previous approaches, the proposed methodology does not require expert opinions to weigh the different criteria and evaluate the patents. The solution of an intuitive linear optimization problem provides optimal weights for the proposed criteria. These weights are subsequently utilized in a systematic multicriteria methodology for patent ranking. The proposed methodology has been implemented in a web-based decision support system and has been validated in the context of identifying the most important patents for the production of twenty-two chemicals.",
     "keywords": ["Decision support systems", "Multiple criteria analysis", "Text analytics", "Patent rankings", "Optimization"]},
    {"article name": "Continuous reactive crystallization of \u03b2-lactam antibiotics catalyzed by penicillin G acylase. Part I: Model development",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.12.029",
     "publication date": "04-2019",
     "abstract": "A model is developed for simulating continuous manufacturing of \u03b2-lactam antibiotics by enzymatic reactive crystallization.\u00a0An amoxicillin case study using the unified model of reaction and crystallization kinetics in different reactor designs is presented. Attainable regions are constructed for conversion of amoxicillin precursors and mean crystal size.\u00a0Attainable regions for conversion show that, with a 2:1 molar ratio of precursors, 98% conversion of the limiting reactant is possible with reactive crystallization while only 72% conversion can be achieved without crystallization. A well-mixed reactive crystallizer is shown to have higher productivity than a plug-flow reactive crystallizer, which is uncommon for positive-order processes like enzyme reactions and crystallization. A Pareto optimal surface is drawn for conversion, productivity, and fractional yield to inform the design of an optimal process. Additional means of optimizing production of amoxicillin, including classified crystal fines removal, are discussed to further enhance crystal size and productivity.",
     "keywords": ["Reactive crystallization", "Penicillin G acylase", "Continuous manufacturing", "\u03b2-lactam antibiotics"]},
    {"article name": "Optimization strategies for chiral separation by true moving bed chromatography using Particles Swarm Optimization (PSO) and new Parallel PSO variant",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.01.020",
     "publication date": "04-2019",
     "abstract": "The Particles Swarm Optimization (PSO) is an optimization technique that has been gaining attention in the last years. In this work, the PSO method is applied to optimize the productivity and the eluent consumption of the separation of the bi-naphthol enantiomers in a True Moving Bed (TMB) device.Three optimization strategies are presented: the two-steps optimization, the single optimization and a new version of the PSO algorithm, the Parallel PSO. All the three strategies showed to be efficient to perform the desired optimization. Comparing in terms of productivity and computation time (represented by the number of iterations), the Parallel PSO appeared to be the best compromise, which emphasizes the relevance of this new version to perform the optimization of the mentioned separation process. Generally, The TMB optimization results presented in this work had an average productivity that was 30% higher than the results previously reported in the literature. The best result was obtained using the Parallel PSO strategy in which a productivity of 209.2\u202fg/Lads/day (corresponding to an eluent consumption of only 83.9 dL/g) was achieved.As the TMB is only a theoretical model, simulations with Simulated Moving Bed (SMB) devices with four, eight and twelve columns were obtained using the equivalence between the two models, and the results were compared.",
     "keywords": ["PSO", "TMB", "SMB", "Chiral separation"]},
    {"article name": "Computer vision system for froth-middlings interface level detection in the primary separation vessels",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.01.017",
     "publication date": "04-2019",
     "abstract": "Bitumen from the surface-mined oil sands is separated through a water-based gravity separation process in the Primary Separation Vessels (PSV). The froth-middlings interface level in the PSVs is the most important control variable in the process. Bitumen recovery and downstream operations are critically dependent on the interface level measurement and control. In this paper, we describe a robust computer vision system that uses image and data processing techniques to estimate the froth-middlings interface level. The algorithm in the computer vision system processes the online video frames of a camera mounted to the PSV sight glasses, and it is based on a combination of static and dynamic image processing steps. Industrial results show that the computer vision algorithm is more accurate and reliable when compared to the other instruments, as well as robust against process and environmental abnormalities.",
     "keywords": ["Computer vision system", "Image processing", "Oil sands extraction", "Primary separation vessels", "Froth-middlings interface level"]},
    {"article name": "Superstructure approach for the design of renewable-based utility plants",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.01.019",
     "publication date": "04-2019",
     "abstract": "This work evaluates the local integration of the renewable resources for the supply of utilities by designing a renewable-based utility plant, which couples different technologies within a steam and power network. A superstructure was developed and a mixed-integer nonlinear programming model was formulated, with the aim of selecting the technologies that process solar, wind, biomass and waste sources to meet the demand for power and steam that minimize the total annual cost of the system. Two case studies, one in Mexico and another one in Scotland, were considered. Results show that the biomass boiler provides the most economical cogeneration scheme. Resource availability plays a major role in the technology selection. As the biomass availability decreases, the solar technologies become the best choice to produce steam and power. The renewable-based utility plant shows a significant reduction in CO2 emissions.",
     "keywords": ["Power plant", "Renewable energy", "Process design", "Mathematical optimization"]},
    {"article name": "Efficient modeling of the nonlinear dynamics of tubular heterogeneous reactors",
     "doi": "https://doi.org/10.1016/j.compchemeng.2019.01.018",
     "publication date": "04-2019",
     "abstract": "The problem of efficiently describing the nonlinear dynamics of spatially distributed tubular heterogeneous reactors is addressed, including multiplicity, stability, and transient behavior. An adjustable-order model is generated with a convergent partial differential equation (PDE)-to-ordinary differential equation (ODE) discretization. Efficiency means the ability to describe the PDE dynamics quantitatively, up to kinetics-transport (KT) parameter error propagation and with the smallest possible order. The problem is solved by combining notions and tools from nonlinear dynamics (bifurcation analysis and structural stability), numerical methods (error propagation analysis and continuation), and chemical reactor engineering. Solvability requires the existence of an order below the critical one for the onset of excessive error propagation. The approach is applied to a 13-profile gasification reactor with experimental data, unknown multiplicity, and finite difference (FD) discretization. It is found that the reactor is robustly bistable, and can be described by a 30th-order model with considerably less equations than in previous related studies.",
     "keywords": ["Tubular heterogeneous reactor", "Gasification reactor", "Distributed system", "Reduced order model", "Multiplicity", "Structural stability"]},
    {"article name": "Process graph approach for two-stage decision making: Transportation contracts",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.07.011",
     "publication date": "02-2019",
     "abstract": "Freight transportation has always been a key factor in industry and market supply and expanding markets relies on its efficacy even more. Uncertainty cannot be avoided in transportation, e.g., traffic or navigability of rivers, and stochastic nature impacts contracts between a firm and a transport company. It is essential to analyze the conditions of contracts, whether they have medium or short term validity, e.g., one costs less while other offers more flexibility. Medium term scenario analysis can be formulated as a two-stage decision problem typically handled by methods applying decision trees. Since realistic problems are complex enough to result in a decision tree of enormous size, application of such methods is practically limited. Our study presents a computer aided algorithmic method based on P-graph framework, which is capable to implicitly involve and enumerate all feasible scenarios instead of explicitly enumerate the possibilities, while keeping the problem formulation compact and visible.",
     "keywords": ["P-graph", "Stochastic optimization", "Supply chain", "Transportation", "Biodiesel"]},
    {"article name": "Performance evaluation of multi-stage reverse osmosis process with permeate and retentate recycling strategy for the removal of chlorophenol from wastewater",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.08.035",
     "publication date": "02-2019",
     "abstract": "Reverse osmosis (RO) is one of the most widely used technologies for wastewater treatment for the removal of toxic impurities, such as phenol and phenolic compounds from industrial effluents. In this research, performance of multi-stage RO wastewater treatment system is evaluated for the removal of chlorophenol from wastewater using model-based techniques. A number of alternative configurations with recycling of permeate, retentate, and permeate-retentate streams are considered. The performance is measured in terms of total recovery rate, permeate product concentration, overall chlorophenol rejection and energy consumption and the effect of a number of operating parameters on the overall performance of the alternative configurations are evaluated. The results clearly show that the permeate recycling scheme at fixed plant feed flow rate can remarkably improve the final chlorophenol concentration of the product despite a reduction in the total recovery rate.",
     "keywords": ["Spiral-wound multi stage reverse osmosis process", "Mathematical modelling", "Chlorophenol removal", "Permeate-retentate recycling design"]},
    {"article name": "Process resilience analysis based data-driven maintenance optimization: Application to cooling tower operations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.10.019",
     "publication date": "02-2019",
     "abstract": "In a process plant system, safe and reliable operations are highly sensitive to utilities such as power, steam, cooling water, nitrogen, and instrument air. These play an important role or act as safety barriers. This is because a disturbance in their supply is likely to affect process operations downstream, may reduce the production efficiency, and may lead to a sudden shutdown or contribute to an unsafe condition. The focus of this paper is to introduce and evaluate a model for survival of a process system under upset conditions using the Process Resilience Analysis Framework (PRAF). Resilience metrics within a data-driven and model-based optimization approach using Bayesian regression are employed integrating both technical (process parameter variations) and social (human and organizational) factors. Based on an optimization objective function accounting for the overall system performance in terms of energy consumption, maintenance costs, safety impact, environmental impact, asset damage, and production loss, the proposed methodology aims to determine the optimal maintenance policy for optimal and safer plant operations. The implementation of the survival model within the PRAF is demonstrated on a cooling tower operation example problem, where optimal operation and maintenance (preventive, corrective, and predictive) strategies are determined based on trade-off analysis of process revenue, safety impact, and maintenance costs.",
     "keywords": ["Maintenance", "Optimization", "Cooling tower", "Resilience", "Bayesian regression"]},
    {"article name": "A robust approach to the design of super-twisting observers \u2013 application to monitoring microalgae cultures in photo-bioreactors",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.07.006",
     "publication date": "02-2019",
     "abstract": "Super twisting observers (STOs) are particularly useful in situations where the process model is uncertain or in the presence of non measurable perturbations. Despite their increasing popularity, the selection of design parameters is delicate and is usually performed in an ad-hoc fashion. In this study, a robust design procedure is developed based on assumptions related to the boundedness of the uncertainties, and a computational procedure based on semi-definite programming is proposed and tested in simulation. Specifically, this procedure is applied to the design of STOs to monitor the internal substrate quota in cultures of micro-algae in photo-bioreactors, which is a key variable in the Droop model and cannot be measured on-line. Using the measurements of the concentrations of biomass and substrate in the culture medium, a state estimation scheme based on two STOs, one for the cell growth rate and the other for the substrate uptake rate, is therefore developed.",
     "keywords": ["State estimation", "Sliding mode observers", "LMIs", "Robust control", "Biological systems"]},
    {"article name": "A model-based optimization of microalgal cultivation strategies for lipid production under photoautotrophic condition",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.10.004",
     "publication date": "02-2019",
     "abstract": "Increasing the lipid production rate stands as one of the challenges for achieving economic feasibility of microalgal biorefinery. Various bioreactor operation strategies have been considered for culturing microalgae, including batch, continuous, fed-batch, semi-batch, and two-stage operations. However, since the previous studies used different experimental criteria, it is hard to draw a conclusion about their relative performances. Motivated by this, the present study compares lipid productivity performances and capital expenditures of the various operation strategies after their operating conditions are optimized. The optimal condition for each strategy is determined by performing a model-based optimization to maximize lipid productivity, which is a good indicator of the overall economics. The two-stage operation with continuous-batch serial connection (w/ stress condition) and the semi-batch operation (w/o stress condition) show outstanding performance compared to the other types of operation. Also, start-up, initial cell concentration, and chemical consumption, which are critical factors in large-scale cultivation, are analyzed.",
     "keywords": ["Microalgae cultivation", "Lipid productivity", "Cultivation strategies", "Model-based optimization", "Capital expenditure"]},
    {"article name": "Deterministic global process optimization: Accurate (single-species) properties via artificial neural networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.10.007",
     "publication date": "02-2019",
     "abstract": "Global deterministic process optimization problems have recently been solved efficiently in a reduced-space by automatic propagation of McCormick relaxations (Bongartz and Mitsos, J. Global Optim, 2017). However, the previous optimizations have been limited to simplified thermodynamic property models. Herein, we propose a method that learns accurate thermodynamic properties via artificial neural networks (ANNs) and integrates those in deterministic global process optimization. The resulting hybrid process model is solved using the recently developed method for deterministic global optimization problems with ANNs embedded (Schweidtmann and Mitsos, J. Optim. Theory Appl., 2018). The optimal operation of a validated steady state model of an organic Rankine cycle is solved as a case study. It is especially challenging as the thermodynamic properties are given by the implicit Helmholtz equation of state. The results show that modeling of thermodynamic properties via ANNs performs favorable in deterministic optimization. This method can rapidly be extended to include properties from existing thermodynamic libraries, based on models or data.",
     "keywords": ["Surrogate-based global optimization", "McCormick relaxations", "Reduced-space formulation", "Organic Rankine cycle", "Thermodynamic properties", "MAiNGO"]},
    {"article name": "A new termination criterion for sampling for surrogate model generation using partial least squares regression",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.10.008",
     "publication date": "02-2019",
     "abstract": "This paper proposes a new incremental sampling method for the generation of surrogate models based on the application of partial least squares regression (PLSR) as a termination criterion. Compared to existing incremental and adaptive methods, the proposed method allows the sampling algorithm to stop without needing to fit a surrogate model at each iteration step. The proposed procedure was applied to a motivating pipe model and two case studies; the reaction and the separation section of an ammonia synthesis loop. In all cases, the new sampling method allows a small number of sampling points, corresponding to a regular grid with less than two points in each independent variable. The two surrogate models of the ammonia loop are combined for overall optimization. The optimum for the combined surrogate models is close to the optimum obtained with the original model.",
     "keywords": ["Partial least squares regression", "Incremental sampling", "Surrogate model", "Optimization", "Integrated processes", "Machine learning", "Design of computer experiments", "Grey box model"]},
    {"article name": "Modeling of reactive batch distillation processes for control",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.10.010",
     "publication date": "02-2019",
     "abstract": "Reactive batch distillation (RBD) is a preferred process intensification technology to carry out equilibrium-limited reactions. It is a multicomponent, multiphase system. Appropriate process description requires dynamic modeling of coupled thermodynamics and transport phenomena including the chemical reactions. Such models are barely applicable to online model based operation technology such as model predictive control, real-time optimization and online process monitoring. Therefore, in this paper, the rigorous dynamic model of an RBD is transformed into a set of decoupled ordinary differential equations using linear transformation matrices, called extent transformation, that preserve the physical meaning of the transformed variables. The resulting model has a state space representation with a diagonal state matrix. This representation is suitable for control purposes and can be considered as a linear parameter varying system. Based on the final structure of the model, controllability conditions are stated, and model reduction scenarios are proposed. Finally, the model based on extent transformations is compared with the rigorous nonlinear model via the simulation of a polyesterification process.",
     "keywords": ["Extent transformation", "Reactive batch distillation", "modeling for control", "linear parameter varying system"]},
    {"article name": "Wide spectrum feature selection (WiSe) for regression model building",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.10.005",
     "publication date": "02-2019",
     "abstract": "Developing predictive models from industrial datasets implies the consideration of many possible predictor variables (features). Using all available features for data-driven modelling is not recommended, as most of them are expected to be irrelevant and their inclusion in the model may compromise robustness and accuracy. In this work, we present, test and compare a new two-stage feature selection method called wide spectrum feature selection for regression (WiSe). In the first stage, a combination of efficient bivariate filters analyzes linear and non-linear association patterns between predictors and responses, screening out clearly noisy features. In the second stage, the reduced set of retained features is subject to further selection in the scope of the predictive methods considered, optimizing their predictive performance. Three simulated datasets and an industrial case illustrate the effectiveness and benefits of applying WiSe to support model development in a wide range of high-dimensional regression problems.",
     "keywords": ["Feature selection", "Filtering methods", "Predictive analytics", "Effect sparsity", "Symmetrical uncertainty"]},
    {"article name": "Design of plantwide control and safety analysis for diethyl oxalate production via regeneration-coupling circulation by dynamic simulation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.09.017",
     "publication date": "02-2019",
     "abstract": "In this article, the plantwide control of a novel process for diethyl oxalate production via two steps is investigated. The unique feature of this process is that there is a closed regeneration-coupling circulation. It results in that two steps should be matched properly and mass balance for overall reaction should be satisfied precisely. An effective control structure using a feedforward ratio with composition controller is determined.Later, safety analysis for this process is investigated by the integration of dynamic simulation and HAZOP (hazard and operability analysis). In comparison with heuristic HAZOP, quantitative deviations can be introduced. Quantitative variation trends and change rates of important variables can be determined. Determining increase rate in temperature and pressure is significantly important, since response time as indirect indictor can be used to assess the possibility of risk. Finally, a general procedure based on simulation for design and safety analysis of chemical process is proposed.",
     "keywords": ["Diethyl oxalate", "Plantwide control", "Safety analysis", "Dynamic simulation"]},
    {"article name": "Design and control of diphenyl carbonate reactive distillation process with thermally coupled and heat-integrated stages configuration",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.10.009",
     "publication date": "02-2019",
     "abstract": "It has been extensively proven that thermally coupled distillation columns can effectively use less energy than their conventional counterparts. Similarly, the extension to reactive systems has also shown that thermally coupled reactive distillation columns can reduce the energy consumption in comparison with their conventional reactive distillation counterparts. This work aims to show that by realizing heat integration between thermally coupled columns at different pressure, further energy savings can be attained. The diphenyl carbonate production process has been taken up to show that there is a synergistic effect when thermally coupling and heat integration are combined in the same distillation sequence. The results showed that the proposed system could attain 47% energy savings in comparison with conventional a reactive distillation sequence while keeping good rejection of throughput and feed composition disturbances.",
     "keywords": ["DPC diphenyl carbonate", "diphenyl carbonate", "DMC dimethyl carbonate", "dimethyl carbonate", "PA phenyl acetate", "phenyl acetate", "MA methyl acetate", "methyl acetate", "MPC Methyl Phenyl Carbonate", "Methyl Phenyl Carbonate", "RD reactive distillation", "reactive distillation", "RDC reactive distillation column", "reactive distillation column", "SC separation column", "separation column", "TCRD thermally coupled reactive distillation", "thermally coupled reactive distillation", "DPTCRD different pressure thermally coupled reactive distillation", "different pressure thermally coupled reactive distillation", "DWC Dividing Wall Columns", "Dividing Wall Columns", "HTCR thermally coupled reactive distillation", "thermally coupled reactive distillation", "RGA Relative Gain Array", "Relative Gain Array"]},
    {"article name": "Classification of states and model order reduction of large scale Chemical Vapor Deposition processes with solution multiplicity",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.08.023",
     "publication date": "02-2019",
     "abstract": "This paper presents an equation-free, data-driven approach for reduced order modeling of a Chemical Vapor Deposition (CVD) process. The proposed approach is based on process information provided by detailed, high-fidelity models, but can also use spatio-temporal measurements. The Reduced Order Model (ROM) is built using the method-of-snapshots variant of the Proper Orthogonal Decomposition (POD) method and Artificial Neural Networks (ANN) for the identification of the time-dependent coefficients. The derivation of the model is completely equation-free as it circumvents the projection of the actual equations onto the POD basis. Prior to building the model, the Support Vector Machine (SVM) supervised classification algorithm is used in order to identify clusters of data corresponding to (physically) different states that may develop at the same operating conditions due to the inherent nonlinearity of the process. The different clusters are then used for ANN training and subsequent development of the ROM. The results indicate that the ROM is successful at predicting the dynamic behavior of the system in windows of operating parameters where steady states are not unique.",
     "keywords": ["Multiplicity of states", "Support Vector Machines", "Classification", "Reduced order modeling", "Artificial Neural Networks", "Data-driven models", "Chemical Vapor Deposition"]},
    {"article name": "A multi-objective optimization approach for sustainable water management for places with over-exploited water resources",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.10.003",
     "publication date": "02-2019",
     "abstract": "Rainwater harvesting (RWH) is analyzed in this work as an option for water supply for places with over-exploited water resources. The model is formulated under a multi-period, multi-objective optimization model. The objective is two fold, first to assess the potential of RWH as an alternative water source, and second to design an optimal water distribution network in which both natural and alternative sources work as an integrated system. The problem is formulated to account for three different objectives, namely maximum profit, minimum groundwater usage and minimum investment cost. A case study for the city of Queretaro in Mexico was considered to show the applicability of the proposed approach. The results show that as much as 27% of the domestic demand in Queretaro City could be supplied by RWH, which would lead to a significant recovery of deep wells currently under depletion.",
     "keywords": ["Rainwater harvesting", "Over-exploited aquifers", "Optimization", "Water supply", "Water networks"]},
    {"article name": "A hybrid time MILP model for the pump scheduling of multi-product pipelines based on the rigorous description of the pipeline hydraulic loss changes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.10.001",
     "publication date": "02-2019",
     "abstract": "As the primary means of transporting refined products, multi-product pipelines play a significant role in ensuring downstream energy supply. The pump scheduling optimization of multi-product pipeline can significantly lower the energy consumption of the pipeline. With the input parameters of pipeline flowrates and batch interface locations, which are determined by a specific detailed schedule, this paper first proposes an innovative method for a rigorous description of pipeline hydraulic loss changes during the multi-batch sequential transportation process. Based on the hybrid time representation, the scheduling horizon is divided into two levels of time windows and a mixed-integer linear programming (MILP) model for the pump scheduling of multi-product pipelines is established according to the proposed method. Various technical and operational constraints are considered. Finally, the established model is successfully applied to a real-world multi-product pipeline, with three operation modes, in China. The superiority, accuracy, and applicability of this model are validated in detail through comparison with a previous model.",
     "keywords": ["Pump scheduling", "Multi-product pipeline", "Mixed-integer linear programming", "Hydraulic loss", "Time continuity"]},
    {"article name": "Optimal placement of imperfect water quality sensors in water distribution networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.10.021",
     "publication date": "02-2019",
     "abstract": "Water Distribution Networks (WDNs) are often susceptible to either accidental or deliberate contamination which can lead to poisoned water, many fatalities and large economic consequences. In order to protect against these intrusions or attacks, an efficient sensor network with a limited number of sensors should be placed in a WDN. In this paper, we focus on optimal sensor placements by introducing two greedy-based algorithms in which the imperfection of sensors and multiple objectives can be taken into account. The algorithms were tested using a medium scale urban WDN. It is shown that our algorithms are able to find sensor placements in reasonable time and that its solutions are close to optimal. Furthermore, relaxing the often used assumption that sensors work perfectly results in different sensor placements than were found before, indicating the importance to take sensor imperfection into account when placing sensors.",
     "keywords": ["Water distribution networks", "Sensor placements", "Contaminant detection", "Imperfect sensors", "Greedy algorithm"]},
    {"article name": "Fast genetic algorithm approaches to solving discrete-time mixed integer linear programming problems of capacity planning and scheduling of biopharmaceutical manufacture",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.09.019",
     "publication date": "02-2019",
     "abstract": "The previous research work in the literature for capacity planning and scheduling of biopharmaceutical manufacture focused mostly on the use of mixed integer linear programming (MILP). This paper presents fast genetic algorithm (GA) approaches for solving discrete-time MILP problems of capacity planning and scheduling in the biopharmaceutical industry. The proposed approach is validated on two case studies from the literature and compared with MILP models. In case study 1, a medium-term capacity planning problem of a single-site, multi-suite, multi-product biopharmaceutical manufacture is presented. The GA is shown to achieve the global optimum on average 3.6 times faster than a MILP model. In case study 2, a larger long-term planning problem of multi-site, multi-product bio-manufacture is solved. Using the rolling horizon strategy, the GA is demonstrated to achieve near-optimal solutions (1% away from the global optimum) as fast as a MILP model.",
     "keywords": ["Capacity planning", "Scheduling", "Genetic algorithm", "Biopharmaceutical"]},
    {"article name": "Economic optimization in transient processes for model predictive control with a dynamic reference trajectory",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.11.006",
     "publication date": "02-2019",
     "abstract": "In this paper, we focus on generating the reference trajectory to improve the economic performance in transient processes for model predictive control (MPC). We convert the trajectory generation problem to the shortest path problem which is one common problem in graph theory. For this purpose we construct a graph, also called the state-input graph, to approximate the dynamics of the system. The nodes of the graph are states obtained by mesh discretization of the state space and the weighted edges represent the costs when driving the system from one state to another under an admissible control input. The admissible control inputs are predetermined by mesh discretization of the input space. The graph describes the accessibility of the mesh points in the state space under control inputs in one sampling time. We run Dijkstra\u2019s algorithm with the obtained graph to find a path from the current state to the optimal point at each sampling time. Then the path is sent to MPC as a reference. The procedure is repeated at the next sampling time and the reference is updated as well. Moreover, we extract a subgraph of the whole graph at each sampling time to accelerate the procedure. A numerical example and a benchmark chemical process are employed to demonstrate the effectiveness of the proposed approach.",
     "keywords": ["Economic optimization", "Model predictive control", "Reference trajectory", "Transient process"]},
    {"article name": "Robust dynamic optimization of enzyme-catalyzed carboligation: A point estimate-based back-off approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.10.006",
     "publication date": "02-2019",
     "abstract": "In this paper, we present a systematic robust dynamic optimization framework applied to the benzaldehyde lyase-catalyzed carboligation of propanal and benzaldehyde to produce (R)-2-hydroxy-1-phenylbutan-1-one (BA). First, the elementary process functions approach was used to screen between different dosing concepts, and it was found that simultaneously dosing propanal and benzaldehyde leads to the highest final concentration of BA. Next, we applied global sensitivity analysis and found that 10 out of 13 kinetic parameters are relevant. Time-varying back-offs were then used to handle parametric uncertainties due to these 10 parameters. A major contribution in our work is the use of the point estimate method instead of Monte Carlo simulations to calculate the back-offs in an efficient and reproducible manner. We show that this new approach is at least 10 times faster than the conventional Monte Carlo approach while achieving low approximation errors.",
     "keywords": ["Enzyme catalysis", "Benzaldehyde lyase", "Optimal reactor design", "Back-off strategy", "Point estimate method", "Dynamic optimization", "Robust optimization"]},
    {"article name": "Modelling steady state intercellular isotopic distributions with isotopomer decomposition units",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.09.024",
     "publication date": "02-2019",
     "abstract": "In the past three decades, various computational frameworks for the modelling of the intracellular isotopic distribution have been proposed by different research groups. Among them the cascaded linear systems are the most popular approaches since they can transfer the nonlinear isotopomer labeling system into cascaded linear sub-systems. In this work, a novel two-step decomposition algorithm to model isotopomer labeling system is proposed. It can be utilized to decompose large metabolic networks. For convenience purpose, a new concept of isotopomer decomposition units (IDUs) is defined to distinguish the approach from others. Comparing to other cascaded linear systems, the IDU method follows a different decomposition procedure and results in another set of cascaded linear systems with different coefficients. Two variants of IDU algorithms are proposed, while one variant of them (IDU-B) has fewer number of balance equations than that of EMU balance equations. The efficiency of the IDU approach is demonstrated through two simulated examples.",
     "keywords": ["Isotopomer decomposition unit", "Steady state isotope labeling experiments", "Isotopomer labeling systems", "Cascaded linear systems"]},
    {"article name": "Model predictive automatic control of sucker rod pump system with simulation case study",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.08.018",
     "publication date": "02-2019",
     "abstract": "This work enables accelerated fluid recovery in oil and gas reservoirs by automatically controlling fluid height and bottomhole pressure in wells. Several literature studies show significant increase in recovered oil by determining a target bottomhole pressure but rarely consider how to control to that value. This work enables those benefits by maintaining bottomhole pressure or fluid height. Moving Horizon Estimation (MHE) determines uncertain well parameters using only common surface measurements. A Model Predictive Controller (MPC) adjusts the stroking speed of a sucker rod pump to maintain fluid height. Pump boundary conditions are simulated with Mathematical Programs with Complementarity Constraints (MPCCs) and a nonlinear programming solver finds a solution in near real-time. A combined rod string, well, and reservoir model simulate dynamic well conditions, and are formulated for simultaneous optimization by large-scale solvers. MPC increases cumulative oil production vs. conventional pump off control by maintaining an optimal fluid level height.",
     "keywords": ["Sucker rod pump", "Model predictive control", "Optimization", "Reservoir modeling"]},
    {"article name": "Analysis of output modifier adaptation for real-time optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.09.028",
     "publication date": "02-2019",
     "abstract": "In the context of real-time optimization, modifier-adaptation schemes update the model-based optimization problem by adding first-order correction terms to the cost and constraint functions of the optimization problem. This guarantees meeting the plant first-order optimality conditions upon convergence, despite the presence of parametric and structural plant-model mismatch. An alternative modifier-adaptation strategy has been proposed, wherein the first-order corrections are applied to the output functions rather than to the cost and constraint functions. This paper analyzes this alternative adaptation scheme in detail and provides arguments for its use in cases where the cost and constraints are nonlinear functions of the inputs and outputs. The approach is illustrated in simulation on the Williams\u2013Otto continuous stirred-tank reactor.",
     "keywords": ["Real-time optimization", "Modifier adaptation", "Plant-model mismatch", "Output adaptation"]},
    {"article name": "Dynamic simulation of the reverse osmosis process for seawater using LabVIEW and an analysis of the process performance",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.11.001",
     "publication date": "02-2019",
     "abstract": "The reverse osmosis (RO) process has become one of the prominent membrane-based technologies for water desalination to meet the demand for fresh water. This paper presents the results of simulations of the RO desalination process based on simplified functional-decomposition approach-based modelling to understand the process dynamics. The simulation model was validated by comparing the transient behaviour predicted by the model with experimental data from an industrial seawater desalination process. The proposed work was carried out using the Control Design and Simulation (CDSim) toolkit in the LabVIEW 2011 environment. The simulation analysis showed that the specific energy consumption of the RO system could be reduced at different fractional recovery rate by maintaining a reasonably high feed temperature.",
     "keywords": ["Reverse osmosis", "Desalination", "Dynamic simulation", "Experimental validation"]},
    {"article name": "Model predictive control of uni-axial rotational molding process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.11.005",
     "publication date": "02-2019",
     "abstract": "This paper addresses the problem of achieving tight product consistency and enabling automated process changes to deliver user-selected criterion based product in a complex industrial batch process such as uni-axial rotational molding. To this end, a data driven state-space model is first identified. For a given trajectory of input moves (heater and compressed air profiles), this dynamic model is able to predict the evolution of the measured variable (internal product temperature). The dynamic model is augmented with a quality model, which relates its own terminal predictions to a selection of key quality variables (sinkhole area, ultrasonic spectra amplitude, impact test metric and viscosity). The dynamic and quality models are in turn utilized within a model predictive control (MPC) framework that enables specifying product quality requirements explicitly. Experimental results demonstrate the ability of the MPC not only in achieving tight quality control but also providing on-spec product for a new specification.",
     "keywords": ["Rotational modeling process", "Batch processes", "Subspace identification", "Model predictive control"]},
    {"article name": "Optimal (n\u22121)-reliable design of distributed energy supply systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.09.029",
     "publication date": "02-2019",
     "abstract": "Distributed energy supply systems are most efficiently designed by mathematical optimization. However, optimization models often assume availability of all components at any time. In practice, security of energy supply is crucial; thus, reliability is mandatory but often neglected in optimization and only implemented subsequently employing expensive rules of thumb.In this work, we propose an exact optimization approach to identify ( n \u2212 1 )-reliable designs for energy systems. The approach guarantees energy supply during the failure of 1 component at any time and is independent of probabilities and the selection of scenarios. ( n \u2212 1 )-reliability is also necessary to allow for maintenance of components. For problems with high computational effort, we propose the inexact but computationally efficient ( n \u2212 1 max )-reliability approach which also guarantees energy supply but allows overproduction. A real-world case study shows that both approaches identify reliable designs at only a small increase of the total annualized costs compared to the unreliable base case.",
     "keywords": ["Energy system", "MILP", "Optimization", "Reliability", "Synthesis"]},
    {"article name": "Well placement optimization using direct mapping of productivity potential and threshold value of productivity potential management strategy",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.11.013",
     "publication date": "02-2019",
     "abstract": "Well placement optimization is usually complex, nonlinear and multimodal in oil field development. Well placement is usually optimized by coupling reservoir numerical simulator with population-based algorithms. This method is computationally expensive, as they require many fitness function calls during the optimization process which limits its optimization efficiency. In this work, the direct mapping of productivity potential (DMPP) technique is applied for well placement optimization problems. Threshold value of productivity potential (TVPP) coupled with particle swarm optimization (PSO) is first proposed as a screening process to reduce the number of objective function evaluation based on DMPP. PUNQ-S3 model is applied to analyze the adaptability of DMPP and feasibility of TVPP for well placement optimization. Results demonstrate TVPP management strategy can significantly reduce the optimization time (OT) and keep the optimization effect at the same time. Taken it total, the proposed management strategy is simple but very efficient.",
     "keywords": ["Well placement optimization", "Direct mapping of productivity potential", "Threshold value of productivity potential", "Optimization time reduction", "Particle swarm optimization"]},
    {"article name": "Municipality solid waste supply chain optimization to power production under uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.11.003",
     "publication date": "02-2019",
     "abstract": "Storage of municipality solid waste (MSW) in landfills is a traditional practice of the past. Negative impacts of landfills have motivated interest in the sustainable utilization of MSW. In this study, we cover the treatment of the MSW supply chain problem. The supply chain network is composed by several arcs connecting sources of MSW by treatment facilities, and markets imposing demand for products. A two-stage stochastic MILP model is formulated to examine the effects of the supply-demand, and power price uncertainties. The first stage decision variables involve technology and capacity selection. The second stage decision variables deal with transportation, and power production. An L-shaped decomposition algorithm is shown to be effective in obtaining solutions for the stochastic model. A risk model is adopted to analyze the financial risk in the organic MSW supply chain problem. A case study is examined to show the application of the mathematical programming formulation.",
     "keywords": ["Organic MSW management", "Supply chain", "L-shaped decomposition algorithm", "Optimization under uncertainty"]},
    {"article name": "Optimization of helium extraction processes integrated with nitrogen removal units: A comparative study",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.11.002",
     "publication date": "02-2019",
     "abstract": "Helium is regarded as a vital gas to various industries such as medicine, aircraft manufacturing, electronics and fiber optics fabrication. Currently, natural gas reserves are considered the only viable resource for this rare element. When processing (helium-rich) natural gas, helium is generally recovered in the most downstream stage in conjunction with the nitrogen rejection unit (NRU). The feed to this unit is a nitrogen rich stream, and the product is either crude helium (50\u201370 mol% purity) or purified helium (99.99 mol% purity). Currently, the cryogenic distillation method is a common technology for a crude helium extraction unit (HeXU). The alternative method for this purpose is a membrane gas separation system, which is successfully used in other applications. This study aims to propose an energy-integrated scheme for each of the two helium separation technologies with a single-column NRU and to evaluate and compare them for different applications. Matlab programming has been used to model the membrane system and incorporate it into Aspen Hysys, which is used to simulate the rest of the process flowsheet. Next, the energy consumption of the systems was optimized using the particle swarm optimization method. An economic analysis was adopted to compare the two technologies for different applications in order to suggest a comprehensive map for HeXU technology selection.",
     "keywords": ["Helium extraction", "Nitrogen removal", "Gas permeation separation", "Cryogenic distillation", "Membrane separation", "Process integration"]},
    {"article name": "The design and scheduling of chemical batch processes: Computational complexity studies",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.11.011",
     "publication date": "02-2019",
     "abstract": "The pharmaceutical industry is quite restrictive concerning quality and safety, the manufacturing disruptions often lead to drug shortages in despite of the high costs involved. Due to the minimization of equipment costs, the design and scheduling of chemical batch processes (DSCBP) is a well-known problem, and various sub-problems are selected from the literature because they were successively enlarging the design and schedule policies: single machine or multiple machines (S or M) in each stage; and single product campaigns or multiple products campaigns (SPC or MPC). In this paper, four problems are studied (by combinatorics: SS, MS, SM, and MM) and it is shown that they are all NP-hard in strong sense through polynomial reduction. This study can support innovative algorithms and methodologies for solving DSCBP problems, in a way to improve equipments sizing and configurations design, and thereby contributing to curb disruptions within pharmaceutical supply chains (PharmSC).",
     "keywords": ["Computational complexity", "Design and scheduling", "Batch processes", "Algorithms", "Pharmaceutical supply chains", "Medicines shortages", "design and scheduling chemical batch processes DSCBP", "DSCBP", "multiple product campaign MPC", "MPC", "single product campaign SPC", "SPC", "multiple processes and MPC problem MM", "MM", "multiple processes and SPC problem MS", "MS", "pharmaceutical supply chain PharmSC", "PharmSC", "single process and MPC problem SM", "SM", "single process and SPC problem SS", "SS"]},
    {"article name": "Modified inferential POD/ML for data-driven inverse procedure of steam reformer for 5-kW HT-PEMFC",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.11.012",
     "publication date": "02-2019",
     "abstract": "In this work, we applied and evaluated modified inferential proper orthogonal decomposition (POD)/machine learning (ML) to a steam reformer for 5-kW high-temperature proton-exchange membrane fuel cells (HT-PEMFC) involving heterogeneous chemical reactions, combustion, and fluid flow. The number of snapshots is limited by the inverse problem of a steam reformer yielding an intractable computational burden, and a limited number of snapshots and modes can yield unfavorable POD subspace projection results. In order to solve this problem, characteristic vectors are derived from the residual after POD projection and employed to the feature. We analyzed the details and distribution of the characteristic vector and investigated the extent of its influence on the inferential POD. Consequently, inferential POD/ML is improved by adding the characteristic vector of observation to the feature for ML.",
     "keywords": ["Inverse problem", "Machine learning", "Proper orthogonal decomposition", "Proton-exchange membrane fuel cells", "Radial basis function network", "Steam reformer"]},
    {"article name": "The set covering problem applied to optimisation of gas detectors in chemical process plants",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.11.008",
     "publication date": "02-2019",
     "abstract": "An approach to optimise the number and location of gas detectors was developed based on the colour pattern of the graph of the set covering problem (SCP). The optimisation problem is combined to Computational Fluid Dynamics (CFD) data and the discrete optimisation problem is solved using Balas algorithm in the framework of a developed Fortran code. Every computational cell is regarded as a node of a graph where the links are the common boards shared by the neighbouring cells. The graph is read into the code via a dedicated algorithm for generation of the set of constrains. The characteristic length that determines the distance among the nodes of the graph is obtained from the set of CFD simulations or any stablished criterion. This work is based on the discovery of the colour pattern observed in the adjacency matrix that represents the set of constrains of the optimisation formulation and also on the limited set of gas dispersion CFD simulations to ensure full coverage of the area. Two cases (covering problem and p-median problem) are used in the validation of the developed code. Results are compared with the results obtained using CPLEX. Additional tests are considered ranging from simple 2D cases with 4 nodes up to 75 nodes. Two real engineering cases are considered and the efficiency of the method is discussed based on the volume of the cloud detected and the time to detection. Results show an efficient method for optimisation of gas detectors able to detect flammable clouds as small as a few cubic meters within 10 seconds.",
     "keywords": ["Set covering problem", "Graph", "Dominating set problem", "Gas detectors", "CFD"]},
    {"article name": "Demand response-oriented dynamic modeling and operational optimization of membrane-based chlor-alkali plants",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.08.030",
     "publication date": "02-2019",
     "abstract": "Power-intensive processes can potentially provide significant demand response (DR) services. Modeling such processes for demand response is not trivial as models must depict plant transient properties under highly dynamic operation while remaining computationally efficient. We develop a demand response-oriented model for an important power-intensive process i.e., chlor-alkali production using membrane cells, and demonstrate the provision of fast demand response by an industrial-size plant. Through an extensive simulation and optimization case study, we show that the fast modulation of the cell power demand is possible without adverse impact on cell concentration and temperature. Additionally, the cell temperature dynamics are found to restrict the demand response capacity of the plant and must to be explicitly accounted for to support dynamic cell operation in DR scenarios. Substantial load curtailment during peak electricity price periods can be achieved and the energy cost to the electrolysis plant can be reduced.",
     "keywords": ["Dynamic modeling", "Chlor-alkali process", "Demand response", "Energy storage", "Electrolysis"]},
    {"article name": "Optimisation of heat exchanger network cleaning schedules: Incorporating uncertainty in fouling and cleaning model parameters",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.11.009",
     "publication date": "02-2019",
     "abstract": "The optimisation of the cleaning schedule in heat exchanger networks (HENs) subject to fouling is presented in which the impact of parametric uncertainty is considered. This work is based on the realisation that these HEN cleaning scheduling problems are multistage mixed-integer optimal control problems (MIOCPs) in which the controls, i.e. cleaning actions, exhibit bang-bang behaviour as they occur linearly in the system equations. A multiple scenario feasible path MIOCP approach is proposed, whereby many scenarios of the HEN multiperiod problems are stacked, sharing the same control actions. This is implemented on a 10 unit and a 25 unit HEN case studies representing crude oil refinery preheat trains (PHTs). Results show that there is a large difference in financial performance for the deterministic case versus the parametric uncertainty problem and hence it is vital that uncertainty be taken into account during the optimisation of schedules for HEN maintenance problems.",
     "keywords": ["Optimal control problem", "Bang-bang control", "Fouling", "Parametric uncertainty"]},
    {"article name": "Application of reduced-order models based on PCA & Kriging for the development of digital twins of reacting flow applications",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.09.022",
     "publication date": "02-2019",
     "abstract": "Detailed numerical simulations of detailed combustion systems require substantial computational resources, which limit their use for optimization and uncertainty quantification studies. Starting from a limited number of CFD simulations, reduced-order models can be derived using a few detailed function evaluations. In this work, the combination of Principal Component Analysis (PCA) with Kriging is considered to identify accurate low-order models. PCA is used to identify and separate invariants of the system, the PCA modes, from the coefficients that are instead related to the characteristic operating conditions. Kriging is then used to find a response surface for these coefficients. This leads to a surrogate model that allows performing parameter exploration with reduced computational cost. Variations of the classical PCA approach, namely Local and Constrained PCA, are also presented. This methodology is demonstrated on 1D and 2D flames produced by OpenSmoke++ and OpenFoam, respectively, for which accurate surrogate models have been developed.",
     "keywords": ["Principal component analysis", "Kriging", "Surrogate models"]},
    {"article name": "Ensemble pattern trees for predicting hot metal temperature in blast furnace",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.10.022",
     "publication date": "02-2019",
     "abstract": "In steel industry, it is crucial to predict hot metal temperature (HMT), which is strongly related to the product quality and the thermal state, to keep high productivity of the blast furnace. The present work proposes a novel ensemble pattern trees model to predict HMT. Ensemble pattern trees is a robust nonlinear modeling method, which aggregates a set of pattern trees models into a single predictive model via the bagging technique. Ensemble pattern trees overcomes the drawback of single pattern trees which may not be robust enough against the random variations such as process perturbations and noises in the blast furnace. In addition, a novel variable importance measure derived from the ensemble pattern trees is proposed to understand which process variables affect the final hot metal quality. The proposed method was validated through an industrial blast furnace ironmaking process, and the results have demonstrated its superiority to several conventional methods.",
     "keywords": ["Virtual sensing", "Steelmaking process", "Blast furnace", "Ensemble pattern trees", "Variable importance measure"]},
    {"article name": "Parameter and state estimation of an agro-hydrological system based on system observability analysis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.11.015",
     "publication date": "02-2019",
     "abstract": "In this work, a systematic approach based on system observability analysis and state estimation is developed to estimate the soil moisture inside an agro-hydrological system where measurements are not easily available. A discrete-time state-space model based on Richards equation is used to describe the agro-hydrological system that considers water dynamics inside soil-plant and atmosphere systems. The nonlinear agro-hydrological system is linearized every sampling time and the observability of the overall system is determined for the locally linearized model every sampling time. Based on the linearized models, we investigate how the number and location of output measurements affect the degree of observability of the system. To demonstrate the efficiency of the proposed approach, state estimation is performed using the extended Kalman filter on both simulated and real field data. The parameters of the model are estimated using prediction error method based on historical output measurements.",
     "keywords": ["Nonlinear systems", "Agro-hydrological systems", "Observability analysis", "Optimal sensor placement"]},
    {"article name": "An adaptive sequential wavelet-based algorithm developed for dynamic optimization problems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.11.007",
     "publication date": "02-2019",
     "abstract": "In this paper we present an adaptive wavelet algorithm (WAA) tailored for dynamic optimization problems (DOP). The main feature of the WAA is the automatic computation of time-domain discretization, generating a self-adapting control parameterization, which depends on the nonlinear characteristics of the mathematical model. For this, the control variables are analyzed and treated at different wavelet levels. First, we have demonstrated the advantages of WAA over heuristic adaptive procedures, proposed in the last years. Second, the results of the proposed strategy are illustrated through the solution of ten case studies. According to the results, the computation cost could be reduced by about 56% on average. Besides, the average NLP size reduction was approximately 49.94%, showing that one of the most considerable advantages of the algorithm is the adaptive discretization without prior information of the control profile.",
     "keywords": ["Dynamic optimization", "Nonlinear programming", "Wavelets", "Thresholding", "Control vector parameterization"]},
    {"article name": "Simulation and analysis of vacuum pressure swing adsorption using the differential quadrature method",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.11.017",
     "publication date": "02-2019",
     "abstract": "A lab-scale vacuum pressure swing adsorption process for oxygen production was investigated both experimentally and theoretically. The experiments were conducted with up to 91% purity and 17% recovery. A complete set of governing equations were solved and compared using the finite difference method (FDM) and differential quadrature method (DQM). Based on the theoretical achievements, a new comprehensive algorithm is proposed, which is compatible with various numerical methods. The DQM method with 12 points combined with the FDM for time integration was determined to be accurate enough for predicting system behaviour. The artificial neural network (ANN) with two hidden layers and up to eight neurons was used to predict the process behaviour at more complex conditions. The agreement between the simulation results and experimental data shows that the algorithm accurately simulates the cyclic adsorption process, and the ANN is reliable for prediction of system behaviour considering variations in all parameters.",
     "keywords": ["Vacuum pressure swing adsorption", "Oxygen production", "Numerical simulations", "Artificial neural network"]},
    {"article name": "Development and implementation of advanced control strategies for power plant cycling with carbon capture",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.11.004",
     "publication date": "02-2019",
     "abstract": "In this paper, three model predictive control (MPC) strategies are developed and implemented for coal-fired power plant cycling applications. These strategies correspond to a dynamic matrix control (DMC)-based linear MPC, the proposed nonlinear MPC (NLMPC) and a nonlinear MPC from the literature (L-NMPC) as benchmark. For application purposes, dynamic models of two different coal-fired power plants with carbon capture are addressed: an integrated gasification combined cycle power plant with a water-gas shift membrane reactor (IGCC-MR) and a supercritical pulverized coal-fired power plant with monoethanolamine-based post-combustion carbon capture (SCPC-MEA). Successful MPC implementations on IGCC-MR system and SCPC-MEA carbon capture subsystem are addressed, including cycling trajectory tracking and disturbance rejection scenarios. The closed-loop results show that the proposed nonlinear MPC (NLMPC) improves the control performance by up to 96% when compared to the DMC controller in terms of the integral squared error (ISE) results.",
     "keywords": ["Nonlinear model predictive control", "IGCC", "MEA post-combustion carbon capture"]},
    {"article name": "Life cycle analysis of milking of microalgae for renewable hydrocarbon production",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.11.019",
     "publication date": "02-2019",
     "abstract": "Botryococcus braunii is a unique microalga which can repeatedly produce the hydrocarbons after their non-destructive extraction \u2013 the process called milking. Botryococcus braunii hydrocarbons can be converted to high-quality fuel or used as other high-value products. In this study, we conduct the life cycle analysis of the milking process for renewable hydrocarbon production focusing on the GHG emissions, the fossil energy consumption, the freshwater consumption and the land use of the process. The total-CO2 emissions and the GHG emissions over 100-year time span for production of B. braunii hydrocarbons were estimated to be \u22120.39\u202fkg CO2-eq/kg hydrocarbons and \u22120.90\u202fkg CO2-eq/kg hydrocarbons, respectively. The fossil energy ratio of the process was found to be 1.04\u202fMJ produced/MJ fossil energy consumed. The fresh water consumption of the process and the land use were estimated to be 1802\u202fkg/kg hydrocarbons and 0.85\u00a0m2/kg of hydrocarbons, respectively.",
     "keywords": ["Renewable fuel", "botryococcus braunii", "microalgae", "CO2 sequestration"]},
    {"article name": "Simulation of the Sour-Compression Unit (SCU) process for CO2 purification applied to flue gases coming from oxy-combustion cement industries",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.11.010",
     "publication date": "02-2019",
     "abstract": "The purpose of the present work was to investigate, for a gas issued from a full oxy-fuel combustion in the cement industry, a CO2 de-SOx and de-NOx process, called \u201cSour-Compression Unit\u201d (SCU), thanks to simulations with Aspen PlusTM. An important stage necessary for the SCU modeling has been the construction of an accurate chemical mechanism. Two-column and single-column configurations have been evaluated and compared. A parametric study and a Design Of Experiments have been conducted on a single-column process to study the influence of the operating parameters on the SOx and NOx abatement ratios. As a demonstration of the effectiveness of the model, three SOx and NOx purity specifications (depending on the further applications of the CO2) were applied to the purified gas. The feature of the investigated model lies on optimizing the way to reach the purity target in order to decrease installation costs (CAPEX) and energy requirements (OPEX).",
     "keywords": ["Full oxy-fuel combustion", "CO2 purification", "Cement industry", "De-SOx", "De-NOx"]},
    {"article name": "An environmental optimization model to configure a hybrid forward and reverse supply chain network under uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.11.014",
     "publication date": "02-2019",
     "abstract": "The configuration of facility locations is a strategic decision which is impossible to be changed in a short-term. There are several parameters incorporating to make such decision that fluctuate in different situations. Those unpredictable changes cause some risks and complexities for the businesses in long-run. The objective of this research is to design a multi-echelon lead acid battery closed-loop supply chain (CLSC) network under uncertainty. To this aim, a fully fuzzy programming (FFP) method and stochastic programming are integrated (FFSP) to address various scenarios to maximize network's profit. Furthermore, second objective is introduced to maximize the environmental compliance of suppliers, plants, and battery recovery centers. Then, distance technique is utilized for solving the fuzzy stochastic multi-objective problem. The application of the proposed model is illustrated in a network in Winnipeg, Canada.",
     "keywords": ["Forward and reverse supply chain", "Fully fuzzy stochastic programming (FFSP)", "Multi-objective model (MOM)", "Closed-loop supply chain", "Optimization"]},
    {"article name": "Multi-timescale, multi-period decision-making model development by combining reinforcement learning and mathematical programming",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.11.020",
     "publication date": "02-2019",
     "abstract": "This study focuses on the linkage between decision layers that have different time scales. The resulting expansion of the boundary of decision-making process can provide more robust and flexible management and operation strategies by resolving inconsistencies between different levels. For this, we develop a multi-timescale decision-making model that combines Markov decision process (MDP) and mathematical programming (MP) in a complementary way and introduce a computationally tractable solution algorithm based on reinforcement learning (RL) to solve the MP-embedded MDP problem. To support the integration of the decision hierarchy, a data-driven uncertainty prediction model is suggested which is valid across all time scales considered. A practical example of refinery procurement and production planning is presented to illustrate the proposed method, along with numerical results of a benchmark case study.",
     "keywords": ["Multi-timescale decision making", "Decision under uncertainty", "Markov decision process", "Mathematical programming", "Reinforcement learning"]},
    {"article name": "Recursive NARX model identification of nonlinear chemical processes with matrix invertibility analysis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.11.022",
     "publication date": "02-2019",
     "abstract": "In the learning techniques based on kernel or orthogonal basis functions, the nonlinearity in the underlying complex dynamic process is modelled by a linear combination of a set of kernel or orthogonal basis functions. Once these functional parameters are selected, the learning task boils down to solving linear least squares (LS). This has motivated the development of various recursive learning algorithms, where matrix inversion is intrinsic in solving LS problems. However, what has not attracted much attention along this track is the analysis of the matrix invertibility conditions in the recursive algorithms. This analysis is especially important when a model is sequentially downdated from the data, which may lead to rank deficiency. The main contribution of this work is the analysis of these conditions, in the formulation of a recursive NARX algorithm based on radial basis functions (RBF-NARX). Aiming at identifying nonlinear and nonstationary time series, RBF-NARX also features a fast algorithm with combined downdating and updating in a single learning step. Both the necessary conditions for checking the singularity of the regressor matrices and the sufficient conditions for ensuring their invertibility are proved. The performance of RBF-NARX and the invertibility conditions are tested and verified by the data from chemical processes.",
     "keywords": ["NARX model", "Radial basis functions", "Matrix invertibility", "Recursive nonlinear least squares", "Nonstationary nonlinear time series"]},
    {"article name": "Artificial Neural Network control of thermoelectrically-cooled microfluidics using computer vision based on IR thermography",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.11.016",
     "publication date": "02-2019",
     "abstract": "High-speed and high-accuracy thermal control of reactors has always been of interest to chemical engineers. In this paper we present a new methodology for thermal control of a continuous-flow chemical reactor using non-contact IR thermography combined with computer vision and a predictive Artificial Neural Network. The system exhibits several key advantages over thermocouples and PID control including the ability to quantify and account for thermal diffusion in the system, to collect and process data very quickly and with high accuracy, to analyze the entire surface of the reactor, and to update its training based not only on the current thermal response, but also on external factors. We have constructed and validated such a system as well as shown improvements in its accuracy, rise time, settling time, set point tracking, and overshoot as compared to more traditional forms of thermal control, validating this as a possible approach for experimental and process control.",
     "keywords": ["Thermal control", "Machine learning", "Neural networks", "Process control", "Autonomous microfluidics", "Process resiliency", "ANN Artificial Neural network", "Artificial Neural network", "CAD Computer Aided Design", "Computer Aided Design", "CAM Computer Aided Modeling", "Computer Aided Modeling", "CNC Computer Numerical Control", "Computer Numerical Control", "CGB Conjugate Gradient Backpropagation", "Conjugate Gradient Backpropagation", "CGBPFRU Conjugate gradient backpropagation with Fletcher\u2013Reeves updates", "Conjugate gradient backpropagation with Fletcher\u2013Reeves updates", "CGBPPBR Conjugate gradient backpropagation with Powell\u2013Beale restarts", "Conjugate gradient backpropagation with Powell\u2013Beale restarts", "CGBPPRU Conjugate gradient backpropagation with Polak\u2013Ribi\u00e9re updates", "Conjugate gradient backpropagation with Polak\u2013Ribi\u00e9re updates", "DLL Direct Link Library, compiled C/C++ code", "Direct Link Library, compiled C/C++ code", "FPS Frames per Second", "Frames per Second", "IoT Internet of Things", "Internet of Things", "LM Levenberg\u2013Marquardt (Damped Least Squares) with forward training", "Levenberg\u2013Marquardt (Damped Least Squares) with forward training", "NARMA Nonlinear Autoregressive Moving Average", "Nonlinear Autoregressive Moving Average", "OSSBP One-step Secant backpropagation", "One-step Secant backpropagation", "PID Proportional Integral Derivative control", "Proportional Integral Derivative control", "PWM Pulse Width Modulation", "Pulse Width Modulation", "q-NBP quasi-Newton backpropagation", "quasi-Newton backpropagation", "RBP Scaled Conjugate Gradient Backpropagation", "Scaled Conjugate Gradient Backpropagation", "UART Universal Asynchronous Receiver\u2013Transmitter", "Universal Asynchronous Receiver\u2013Transmitter", "UFPA Uncooled Focal Plane Array", "Uncooled Focal Plane Array", "USB Universal Serial Bus", "Universal Serial Bus", "VOx Vanadium Oxide thin film microbolometer", "Vanadium Oxide thin film microbolometer"]},
    {"article name": "Proxy models for caprock pressure and temperature dynamics during steam-assisted gravity drainage process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.10.023",
     "publication date": "02-2019",
     "abstract": "The usage of first principles-based dynamic models for advanced control, monitoring and optimization of petroleum reservoirs is constrained by the large scale nature of the models. The parametric uncertainty makes the process even more challenging; hence, development of proxy models is an attractive proposition. In this work, proxy models are developed from spatio-temporal data of pressure and temperature in the caprock during steam-assisted gravity drainage operation. The first proxy model addresses the issue of reduced-order dynamic modelling of the caprock pressure and temperature fields based on proper orthogonal decomposition and system identification using data from the first principles model. The second proxy model takes the first step towards dynamic analysis of factor of safety in reservoir management by modelling the evolution of clusters of high, medium and low pressure regions using graph theory and subspace modelling. System theoretic properties of these proxy models and their practical relevance is also analysed.",
     "keywords": ["Steam assisted gravity drainage (SAGD)", "Proxy modeling", "Proper orthogonal decomposition (POD)", "Clustering", "Graph theory", "Factor of safety(FoS)"]},
    {"article name": "Modelling and particle based simulation of electro-coalescence of a water-in-oil emulsion",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.12.003",
     "publication date": "02-2019",
     "abstract": "Electrocoalescence is an industrially relevant process, especially in oil exploration and petroleum refining industries, and is the working principle of the process equipment, an electrocoalescer. In this work, electrocoalescence is modelled using dipolar electrostatic forces which bring the droplets together but are resisted by Stokesian drag forces. We demonstrate that hydrodynamic interactions are important to model electrocoalescence. Similarly, a multi box methodology is necessary for getting improved statistics for drop size distribution, since any coagulation/coalescence process, typically results in the loss of droplets. Moreover, the process of chaining can be modelled by assuming a \u201cwait-time\u201d on contact of the interacting drops, and n-mer formation can be predicted. Our results show a reasonable agreement between model predictions and experimental data. The simulations also show that an increase in volume fraction, average droplet size, and polydispersity enhance the dynamics of the coalescence process.",
     "keywords": ["Electro-coalescence", "Dipole-dipole force", "Stokeslet", "Wait-time", "Water-in-oil emulsion"]},
    {"article name": "Computational approaches to kinetic model selection",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.12.002",
     "publication date": "02-2019",
     "abstract": "This paper demonstrates how the stoichiometry and kinetic model of a chemical synthesis involving multiple reactions can be selected via a computational approach which uses consecutive optimisation steps. First, a list of all feasible stoichiometric relations consistent with the molecular weights or the elemental makeup of participating species is developed using integer linear programming (ILP). A second ILP is then used to construct all plausible stoichiometric schemata (combinations of the stoichiometric equations) which are used to instantiate kinetic model structures. Using a numerical integration routine, the models are simulated and unknown parameters estimated using an iterative optimisation algorithm. Produced model structures are then numerically scored, ranked and compared. This allows selection between competing models using both physical and the statistical evidence the data provides. The methods are demonstrated using synthetic and experimental data sets assuming liquid-phase reactions occurring in a well-mixed isothermally operated batch reactor.",
     "keywords": ["Reaction engineering", "Kinetic modelling", "Model structure selection", "Parameter identification", "Statistical inference"]},
    {"article name": "A novel cost-effective silica membrane-based process for helium extraction from natural gas",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.12.013",
     "publication date": "02-2019",
     "abstract": "Natural gas reserves with 0.3\u20132 mol% helium are considered as the only viable source for this noble gas. Currently, cryogenic separation is used to extract helium, but it is energy-intensive. While membrane-based separation is a promising alternative, it is still not considered economical. Even for an inorganic/silica membrane with relatively high selectivity and permeance, a multi-stage membrane system with inter-stage compression is required, which necessitates high CAPEX and OPEX. This study proposes a novel process to enhance the selectivity and permeance of an inorganic/silica membrane system to eliminate the costly inter-stage compression. A 16\u201324% reduction in the CAPEX and 23\u201357% in the OPEX are achievable for a natural gas feed with 3\u20135 mol% helium. In contrast, for a 2 mol% helium feed, the OPEX increases by 34%. However, a 30% decrease in the capital cost outweighs the OPEX increase to make the new process more profitable.",
     "keywords": ["Helium extraction", "Gas permeation separation", "Sweep gas", "Inorganic membrane", "Organic membrane"]},
    {"article name": "Direct computation of Hopf bifurcation points in differential-algebraic equations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.12.008",
     "publication date": "02-2019",
     "abstract": "In this work, we address the bifurcation theory for differential-algebraic equations (DAEs). Our main subject is the direct computation of Hopf bifurcation points, for which we present a novel methodology. In order to achieve this goal, the theory of ordinary differential equations (ODEs) was extended to formulate a concise stability criterion and provide a procedure for the computation of characteristic polynomials of DAEs. Hopf bifurcation points of DAE models of any index and with any number of parameters can now be easily handled. The methodology is tested for the calculation of Hopf bifurcation points of a benchmark model in chemical engineering in a fast and accurate way.",
     "keywords": ["Hopf", "Bifurcation", "Differential-algebraic equations", "Stability", "Characteristic polynomials"]},
    {"article name": "Efficient online model-based design of experiments via parameter subset selection for batch dynamical systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.12.005",
     "publication date": "02-2019",
     "abstract": "Model-based design of experiments (MBDOE) is being widely used for the efficient identification of complex dynamical systems. Given real-time measurements, online MBDOE can be formulated. However, conventional real-time MBDOE requires considerable computational time for finding a solution which makes real-time implementation impossible. Moreover, the optimality of experimental design and the accuracy of parameter estimates are not ensured. We propose a new algorithm that advances online MBDOE by focusing on the subset of parameters at each design instant. It considerably reduces the numerical complexity of the problem while almost completely preserving its optimality and allowing for faster and more accurate calculation. A case study is presented, wherein the proposed algorithm is applied to a fed-batch bioreactor model with 14 parameters.",
     "keywords": ["Model-Based design of experiment", "Parameter estimation", "Parameter subset", "System identification"]},
    {"article name": "Advanced modeling of vegetable oils steam stripping with structured packing columns",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.12.007",
     "publication date": "02-2019",
     "abstract": "This work presents an advanced modeling of the deodorization and the physical refining (steam stripping) of vegetable oils using structured packed columns, applying the non-equilibrium model developed by (Taylor and Krishna, 1993) and (Taylor and Kooijman, 1993). In this model, mass and energy balances are calculated separated through the interface for each liquid and vapor phase by using mass and energy transfer rates.The proposed modeling is implemented in a process simulation platform, which involves the collection of basic properties, the selection of equations for temperature dependent properties of each compound present in vegetable oils and the construction of a deodorization column as a stage to stage unit operation. Two cases are studied by applying the process model proposed: a free fatty acids stripping and deodorization process of bleached palm oil and a tocopherol stripping and deodorization process of soybean oil.",
     "keywords": ["Vegetable oils", "Deodorization", "Non-equilibrium modeling", "Simulation"]},
    {"article name": "Customized code generation based on user specifications for simulation and optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.12.006",
     "publication date": "02-2019",
     "abstract": "Model-driven software engineering is a well-known concept in computer science, but scarcely applied in chemical engineering. This contribution presents a first implementation of model-driven development of customized code for simulation and optimization based on equation-oriented models in process science. Transferring the model-driven approach to chemical engineering allows for transformations or discretizations before automated code generation. The customization of this concept poses additional challenges regarding flexibility and tailoring for the user\u2019s needs. We propose the so-called \u201cUser-defined Language Specificators (UDLS)\u201d, based on free standards (MathML, XML), to combine the benefits of automated code generation with the flexibility of customization, therefore still avoiding error-prone manual model implementation. The case studies show the successful application of this approach for equation-based flowsheet simulation (using CAPE-OPEN interfaces) as well as optimization.",
     "keywords": ["MathML model", "Model transformation", "Code generation", "Customized simulation"]},
    {"article name": "Techno-economic and life cycle analysis of different types of hydrolysis process for the production of p-Xylene",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.11.018",
     "publication date": "02-2019",
     "abstract": "The need for producing renewable chemicals and fuels from lignocellulosic biomass has increased due to economic and environment reasons. However, most of the chemicals studied to date are produced using sugars, which is readily available from biomass as their starting raw materials rather than lignocellulose, which increase the minimum selling price of produced chemicals. In this work a novel one-step saccharification (combining pretreatment and hydrolysis) process of biomass using molten salt hydrates (MSHs), an inorganic salt solution, to produce sugars and further integration of the upstream process to produce p-Xylene is investigated. We compared our one-step process with reported commercial relevant dilute acid (DA) and concentrated acid (CA) processes using detailed techno-economic analysis and life cycle analysis. The production of p-Xylene using the DA process is more costly compared to the MSH and CA processes. The cost of raw materials and utilities are the two major cost factors for the MSH process. From the life cycle analysis point of view, CA perform better than MSH process, while the DA process is less environmentally friendly mainly due to high reaction temperatures and usage of steam because of the dilute nature of the process.",
     "keywords": ["P-Xylene Production", "Techno-economic analysis", "Life-cycle analysis", "Bio-refinery", "Hydrolysis"]},
    {"article name": "Conceptual design of a lignocellulosic biorefinery and its supply chain for ethanol production in India",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.11.021",
     "publication date": "02-2019",
     "abstract": "A mixed integer linear programming model for the optimal synthesis of a biorefinery and its supply chain for producing ethanol from agricultural residue in the Indian context is developed. A superstructure considering multiple options for each processing stage is developed. Mass and energy balance equations along with fuel demand and feedstock availability equations constitute the model constraints. Process selection decisions and the mass flows are the decision variables, and minimization ethanol production cost for a specific production target is the objective function. The model determined the minimum ethanol cost to be Rs. 70/l ($ 1.05/l) for a biorefinery producing 100\u00a0Mg/month of ethanol. Dilute acid pretreatment and simultaneous saccharification and fermentation (SSCF) were the optimal process selections. Pretreatment selection had a significant impact on the ethanol cost. The ethanol cost reduced to Rs. 43.9/l ($ 0.65/l) for a biorefinery producing 1000\u00a0Mg/month of ethanol.",
     "keywords": ["Lignocellulosic biorefinery", "India", "Optimization", "Supply chain", "Economics"]},
    {"article name": "Direct coupling of continuum and kinetic Monte Carlo models for multiscale simulation of electrochemical systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.12.016",
     "publication date": "02-2019",
     "abstract": "Electrochemical systems include atomistic processes at electrochemical interfaces and macroscopic transport processes, which can be modeled using kinetic Monte Carlo (kMC) simulation and continuum equations, respectively. Multiparadigm algorithms are applied to directly couple such models to study multiscale interactions. This article compares different algorithms for an example problem. Results quantify the effect of computational cost and numerical accuracy by the choice of algorithm and its configuration. The stochastic fluctuations of kMC simulations as well as sequential data exchange between the models generate errors in coupled simulations. Measures to reduce stochastic fluctuations or revise exchanged data can be either highly successful or futile, depending on the dominant cause of the error. Hence, we strongly advise to identify the different causes of errors and their mechanics when selecting a coupling algorithm or optimizing its configuration. This article provides various algorithms and suggestions for their configuration to enable efficient and robust multiscale simulations.",
     "keywords": ["Multiparadigm", "Multiscale", "Electrochemcial systems", "Interfaces", "Computational methods"]},
    {"article name": "Logarithmic mean: Chen's approximation or explicit solution?",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.10.002",
     "publication date": "01-2019",
     "abstract": "An explicit solution has been obtained for the logarithmic mean temperature difference method of heat exchanger calculation by making use of the Lambert W-function. The results might be of use where an explicit solution involving the logarithmic mean is required.",
     "keywords": ["Logarithmic mean", "Approximation", "HEN", "Lambert W-function"]},
    {"article name": "Model-based bidding strategies on the primary balancing market for energy-intense processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.09.026",
     "publication date": "01-2019",
     "abstract": "Energy-intense enterprises that flexibilize their electricity consumption can market this either at electricity spot markets or by offering ancillary services on demand, such as balancing power. We formulate optimization of the balancing power bidding strategy as a mixed-integer nonlinear program considering both price forecasts for the ancillary service market and hourly varying spot market prices. We solve this two-stage approach by decomposition into a nonlinear bidding problem and a mixed-integer linear scheduling problem. We consider aluminum electrolysis participating in the German primary balancing market. We show savings in weekly production costs of 5\u201320% compared to stationary operation. The savings due to the optimal bidding strategy are up to twice the savings from pure exploitation of electricity spot market price spreads. We thus demonstrate that energy-intense processes can systematically take advantage of highly profitable demand-side management measures beyond a spot market price adjusted production.",
     "keywords": ["Demand side management", "Aluminum electrolysis", "Ancillary service markets", "Mixed-integer nonlinear programming", "Model-based bidding strategies"]},
    {"article name": "Dynamic optimization of batch free radical polymerization with conditional modeling formulation through the adaptive smoothing strategy",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.09.023",
     "publication date": "01-2019",
     "abstract": "With the increase of conversion and viscosity, a batch free radical polymerization (FRP) may encounter complicated phenomena, including the gel, glass, and cage effects. These effects normally lead to a conditional modeling formulation described by a set of differential-algebraic equations (DAEs). Discontinuous gradient exists at model switches triggered by occurrence of the effects, which cause difficulty for dynamic optimization on the batch process. To deal with the discontinuity, interior-point smooth approximations are proposed to describe the model switches at the occurrence of the effects. An adaptive smoothing strategy, which can efficiently balance the efficiency of optimization and the accuracy of the approximated model, is proposed for the dynamic optimization. To show the effectiveness of this smoothing strategy, case studies for optimal operating profiles in a polymerization process of methyl methacrylate are conducted. Good computational performance is achieved with the proposed strategy.",
     "keywords": ["Free radical polymerization", "Conditional modeling", "Adaptive smoothing", "Dynamic optimization"]},
    {"article name": "Performance enhancement of pressure-swing distillation process by the combined use of vapor recompression and thermal integration",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.09.014",
     "publication date": "01-2019",
     "abstract": "The intensified pressure-swing distillation (PSD) process is explored by applying vapor recompression and thermal integration. The evaluation indicators of second-law efficiency and CO2 emissions are adopted to rank different pressure swing-top vapor recompressed (PSDVRC) configurations. Compared to the conventional PSD process, the economically optimum flowsheet is the thermal integrated PSDVRC process with overhead vapor splitting since it can reduce 67.05% in energy consumption, 72.66% in CO2 emissions and 34.14% in total annual cost (TAC) as well as enhance 159.06% in thermodynamic efficiency. Besides, PSDVRC processes with overhead vapor splitting (PSDVRC-VS) are also superior to other PSDVRC configurations where economic-efficient process is PSD process (PSDSVRC) with single vapor recompression. Dynamic controllability for the highly integrated and interacting economic-efficient process is also investigated. An effective control structure is developed that only handles small (5%) disturbances in throughput and feed composition. The modified process is further developed to achieve robust control performance in terms of peak dynamic transients, settling time, oscillation and steady-state offsets. Meanwhile, control performance comparisons for PSDVRC-VS and PSDSVRC processes are also studied and showed that there is conflict between the steady-state economics and dynamic controllability.",
     "keywords": ["Pressure-swing distillation", "Vapor recompression", "Thermal integration", "Dynamic controllability"]},
    {"article name": "Determining the number of segments for piece-wise linear representation of discrete-time signals",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.08.034",
     "publication date": "01-2019",
     "abstract": "Piece-wise linear representation (PLR) separates a discrete-time continuous-valued signal into a few short data segments, each of which is represented by a straight line. One prerequisite parameter in the PLR is the number of data segments. This paper proposes a new method to determine the number of data segments. The method is based on a key observation on two balancing effects of the number of data segments on the confidence intervals of line approximations. The confidence intervals define parallelogram-shaped spaces, and an index is formulated as the weighted percentage of these spaces occupied by data points in data segments. The number of data segments is determined as the one achieving the maximum value of the formulated index. Numerical and industrial examples are provided to illustrate the effectiveness of the proposed method.",
     "keywords": ["Data segmentation", "Piecewise linear representation", "Linear regression", "Confidence intervals"]},
    {"article name": "Parallel cyclic reduction decomposition for dynamic optimization problems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.09.023",
     "publication date": "01-2019",
     "abstract": "Direct transcription of dynamic optimization problems, with differential-algebraic equations discretized and written as algebraic constraints, can create very large nonlinear optimization problems. When this discretized optimization problem is solved with an NLP solver, such as IPOPT, the dominant computational cost often lies in solving the linear system that generates Newton steps for the KKT system. Computational cost and memory constraints for this linear system solution raise many challenges as the system size increases. On the other hand, the linear KKT system for our dynamic optimization problem is sparse and structured, and can be permuted to form a block tridiagonal matrix. This study explores a parallel decomposition strategy for block tridiagonal systems that is based on cyclic reduction (CR) factorization of the KKT matrix. The classical CR method has good observed performance, but its numerical stability properties need further study for our KKT system. Finally, we discuss modifications to the CR decomposition that improve performance, and we apply the approach to four industrially relevant case studies. On the largest problem, a parallel speedup of a factor of four is observed when using eight processors.",
     "keywords": ["Dynamic optimization", "KKT systems", "Cyclic reduction", "Parallel computation"]},
    {"article name": "A multi-objective robust optimization scheme for reducing optimization performance deterioration caused by fluctuation of decision parameters in chemical processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.08.037",
     "publication date": "11-2018",
     "abstract": "The fluctuation of decision parameters will deviate from the optimal decision, which will have significant impact on the optimization performance of chemical processes. To reduce optimization performance deterioration caused by fluctuation of decision parameters in chemical processes, a multi-objective robust optimization scheme is developed to assess performance robustness. In addition, based on the model that maps decision parameters to objective performance through neural network, a new robustness evaluation metric is created as the fitness value of the multi-objective evolutionary algorithm (for improving the strength Pareto evolutionary algorithm (SPEAII)) to elaborate the relationship between robustness and fluctuation. The efficacy of the proposed method is verified with HCN production process application by comparing with genetic algorithm (GA) and weighted single-objective robust optimization.",
     "keywords": ["Decision parameters fluctuation", "Robustness evaluation metric", "Chemical process", "Multi-objective robust optimization"]},
    {"article name": "Solution of population balance equations by logarithmic shape preserving interpolation on finite elements",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.08.008",
     "publication date": "11-2018",
     "abstract": "A new numerical approach for solving population balance equations (PBE) is proposed and validated. The method employs a combination of basis functions, defined on finite elements, to approximate the sought distribution function. Similarly to other methods of the same family, the PBE are solved only in a finite number of values of the internal coordinate (grid points). The peculiarity of the method is the use of a logarithmic, shape-preserving interpolation (LSPI) procedure to estimate the values of the distribution in between grid points. The main advantages of the LSPI method compared to other approaches of the same category are: (i) the stability of the numerical approach (i.e., the absence of oscillations in the distribution function occurring when using \u201cstandard\u201d cubic splines and a low number of elements), and (ii) the conceptual and implementation simplicity, as no mathematical manipulation of the PBE is required.",
     "keywords": ["Population balance equations", "Aggregation", "Breakage", "Finite elements", "Interpolation"]},
    {"article name": "Multi-objective Bayesian optimization of chemical reactor design using computational fluid dynamics",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.08.005",
     "publication date": "11-2018",
     "abstract": "This study presents a computational fluid dynamics (CFD) based optimal design tool for chemical reactors, in which multi-objective Bayesian optimization (MBO) is utilized to reduce the number of required CFD runs. Detailed methods used to automate the process by connecting CFD with MBO are also proposed. The developed optimizer was applied to minimize the power consumption and maximize the gas holdup in a gas-sparged stirred tank reactor, which has six design variables: the aspect ratio of the tank, the diameter and clearance of each of the two impellers, and the gas sparger. The saturated Pareto front is obtained after 100 iterations. The resulting Pareto front consists of many near-optimal designs with significantly enhanced performances compared to conventional reactors reported in the literature. We anticipate that this design approach can be applied to any process unit design problems that require a large number of CFD simulation runs.",
     "keywords": ["Multi-objective optimization", "Bayesian optimization", "Computational fluid dynamics", "CFD-based optimization", "Reactor design", "Machine learning"]},
    {"article name": "Semi-batch industrial process of nitriles production: Dynamic simulation and validation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.08.013",
     "publication date": "11-2018",
     "abstract": "A semi-batch reactor is used to produce nitriles by reacting fatty acids and ammonia. Despite it is an old chemical process and many patents are available in the literature, its mathematical model was not developed until now. This process comprises a semi-batch reactor with recycle, which makes the numerical solution a challenge to face. In this paper, the entire process has been computer simulated for the very first time. Aspen Plus software was used to find the initial condition for the set of differential algebraic equations, which will be solved by Aspen Plus Dynamics, the key tool to ensure that the entire batch cycle could be properly programmed. Besides that, PI controllers were implemented to guarantee normal and safety operation of the process evaluated, in other words, controllers performed well in tracking pressure and composition profiles, target conversion for the desired product and level behavior. The simulated results were validated with industrial data and opportunities for improving the process were evaluated. The main result was a reduction in batch cycle time for around 40\u00a0min.",
     "keywords": ["Nitrile production", "Fatty acid", "Semi-batch reactor", "Dynamic simulation", "Aspen Dynamics"]},
    {"article name": "ARES: An efficient approach to adaptive time integration for stiff Differential-Algebraic Equations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.08.009",
     "publication date": "11-2018",
     "abstract": "In the broad context of solving systems of stiff Differential-Algebraic Equations (DAE), in-between a basic Euler implicit scheme with a fixed timestep and adaptive timestep and higher order approaches, we propose the Adaptive Relaxed Euler Scheme (ARES): an implicit Euler scheme with an adaptive timestep, in conjunction with a nonlinear solver using the Newton method. We stick to a 1st-order time scheme and the adaptive quality uses very few additional operations and is therefore much less costly and easier to implement, while remaining adaptive to the local stiffness of the system. The overall principle of ARES: allowing to reduce accuracy of a transient calculation in order to get faster to a steady state, proves to be especially relevant in the context of complex industrial reactive transport simulations, where only the steady state of the plant is of interest, while eluding often evaluation through a direct calculation. In cases where computational time is of the essence, our approach is demonstrated through practical examples to offer a simple and valid way to obtain steady-state solutions reliably and fast.",
     "keywords": ["Reactive transport", "Adaptive timestep integrators", "Nonlinear Differential-Algebraic Equations", "Pulsed columns", "Stiff chemistry"]},
    {"article name": "Optimizing the inventorying and distribution of chemical fluids: An innovative nested column generation approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.08.004",
     "publication date": "11-2018",
     "abstract": "Vendor-managed-inventory is a successful business practices based on the cooperation between a supplier and its customers in which demand and inventory information from the customers are shared with the supplier. This practice is gaining popularity in the chemical industry and relies on the inventory-routing-problem, which integrates inventory management, vehicle routing, and delivery scheduling decisions. This one is a difficult combinatorial optimization problem both theoretically and practically. However, because of the large expenses involved in distribution and inventorying of chemical products, it is attractive to make use of optimization tools for exploiting as many degrees of freedom as possible with the goal of minimizing both distribution and inventorying costs. Consequently, we propose a nested column generation algorithm for solving an inventorying and distribution problem that models the delivery of several chemicals fluids. The approach is building on a column generation & incomplete branch-and-price algorithm in which for each delivery route, the delivery patterns of fluids are also determined by column generation. We detail the implementation and provide computational results for realistic test instances.",
     "keywords": ["Multi-commodity", "Nested column generation", "Incomplete branch-and-price", "Multi-compartment vehicles", "Inventory-routing-problem"]},
    {"article name": "Integration of the biorefinery concept for the development of sustainable processes for pulp and paper industry",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.07.019",
     "publication date": "11-2018",
     "abstract": "This work aims at developing sustainable processes for pulp and paper industry by integration of the biorefinery concept to an existing pulp and paper process. A systematic methodology employing a superstructure-based process synthesis approach is employed with support from computer-aided tools to determine potential pathways for a long-term sustainable growth objective. A superstructure of the multi-product biorefinery process network for the pulp and paper industry is developed. It is divided into three sub-networks, a chemical pulping section, a biochemical production section and a black liquor utilization section. Superstructure optimization is performed with the objective to maximize profit to determine optimal integrated networks for three scenarios. The obtained results provide useful insights for further development of the optimal networks as sustainable integrated biorefinery combined with pulp and paper mills.",
     "keywords": ["Pulp and paper industry", "Biorefinery", "Process integration", "Process synthesis", "Superstructure optimization"]},
    {"article name": "Development of a plant-wide Dimethyl Oxalate (DMO) synthesis process from syngas: Rigorous design and optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.08.025",
     "publication date": "11-2018",
     "abstract": "The rigorous design and optimization of a complex, plant-wide dimethyl oxalate (DMO) synthesis process from syngas is firstly developed in this work. The whole process can be roughly divided into two sections. The first one is to produce DMO through the coupling reaction, in which CO is reacted with the intermediate, methyl nitrite (MN). This section includes the production, and also the purification of DMO. The second one is the regeneration reaction to get back MN through a packed column reactor, which is simulated as a modified version of reactive distillation column. After the basic design, systematic optimization is investigated through minimizing total annual cost, and the potential heat integration strategy is also proposed. During optimization, we found that the methanol circulation rate inside the process is the most influential variable, and a higher methanol flow rate (defined as MeOH/NOs) within an acceptable region leads to better economic performance.",
     "keywords": ["Process design", "Optimization", "Dimethyl Oxalate", "Coupling reaction", "Regeneration reaction"]},
    {"article name": "Data-driven identification of interpretable reduced-order models using sparse regression",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.08.010",
     "publication date": "11-2018",
     "abstract": "Developing physically interpretable reduced-order models (ROMs) is critical as they provide an understanding of the underlying phenomena apart from computational tractability for many chemical processes. In this work, we re-envision the model reduction of nonlinear dynamical systems from the perspective of regression. In particular, we solve a sparse regression problem over a large set of candidate functional forms to determine the structure of the ROM. The method balances model complexity and accuracy by selecting a sparse model that avoids overfitting to accurately represent the system dynamics when subjected to a different input profile. By applying to a hydraulic fracturing process, we demonstrate the ability of the developed models to reveal important physical phenomena such as proppant transport and fracture propagation inside a fracture. It also highlights how a priori knowledge can be incorporated easily into the algorithm and results in accurate ROMs that are used for controller synthesis.",
     "keywords": ["Reduced-order model", "Sparse regression", "Hydraulic fracturing", "Model predictive control"]},
    {"article name": "A steady-state analysis method for optimal operation of dividing-wall column",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.05.010",
     "publication date": "11-2018",
     "abstract": "Optimal operation indicates energy consumption (Q) close to its Qmin while product specifications are satisfied. Generally, for DWC, there are two manipulated variables (MVs) to keep Q close to Qmin. Therefore, there are three operation mode, which are two MVs online optimization (Mode 2), single MV online optimization (Mode 1) and two MVs fixed (Mode 0). A steady-state analysis method for optimal operation of DWC with disturbance is presented. In the method, whether optimal operation can be achieved under Mode 0/ Mode 1 can be determined. Feedback is used for the online optimization under Mode 1 and Mode 2. The relation between degree of closeness (between Q and Qmin) and setpoint range of manipulated variable/control variable can be acquired. For ternary separation, dilution effect in DWC is proposed. It reveals DWC may need online optimization. Implementation error is discussed. Disturbance and implementation error can be handled by using the proposed method simultaneously. The important ternary separation of benzene, toluene, and o-xylene in industry is used as an example to illustrate the method. In the case, optimal operation can be realized under Mode 1 and Mode 2.",
     "keywords": ["Dividing-wall column", "Optimal operation", "Online optimization", "Feedback", "Sensitivity analysis", "Steady state", "A light component", "light component", "B middle component", "middle component", "C heavy component", "heavy component", "xA mole fraction of A in feed", "mole fraction of A in feed", "xB mole fraction of B in feed", "mole fraction of B in feed", "xC mole fraction of C in feed", "mole fraction of C in feed", "DWC dividing-wall column", "dividing-wall column", "Q energy consumption", "energy consumption", "LQR low energy consumption region", "low energy consumption region", "F feed condition", "feed condition", "MV manipulated variable", "manipulated variable", "MVO manipulated variable for optimization", "manipulated variable for optimization", "MVLQR manipulated variables low energy consumption region", "manipulated variables low energy consumption region", "CV controlled variable", "controlled variable", "CVLQR controlled variables low energy consumption region", "controlled variables low energy consumption region", "HVLQR hybrid variables low energy consumption region", "hybrid variables low energy consumption region", "Lmp liquid flow entering the top of prefractionator from the main column", "liquid flow entering the top of prefractionator from the main column", "Vmp vapor flow entering the bottom of prefractionator form main column", "vapor flow entering the bottom of prefractionator form main column", "RL the ratio of Lmp to liquid flow above the dividing wall", "the ratio of Lmp to liquid flow above the dividing wall", "RV the ratio of Lmp to liquid flow above the dividing wall", "the ratio of Lmp to liquid flow above the dividing wall", "xpm,A the mole fraction of light component A in liquid flow entering the main column from the bottom of prefractionator", "the mole fraction of light component A in liquid flow entering the main column from the bottom of prefractionator", "ypm,C the mole fraction of heavy component C in vapor flow entering the main column from the top of prefractionator", "the mole fraction of heavy component C in vapor flow entering the main column from the top of prefractionator", "k the degree of closeness between Q and Qmin", "the degree of closeness between Q and Qmin", "kmin,mode 0 the kmin under Mode 0", "the kmin under Mode 0", "kmin,mode 1 the kmin under Mode 1", "the kmin under Mode 1", "kmin,mode 2 the kmin under Mode 2", "the kmin under Mode 2", "kFBmin,mode 1 the kmin under Mode 1, when feedback is used for online optimization", "the kmin under Mode 1, when feedback is used for online optimization", "kFBmin,mode 2 the kmin under Mode 2, when feedback is used for online optimization", "the kmin under Mode 2, when feedback is used for online optimization", "kIEmin,mode 1 the kmin under Mode 1, when implementation error and disturbance are considered simultaneously and feedback is used for online optimization", "the kmin under Mode 1, when implementation error and disturbance are considered simultaneously and feedback is used for online optimization", "kIEmin,mode 2 the kmin under Mode 2, when implementation error and disturbance are considered simultaneously and feedback is used for online optimization", "the kmin under Mode 2, when implementation error and disturbance are considered simultaneously and feedback is used for online optimization"]},
    {"article name": "A parallel unidirectional coupled DEM-PBM model for the efficient simulation of computationally intensive particulate process systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.08.006",
     "publication date": "11-2018",
     "abstract": "The accurate modeling of the physics underlying particulate processes is complicated and requires significant computational capabilities to solve using particle-based models. In this work, a unidirectional multi-scale approach was used to model the high shear wet granulation process. A multi-dimensional population balance model (PBM) was developed with a mechanistic kernel, which in turn obtained collision data from the discrete element modeling (DEM) simulation. The PBM was parallelized using a hybrid OpenMP+MPI approach. The DEM simulations were performed using LIGGGHTS, which was parallelized using MPI. Speedups of approximately 14 were obtained for the PBM simulations and approximately 12 for the DEM simulations. The uni-directional coupling of DEM to PBM was performed using middle-ware components (RADICAL-Pilot) that did not require modifications of the DEM or PBM codes, yet supported flexible execution on high-performance platforms. Results demonstrate scaling from 1 to 128 cores for the PBM and up to 256 cores for the DEM. The proposed method, implementations and middle-ware enable the modeling of high shear wet granulation process faster than existing approaches in literature.",
     "keywords": ["Population balance model", "Granulation", "Discrete element method", "MPI and OpenMP", "Pharmaceutical process design"]},
    {"article name": "Surrogate model generation using self-optimizing variables",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.08.031",
     "publication date": "11-2018",
     "abstract": "This paper presents the application of self-optimizing concepts for more efficient generation of steady-state surrogate models. Surrogate model generation generally has problems with a large number of independent variables resulting in a large sampling space. If the surrogate model is to be used for optimization, utilizing self-optimizing variables allows to map a close-to-optimal response surface, which reduces the model complexity. In particular, the mapped surface becomes much \u201cflatter\u201d, allowing for a simpler representation, for example, a linear map or neglecting the dependency of certain variables completely. The proposed method is studied using an ammonia reactor which for some disturbances shows limit-cycle behaviour and/or reactor extinction. Using self-optimizing variables, it is possible to reduce the number of manipulated variables by three and map a response surface close to the optimal response surface. With the original variables, the response surface would include also regions in which the reactor is extinct.",
     "keywords": ["Self-optimizing control", "Surrogate model", "Sampling domain definition", "B-Splines", "Optimization of integrated processes", "Steady-state optimization"]},
    {"article name": "Simulation and optimization of dynamic flux balance analysis models using an interior point method reformulation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.08.041",
     "publication date": "11-2018",
     "abstract": "This work presents a novel, differentiable, way of solving dynamic Flux Balance Analysis (dFBA) problems by embedding flux balance analysis of metabolic network models within lumped bulk kinetics for biochemical processes. The proposed methodology utilizes transformation of the bounds of the embedded linear programming problem of flux balance analysis via a logarithmic barrier (interior point) approach. By exploiting the first-order optimality conditions of the interior-point problem, and with further transformations, the approach results in a system of implicit ordinary differential equations. Results from four case studies, show that the CPU and wall-times obtained using the proposed method are competitive with existing state-of-the art approaches for solving dFBA simulations, for problem sizes up to genome-scale. The differentiability of the proposed approach allows, using existing commercial packages, its application to the optimal control of dFBA problems at a genome-scale size, thus outperforming existing formulations as shown by two dynamic optimization case studies.",
     "keywords": ["Dynamic flux balance analysis", "Ordinary differential equations with embedded optimization", "Linear programming", "Genome-scale metabolic network"]},
    {"article name": "Strategic decision-making in the pharmaceutical industry: A unified decision-making framework",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.09.010",
     "publication date": "11-2018",
     "abstract": "The implementation of efficient strategic decisions such as process design and capacity investment under uncertainty, during the product development process, is critical for the pharmaceutical industry. However, to tackle these problems the widely used multi-stage/scenario-based optimization formulations are still ineffective, especially for the first-stage (here-and-now) solutions where uncertainty has not yet been revealed.This study extends the authors\u2019 previous work addressing the stochastic product-launch planning problem, by developing a new Multi-Objective Integer Programming model, embedded in a unified decision-making framework, to obtain the final design strategy that \u201cmaximizes\u201d productivity while considering the decision-maker preferences.An approximation of the efficient Pareto-front is determined, and a subsequent Pareto solutions analysis is made to guide the decision process. The developed approach clearly identifies the process designs and production capacities that \u201cmaximize\u201d productivity as well as the most promising solutions region for investment. Moreover, a good balance between investment and capacity allocation was achieved.",
     "keywords": ["Uncertainty", "Strategic decisions", "Process design", "Capacity planning", "Multi-Objective Integer Programming", "Pharmaceutical industry"]},
    {"article name": "A Kriging-based approach for conjugating specific dynamic models into whole plant stationary simulations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.09.009",
     "publication date": "11-2018",
     "abstract": "Steady-state simulators are usually applied for design, techno-economic analysis and optimization of industrial processes. However, sometimes dynamic systems are important parts of the process, which cannot be disregarded. Coupling a dynamic model within a full-plant for steady-state simulation is a challenging task, whatever might be the simulator concept, either sequential or equation-oriented. An alternative to solve this problem is the use of surrogate models to substitute specific dynamic models, by taking the variable time as an extra input of the meta-model. This methodology was applied in an equation-oriented simulator (EMSO) by the use of Kriging meta-models. A case study involving the production of bioethanol from sugarcane was used to demonstrate the capability of this approach. A Kriging meta-model used to substitute the kinetic model of an enzymatic hydrolysis reactor was conjugated into the global plant simulation and an optimization problem was successfully solved.",
     "keywords": ["Kriging", "Surrogate model", "Dynamic and steady-state simulation", "Plant design and optimization"]},
    {"article name": "An optimization framework for scheduling of converter aisle operation in a nickel smelting plant",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.08.024",
     "publication date": "11-2018",
     "abstract": "The scheduling of converter aisle operation in a nickel smelting plant is a complex task with significant ramifications to plant profitability and production. An optimization-based scheduling formulation is developed using a continuous-time paradigm to accurately represent event timings. The formulation accounts for environmental restrictions on sulfur dioxide emissions, using event timing constraints. Flash furnaces are characterized by a continuous inlet flow and intermittent, discrete material removal, which is captured via novel semi-continuous modeling. An innovative sequencing and symmetry-breaking scheme is introduced to exploit identical units operating in parallel. A rolling horizon feature is included to accommodate real-time optimization. Tightening constraints are developed to improve the computational performance. A unique, multi-tiered procedure enhances the practicality of the solution and supports additional operability objectives, without compromising the optimality of the primary objective. The success of the approach is demonstrated via case studies arising from industrial production scenarios.",
     "keywords": ["Scheduling", "Continuous-time formulation", "Semi-continuous operation", "Mixed-integer linear programming", "Nickel smelting", "Tiered optimization"]},
    {"article name": "Optimal operation of water distribution networks with intermediate storage facilities",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.04.017",
     "publication date": "11-2018",
     "abstract": "The nexus between water and energy reveals that transporting water for end use is a highly energy intensive operation. In this work we consider the optimal operation of a water distribution network consisting of pumps delivering water to different reservoirs, with each reservoir catering to a time varying demand. Pumps and ON/OFF valves are used as manipulated variables to minimize energy consumption while meeting the demand. Due to the nonlinear nature of the pump operating curve and the hydraulics, this results in a Mixed Integer Nonlinear Program (MINLP). We propose a three step decomposition approach to solve this problem efficiently. The applicability of this technique is demonstrated on a water network proposed for a municipality in India and the potential advantages are reported. We also compare the solution times required for the proposed technique and a standard solver and demonstrate the efficiency of the proposed approach.",
     "keywords": ["Water distribution networks", "Energy minimization", "Scheduling", "Mixed integer non-linear programming"]},
    {"article name": "Utilizing big data for batch process modeling and control",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.09.013",
     "publication date": "11-2018",
     "abstract": "This manuscript illustrates the use of big data for modeling and control of batch processes. A modeling and control framework is presented that utilizes data variety (temperature or concentration measurements along with size distribution) to achieve newer control objectives. For an illustrative crystallization process, an approach is proposed consisting of a subspace state-space model augmented with a linear quality model, able to model and predict, and therefore control the particle size distribution (PSD). The identified model is deployed in a linear model predictive control (MPC) with explicit model validity constraints. The paper presents two formulations: a) one that minimizes the volume of fines in the product by leveraging the variety of measurements and b) the other that directly controls the shape of the particle size distribution in the product. The former case is compared to traditional control practice while the latter\u2019s superior ability to achieve desired PSD shape is demonstrated.",
     "keywords": ["Batch process", "Subspace identification", "Model predictive control", "Big-data", "Data driven model predictive control"]},
    {"article name": "Extended cross decomposition for mixed-integer linear programs with strong and weak linking constraints",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.09.011",
     "publication date": "11-2018",
     "abstract": "Large-scale mixed-integer linear programming (MILP) problems, such as those from two-stage stochastic programming, usually have a decomposable structure that can be exploited to design efficient optimization methods. Classical Benders decomposition can solve MILPs with weak linking constraints (which are decomposable when linking variables are fixed) but not strong linking constraints (which are not decomposable even when linking variables are fixed). In this paper, we first propose a new rigorous bilevel decomposition strategy for solving MILPs with strong and weak linking constraints, then extend a recently developed cross decomposition method based on this strategy. We also show how to apply the extended cross decomposition method to two-stage stochastic programming problems with conditional-value-at-risk (CVaR) constraints. In the case studies, we demonstrate the significant computational advantage of the proposed extended cross decomposition method as well as the benefit of including CVaR constraints in stochastic programming.",
     "keywords": ["Cross decomposition", "Benders decomposition", "Dantzig\u2013Wolfe decomposition", "Risk-averse stochastic programming", "Mixed-integer linear programming"]},
    {"article name": "On improving the hydrogen and methanol production using an auto-thermal double-membrane reactor: Model prediction and optimisation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.09.006",
     "publication date": "11-2018",
     "abstract": "The concentric configured thermally-coupled double-membrane reactor (TCDMR) was optimised to improve the co-production of hydrogen and methanol. Using a detailed approach, we identified the non-linear differential evolution (DE) algorithm as the most suitable optimisation tool among the most used optimisation algorithms in reactor design (GA, PSO, and DE) due to its ability to converge to the optimal solution with fewer iterations. Considering DE algorithm with the industry benchmark data, we optimised the key operational parameters of TCDMR (as OTCDMR), leading to the improved reactor performance (regarding the overall heat transfer and methanol/hydrogen production) compared to the conventional methanol reactor (CMR) and TCDMR. Simulation results show that the methanol production rate of OTCDMR could reach 315.7 tonnes day\u22121, representing a 22.6% enhancement than CMR (257 tonnes day\u22121). For the hydrogen production, OTCDMR is predicted to deliver 19.7 tonnes of hydrogen per day, surpassing the 15.5 tonnes day\u22121 production rate by TCDMR.",
     "keywords": ["Non-linear optimisation", "Auto-thermal reactor", "Membrane reactor", "Hydrogen", "Methanol"]},
    {"article name": "Optimal Laypunov exponent parameters for stability analysis of batch reactors with Model Predictive Control",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.08.038",
     "publication date": "11-2018",
     "abstract": "Thermal runaways in exothermic batch reactions are a major economic, health and safety risk in industry. In literature most stability criteria for such behaviour are not reliable for nonlinear non-steady state systems. In this work, Lyapunov exponents are shown to predict the instability of highly nonlinear batch processes reliably and are hence incorporated in standard MPC schemes, leading to the intensification of such processes. The computational time is of major importance for systems controlled by MPC. The optimal tuning of the initial perturbation and the time frame reduces the computational time when embedded in MPC schemes for the control of complex batch reactions. The optimal tuning of the initial perturbation and time horizon, defining Lyapunov exponents, has not been carried out in literature so far and is here derived through sensitivity analyses. The computational time required for this control scheme is analysed for the intensification of complex reaction schemes.",
     "keywords": ["Optimal Lyapunov exponent parameters", "Thermal stability analysis", "Model Predictive Control", "Batch reactors"]},
    {"article name": "Numerical investigation of selective withdrawal in a pancreatic cell islet encapsulation apparatus",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.09.021",
     "publication date": "11-2018",
     "abstract": "The development of an efficient microencapsulation apparatus is a major challenge for islet transplantation. To that end, the flow generated by selective withdrawal in an apparatus for micro-encapsulation of pancreatic islets is studied through CFD simulations. Each islet enters individually a chamber containing a twolayer system in which is encapsulated by selective withdrawal. Optimal encapsulation occurs, when the perturbed interface is kept stable and transition to viscous entrainment is prevented. Simulations were validated with experimental data. Contrary to previous studies that simplify the problem by approximating the tubes as a doublet of a point mass source and sink, the model presented here employs a detailed geometry. Numerical results shed light on the dependence of the shape of the interface on flow, geometry and physical parameters. These observations can contribute to the design of encapsulation apparatuses considering polydispersity in size and the different shape of the islets.",
     "keywords": ["Cell encapsulation", "Medical device", "Diabetes", "Fluid-fluid interface", "Similarity analysis"]},
    {"article name": "A mixed-integer conic programming formulation for computing the flexibility index under multivariate gaussian uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.09.005",
     "publication date": "11-2018",
     "abstract": "We present a methodology for computing the flexibility index when uncertainty is characterized using multivariate Gaussian random variables. Our approach computes the flexibility index by solving a mixed-integer conic program (MICP). This methodology directly characterizes ellipsoidal sets to capture correlations in contrast to previous methodologies that employ approximations. We also show that, under a Gaussian representation, the flexibility index can be used to obtain a lower bound for the so-called stochastic flexibility index (i.e., the probability of having feasible operation). Our results also show that the methodology can be generalized to capture different types of uncertainty sets.",
     "keywords": ["Flexibility", "Uncertainty", "Ellipsoidal", "Mixed-Integer"]},
    {"article name": "Exo-parametric (\u201cinside-out\u201d) model of discounted cash flow calculations using NPV%: Macro calculation of coefficients for an exact, collapsed financial model",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.09.001",
     "publication date": "11-2018",
     "abstract": "In a 2013 paper (Mellichamp, 2013), the author developed an internally-consistent discounted cash flow (DCF) model that represents the key financial outputs of a conceptual chemical process design using just four variables: \u2022 Total investment, TI (capital required to construct and operate a project for NTotal years), \u2022 ROIBT and NPVproj (the traditional annual and long-term (NPV) profitabilities), and \u2022 NPV% [a new metric whose reciprocal, (NPV%)\u22121, directly expresses short-term project risk].A traditional spreadsheet relating these four dependent long- and short-term financial metrics to a project's two independent design variables, PBT and FC, is functionally dependent on the many process design parameters (tax rate, discount factor, etc.) and highly complicated, involving non-linear (mostly geometric) relationships. Surprisingly, an exact linear-in-the-parameters \u201cinner model\u201d including the usual nonlinear relation for ROIBT is obtained by collapsing the original large-scale financial model (the complicated spreadsheet) using these four dependent variables, if the coefficients are functions of fixed design parameters , as when held constant in design. The power/utility to understand several key features of a project's design financial characteristics are revealed via the \u201cinner model\u201d through this reversal of the usual modeling hierarchy. The \u201couter model\u201d coefficients incorporate the highly non-linear DCF functionalities. The new form is referred to as \u201cexo-parametric\u201d or an \u201cinside-out\u201d model.",
     "keywords": ["Profitability", "Discounted cash flow calculations", "Net present value_%", "Inside-out model", "Exo-parametric model"]},
    {"article name": "Benchmarking ADMM in nonconvex NLPs",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.08.036",
     "publication date": "11-2018",
     "abstract": "We study connections between the alternating direction method of multipliers (ADMM), the classical method of multipliers (MM), and progressive hedging (PH). The connections are used to derive benchmark metrics and strategies to monitor and accelerate convergence and to help explain why ADMM and PH are capable of solving complex nonconvex NLPs. Specifically, we observe that ADMM is an inexact version of MM and approaches its performance when multiple coordination steps are performed. In addition, we use the observation that PH is a specialization of ADMM and borrow Lyapunov function and primal-dual feasibility metrics used in ADMM to explain why PH is capable of solving nonconvex NLPs. This analysis also highlights that specialized PH schemes can be derived to tackle a wider range of stochastic programs and even other problem classes. Our exposition is tutorial in nature and seeks to to motivate algorithmic improvements and new decomposition strategies",
     "keywords": ["Decomposition", "Augmented Lagrangian", "ADMM", "NLP", "Large-scale", "Coordination"]},
    {"article name": "Modeling and control of battery systems. Part II: A model predictive controller for optimal charging",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.08.017",
     "publication date": "11-2018",
     "abstract": "In this part of the paper, a control strategy for optimal charging is discussed. This work seeks to develop a capacity fade minimizing model predictive control (MPC) framework, which can help in identification and realization of optimum charge-discharge cycles in Lithium-ion (Li-ion) batteries. Although the model developed in the first part is a good representation for a battery, it has limitations for on-line applications due to its complexity. For on-line applications, it is important that the model is computationally fast, but at the same time incorporate the effects of various capacity fade mechanisms. Development of a simple lumped model to meet these requirements is also a part of this work.",
     "keywords": ["Model predictive control", "Optimal charging", "Batteries"]},
    {"article name": "Modeling and control of battery systems. Part I: Revisiting Butler\u2013Volmer equations to model non-linear coupling of various capacity fade mechanisms",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.08.016",
     "publication date": "11-2018",
     "abstract": "Lithium-ion batteries, affected by various capacity fade mechanisms, require an efficient battery management system that can prolong battery lifetime by periodic diagnosis, subsequent management and control. The work presented in this two-part paper is an investigation and development of strategies for battery modeling and controller implementation, which are two of the essential components of any battery management system. In this first part, a generalized approach to incorporate non-linear coupling of various capacity fade mechanisms is proposed. Though there exist numerous models to capture effects of various capacity fade mechanisms, they fail to model non-linear coupling as they assume linear superposition of individual current densities (provided by individual Butler\u2013Volmer equations). Considering battery as a system with multiple reactions (both desired and undesired reactions), rate equations can be written for the overall system. Re-deriving a single Butler\u2013Volmer equation from this rate equation provided insights regarding the true nature of coupling between various reactions inside a battery. Incorporating various side reactions using this framework to a detailed ideal battery model would help in understanding the behavior of a battery with aging and this information can be useful to diagnose various problems in the battery. For demonstrating the implementation and usefulness of this approach, SEI layer formation and Li plating are incorporated to a detailed battery model in this article.",
     "keywords": ["Capacity fade modeling", "Non-linear coupling", "Reaction kinetics approach", "Batteries"]},
    {"article name": "LEAPS2: Learning based Evolutionary Assistive Paradigm for Surrogate Selection",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.09.008",
     "publication date": "11-2018",
     "abstract": "We propose a learning-based paradigm (LEAPS2) to recommend the best surrogate/ with minimal computational effort using the input-output data of a complex physico-numerical system. Emulating the knowledge pyramid, LEAPS2 uses several attributes to extract system information from the data, correlates them with surrogate performances, stores this attribute-surrogate knowledge in a regression tree ensemble, and uses the ensemble to recommend surrogates for unknown systems. We implement LEAPS2 using data from 66 diverse analytical functions, 18 attributes, and 25 surrogates. By progressively adding data, we demonstrate that LEAPS2 learns to improve computational efficiency and functional accuracy. Besides, the architecture of LEAPS2 enables its evolution via more attributes and surrogates. We employ LEAPS2 to recommend surrogates for estimating the bubble and dew point temperatures of LNG. Interestingly, our assistive tool suggests a different surrogate for each temperature, and hints that DPT may be harder to approximate than BPT.",
     "keywords": ["Surrogate model", "Meta-model", "System attributes", "Knowledge pyramid", "Machine learning"]},
    {"article name": "Chromatographic studies of n-Propyl Propionate: Adsorption equilibrium, modelling and uncertainties determination",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.09.020",
     "publication date": "11-2018",
     "abstract": "The n-Propyl Propionate (ProPro) is a compound that has several possible industrial applications. However, the current production route of this component presents several problems, such as the downstream purification. In this way, chromatographic separation could be an alternative solution to the downstream purification. In this work experimental studies of the ProPro reaction system separation in a chromatographic fixed bed unit packed with Amberlyst 46 were performed. The adsorption equilibrium isotherms and the corresponding Langmuir model parameters were determined. A phenomenological model to represent the process was developed and validated through the experimental data. Meanwhile, it is proposed the characterization of the uncertainties of all steps and its extension to the model prediction, which allowed to estimate the model parameters with a reduced number of experiments, when compared with other reports in the literature; nevertheless, the final results lead to a statistically more reliable model.",
     "keywords": ["Fixed bed adsorptive unit", "Particle swarm optimization", "Confidence region", "Adsorption equilibrium isotherms", "n-Propyl Propionate"]},
    {"article name": "Multi-objective optimisation for biopharmaceutical manufacturing under uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.09.015",
     "publication date": "11-2018",
     "abstract": "This work addresses the multi-objective optimisation of manufacturing strategies of monoclonal antibodies under uncertainty. The chromatography sequencing and column sizing strategies, including resin at each chromatography step, number of columns, column diameters and bed heights, and number of cycles per batch, are optimised. The objective functions simultaneously minimise the cost of goods per gram and maximise the impurity reduction ability of the purification process. Three parameters are treated as uncertainties, including bioreactor titre, and chromatography yield and capability to remove impurities. Using chance constraint programming techniques, a multi-objective mixed integer optimisation model is proposed. Adapting both \u03b5-constraint method and Dinkelbach's algorithm, an iterative solution approach is developed for Pareto-optimal solutions. The proposed model and approach are applied to an industrially-relevant example, demonstrating the benefits of the proposed model through Monte Carlo simulation. The sensitivity analysis of the confidence levels used in the chance constraints of the proposed model is also conducted.",
     "keywords": ["Biopharmaceutical manufacturing", "Multi-objective optimisation", "Uncertainty", "Chance constrained programming", "Mixed integer programming"]},
    {"article name": "Optimal municipal solid waste energy recovery and management: A mathematical programming approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.09.025",
     "publication date": "11-2018",
     "abstract": "A multi-period approach to municipal solid waste (MSW) management is proposed. The analysis includes the optimization of a MSW network considering waste reduction processes and landfilling. The optimization of the transportation of MSW to its potential destinations has been addressed using a direct-hauling system and an optimally allocated off-municipality transfer station. As the main component in the formulation, an optimal landfill gas (LFG) to energy design is obtained to improve the economics of the landfill operation; the design involves the installation of several harnessing technologies according to the annual increase or decay of the LFG flow rate. A case-study for a municipality in Mexico has been solved through the GAMS\u00ae modeling environment. The resulting mixed-integer linear programming (MILP) model has been assessed through several scenarios. The results show that the installation of an LFG-to-electricity system and a materials recycling facility achieve the minimum overall cost of the MSW management.",
     "keywords": ["LFG-to-energy", "Municipal solid waste management", "Mathematical programming", "Modeling", "Optimization", "Landfill gas utilization", "GDP general disjunctive programming", "general disjunctive programming", "GHG green house gas", "green house gas", "GT gas turbine", "gas turbine", "LFG landfill gas", "landfill gas", "LICE large internal combustion engine", "large internal combustion engine", "MILP mixed integer linear programming", "mixed integer linear programming", "MRF materials recycling facility", "materials recycling facility", "MSW municipal solid waste", "municipal solid waste", "MT micro turbine", "micro turbine", "NPV net present value", "net present value", "SICE small internal combustion engine", "small internal combustion engine", "WTE waste to energy", "waste to energy"]},
    {"article name": "Enhancing the performance of a solar-assisted adsorption chiller using advanced composite materials",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.09.018",
     "publication date": "11-2018",
     "abstract": "This paper presents multi-level modeling and simulation techniques for a new solar-assisted adsorption chiller using a novel combination of heat transfer fluids (HTF) and composite adsorbents under transient conditions. Multi-walled carbon nanotube and graphene nanofluids (MWCNT/GNF) composite is used as a cooling liquid in the cooling circuit of the chiller, while activated carbon fiber with either barium chloride salt (ACF/BCS) or nickel chloride salt (ACF/NCS) composite are used as adsorbers. The key system components \u00a0are the adsorption chiller with two beds, an evaporator and condenser, as well as an evacuated tubes collector (ETC), a hot water storage tank and an open circuit cooling tower. Computational fluid dynamics analysis examines the properties of water and ammonia at adsorbed and gaseous phases for adsorption on silica gel and ACF/BCS adsorbers. The non-dimensional specific cooling capacity (NSCC) and the coefficient of performance (COP) are compared for the chiller using the proposed material sets vs the conventional silica gel counterparts. It is found that ACF/NCS and ACF/BCS can improve the COP by 23.4% and 95.7%, respectively. This demonstrates the utility of computation in simulating an adsorption chiller systems carrying advanced composite materials that lead to higher energy conversion efficiency, smaller size solar collectors, and compact storage tanks resulting in tangible economic benefits.",
     "keywords": ["Adsorption chiller", "Activated carbon fiber", "Solar energy", "Multi-walled carbon nanotubes", "Graphene nanofluid", "Evacuated tubes collector."]},
    {"article name": "Supply-demand pinch based methodology for multi-period planning under uncertainty in components qualities with application to gasoline blend planning",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.09.016",
     "publication date": "11-2018",
     "abstract": "Uncertainty in component quality in gasoline blending due to measurement errors and variation in operation leads to planned blends which may not meet quality specifications and re-blending is required. Formulating gasoline blending as chance constrained programming enables a decision maker to decide what percentage of blends will be guaranteed to meet the specifications and balance the increased cost of blends vs. the cost of having to re-blend the off-spec blends. Chance constrained formulation makes the gasoline blend problem nonlinear and nonconvex. In this work, we employ a supply-demand pinch based algorithm to optimize gasoline blend planning with uncertainty in components qualities and examine its performance vs. full-space model. The supply-demand pinch algorithm decomposes the problem into two sub-problems, top-level (NLP) computes optimal blend recipes and the bottom-level (MILP) computes an optimal production plan using the recipes computed at the top-level. Computational efficiency of the algorithm is verified by case studies.",
     "keywords": ["Production planning under uncertainty in raw material qualities", "Gasoline blending planning with uncertainty in component qualities", "Supply-demand pinch", "Inventory pinch", "Joint Chance constrained programming"]},
    {"article name": "An improvement scheme for pressure-swing distillation with and without heat integration through an intermediate connection to achieve energy savings",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.09.012",
     "publication date": "11-2018",
     "abstract": "Pressure-swing distillation is widely studied in academia and industry. Energy consumption is a key criterion for judging whether a pressure-swing distillation is better than other distillation methods. An improved process with an intermediate connection was developed based on the two-column pressure-swing distillation for separating a binary azeotropic mixture to save energy and reduce the total annual cost. The proposed pressure-swing distillation was applied to three case systems, two minimum-boiling azeotropic mixtures of ethyl acetate/ethanol and methanol/chloroform and one maximum-boiling azeotropic mixture of ethylenediamine/water. According to our analysis, the proposed pressure-swing distillation and heat-integrated scheme can reduce energy consumption and total annual cost. The pressure-swing distillation with the intermediate connection is promising for saving energy. This advantage assists in promoting the application of pressure-swing distillation and overall energy savings in the chemical process industry.",
     "keywords": ["Pressure-swing distillation", "Heat integration", "Intermediate connection", "TAC", "Energy consumption"]},
    {"article name": "Side stream control in semicontinuous distillation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.09.002",
     "publication date": "11-2018",
     "abstract": "The idea to reduce cycle time (T), by controlling the side stream flow rate using a feedforward control model \u2013 the ideal side draw recovery arrangement (ISR) \u2013 was standard in most semicontinuous distillation studies. However, its effect particularly on \u2018T\u2019 and more broadly on the system dynamics was not clearly understood. In the current study, we compare the performance of using a modified form of ISR model with the status quo based on the criteria T and separating cost (SC) on different case studies. Results show that the modified control model performed better with a 10-20% reduction in SC while maintaining product purities. Furthermore, the side stream flow rate trajectory that minimizes SC was found by using dynamic optimization and it did not differ a lot from the trajectory generated by the modified control model. The improvement in SC was at most 2%.",
     "keywords": ["Semicontinuous distillation", "side stream control", "dynamical system analysis", "dynamic optimization", "A Low volatile component", "Low volatile component", "B Intermediate boiling component", "Intermediate boiling component", "C High volatile component in ternary mixture / Intermediate boiling component in quaternary mixture", "High volatile component in ternary mixture / Intermediate boiling component in quaternary mixture", "D High volatile component in quaternary mixture", "High volatile component in quaternary mixture", "MV Middle Vessel", "Middle Vessel", "MV1 Middle Vessel to concentrate B in the quaternary separation", "Middle Vessel to concentrate B in the quaternary separation", "MV2 Middle Vessel to concentrate C in the quaternary separation", "Middle Vessel to concentrate C in the quaternary separation", "Ch Charging Mode", "Charging Mode", "Dis Discharging Mode", "Discharging Mode", "Sep Separating Mode", "Separating Mode", "spec Specification", "Specification", "DB Distillate-Bottom control configuration", "Distillate-Bottom control configuration", "P Proportional controller", "Proportional controller", "PI Proportional Integral controller", "Proportional Integral controller", "BTX Benzene, Toluene, and O-Xylene", "Benzene, Toluene, and O-Xylene", "DME Dimethyl Ether", "Dimethyl Ether", "MeOH Methanol", "Methanol", "ISR Ideal Side draw recovery", "Ideal Side draw recovery", "PV Process Variable", "Process Variable", "SP Setpoint", "Setpoint"]},
    {"article name": "Optimization and control of a thin film growth process: A hybrid first principles/artificial neural network based multiscale modelling approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.08.029",
     "publication date": "11-2018",
     "abstract": "This work details the construction and evaluation of a low computational cost hybrid multiscale thin film deposition model that couples artificial neural networks (ANNs) with a mechanistic (first-principles) multiscale model. The multiscale model combines continuum differential equations, which describe the transport of the precursor gas phase, with a stochastic partial differential equation (SPDE) that predicts the evolution of the thin film surface. In order to allow the SPDE to accurately predict the thin film growth over a range of system parameters, an ANN is developed and trained to predict the values of the SPDE coefficients. The fully-assembled hybrid multiscale model is validated through comparison against a kinetic Monte Carlo-based thin film multiscale model. The model is subsequently applied to a series of optimization and control studies to test its performance under different scenarios. These studies illustrate the computational efficiency of the proposed hybrid multiscale model for optimization and control applications.",
     "keywords": ["Artificial neural networks", "Multiscale modelling", "Thin film deposition", "Hybrid modelling", "Stochastic partial differential equations"]},
    {"article name": "Model-based optimization of biopharmaceutical manufacturing in Pichia pastoris based on dynamic flux balance analysis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.07.013",
     "publication date": "10-2018",
     "abstract": "Biologic drugs are promising therapeutics, and their efficient production is essential for a competitive pharma industry. Dynamic flux balance analysis (dFBA) enables the dynamic simulation of the extracellular bioreactor environment and intracellular fluxes in microorganisms, but it is rarely used for model-based optimization of biopharmaceutical manufacturing in Pichia pastoris. To bridge this gap, we present a model-based optimization approach based on dFBA to produce biologics in P. pastoris that combines ideas from bilevel optimization, penalization schemes, and direct dynamic optimization. As a case study, we consider the production of recombinant erythropoietin in P. pastoris growing on glucose, and predict a 66% improvement in the productivity of erythropoietin. We show that this improvement could be obtained by implementing an almost constant optimal feeding strategy which is different from typical exponential feeding strategies and that a high activity of most pathways in the central carbon metabolism is crucial for a high productivity.",
     "keywords": ["Biopharmaceutical manufacturing", "Pichia pastoris", "Dynamic flux balance analysis", "Elementary process functions", "Bilevel optimization", "Complementarity constraints"]},
    {"article name": "Numerical analysis of water distribution in the various layers of proton exchange membrane fuel cells",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.07.004",
     "publication date": "10-2018",
     "abstract": "In this paper, a two-dimensional, two-phase, isothermal model is presented to investigate the water transport characteristic and water distribution in a proton exchange membrane fuel cell (PEMFC) with emphasis on exploring the water distribution in the layers of PEMFC. The governing equations have been solved over a single computational domain, which consists of all the layers. The result of polarization curve has been validated against the experimental data in an extensive range of operating conditions. A parametric study has also been performed to examine the effects of the diffusion layer (DL) permeability, contact angle, and cell output voltage on water distribution in the various layers of PEMFC. Numerical results elucidate that water distribution level in the cathode side is decreased with the increment in the cell voltage. Furthermore, the permeability of the cathode DL and contact angle significantly influence the water saturation level in the cathode side.",
     "keywords": ["Proton exchange membrane fuel cells", "Modeling", "Two-phase", "Water transport"]},
    {"article name": "Economics optimizing control of a multi-product reactive distillation process under model uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.07.003",
     "publication date": "10-2018",
     "abstract": "By the use of online optimization, the profitability of chemical processes can be enhanced while meeting process-related, environmental and ecological constraints. Two well-known techniques to achieve an economically optimal operation of chemical processes are repetitive stationary optimization (RTO) combined with advanced control and direct economics optimizing control on a receding horizon (also called one-layer approach or dynamic RTO). Direct economics optimizing control can react faster to disturbances and is better suited for the optimization of transients between different products. On the other hand it is computationally demanding and requires sufficiently accurate dynamic models.In this paper, we discuss economics optimizing control of a very complex process, a multi-product transesterification reaction that is realized in a reactive distillation process. The process model, the specification of the economics optimizing controller, the implementation of the dynamic optimization, and the robustification of the controller by a multi-stage approach are discussed using simulation studies.",
     "keywords": ["Online economics optimizing control", "Model predictive control", "Nonlinear DAE-systems", "Reactive distillation", "Two-step transesterification", "Robust model predictive control", "ACM Aspen Custom Modeler\u00ae", "Aspen Custom Modeler\u00ae", "ACR Alcohol-to-carbonate ratio", "Alcohol-to-carbonate ratio", "cat Catalyst", "Catalyst", "D Distillate", "Distillate", "DEC Diethyl carbonate", "Diethyl carbonate", "DF Distillate-to-feed ratio", "Distillate-to-feed ratio", "DMC Dimethyl carbonate", "Dimethyl carbonate", "D-RTO Dynamic real-time optimization", "Dynamic real-time optimization", "ECO Economic objective", "Economic objective", "EKF Extended Kalman filter", "Extended Kalman filter", "EMC Ethyl methyl carbonate", "Ethyl methyl carbonate", "EtOH Ethanol", "Ethanol", "inf infinity", "infinity", "HETS Height equivalent of equilibrium stage", "Height equivalent of equilibrium stage", "GC Gas chromatography", "Gas chromatography", "MeOH Methanol", "Methanol", "MHE Moving horizon estimator", "Moving horizon estimator", "NLP Nonlinear programming problem", "Nonlinear programming problem", "RR Reflux Ratio", "Reflux Ratio", "RD Reactive distillation", "Reactive distillation", "RMSE Root mean squared error", "Root mean squared error", "R-D-RTO Robust dynamic real-time optimization", "Robust dynamic real-time optimization", "RTO Real-time optimization", "Real-time optimization", "SR Single-rate", "Single-rate", "tot Total", "Total", "VLE Vapor-liquid equilibrium", "Vapor-liquid equilibrium"]},
    {"article name": "An ontology framework towards decentralized information management for eco-industrial parks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.07.010",
     "publication date": "10-2018",
     "abstract": "In this paper, we develop a skeletal ontology for eco-industrial parks. A top-down conceptual framework including five operating levels (unit operations, processes, plants, industrial resource networks and eco-industrial parks) is employed to guide the design of the ontology structure. The detailed ontological representation of each level is realized through adapting and extending OntoCAPE, an ontology of the chemical engineering domain. Based on the proposed ontology, a framework for distributed information management is proposed for eco-industrial parks. As an example, this ontology is used to create a knowledge base for Jurong Island, an industrial park in Singapore. Its potential uses in supporting process modeling and optimization and facilitating industrial symbiosis are also discussed in the paper.",
     "keywords": ["Eco-industrial park", "Knowledge base", "Ontology", "Process modelling"]},
    {"article name": "Self-adaptive differential evolution with a novel adaptation technique and its application to optimize ES-SAGD recovery process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.07.018",
     "publication date": "10-2018",
     "abstract": "Differential evolution (DE) algorithm has shown good performance in many optimization problems. However, its control parameters greatly affect its performance and require many trials to determine the optimum values of control parameters for each specific optimization problem. In this paper, we present a self-adaptive DE with a new adaptation technique to improve the solution quality as well as increase the speed of convergence with a reduction in the computational cost. The proposed approach, called modified self-adaptive differential evolution (MSaDE), employs a new success-rate indicator of the strategies used to generate the trial vectors in conventional self-adaptive differential evolution (SaDE) algorithm. The proposed method has been tested on 22 benchmark problems and on the expanded solvent steam-assisted gravity drainage (ES-SAGD) recovery process. The results show that a significant speed-up is achieved in the exploitation and exploration capabilities of the self-adaptive DE algorithm.",
     "keywords": ["Differential evolution", "Self-adaptive DE", "Global optimization", "Trial-vector generation strategies", "ES-SAGD optimization", "Net present value"]},
    {"article name": "Feature learning and process monitoring of injection molding using convolution-deconvolution auto encoders",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.07.009",
     "publication date": "10-2018",
     "abstract": "Feature learning is a generic and fundamental problem in data-based process monitoring of batch processes, such as injection molding. This paper proposes an automatic feature learning method, considering the following two vital characteristic aspects: the mechanical characteristics within a batch and the inherent characteristics among batches. Unsupervised feature learning is performed using convolution-deconvolution auto encoders, and the learned features are applied as predefined parameters for process monitoring, including process condition identification and fault detection. Experiments are carried out with different process conditions. The results indicate that the proposed method achieves improved model generalization ability under various process conditions, which means it can precisely reveal the variable autocorrelations and cross-correlations among different variables. Meanwhile, the learned features achieve higher classification accuracies and offer more optimal solutions for process monitoring. This method has been proven as an efficient means of feature learning, which should be appropriate for other batch processes.",
     "keywords": ["Feature learning", "Process monitoring", "Fault detection", "Injection molding", "Auto encoders"]},
    {"article name": "Enhanced surrogate assisted framework for constrained global optimization of expensive black-box functions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.06.027",
     "publication date": "10-2018",
     "abstract": "An enhanced surrogate assisted framework, based on Probability of Improvement (PI) method, is proposed in this paper. We made some modifications to the original PI approach to enhance the performance of the modeling and optimization framework, leading to fewer rigorous simulations to find the optimal solution without loss of accuracy. We also extended the algorithm for handling general constraints using a fully probabilistic approach. The behavior of the proposed framework was investigated through a set of 9 Unconstrained Test Functions (UTF), 7 Constrained Optimization Problems (COP) and 3 Chemical Engineering Problems (CEP). The numerical results indicate that a lower number of rigorous model simulations were needed for optimizing UTF compared to the classic PI method and that the proposed framework was capable of achieving sustained near optimal solutions for COP and CEP. These results indicate that the proposed framework is suitable for solving computationally expensive constrained black-box optimization problems.",
     "keywords": ["Black-box optimization", "Kriging", "Probability of improvement"]},
    {"article name": "Supplier selection and operation planning in biomass supply chains with supply uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.07.012",
     "publication date": "10-2018",
     "abstract": "Bioenergy is considered a potential solution to reduce carbon footprint and fight against global warming. However, uncertainty in the harvest of biomass could lead to the instability of feedstock supply that has a significant impact on the sustainability of biomass supply chain. In this paper, we present a two-stage stochastic programming model dealing with supplier selection to stabilize feedstock supply of a biomass supply chain in uncertain environments. The model involves the first stage decisions for the supplier selection and the second-stage decisions for planning transportation, inventory and production operations. To reduce the computational burden for large instances, we propose an enhanced and regularized L-shaped decomposition algorithm to solve the model. The applicability of this model and the performance of the solution method are evaluated by numerical studies. Sensitivity analysis shows that the values of some parameters related to suppliers have significant impacts on the optimal expected cost and supplier selection.",
     "keywords": ["Biomass supply chain", "Supplier selection", "Uncertainty", "Stochastic programming"]},
    {"article name": "Product design: Impact of government policy and consumer preference on company profit and corporate social responsibility",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.06.026",
     "publication date": "10-2018",
     "abstract": "A multi-objective optimization framework that considers the influence of government policy on product design is presented. The government policy affects product design in the form of financial and non-financial incentives, and regulations. The consumers affect product design by their purchase behavior that is in turn influenced by price, product quality, and the presence of competing products. The company responds to these influencing factors to design a product with as high a profit as possible while satisfying corporate social responsibility. Different models as well as rule-based methods \u2013 quality, consumer utility, product demand, product cost, capital budgeting, social indices, and government policy \u2013 are presented. A solar photovoltaic case study is used to illustrate the framework.",
     "keywords": ["Product development", "Government policy", "Corporate social responsibility", "Multi-objective optimization"]},
    {"article name": "Surrogate-assisted modeling and optimization of a natural-gas liquefaction plant",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.08.003",
     "publication date": "10-2018",
     "abstract": "In this study, surrogate-assisted modeling and optimization of the single mixed refrigerant process of natural-gas liquefaction is presented. The mixed refrigerant liquefaction process is highly nonlinear owing to the involved thermodynamics that increase the computational burden of any optimization algorithm. To address the computational-burden issue and obtain the results in a reasonable time for the complex single mixed refrigerant process, an approximate surrogate model was developed using a radial basis function combined with a thin-plate spline approach. Even with the reduced model, all the results obtained were comparable with those by rigorous first-principle models. This confirms that all the important characteristics of the model are correctly captured, and the surrogate models of the liquefaction plant are acceptable replacements of first-principle models, especially in computationally demanding situations.",
     "keywords": ["Surrogate model", "Radial-based function", "Single mixed refrigerant process", "Natural-gas liquefaction", "Optimization"]},
    {"article name": "Dynamic modeling and optimization of sustainable algal production with uncertainty using multivariate Gaussian processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.07.015",
     "publication date": "10-2018",
     "abstract": "Dynamic modeling is an important tool to gain better understanding of complex bioprocesses and to determine optimal operating conditions for process control. Currently, two modeling methodologies have been applied to biosystems: kinetic modeling, which necessitates deep mechanistic knowledge, and artificial neural networks (ANN), which in most cases cannot incorporate process uncertainty. The goal of this study is to introduce an alternative modeling strategy, namely Gaussian processes (GP), which incorporates uncertainty but does not require complicated kinetic information. To test the performance of this strategy, GPs were applied to model microalgae growth and lutein production based on existing experimental datasets and compared against the results of previous ANNs. Furthermore, a dynamic optimization under uncertainty is performed, avoiding over-optimistic optimization outside of the model\u2019s validity. The results show that GPs possess comparable prediction capabilities to ANNs for long-term dynamic bioprocess modeling, while accounting for model uncertainty. This strongly suggests their potential applications in bioprocess systems engineering.",
     "keywords": ["Optimization under uncertainty", "Gaussian process", "Artificial neural network", "Machine learning", "Dynamic bioprocess"]},
    {"article name": "Data-driven soft-sensors for online monitoring of batch processes with different initial conditions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.07.014",
     "publication date": "10-2018",
     "abstract": "A soft-sensing methodology applicable to batch processes operated under changeable initial conditions is presented. These cases appear when the raw materials specifications differ from batch to batch, different production scenarios should be managed, etc. The proposal exploits the capabilities of the machine learning techniques to provide practical soft-sensing approach with minimum tuning effort in spite of the fact that the inherent dynamic behavior of batch systems are tracked through other online indirect measurements. Current data modeling techniques have been also tested within the proposed methodology to demonstrate its advantages. Two simulation case-studies and a pilot-plant case-study involving a complex batch process for wastewater treatment are used to illustrate the problem, to assess the modeling approach and to compare the modeling techniques. The results reflect a promising accuracy even when the training information is scarce, allowing significant reductions in the cost associated to batch processes monitoring and control.",
     "keywords": ["Soft-sensors", "Batch processes", "Ordinary Kriging", "Support vector machines", "Artificial neural networks", "Photo-Fenton"]},
    {"article name": "A switched dynamical system approach towards the optimal control of chemical processes based on a gradient-based parallel optimization algorithm",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.08.007",
     "publication date": "10-2018",
     "abstract": "This paper considers an optimal control problem of chemical processes with a novel piecewise state-feedback controller. Firstly, the chemical process optimal control problem is formulated as a switched dynamical system optimal control problem, which can be transformed into a parameter optimization problem. Next, to achieve rapid convergence from remote starting points, we propose a novel gradient-based optimization algorithm, which is suitable to a parallel implementation because the step is improved by updating its direction as well as its length simultaneously before moving to the next iteration, and the step computation involves only the inner products of vectors. Then, the convergent properties of the parameter optimization problem to the original optimal control problem are discussed. Finally, the numerical simulation results show that the gradient-based parallel optimization algorithm is an effective alternative method for solving the chemical process optimal control problems.",
     "keywords": ["Chemical process", "Optimal control", "Piecewise state feedback controller", "Switched dynamical system", "Parallel optimization algorithm"]},
    {"article name": "Approximation techniques for dynamic real-time optimization (DRTO) of distributed MPC systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.08.002",
     "publication date": "10-2018",
     "abstract": "A dynamic real-time optimization (DRTO) strategy has recently been proposed for coordination of distributed MPC systems. The scheme utilizes a prediction model that accounts for the impact of the distributed controllers on the plant response. Two techniques are presented for approximating the closed-loop prediction within the DRTO formulation - a hybrid closed-loop and an input clipping formulation. The hybrid formulation generates closed-loop predictions for a limited number of time intervals along the DRTO prediction horizon, followed by an open-loop optimal control formulation for the rest of the horizon. The input clipping formulation utilizes an unconstrained MPC optimization formulation for each distributed MPC, coupled with an input saturation mechanism. The performance of the approximation techniques is evaluated through application to case studies based on linear and nonlinear dynamic plant models respectively. The approximation techniques are demonstrated to be more computationally efficient than the rigorous counterpart without significant loss in performance.",
     "keywords": ["Dynamic real-time optimization", "Model predictive control", "Distributed MPC", "Economic optimization", "Coordination", "Closed-loop prediction approximation"]},
    {"article name": "Constrained optimization of black-box stochastic systems using a novel feasibility enhanced Kriging-based method",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.07.016",
     "publication date": "10-2018",
     "abstract": "Stochastically constrained simulation optimization problems are challenging because the inherent noise terms to a black-box system lead to the need of considering the uncertainty at both the objective and the constraint function. To address this problem, we propose a Kriging-based optimization framework, which uses stochastic Kriging models to approximate the objective and the constraint functions and an adaptive sampling approach to sequentially search for the next point that is promising to have a better objective. The main contribution of this work is to incorporate a feasibility-enhanced term to the infill sampling criterion, which improves the Kriging-based algorithm's capabilities of returning a truly feasible near-optimal solution for stochastic systems. The efficacy of the Kriging-based algorithm is demonstrated with eight benchmark problems and a case study from the pharmaceutical manufacturing domain.",
     "keywords": ["Simulation optimization", "Stochastic constraint", "Kriging", "Adaptive sampling", "Feasibility analysis"]},
    {"article name": "Process modelling, design and technoeconomic evaluation for continuous paracetamol crystallisation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.03.020",
     "publication date": "10-2018",
     "abstract": "Continuous Pharmaceutical Manufacturing (CPM) has a strong potential to catalyse pharmaceutical innovation. This paper analyses Continuous Oscillatory Baffled Crystalliser (COBC) optimal design and performance for paracetamol crystallisation, via systematic modelling and nonlinear optimization (NLP). Clear trends emerge, with rate of antisolvent use having a marked impact on COBC volumes; crystal seed mass loading also has a strong effect. For the base case studied (inlet temperature of 50\u202f\u00b0C, seed crystal size of 40\u00a0\u00b5m) the optimal operation has been determined to be under using 2% seed mass loading (with respect to solute mass) and with 80% water antisolvent use (by mass with respect to process solvent acetone); the crystalliser size required has been computed equal to 4.25 L with a total cost of 101,370 GBP, achieving a product yield of 50% with a product crystal size of 83.6\u00a0\u00b5m. Clear trade-offs among mass efficiency, volume, cost and product crystal size have been illustrated, providing valuable quantitative insights into process performance.",
     "keywords": null},
    {"article name": "Robust design of ambient-air vaporizer based on time-series clustering",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.08.026",
     "publication date": "10-2018",
     "abstract": "A methodology for the robust design of an ambient-air vaporizer under time-series weather conditions is proposed. Two techniques are used to extract representative features in the time-series data. (i) The major trend of a day is rapidly identified by the discrete wavelet transform (DWT), in which a high level of Haar function reflects the trend of a day and drastically reduces the data size. (ii) The k-means clustering method groups the similar features of a year, and the reconstructed time-series dataset extracted by the centroids of clusters represents the weather conditions of a year. The results of the multi-feature-based optimization were compared with non-wavelet based and multi-period optimization by simulation under a year of data. The design structure from the feature extraction shows 22.92% better performance than the original case and is 12 times more robust in different weather conditions than clustering with raw data.",
     "keywords": ["Ambient air vaporizer", "Wavelet transform", "k-means clustering", "Feature extraction", "Robust design", "Global sensitivity analysis", "AAV Ambient-air vaporizer", "Ambient-air vaporizer", "DIRECT DIviding hyper-RECTangle", "DIviding hyper-RECTangle", "DWT Discrete wavelet transform", "Discrete wavelet transform", "LNG Liquefied natural gas", "Liquefied natural gas"]},
    {"article name": "Simultaneous heat integration and economic optimization of the coal-to-SNG process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.08.027",
     "publication date": "10-2018",
     "abstract": "The area of the heat recovery network has a significant impact on the economic performance of the coal-to-SNG process. This paper proposes a superstructure of the methanation unit, and the number of the methanator, recycle gas extraction position (RGEXP), as well as the operating conditions, are optimized. Simultaneous optimization and heat integration are performed along with the area targeting to weigh against the operating cost and capital cost of the coal-to-SNG process. Area cost is evaluated by assuming vertical heat transfer between cold and hot composite curves. A sequential method is used to provide initial values for the simultaneous model. Results show that Case7 (2) has the best economic performance, which has 7 methanators and recycled gas is extracted from the 2nd methanator. The total annual cost of Case7 (2) can be reduced by 10.36 MM$\u00b7a\u22121, and exergy efficiency can be also improved by 0.77% compared with an industrial plant.",
     "keywords": ["Heat integration", "Area targeting", "Coal-to-SNG", "Economic optimization"]},
    {"article name": "Effect of feed composition on cryogenic distillation precooling configurations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.08.019",
     "publication date": "10-2018",
     "abstract": "Precooling the feed of cryogenic distillation columns is achieved by using the cold distillate and bottoms streams. The distillate is typically removed from the top of the reflux drum as a vapor to reduce the condenser refrigeration heat duty. The bottoms stream is typically a liquid at a somewhat higher temperature than the distillate. The colder vapor distillate can only provide sensible heat removal. The warmer bottoms liquid stream can provide more heat removal because of vaporization.The purpose of this paper is to demonstrate that the appropriate arrangement of the precooling heat exchangers depends on the column feed composition. If the feed contains little light key component, the bottoms stream is large and the feed can be easily precooled. If the feed contains little heavy key component, the bottoms stream is small and precooling the feed is more difficult.",
     "keywords": ["Cryogenic distillation", "Refrigeration", "Heat exchanger network"]},
    {"article name": "On the use of physical boundary conditions for two-phase flow simulations: Integration of control feedback",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.08.012",
     "publication date": "10-2018",
     "abstract": "The sensitivity of two-phase flow simulations using the Euler\u2013Euler model on the inlet boundary conditions (BCs) is studied. Specifically, the physical relevance of Dirichlet uniform inlet velocity BCs is studied which are widely used due their simplicity and the lack of a priori knowledge of the slip velocity between the phases. It is found that flow patterns obtained with the more physically realistic uniform inlet pressure BCs are radically different from the results obtained with Dirichlet inlet velocity BCs, refuting the argument frequently put forward that Dirichlet uniform inlet velocity BCs can be interchangeably used because the terminal slip velocity is reached after a short entrance region. A comparison with experimental data is performed to assess the relevance of the flows obtained numerically. Additionally, a multivariable feedback control method is demonstrated to be ideal for enforcing desired flow rates for simulations using pressure BCs.",
     "keywords": ["Two-phase flow", "Computational fluid dynamics", "Euler\u2013Euler model", "Simulations", "Control"]},
    {"article name": "Kinetic Monte Carlo modeling of multivalent binding of CTB proteins with GM1 receptors",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.08.011",
     "publication date": "10-2018",
     "abstract": "The binding mechanism between a cholera toxin subunit B (CTB) protein and ganglioside receptors on a host cell membrane still remains elusive due to multivalent surface binding reactions, dependency on the surface micro-configuration, and intrinsic stochasticity. In this work, a kinetic Monte Carlo (kMC) modeling framework coupled with a continuum model is proposed to incorporate the major characteristics of the binding process between CTB proteins and GM1 gangliosides, which are primary CTB receptors. First, a steady-state diffusion equation is introduced to model the diffusion of CTB proteins in the solution phase. Second, a kMC model is proposed to describe a fluctuation in the surface reaction rates as well as the evolution of surface configurations, and their effects on the binding kinetics. Lastly, the comparison with an experimentally validated deterministic model demonstrates the predictive capability of the proposed modeling framework in describing the kinetics of CTB-GM1 binding systems.",
     "keywords": ["Cholera toxin", "Kinetic Monte Carlo", "Stochastic model", "GM1 ganglioside"]},
    {"article name": "Optimal tracking control of artificial gas-lift process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.05.011",
     "publication date": "09-2018",
     "abstract": "Artificial gas-lift (AGL) technique is commonly used to enhance oil production when the reservoir pressure in wells is not enough to sustain acceptable oil flow rate. However, the gas-lift wells are prone to instability, characterized by regular oscillations of pressure and flow. This phenomenon is known as casing-heading instability. It results in production loss and negative impact on downstream equipment, and has been a challenging problem to both industry and academia. In this paper, a novel concept of optimal tracking control is proposed for stabilization and operating mode transition in gas-lift wells when casing-heading phenomenon occurs. The stability of artificial gas-lift process is ensured by manipulating both gas lift choke and oil production choke, where the openings of both choke valves can vary from fully closed to fully open. Through the simulation of the open-loop system, a stability map of AGL process is produced. Then a trajectory optimization algorithm is developed based on this stability map, which is synthesized with a tracking controller to achieve trajectory optimization control. Also, a nonlinear state observer is designed to ensure estimation of unmeasurable variables. Through simulation studies, the effectiveness of proposed trajectory optimization control is demonstrated.",
     "keywords": ["Artificial gas-lift process", "Casing-heading phenomenon", "Optimal tracking control", "Trajectory optimization", "Nonlinear state observer"]},
    {"article name": "A novel tool for the modeling, simulation and costing of membrane based gas separation processes using Aspen HYSYS: Optimization of the CO2/CH4 separation process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.05.013",
     "publication date": "09-2018",
     "abstract": "A key tool called \u201cMemCal\u201d is developed to support the simulation of membrane-based gas separation processes using Aspen HYSYS. By integrating \u201cMemCal\u201d with the simulator, users can simulate complex multi-component/multi-stage processes, perform sensitivity studies, and utilize the default HYSYS features to cost and optimize processes. Industrially, \u201cMemCal\u201d can be used to simulate the separation of air, biogas, natural gas, olefins-paraffins and other gases. The CO2/CH4 binary mixture separation was demonstrated in this manuscript as the first and the simplest application of \u201cMemCal\u201d. The simulation of various multi-stage processes suggested that the treatment cost can be dropped by optimizing the separation load distribution among the stages. For CO2/CH4 separation, the simulations suggested that the optimized two-stage process plus recycle resulted in the least separation cost. The installation of a third stage for boosting hydrocarbons recovery was found unjustified when membranes equivalent to or better than cellulose acetate are adopted.",
     "keywords": ["Simulation", "Process configuration", "Membrane stage", "Treatment cost", "CO2 removal", "CA cellulose acetate", "cellulose acetate", "CAPEX capital expenditure", "capital expenditure", "LNG liquefied natural gas", "liquefied natural gas", "MMBTU million British thermal unit", "million British thermal unit", "Mscfd thousand standard cubic feet per day", "thousand standard cubic feet per day", "MMscfd million standard cubic feet per day", "million standard cubic feet per day", "OPEX operating expenditure", "operating expenditure", "Ppm part per million", "part per million"]},
    {"article name": "An optimal control approach to steam distillation of essential oils from aromatic plants",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.05.009",
     "publication date": "09-2018",
     "abstract": "In this work, an optimal steam flow trajectory for essential oils extraction from aromatic plants is derived, minimizing energy consumption. A phenomenological dynamic model of the oil extraction process is adopted from literature and a multi-objective optimal control problem is formulated, in order to minimize energy consumption and at the same time maximize the yield of extraction. The resulting optimal control problem is highly non-linear and it is solved by numerical methods. Simulation results are presented for three scenarios: (i) Maximum yield, (ii) minimum energy consumption and (iii) trade-off between yield and energy. It is shown that the optimal steam flow rate trajectory is not necessarily constant. Using a mixed cost-function, it is possible to extract almost 100% of oil essential while saving 60% of energy. Finally, a sensitivity analysis shows that the optimal steam trajectory has few variations when parameters of the plant physiology change.",
     "keywords": ["Essential oils", "Optimal control", "Nonlinear optimization", "Steam distillation"]},
    {"article name": "Harvest time prediction for batch processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.05.019",
     "publication date": "09-2018",
     "abstract": "Batch processes usually exhibit variation in the time at which individual batches are stopped (referred to as the harvest time). Harvest time is based on the occurrence of some criterion and there may be great uncertainty as to when this criterion will be satisfied. This uncertainty increases the difficulty of scheduling downstream operations and results in fewer completed batches per day. A real case study is presented of a bacteria fermentation process. We consider the problem of predicting the harvest time of a batch in advance to reduce variation and improving batch quality. Lasso regression is used to obtain an interpretable model for predicting the harvest time at an early stage in the batch. A novel method for updating the harvest time predictions as a batch progresses is presented, based on information obtained from online alignment using dynamic time warping.",
     "keywords": ["Batch process", "Prediction", "Dynamic time warping", "Partial least squares", "Lasso regression"]},
    {"article name": "A novel MINLP model of front-end crude scheduling for refinery with consideration of inherent upset minimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.05.022",
     "publication date": "09-2018",
     "abstract": "In this paper, a new methodology has been developed to deal with the front-end crude scheduling (FECS) problems with the consideration of inherent upset minimization (IUM). Specifically, the primary and secondary inherent upsets (IUs) have been defined and modeled to, respectively, address flowrate fluctuations of feeding crude distillation units and the long-distance pipeline (LDPL). Based on such IU characterizations, a new MINLP model with the unit-based continuous time representation is developed to determine the optimal FECS solution by minimizing the overall operating cost and instability along the entire scheduling time horizon. In addition to the IUM, another two merits are also included in this study: (i) the trans-mixing (TM) issue along with crude transportation inside the LDPL has been modeled; (ii) multiple types of crudes with multiple key properties have been simultaneously considered. The developed FECS MINLP model is solved by the ANTIGONE solver to obtain the global optima. The efficacy of the proposed methodology and the FECS model, and the effect of IUM have been investigated through various case studies.",
     "keywords": ["Front-end crude scheduling", "Inherent upset minimization", "Trans-mixing", "MINLP"]},
    {"article name": "A MILP model based on flowrate database for detailed scheduling of a multi-product pipeline with multiple pump stations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.05.002",
     "publication date": "09-2018",
     "abstract": "Multi-product pipelines usually transport several products in batches to respective delivery stations. As for a multi-product pipeline with multiple pump stations, this paper develops a continuous-time mixed-integer linear programming (MILP) model based on flowrate database to optimize its detailed scheduling. In the proposed model, various unit pump cost and flowrate constraints, which strongly depend on pump operation schemes, are introduced for the economy and safety of solved scheduling plans. Moreover, this paper considers the actual field processing constraints which vary with batch interface migration and rarely considered in previous work. And a novel method of historical flowrate database preprocessing is presented to enhance solving efficiency. Finally, through comparing with three real-world cases solved by another two available models, the proposed one performs the best in scheduling optimization as well as substantial reduction of pump cost.",
     "keywords": ["Multi-product pipelines", "Mixed-integer linear programming (MILP)", "Detailed scheduling optimization", "Pump cost", "Flowrate database"]},
    {"article name": "Heat exchanger network synthesis using genetic algorithm and differential evolution",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.06.005",
     "publication date": "09-2018",
     "abstract": "Heat Exchanger Networks (HEN) synthesis is a process engineering problem that can be mathematically characterized as highly combinatory, non-linear and non-convex. All these aspects bottleneck the identification of locally optimal solutions at acceptable computational time. This work proposes an optimization algorithm based on a superstructure considering non-isothermal mixing and stream splitting. HENs are optimized through the application of a bi-level new hybrid method that works at an upper level with Genetic Algorithm (GA) to optimize discrete variables and at a lower level with Differential Evolution (DE) for optimizing heat loads and stream split fractions in order to find solutions with low total annual costs (TAC). The proposed method was applied to six literature case studies and was efficient in obtaining solutions with TAC comparable or lower than those previously reported.",
     "keywords": ["Heat exchanger networks synthesis", "Optimization", "Genetic algorithm", "Differential evolution"]},
    {"article name": "Corrosion effect on inspection and replacement planning for a refinery plant",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.05.027",
     "publication date": "09-2018",
     "abstract": "This paper presents an optimization model of inspection and replacement planning for a refinery plant under the consideration of corrosion in terms of cost. The management of corrosion is an essential task for processes that operate over several years without a shutdown. This is because corrosion can cause severe failures by thinning the wall thickness and eventually cause pipes or equipment to burst. However, required safety measures, such as the corrosion management, involve costly inspection and replacement. Therefore, a cost-effective safety-action strategy is proposed in this paper. The developed model presents an optimal combination of steel grade, design wall thickness, inspection number, and inspection timing under a given corrosion rate to minimize the cost of design, inspection, replacement, and failure. Three case studies using sensitivity analyses are applied to three major processes in a refinery plant: a crude distillation unit, visbreaker, and hydrocracker.",
     "keywords": ["Crude distillation unit", "Heavy oil upgrade", "Hydrocracking", "Visbreaking", "Carbon steel", "Low alloy steel"]},
    {"article name": "Energy efficient design of membrane processes by use of entropy production minimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.06.002",
     "publication date": "09-2018",
     "abstract": "To minimize entropy production means to reduce the lost work in a process, and to optimize the use of energy resources. Due to the need for re-compression, membrane units for separation of CO2 from natural gas require large amounts of electrical power. We show that this power requirement can be reduced by controlling the permeation process so that the entropy production is minimum. With the use of optimal control theory, we develop in this work a detailed and robust method to minimize the entropy production of a membrane unit for separation of CO2 from natural gas, by control of the partial and total pressures on the permeate side. Moreover, we show how the continuous optimal results can serve as ideal limits for the practical design. A three-step permeate pressure that approximates the optimum reduces both the entropy production and the compressor power, when the permeate gas is re-compressed.",
     "keywords": ["Optimal control theory", "Membrane", "Gas separation", "Lost work"]},
    {"article name": "Simultaneous optimization and heat integration of the coal-to-SNG process with a branched heat recovery steam cycle",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.02.008",
     "publication date": "09-2018",
     "abstract": "The coal-to-SNG process is an energy-intensive process, and optimizing the heat recovery network can improve the economy and energy efficiency. This study proposes a branched, triple pressure level heat recovery steam cycle (HRSC) to recover waste heat, in which one branch is responsible for recovering the waste heat from the water gas shift (WGS) unit, and the other branch is responsible for the methanation (METH) unit. The extended Duran\u2013Grossmann model is used to optimize two heat exchanger networks to match the branched HRSC superstructure. The temperature/pressure/flow rates of the HRSC streams and the operating temperature of the WGS and METH units are optimized. The optimal bypass ratio of the WGS unit as well as the recycle ratio and split ratio of the METH unit, are 0.506, 0.681 and 0.456, respectively. The exergy efficiency of the coal-to-SNG plant is improved by 1.28% compared with the industrial plant, which can reach 54.17%.",
     "keywords": null},
    {"article name": "Performance of an active disturbance rejection control on a simulated continuous microalgae photobioreactor",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.06.006",
     "publication date": "09-2018",
     "abstract": "Microalgae are used for the industrial production of high value compounds. The aim in continuous bioreactors is to obtain the highest biomass production. It is necessary to guarantee that the bioprocesses attain and maintain the optimal reference biomass CX*(t), despite endogenous and exogenous disturbances. This paper describes the numerical simulation of the application of active disturbance rejection control (ADRC) to control the dilution rate (D(t))\u2009in a continuous culture of the microalga Chlorella vulgaris. To reduce the bioprocess to a \u201cSISO\u201d system, the authors chose the dilution rate, D(t), to be the only control signal. The control proposal was illustrated and evaluated through a numerical simulation using MATLAB/Simulink\u2122 environment. The performance of the ADRC was tested by the application of external perturbations and variation of parameters over a nominal case. At nominal conditions, D(t) was always maintained within the physical limits imposed by the bioprocess. Step and smooth type signals, at 96.4%\u00b7|Dmax(t)|, were imposed as external perturbation on the control signal input, D(t). The controller response kept the output signal CX(t) within an insignificant 0.0043%\u00b7|CXmax(t)|. The algal culture had a strongly asymmetric response to variations of the ideal maximum growth rate, \u03bcmax(t)\u202f\u00b1\u202f30%\u00b7|\u03bcmax(t)|, and of the nominal light intensity, Iin(t)\u202f\u00b1\u202f30%\u00b7|Iin(t)|. Nonetheless, the controller promptly returned the output signal to its reference value, CX(t)*. The numerical test of the control proposal, in summary, showed that the ADRC strategy ensures excellent reference tracking capability and robustness towards parametric uncertainties, un-modeled dynamics, and external disturbances.",
     "keywords": ["Active disturbance rejection control", "Nonlinear system", "Robust control", "Growth of microalgae", "Continuous culture"]},
    {"article name": "On the stochastic modelling of surface reactions through reflected chemical Langevin equations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.05.003",
     "publication date": "09-2018",
     "abstract": "Modelling of small-scale heterogeneous catalytic systems with master equations captures the impact of molecular noise, but can be computationally expensive. On the other hand, the chemical Fokker\u2013Planck approximation offers an excellent alternative from an efficiency perspective. The Langevin equation can generate stochastic realisations of the Fokker\u2013Planck equation; yet, these realisations may violate the conditions 0\u202f\u2264\u202f\u03b8\u202f\u2264\u202f1 (where \u03b8 is surface coverage). In this work, we adopt Skorokhod\u2019s formulations to impose reflective boundaries that remedy this issue. We demonstrate the approach on a simple system involving a single species and describing adsorption, desorption, reaction and diffusion processes on a lattice. We compare different numerical schemes for the solution of the resulting reflected Langevin equation and calculate rates of convergence. Our benchmarks should guide the choice of appropriate numerical methods for the accurate and efficient simulation of chemical systems in the catalysis field.",
     "keywords": ["Molecular noise", "Chemical master equation", "Fokker\u2013Planck equation", "Langevin equation", "Reflective boundary", "Gillespie algorithm", "00-01", "99-00"]},
    {"article name": "Comparison of different kraft lignin-based vanillin production processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.05.020",
     "publication date": "09-2018",
     "abstract": "This work employed a systematic methodology and process intensification concepts to improve the state-of-the-art vanillin production process from kraft lignin. Particular focus was put on the use of different product separation techniques, including solvent extraction using ethyl acetate, benzene, organic nanofiltration, and adsorption by zeolite. The performance of each proposed process was evaluated in terms of energy consumption, process economics, and environmental impacts. Simulation of the vanillin production processes was performed using the Aspen Plus program while the economic and environmental analyses were conducted using economic analysis tools and LCSoft, respectively. The results revealed that the process using adsorption by zeolite is the best alternative. It offered a slight improvement in energy consumption while its economic result showed 7.37% improvement. It also caused the least environmental impact in all categories.",
     "keywords": ["Vanillin production", "Kraft lignin", "Process intensification", "Process design", "Biorefinery"]},
    {"article name": "Optimal design of boil-off gas reliquefaction process in LNG regasification terminals",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.06.003",
     "publication date": "09-2018",
     "abstract": "Boil-off gas (BOG) generation in liquefied natural gas (LNG) regasification terminals is substantial and unavoidable. Most terminals employ a cost-intensive BOG reliquefaction process using the send-out LNG. In this work, we study the preliminary design of this process with the objective of minimizing its total annualized cost (TAC). We present a comprehensive superstructure for the reliquefaction process that incorporates several process options for cooling BOG using LNG at different pressures, and allows recondensation in multiple stages. Then, we develop custom simulation/sizing modules for the process units in our superstructure, and implement a procedure to reduce explicit constraints during optimization. Considering realistic design specifications and operational constraints, we optimize a case study terminal for various BOG rates and conditions. While the TAC increases substantially with BOG rate, two-stage recondensation is always optimal. A 2-recondenser scheme with BOG cooling by the high-pressure LNG before the first recondenser is optimal for most cases.",
     "keywords": ["Boil-off gas", "LNG", "Regasification terminal", "Reliquefaction"]},
    {"article name": "A process design approach to manage the uncertainty of industrial flaring during abnormal operations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.06.011",
     "publication date": "09-2018",
     "abstract": "Flare management challenges are related to the flaring uncertainty during abnormal situations. In this work, a multi-objective optimization framework is upgraded with multi-period optimization and Monte Carlo simulation to incorporate the risk associated with uncertain flare events. An ethylene plant is used to present the developed framework. Using the ethylene process historical flaring data, Monte Carlo simulation generates probabilistic values for flaring events and event duration. Here, cogeneration unit (COGEN) is considered as the flare reduction alternative. The results of the formulations are presented as a set of Pareto fronts providing insights into the competing techno-economic and environmental objectives. Sensitivity analysis on the factors for the case suggests that some factors such as CO2 tax savings are severely affected by minor variations in flaring profiles, whereas others such as the fixed and operating costs are less sensitive. Hence, using this approach, the decision maker gains techno-economic-environmental insights regarding the flare reduction alternative (COGEN).",
     "keywords": ["Design under uncertainty", "Abnormal situation management (ASM)", "Industrial flaring", "Flare utilization", "Multi-objective optimization", "Monte Carlo simulation", "Greenhouse gas emissions"]},
    {"article name": "A meta-optimized hybrid global and local algorithm for well placement optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.06.013",
     "publication date": "09-2018",
     "abstract": "Well placement optimization is a complex and time-consuming task. An efficient and robust algorithm can improve the optimization efficiency. In this work, we propose a meta-optimized hybrid cat swarm mesh adaptive direct search (O-CSMADS) algorithm for well placement optimization. By coupling Cat Swarm Optimization (CSO) algorithm, Mesh Adaptive Direct Search (MADS) algorithm, and Particle Swarm Optimization (PSO) meta-optimization approach, O-CSMADS has global search ability and local search ability. We perform detailed comparisons of optimization performances between O-CSMADS, hybrid cat swarm mesh adaptive direct search (CSMADS) algorithm, CSO, and MADS in three different examples. Results show that O-CSMADS algorithm outperforms stand-alone CSO, MADS, and CSMADS. Besides, optimal controlling parameters are not same for different problems, which indicates that the optimization of algorithmic parameters is necessary. The proposed method also shows great potential for other petroleum engineering optimization problems, such as well type optimization and joint optimization of well placement and control.",
     "keywords": ["Well placement", "Optimization efficiency", "Hybrid optimization algorithm", "Meta-optimization approach", "Cat swarm optimization algorithm", "Mesh Adaptive Direct Search"]},
    {"article name": "Carbon dioxide adsorption separation from dry and humid CO2/N2 mixture",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.06.016",
     "publication date": "09-2018",
     "abstract": "In this study, we report the effect of water vapor on CO2 uptake using Mg-MOF-74 via adsorption breakthrough modeling and lab experiments. Carbon dioxide is the most influencing gas that significantly expedites global warming. Therefore, it is ultimately necessary to reduce the rapid increase of CO2 concentration in the atmosphere by means of Carbon Capture and Storage (CCS). CO2 separation by physical adsorption is an interesting technology to achieve CO2 capture with minimum energy penalties. Metal-organic framework (MOF) adsorbents forms a class of adsorbents with much higher specific surface areas than conventional porous materials such as activated carbons, and zeolites. However, most MOFs show notable hydro instability for CO2 separation from humid flue gas. Mg-MOF-74 is a superior adsorbent amongst other adsorbents owing to its high CO2 uptake at flue gas conditions. A model is developed using User-Defined-Function in an ANSYS Fluent program. Two and three-dimensional models are validated by comparing their results with experimental work carried out by the authors, at ambient temperature, and published experimental data for high temperature conditions. The effect of water vapor is studied at different temperatures and various relative humidity values for Mg-MOF-74. Results indicate that CO2 uptake has been significantly reduced with the existence of more than 5% water vapor when Mg-MOF-74 is used as an adsorbent.",
     "keywords": ["Carbon capture", "Humidity, MOFs", "Adsorption", "Global warming", "Mg-MOF-74"]},
    {"article name": "On the computation and physical interpretation of semi-positive reaction network invariants",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.06.009",
     "publication date": "09-2018",
     "abstract": "In this paper, we examine the mathematical structure of chemical reaction networks with the goals of identifying reaction invariant states and determining their physical significance. A combined species-reaction graph/convex analysis approach is developed to find semi-positive invariant states associated with a reaction network. Application of this graphical/algebraic reaction network analysis approach to four different chemical processes reveals that reaction invariants can represent conserved quantities other than elemental balances.",
     "keywords": ["Reaction network analysis", "Semi-positive invariant", "Species-reaction graph"]},
    {"article name": "New methodology for parameter estimation of offshore slug models with Hopf bifurcation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.06.012",
     "publication date": "09-2018",
     "abstract": "Offshore well elevation systems often present limit cycles caused by oil-gas slugging. The latest works in control solutions use models based on simple Ordinary Differential Equations to suppress slug flow and increase overall production. However, obtaining parameters for these models is not straightforward due to the occurrence of Hopf bifurcations. Therefore, this work aims to propose a novel methodology to improve parameter estimation. The limit cycle readings of pressure instruments are condensed into a single characteristic cycle with mean value and variance for each sample. Each iteration then uses a modified weighted sum of least squares. This creates a more convex region around the optimum that improves the performance of optimization algorithms. The results were obtained using previously published data for two models. Using the F-test, it was statistically proven that the parameters obtained from the proposed method provided an equivalent or better model fit by employing strictly numeric methods.",
     "keywords": null},
    {"article name": "Multi-objective optimization of an integrated gasification combined cycle for hydrogen and electricity production",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.06.004",
     "publication date": "09-2018",
     "abstract": "In this paper, an integrated coal gasification combined cycle system for the production of hydrogen and electricity is optimized in terms of energy and exergy efficiencies, and the amount and cost of the produced hydrogen and electricity. The integrated system is optimized by focusing on the conversion process of coal to syngas. A novel optimization process is developed which integrates an artificial neural network with a genetic algorithm. The gasification system is modeled and simulated with Aspen Plus for large ranges of operating conditions, where the artificial neural network method is used to represent the simulation results mathematically. The mathematical model is then optimized using a genetic algorithm method. The optimization demonstrates that the lower is the grade of coal of the three considered coals, the less expensive is the hydrogen and electricity that can be produced by the considered integrated gasification combined cycle (IGCC) system.",
     "keywords": ["Gasification", "Hydrogen Production", "Modeling", "Efficiency", "Optimization"]},
    {"article name": "Development of a reaction/distillation matrix for systematic generation of sequences in a single two component reaction-separation case study",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.06.025",
     "publication date": "09-2018",
     "abstract": "Studies on simultaneous synthesis of the reaction-separation processes are limited. The initial step is choosing a method to generate all possible arrangements. In this work, the Separation matrix algorithm previously developed by the authors has been expanded to include the reaction part of the process to form a Reaction/Distillation matrix.In order to achieve a comprehensive algorithm, initially all feasible matrices have been analyzed to find common points among non-synthesizable matrices. The ability of new matrix to include a broad selection of flowsheets has then been investigated. A single two-component reaction, is assumed to prevail within a four-component mixture, the fourth being the initially nonexistent reaction product. A systematic approach is presented to determine a large selection of sequences for a four-component system including three distillation columns (simple or complex) and a reactor. A practical case study of the isomerization of n-Butane has been analyzed using the procedure developed here.",
     "keywords": ["Process synthesis", "Reaction-separation systems", "Reaction/Distillation matrix", "Probable sequence generation"]},
    {"article name": "Improved capacity estimation technique for the battery management systems of electric vehicles using the fixed-point iteration method",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.06.023",
     "publication date": "09-2018",
     "abstract": "This paper presents an improved scheme for the state-of-health (SOH) estimation, applicable to battery management system (BMS) of electric vehicles. The original scheme requires the prior information for the estimation, which is the prior SOH identified from the last charging. This limit implies that if a battery or its BMS is replaced, the prior SOH stored within the BMS no longer matches with the actual SOH, resulting in a critical error. To avoid this potential but critical pitfall, we newly devise an improved SOH estimation scheme. The original scheme is revised by adopting the fixed-point iteration method into its parameter estimation. By removing dependencies on the prior information, the revised scheme can function regardless of such replacements. The revised scheme is experimentally validated and demonstrated that even without the prior information, it can satisfy the requirement of the SOH estimation (within 3%) thanks to its improved design.",
     "keywords": ["Lithium-ion battery", "Battery management system", "Electric vehicle", "State-of-health (SOH)", "Fixed-point iteration method"]},
    {"article name": "Optimization of multistage fractured horizontal well in tight oil based on embedded discrete fracture model",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.06.015",
     "publication date": "09-2018",
     "abstract": "Optimizing multistage fractured horizontal wells (MFHW) can tap the full potential of tight oil reservoirs. Although recent studies have introduced various frameworks, most of the significant parameters for MFHW are not optimized simultaneously, which may lead to actual performance that is far below expected performance, especially in heterogeneous reservoirs. Here, we present an efficient optimization framework that couples embedded discrete fracture model (EDFM) and intelligent algorithms to maximize net present value. We also examined the performance of four optimization algorithms in our model: genetic algorithm (GA), multilevel coordinate search (MCS), covariance matrix adaptation evolution strategy (CMA-ES), and generalized pattern search (GPS). Our results suggest that because CMA-ES handles MFHW optimization robustly and effectively, it may be utilized in future applications. Our framework serves as an efficient tool to optimize MFHW design, which plays an increasingly significant role in the enhancement of tight oil recovery.",
     "keywords": ["Tight oil", "Optimization", "Hydraulic fracture", "Horizontal well", "Embedded discrete fracture model"]},
    {"article name": "Locality preserving discriminative canonical variate analysis for fault diagnosis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.06.017",
     "publication date": "09-2018",
     "abstract": "This paper proposes a locality preserving discriminative canonical variate analysis (LP-DCVA) scheme for fault diagnosis. The LP-DCVA method provides a set of optimal projection vectors that simultaneously maximizes the within-class mutual canonical correlations, minimizes the between-class mutual canonical correlations, and preserves the local structures present in the data. This method inherits the strength of canonical variate analysis (CVA) in handling high-dimensional data with serial correlations and the advantages of Fisher discriminant analysis (FDA) in pattern classification. Moreover, the incorporation of locality preserving projection (LPP) in this method makes it suitable for dealing with nonlinearities in the form of local manifolds in the data. The solution to the proposed approach is formulated as a generalized eigenvalue problem. The effectiveness of the proposed approach for fault classification is verified by the Tennessee Eastman process. Simulation results show that the LP-DCVA method outperforms the FDA, dynamic FDA (DFDA), CVA-FDA, and localized DFDA (L-DFDA) approaches in fault diagnosis.",
     "keywords": ["Fault diagnosis", "Canonical variate analysis", "Fisher discriminant analysis", "Locality preserving projection", "Tennessee Eastman process"]},
    {"article name": "Plant-wide oscillation detection using multivariate empirical mode decomposition",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.06.007",
     "publication date": "09-2018",
     "abstract": "Plant-wide oscillation detection is an important task in the maintenance of large-scale industrial control systems, owing to the fact that in an interactive multi-loop environment oscillation generated in one loop may propagate to the different parts of the plant. In such a scenario, it is required that different loops oscillating due to a common cause and hence similar frequency may be grouped together. In this paper an adaptive method for plant-wide oscillation detection based on multivariate empirical mode decomposition (MEMD) along with a grouping algorithm is proposed. The method can identify multiple oscillation groups among different variables as well as variables with random noise only. The proposed method is also applicable to both non-linear and non-stationary time series where the techniques based on the conventional Fourier analysis are prone to errors. Within each group that oscillate due to a common cause, the method can also indicate the location of the probable root cause of oscillations. The efficacy of the proposed method is established with the help of both simulation and industrial case studies.",
     "keywords": ["Multivariate empirical mode decomposition", "Mode alignment", "Grouping algorithm", "Dyadic filter bank property"]},
    {"article name": "Deviation propagation analysis along a cumene process by using dynamic simulations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.06.010",
     "publication date": "09-2018",
     "abstract": "The dynamic response of benzene alkylation process to a set of deviations is analyzed with Aspen Plus Dynamics. A quantitative risk assessment is developed through simulations of deviation scenarios. The process comprises a reactor and three distillation columns with a recycle stream. The simulation scenarios are determined according to lessons learnt from accidents. This study underlines the conditions that induce an overpressure or a flooding in a distillation column. Three scenarios are proposed: feed flowrate variations, coolant flowrate reduction and cooling of the reboiler steam. Thereafter, the results allow calculating a set of risk indexes related to flooding and overpressure phenomena. This study underlines the deviation propagation effects that can be expected in all the process equipment. Moreover, it represents a significant contribution to the definition of the process control strategy and the necessary safety barriers.",
     "keywords": ["Dynamic simulation", "Process safety", "Risk assessment", "Consequence analysis", "Sensitivity analysis"]},
    {"article name": "Model on transport phenomena and control of rod growth uniformity in siemens CVD reactor",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.07.001",
     "publication date": "09-2018",
     "abstract": "The transport phenomena in Siemens reactors has been investigated by using computational fluid dynamics (CFD) technique. The reaction kinetics was validated against two data sets of silicon epitaxial deposition experiments. Comparative simulations were carried out for four types of reactors with different configurations of gas supplying nozzles and offgas ports. The uniformity index was introduced to evaluate rod growth uniformity. The result showed that improper gas distributing system would result in zones of low velocity together with high temperature, and uneven species concentration distribution, finally cause low uniformity of rod growth. The reactor with a single offgas port at the center of the bottom plate was supposed to be best design among the four from both the performance and the equipment complexity points of view.",
     "keywords": ["Siemens Reactor", "CVD", "Polysilicon", "CFD", "Photovoltaics"]},
    {"article name": "Dynamic graph embedding for fault detection",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.05.018",
     "publication date": "09-2018",
     "abstract": "Using sequence information can improve performances in fault detection for serial (temporal) correlated process data. Classical methods firstly construct extended vectors through concatenating current process data and a certain number of previous process data, and then take dimension reduction methods. However, the simple extension of process data may distort the correlation between variables and largely increase the dimensionality. This paper proposes a novel algorithm, called Dynamic Graph Embedding (DGE), for fault detection. DGE adopts augmented matrices instead of extended vectors to encode sequence information. Furthermore, DGE incorporates both time information and neighborhood information to form similarities of different process data. And then DGE is designed to obtain embedding matrices with Markov chain analysis of the similarities. Extensive experimental results on the Tennessee Eastman (TE) benchmark process show the superiority of DGE in terms of missed detection rate (MDR) and false alarm rate (FAR).",
     "keywords": ["Process monitoring", "Fault detection", "Dimension reduction", "Dynamic feature extraction"]},
    {"article name": "LAMOS: A linear algorithm to identify the origin of multiple optimal flux distributions in metabolic networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.06.014",
     "publication date": "09-2018",
     "abstract": "In flux balance analysis, where flux distribution within a cell metabolic network is estimated by optimizing an objective function, there commonly exist multiple optimal flux distributions. Although finding all optimal solutions is possible, their interpretation is a challenge. A new four-phase algorithm (LAMOS) is therefore proposed in this work to efficiently enumerate all of these solutions based on iterative substitution of a current non-basic variable with a basic variable. These basic and non-basic variables are called key reaction pairs that their successive activity or inactivity causes alternate optimal solutions. LAMOS was implemented on E. coli metabolic models and the results proved it as a simple and fast method capable of finding the key reactions as well as reactions participating in the futile cycles. Key reactions were 1\u20133% of all reactions for the large-scale models and these reactions were identified using only 1% of optimal solutions.",
     "keywords": ["Genome- scale metabolic network", "Flux balance analysis", "Multiple optimal solution", "Linear algorithm", "Internal and futile cycles, Key reactions"]},
    {"article name": "A biologically-inspired approach for adaptive control of advanced energy systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.07.002",
     "publication date": "09-2018",
     "abstract": "In this article, a novel approach is proposed for integrating a Biologically-Inspired Optimal Control Strategy (BIOCS) with an Artificial Neural Network (ANN)-based adaptive component for advanced energy systems applications. Specifically, BIOCS employs gradient-based optimal control solvers in a biologically-inspired manner, following the rule of pursuit for ants, to simultaneously control multiple process outputs at their desired setpoints. Also, the ANN component captures the mismatch between the controller and the plant models by using a single-hidden-layer technique with online learning capabilities to augment the baseline BIOCS control laws. The resulting approach is a unique combination of biomimetic control and data-driven methods that provides optimal solutions for dynamic systems. The applicability of the proposed framework is illustrated via an Integrated Gasification Combined Cycle (IGCC) process with carbon capture as an advanced energy system example. In particular, a multivariable control structure associated with a subsystem of the IGCC plant simulation in DYNSIM\u00ae is addressed. The proposed control laws are derived in MATLAB\u00ae, while the plant models are built in DYNSIM\u00ae, and a previously developed MATLAB\u00ae-DYNSIM\u00ae link is employed for implementation purposes. The proposed integrated approach improves the overall performance of the process up to 85% in terms of reducing the output tracking error when compared to stand-alone BIOCS and Proportional-Integral (PI) controller implementations, resulting in faster setpoint tracking. The proposed framework thus provides a promising alternative for advanced control of energy systems of the future.",
     "keywords": ["Biomimetic control", "Adaptive control", "Artificial neural network", "Advanced energy systems"]},
    {"article name": "Optimal molecular design towards an environmental friendly solvent recovery process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.06.008",
     "publication date": "09-2018",
     "abstract": "This paper presents a novel Computer Aided Molecular Design (CAMD) methodology for the design of solvents with consideration of environmental impact during the extraction and recovery processes. Throughout the years, CAMD techniques have been widely applied to design novel molecules for various applications, especially solvents. Most of the works in this area are only focusing on designing solvents that can achieve its functionality. However, limited works consider safety, health and environmental impacts of the process in which the solvent is applied. In this work, a quantitative assessment of the total environmental burden for solvent recovery process is incorporated into a single stage CAMD framework. The CAMD formulation includes molecular properties that affect the quantitative assessment of the environmental impact of a process. A multi-objective solvent design framework is then solved using Fuzzy Analytic Hierarchy Process (FAHP) weighting approach to design solvents that satisfy various target properties. With this approach, the generated solvents can simultaneously improve the overall environmental characteristic of a process and give a better balance of performance for a set of predefined properties. To illustrate the proposed methodology, a case study on solvent design for residual oil extraction from palm pressed fibre is presented.",
     "keywords": ["Computer Aided Molecular Design (CAMD)", "Fuzzy Analytic Hierarchy Process (FAHP)", "Multi-objective optimisation", "Safety", "Health and Environment (SHE), Solvent design"]},
    {"article name": "Microalgae growth optimization in open ponds with uncertain weather data",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.07.005",
     "publication date": "09-2018",
     "abstract": "Although microalgae-based processes are currently one of the most promising new technologies for the substitution of fossil fuels and chemicals, the theoretical potential of these technologies is currently limited by their low profitability, hence hindering the development of large scale plants in an economically feasible way. One of the process bottlenecks is the cultivation phase, whose operation is complicated by both the involved biological mechanisms complexity and the highly fluctuating weather conditions affecting the system. Available mathematical models describing microalgae growth and pond temperature dynamics through weather data implementation assume perfect knowledge of weather conditions, hence neglecting the inaccuracy of meteorological predictions that is expected even considering short-term forecasts. In this study a sensitivity study is first carried out to evaluate the weather variables that most impact on productivity. Then, two optimization approaches are proposed to prevent potential critical conditions (such as cell death due to too high temperatures) that may arise by using inaccurate weather forecast. The study demonstrates the reliability of the proposed methodologies and compares them in terms of productivity loss and water demand.",
     "keywords": ["Microalgae", "Biofuels", "Open ponds", "Optimization", "Weather uncertainty"]},
    {"article name": "A dynamical model of an aeration plant for wastewater treatment using a phenomenological based semi-physical modeling methodology",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.07.008",
     "publication date": "09-2018",
     "abstract": "Diffused aeration is a sensitive process for wastewater treatment. Because of the nonlinearity and complexity of aerator dynamics due to microorganism metabolism and oxygen transfer, reliable mathematical models are needed to perform control-oriented tasks. To this end, in this study we develop a phenomenological based semi-physical model (PBSM) to predict and describe the dynamic behavior of the oxygen transfer in a diffused aeration process by means of a formal modeling methodology. This model will then be validated by using data from an aeration pilot plant. In this paper, we also show a lack of agreement in the literature in terms of the different available ways to represent the volumetric oxygen transfer coefficient kLa. Reasonable agreement between the developed model and plant data is found by considering a phenomenological approach of the kLa instead of considering many of the available empirical correlations in the literature.",
     "keywords": ["Oxygen transfer", "Aeration processes", "Phenomenological based semi-physical model", "Volumetric mass transfer coefficient", "Oxygen scavenger"]},
    {"article name": "Analysis of the financial risk under uncertainty in the municipal solid waste management involving multiple stakeholders",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.07.017",
     "publication date": "09-2018",
     "abstract": "This paper presents a multi-objective optimization approach for the strategic planning of a municipal solid waste (MSW) management system under uncertainty. The formulation considers the involved tasks such as recycle, reuse, transportation, separation and distribution. The proposed approach also accounts for the multiple involved stakeholders with the objective of maximizing the benefit to all the participants. The main variables that are considered under uncertainty were the MSW availability and the prices of the products made from the recovered MSW. A case study for Mexico is analyzed, where a random generation approach based on historical data is used to introduce three different financial risk levels: the optimistic, mean and pessimistic cases. This way, the obtained results provide additional information about the system. Then, the stakeholders will be more certain of making any decision.",
     "keywords": ["Multi-objective optimization", "Multi-stakeholders", "Uncertainty", "Waste management"]},
    {"article name": "Dynamic self-optimizing control for unconstrained batch processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.06.024",
     "publication date": "09-2018",
     "abstract": "In this paper, we consider near-optimal operation for a class of unconstrained batch processes using the self-optimizing control (SOC) methodology. The existing static SOC approach is extended to the dynamic case by means of a static reformulation of the dynamic optimization problem. However, the dynamic SOC problem is posed as a structure-constrained controlled variable (CV) selection problem, which is different from the static cases. A lower-block triangular structure is specified for the combination matrix, H, to allow for optimal operation whilst respecting causality. A new result is that the structure-constrained SOC problem still results in a convex formulation, which has an analytic solution where the optimal CVs associated with discrete time instants are solved separately. In addition, the inputs are directly determined based on current CV functions for on-line utilization. A fed-batch reactor and a batch distillation column are used to demonstrate the usefulness of the proposed approach.",
     "keywords": ["Self-optimizing control", "Controlled variable", "Dynamic optimization", "Batch process"]},
    {"article name": "Mixed-integer nonlinear programming models for optimal design of reliable chemical plants",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.08.013",
     "publication date": "08-2018",
     "abstract": "Motivated by reliability/availability concerns in chemical plants, this paper proposes MINLP models to determine the optimal selection of parallel units considering the trade-off between availability and cost. Assuming an underlying serial structure for availability, we consider first a case where the system transitions between available and unavailable states, and second the case with an intermediate state at half capacity. Two non-convex MINLP models maximizing net profit are introduced for the two cases. In addition, a bi-criterion MINLP model is proposed to maximize availability and to minimize cost for the first case. It is shown that the corresponding epsilon-constrained model, where the availability is maximized subject to parametrically varying upper bound of the cost, can be reformulated as a convex MINLP. Availability is also incorporated in the superstructure optimization of process flowsheets. The performances of the proposed models are illustrated with a methanol synthesis and a toluene hydrodealkylation process.",
     "keywords": ["Reliability", "Availability", "Design", "MINLP", "Parallel units", "Serial structure"]},
    {"article name": "Reprint of: Data-driven robust optimization under correlated uncertainty: A case study of production scheduling in ethylene plant",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.10.039",
     "publication date": "08-2018",
     "abstract": "To hedge against the fluctuations generated from continuous production processes, practical solutions can be obtained through robust optimization induced by the classical uncertainty sets. However, uncertainties are sometimes correlated in industrial scheduling problems because of the connected process and various random factors. To capture and enrich the valid information of uncertainties, copulas are introduced to estimate the joint probability distribution and simulate mutual scenarios for uncertainties. Cutting planes are generated to remove unnecessary uncertain scenarios in the uncertainty sets, and then robust formulations induced by the cut set are proposed to reduce conservatism and improve the robustness of scheduling solutions. A real-world process of ethylene plant is introduced as the numerical case, and high-dimensional data-driven uncertainty sets are illustrated in detail. The proposed models are proved to control the fluctuation of consumed fuel gas below a lower level of conservatism.",
     "keywords": ["Data-driven", "Robust optimization", "Correlated uncertainty", "Production scheduling", "Copula", "Fuel gas"]},
    {"article name": "Computer aided chemical product design \u2013 ProCAPD and tailor-made blended products",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.03.029",
     "publication date": "08-2018",
     "abstract": "In chemical product design, application of computer-aided methods helps to design as well as improve products to reach the market faster by reducing time-consuming experiments at the early stages of design. That is, experiments are performed during the later stages as a verification or product refinement step. Computer-aided molecular and mixture-blend design methods are finding increasing use because of their potential to quickly generate and evaluate thousands of candidate products; to estimate a large number of the needed physico-chemical properties; and to select a small number of feasible product candidates for further verification and refinement by experiments. In this paper, an extended computer-aided framework and its implementation in a product design software tool is presented, highlighting the new features together with an overview on the current state of the art in computer-aided chemical product design. Results from case studies involving tailor-made blend design are presented to highlight the latest developments.",
     "keywords": ["CAMD, CAMbD", "ProCAPD", "Tailor-made surrogate blends", "Solvents", "Lubricants", "Fuels"]},
    {"article name": "On the estimation of high-dimensional surrogate models of steady-state of plant-wide processes characteristics",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.02.014",
     "publication date": "08-2018",
     "abstract": "This work generalizes a preliminary investigation (Georgakis and Li, 2010) in which we examined the use of Response Surface Methodology (RSM) for the estimation of surrogate models as accurate approximations of high-dimensional knowledge-driven models. Three processes are examined with higher complexity than before, accounting for a much larger number of input and output variables. The surrogate models obtained are used to analyze several steady-state plant-wide characteristics. In all processes, the knowledge-driven model is a dynamic simulation with a plant-wide control structure of multiple SISO controllers. This type of controller proves to not be robust enough in its stability characteristics to enable substantial changes in the set-points. The net-elastic regularization is successfully used for the estimation of the metamodel parameters, avoiding overfitting and eliminating insignificant terms. Cross validation is used to compare and evaluate the relative accuracy of the quadratic and cubic models.",
     "keywords": ["Surrogate model", "Metamodel", "Process operability", "Process optimization", "Design of experiments", "Plant-wide control", "Net-elastic regularization"]},
    {"article name": "Infeasibility resolution for multi-purpose batch process scheduling",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.03.005",
     "publication date": "08-2018",
     "abstract": "Scheduling decisions give rise to some of the most challenging optimization problems in the process industry. Formulating mathematical models for scheduling problems and devising tailored solution algorithms for these models has been the main thrust of previous literature. In this work, we focus on analyzing and resolving the cause of bottlenecks and infeasibilities in generic scheduling problems. We present a systematic approach for infeasibility diagnosis. Our approach exploits the known structure of scheduling models to isolate interpretable infeasible sets of constraints. We demonstrate the power of the algorithm on infeasible instances of the Westenberger\u2013Kallrath multipurpose batch process modeled using a state-task network (STN) representation. The methodology presented in the paper is able to successfully analyze the cause of infeasibility and provide recommendations for resolving it. We also demonstrate how these insights and recommendations can be presented to scheduling operators with little or no optimization expertise in an intuitive manner.",
     "keywords": ["Infeasibility analysis", "Irreducible infeasible sets", "Mixed-integer optimization", "Batch-scheduling", "Multipurpose plants", "Planning and scheduling"]},
    {"article name": "An anchor-tenant approach to the synthesis of carbon-hydrogen-oxygen symbiosis networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.02.024",
     "publication date": "08-2018",
     "abstract": "Sustainable development of industrial cities and eco-industrial parks (EIPs) requires careful consideration and creation of synergistic opportunities among the participating entities. Recently, a multi-scale design approach was developed for carbon-hydrogen-oxygen symbiosis networks (CHOSYNs) with focus on the targeting, integration, and retrofitting of EIPs involving a set of existing facilities. Another important category of EIPs involves the grass-root design of industrial cities in which the participants are not originally known. Instead, \u201canchor\u201d plants are first invited followed by the consideration and invitation of supporting facilities (referred to as \u201ctenants\u201d) that are to be determined according to integration opportunities with the anchors, other tenants, common infrastructure while accounting for resource limitations, market demands, and environmental regulations. The purpose of this work is to introduce a multi-scale targeting, synthesis, and optimization approach for the grass-root design of EIPs with known anchors. The CHOSYN framework is extended to tackle the case of candidate tenants with the objective of identifying industrial facilities, raw materials, byproducts, products, and wastes that can be effectively integrated with the anchors, among the participating tenants, and with the surrounding markets. Atomic-based and techno-economic targeting approaches are developed to identify benchmarks for mass integration within the EIP and to provide preliminary screening of the type and size of candidate tenants. Next, an optimization framework is developed to synthesize a highly-integrated and cost-effective cluster of anchors and tenants with sufficient design details on the individual facilities and the interaction among the participating plants. A case study is solved to demonstrate the multi-scale targeting, synthesis, and optimization approaches for the grass-root design of EIPs.",
     "keywords": ["Eco-industrial parks", "Sustainability", "Mass integration", "Targeting", "Design", "Synthesis"]},
    {"article name": "An optimization based strategy for crude selection in a refinery with lube hydro-processing",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.09.025",
     "publication date": "08-2018",
     "abstract": "An optimization-based strategy is developed for crude selection in a refinery with lube base oil (LBO) producing capability. Crude oil is classified into five types based on the viscosity index (VI) and the compositional aspect of vacuum gas oil (VGO). A prediction model for the VI, the most important quality variable in LBO processing, is developed for VGO based on its bulk properties. For each crude type, prediction models for the yield and the VI change vs. the conversion rate during lube hydro-processing are developed. The lube hydro-processing models are then incorporated into the overall refinery optimization to maximize the overall margin while satisfying all the specifications of the lube and fuel products simultaneously. A case study involving several price scenarios illustrates the benefits from using the model-based optimization method for deciding crude procurement, the grade of the LBO to be produced, and the conversion rate in the lube process.",
     "keywords": ["Lube base oil", "Refinery", "Crude selection", "Viscosity index (VI)", "Hydro-processing", "Hydro-isomerization", "Lube hydro-cracking optimization", "Group III and group II base oils"]},
    {"article name": "Computational design of thermostable mutants for cephalosporin C acylase from Pseudomonas strain SE83",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.05.014",
     "publication date": "08-2018",
     "abstract": "Computational protein design strategies can be used to increase enzyme stability without the need for high-throughput screening. In this report, computational methods were used to redesign cephalosporin C acylase from Pseudomonas strain SE83 to enhance its stability by repacking the hydrophobic core regions and reconstructing the protein-protein interactions in the segment interface regions. A nine-fold mutant with enhanced catalytic activity in the hydrolysis of cephalosporin C to 7-aminocephalosporanic acid, but with low stability, was used as a starting point. A computational enzyme design strategy was used to identify target regions to increase the protein melting temperature (Tm). Single point mutations Asn2\u03b2Thr, Asn2\u03b2Val, Cys470\u03b2Ser, Leu154\u03b2Phe, and Leu180\u03b2Phe in hydrophobic core regions, and Ala100\u03b1Ser and Ala37\u03b2Ser in segment-segment interface regions, increased the Tm by 4.7\u201319.7\u00b0 C, while combining these confirmed single mutations increased the Tm by up to 20.5\u00b0 C.",
     "keywords": ["Computational protein design", "Cephalosporin C acylase", "Computational enzyme design", "Protein-protein interaction", "Enzyme engineering"]},
    {"article name": "Solving global optimization problems using reformulations and signomial transformations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.10.035",
     "publication date": "08-2018",
     "abstract": "In this paper, a framework for reformulating nonconvex mixed-integer nonlinear programming (MINLP) problems containing twice-differentiable (C2) functions to convex relaxed form is discussed. To provide flexibility and for utilizing more effective transformation strategies, the twice-differentiable functions can be partitioned into convex, signomial and general nonconvex functions. The latter two can then be convexified using lifting transformations in combination with approximations using piecewise linear functions (PLFs). However, since there are many degrees of freedom in how to select the set of transformations, an optimization-based method is proposed for finding an optimal set. The lifting transformations are based on single-variable power and exponential transformations for signomials. For nonconvex C2-functions the \u03b1 reformulation (\u03b1R) technique as well as more generally the method of difference of convex functions can be applied. In the \u03b1R, the \u03b1BB convex underestimator can be used. The framework is utilized in the \u03b1 signomial global optimization (\u03b1SGO) algorithm to find the \u03f5-global solution to a nonconvex problem by iteratively updating the approximations provided by the PLFs. The framework can also be used to directly obtain a convex relaxation of any nonconvex MINLP problem of the specified type to a determined accuracy.",
     "keywords": ["Global optimization", "Nonconvex MINLP", "Reformulation techniques", "Signomial functions", "Twice-differentiable nonconvex functions", "Power transformations", "Exponential transformations", "\u03b1 reformulation", "\u03b1BB convex underestimator", "Difference of convex functions"]},
    {"article name": "Fast computation of Lipschitz constants on hyperrectangles using sparse codelists",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.04.016",
     "publication date": "08-2018",
     "abstract": "We propose and compare methods for the efficient calculation of Lipschitz constants of differentiable functions on hyperrectangular sets. Two new approaches are presented here that are inspired by techniques for the calculation of Hessian spectral bounds for \u03b1BB, a method that Professor Christodoulos A. Floudas proposed as a young researcher and continuously refined over his entire career.We compare the new approaches to existing ones with respect to two features, their computational cost, and the tightness of the resulting constants. The new algorithms are designed to require fewer operations than the existing ones. Not surprisingly, they result in fairly conservative Lipschitz constants. We show, however, that this conservatism can be mitigated considerably by incorporating structural information. More specifically, all methods considered here use codelists that are extended by first order derivatives. Extended codelists involve sparse intermediate gradients, since early codelist lines depend on very few variables by construction (for any nontrivial function, not just in special cases). This sparsity can be used to improve the Lipschitz constants.We compare the computational cost and tightness of the existing and new approaches from a theoretical point of view and corroborate the results with a large number of computational experiments involving several hundred sample functions on random hyperrectangular sets. We claim that one of the new sparse methods is an attractive option for algorithms that require Lipschitz constants on many hyperrectangular sets (such as global branch and bound Lipschitz optimization), since it is less computationally expensive than existing approaches and provides similar Lipschitz constants.",
     "keywords": ["Lipschitz constant", "Global optimization", "Gradient bounds"]},
    {"article name": "Reprint of: Optimal decomposition for distributed optimization in nonlinear model predictive control through community detection",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.10.011",
     "publication date": "08-2018",
     "abstract": "Distributed optimization, based on a decomposition of the entire optimization problem, has been applied to many complex decision making problems in process systems engineering, including nonlinear model predictive control. While decomposition techniques have been widely adopted, it remains an open problem how to optimally decompose an optimization problem into a distributed structure. In this work, we propose to use community detection in network representations of optimization problems as a systematic method of partitioning the optimization variables into groups, such that the variables in the same groups generally share more constraints than variables between different groups. The proposed method is applied to the decomposition of the optimal control problem involved in the nonlinear model predictive control of a reactor-separator process, and the quality of the resulting decomposition is examined by the resulting control performance and computational time. Our result suggests that community detection in network representations of the optimization problem generates decompositions with improvements in computational performance as well as a good optimality of the solution.",
     "keywords": ["Network decomposition", "Distributed optimization", "Community detection", "Nonlinear model predictive control"]},
    {"article name": "Elucidating and handling effects of valve-induced nonlinearities in industrial feedback control loops",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.08.008",
     "publication date": "08-2018",
     "abstract": "In this work, we investigate the effects of various types of valve behavior (e.g., linear valve dynamics and stiction) on the effectiveness of process control in a unified framework based on systems of nonlinear ordinary differential equations that characterize the dynamics of closed-loop systems including the process, valve, and controller dynamics. By analyzing the resulting dynamic models, we demonstrate that the responses of the valve output and process states when valve behavior cannot be neglected (e.g., stiction-induced oscillations in measured process outputs) are closed-loop effects that can be difficult to predict a priori due to the coupled and typically nonlinear dynamics of the process-valve model. Subsequently, we discuss the implications of this closed-loop perspective on the effects of valve dynamics in closed-loop systems for understanding valve behavior compensation techniques and developing new ones. We conclude that model-based feedback control designs that can account for process and valve constraints and dynamics provide a systematic method for handling the multivariable interactions in a process-valve system, where the models in such control designs can come either from first-principles or empirical modeling techniques. The analysis also demonstrates the necessity of accounting for valve behavior when designing a control system due to the potentially different consequences under various control methodologies of having different types of valve behavior in the loop. Throughout the work, a level control example and a continuous stirred tank reactor example are used to illustrate the developments.",
     "keywords": ["Model predictive control", "Valve dynamics", "Chemical processes", "Process control", "Stiction", "Empirical modeling"]},
    {"article name": "Combining the advantages of discrete- and continuous-time scheduling models: Part 1. Framework and mathematical formulations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.12.003",
     "publication date": "08-2018",
     "abstract": "We propose a general method for the solution of chemical production scheduling problems in network environments. The method consists of three stages. In the first stage, a discrete-time mixed-integer programming (MIP) model is solved to quickly obtain an approximate solution. In the second stage, the solution is mapped onto newly introduced unit- and material-specific continuous-time grids, using a mapping algorithm. In the third stage, a continuous-time linear programming (LP) model is solved to improve the accuracy of the mapped discrete-time solution by refining the timing of events and batch sizes. The proposed method takes advantage of the complementary strengths of discrete- and continuous-time formulations, which enables us to not only handle various processing features (e.g., intermediate deliveries and orders, time-varying resource availability and cost, variable processing times), but also obtain order of magnitude speedups in the solution of large-scale instances.",
     "keywords": ["Network environment", "Discrete- and continuous-time representation", "Solution refinement method"]},
    {"article name": "Medium-term optimization-based approach for the integration of production planning, scheduling and maintenance",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.04.030",
     "publication date": "08-2018",
     "abstract": "A medium-term optimization-based approach is proposed for the integration of production planning, scheduling and maintenance. The problem presented in this work considers a multiproduct single-stage batch process plant with parallel units and limited resources. An MILP continuous-time formulation is developed based on the main ideas of travelling salesman problem and precedence-based constraints to deal with, sequence-dependent unit performance decay, flexible recovery operations, resource availability and product lifetime. Small scheduling examples have been solved and compared with adapted formulations from the literature, based on discrete-time and global-time events, demonstrating the effectiveness of the proposed solution approach. Additional planning and scheduling problems have been proposed by considering several time periods. Multi-period examples have been efficiently solved by the model showing the applicability of the solution approach for medium-size problems.",
     "keywords": ["MILP-based approach", "Planning", "Scheduling and Maintenance", "Traveling Salesman Problem Precedence-based representation", "Performance decay", "Flexible recovery operations"]},
    {"article name": "Reprint of: Optimal scheduling of interconnected power systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.10.012",
     "publication date": "08-2018",
     "abstract": "This paper presents an optimization-based approach to address the problem of the optimal daily energy scheduling of interconnected power systems in electricity markets. More specifically, a Mixed Integer Linear Programming model (MILP) has been developed to address the specific challenges of the underlying problem. The main focus of the proposed framework is to examine the importance and the impacts of electricity interconnections and cross-border electricity trade on the scheduling of power systems, both at a technical and economic level. The applicability of the proposed approach has been tested on an illustrative case study including five power systems which can be interconnected (with a certain interconnection structure) or not. The proposed model determines in a detailed and analytical way the optimal power generation mix, the electricity trade among the systems, the electricity flows (in case of interconnection options), the marginal price of each system, as well as it investigates through a sensitivity analysis the effects of the available interconnection capacity on the resulting power production mix. The work demonstrates that the proposed optimization approach is able to provide important insights into the appropriate energy strategies followed by the market participants, as well as on the strategic long-term decisions to be implemented by investors and/or policy makers at a national and/or regional level, underlining potential risks and providing appropriate price signals on critical energy infrastructure projects under real market operating conditions.",
     "keywords": ["Energy scheduling", "Electricity interconnections", "Unit commitment", "Electricity trade", "Day-ahead market"]},
    {"article name": "Resilient design and operations of process systems: Nonlinear adaptive robust optimization model and algorithm for resilience analysis and enhancement",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.11.002",
     "publication date": "08-2018",
     "abstract": "This paper is concerned with the resilient design and operations of process systems in response to disruption events. A general framework for resilience optimization is proposed that incorporates an improved quantitative measure of resilience and a comprehensive set of resilience enhancement strategies for process design and operations. The proposed framework identifies a set of disruptive events for a given system, and then formulates a multiobjective two-stage adaptive robust mixed-integer fractional programming model to optimize the resilience and economic objectives simultaneously. The model accounts for network configuration, equipment capacities, and capital costs in the first stage, and the number of available processes and operating levels in each time period in the second stage. A tailored solution algorithm is developed to tackle the computational challenge of the resulting multi-level optimization problem. The applicability of the proposed framework is illustrated through applications on a chemical process network and a shale gas processing system.",
     "keywords": ["Resilience", "Process design and operations", "Two-stage adaptive robust optimization", "Superstructure optimization", "Mixed-integer fractional programming"]},
    {"article name": "Robust dynamic optimization of batch processes under parametric uncertainty: Utilizing approaches from semi-infinite programs",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.05.025",
     "publication date": "08-2018",
     "abstract": "The optimal solution in dynamic optimization of batch processes often exhibits active path constraints. The goal of this work is the robust satisfaction of path constraints in the presence of parametric uncertainties based on known worst-case formulations. These formulations are interpreted as semi-infinite programs (SIP). Two known SIP algorithms are extended to the dynamic case and assessed. One is a discretization approach and the other a local reduction approach. With these presented concepts, robust path constraint satisfaction is in principle guaranteed. In this work, however, local methods are used to approximate the global solution of the lower-level problem with local solvers thus allowing for (rather unlikely) constraint violations. Finally, the penicillin fermentation is introduced as a well-known case study with uncertainties, which is modified in this work by adding further dependencies. The adaptation of the SIP concepts to dynamic optimization problems are shown to be successful for this case study.",
     "keywords": ["Robust dynamic optimization", "Optimal control problem", "Parameter uncertainty", "Uncertain dynamic systems", "Semi-infinite programs"]},
    {"article name": "Reprint of: A scheduling perspective on the monetary value of improving process control",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.10.013",
     "publication date": "08-2018",
     "abstract": "The goal of quantifying the monetary value of process control has been a target of much research since the inception of the field, and methods have been developed for quantifying the value of control in the case of predominantly steady-state processes. However, there has been no attempt to quantify the monetary value of control for predominantly transient processes. In this work, we utilize the general framework of integrated scheduling and control to develop novel performance functions that enable the quantification of the monetary value of control from a scheduling perspective for a predominantly transient process. Specifically, we posit that the transition time between one product and the next in a production sequence can be used as a performance metric over which the value of control can be quantified. We demonstrate the utility of the developed performance functions using a case study of the scheduling of a multi-product CSTR.",
     "keywords": ["Value of control", "Production scheduling", "Process economics", "Process control", "Economic benefit analysis"]},
    {"article name": "Multi-parametric mixed integer linear programming under global uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.04.015",
     "publication date": "08-2018",
     "abstract": "Major application areas of the process systems engineering, such as hybrid control, scheduling and synthesis can be formulated as mixed integer linear programming (MILP) problems and are naturally susceptible to uncertainty. Multi-parametric programming theory forms an active field of research and has proven to provide invaluable tools for decision making under uncertainty. While uncertainty in the right-hand side (RHS) and in the objective function\u2019s coefficients (OFC) have been thoroughly studied in the literature, the case of left-hand side (LHS) uncertainty has attracted significantly less attention mainly because of the computational implications that arise in such a problem. In the present work, we propose a novel algorithm for the analytical solution of multi-parametric MILP (mp-MILP) problems under global uncertainty, i.e. RHS, OFC and LHS. The exact explicit solutions and the corresponding regions of the parametric space are computed while a number of case studies illustrates the merits of the proposed algorithm.",
     "keywords": ["Optimisation under uncertainty", "Multi-parametric programming", "Mixed integer linear programming", "Cylindrical algebraic decomposition", "Grobner bases", "Process scheduling"]},
    {"article name": "Advances in clean and low-carbon power generation planning",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.02.012",
     "publication date": "08-2018",
     "abstract": "Increasing global energy consumption and consequent greenhouse gas emissions pose great challenges to the sustainable development of international human society. Electricity constitutes the largest part of energy carriers, and the power sector is identified as the key sector with great carbon dioxide mitigation potential. Therefore, power generation expansion planning (GEP) problem has drawn great attention due to its important role in global energy supply, renewable energy utilization and carbon dioxide mitigation. Several important issues, including renewable energy sources integration, operating reserve, deregulated power market, demand response and carbon pricing mechanism should be incorporated in a GEP problem. Energy system engineering provides a methodological framework to address the complex energy, economic and environmental problems by adopting an integrated systematic approach, featuring superstructure-based modeling, mixed-integer programming, multi-objective optimization, and optimization under uncertainty. Recent advances of these approaches in GEP problems related to the five issues mentioned above are reviewed and discussed in this article.",
     "keywords": ["Modeling and optimization", "Power generation expansion planning", "Energy system engineering"]},
    {"article name": "A trust region-based two phase algorithm for constrained black-box and grey-box optimization with infeasible initial point",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.12.011",
     "publication date": "08-2018",
     "abstract": "This paper presents an algorithm for constrained black-box and grey-box optimization. It is based on surrogate models developed using input-output data in a trust-region framework. Unlike many current methods, the proposed approach does not require feasible initial point and can handle hard constraints via a novel optimization-based constrained sampling scheme. A two-phase strategy is employed, where the first phase involves finding feasible point through minimizing a smooth constraint violation function (feasibility phase). The second phase improves the objective in the feasible region using the solution of the feasibility phase as starting point (optimization phase). The method is applied to solve 92 test problems and the performance is compared with established derivative-free solvers. The two-phase algorithm outperforms these solvers in terms of number of problems solved and number of samples used. We also apply the algorithm to solve a chemical process design problem involving highly-coupled, nonlinear algebraic and partial differential equations.",
     "keywords": ["Constrained derivative-free optimization", "Black-box optimization", "Grey-box optimization", "Simulation-based optimization", "Surrogate model", "Data-driven optimization"]},
    {"article name": "Elucidating the multi-targeted anti-amyloid activity and enhanced islet amyloid polypeptide binding of \u03b2-wrapins",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.02.013",
     "publication date": "08-2018",
     "abstract": "\u03b2-wrapins are engineered binding proteins stabilizing the \u03b2-hairpin conformations of amyloidogenic proteins islet amyloid polypeptide (IAPP), amyloid-\u03b2, and \u03b1-synuclein, thus inhibiting their amyloid propensity. Here, we use computational and experimental methods to investigate the molecular recognition of IAPP by \u03b2-wrapins. We show that the multi-targeted, IAPP, amyloid-\u03b2, and \u03b1-synuclein, binding properties of \u03b2-wrapins originate mainly from optimized interactions between \u03b2-wrapin residues and sets of residues in the three amyloidogenic proteins with similar physicochemical properties. Our results suggest that IAPP is a comparatively promiscuous \u03b2-wrapin target, probably due to the low number of charged residues in the IAPP \u03b2-hairpin motif. The sub-micromolar affinity of \u03b2-wrapin HI18, specifically selected against IAPP, is achieved in part by salt-bridge formation between HI18 residue Glu10 and the IAPP N-terminal residue Lys1, both located in the flexible N-termini of the interacting proteins. Our findings provide insights towards developing novel protein-based single- or multi-targeted therapeutics.",
     "keywords": ["Protein aggregation", "Intrinsically disordered proteins", "\u03b1-synuclein", "Amyloid-\u03b2", "Amylin", "Molecular dynamics"]},
    {"article name": "Reactive scheduling of crude oil using structure adapted genetic algorithm under multiple uncertainties",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.04.005",
     "publication date": "08-2018",
     "abstract": "Crude oil processed in marine access refineries contributes about 15% of the total energy production worldwide. An optimized schedule of crude unloading and charging in these offers the best utilization of available resources to increase the profitability and also helps in incorporating the future uncertainties commonly encountered in the operation. In the present study, a new reactive crude oil scheduling methodology is developed for marine-access refinery using a structured adapted genetic algorithm to handle the commonly encountered uncertainties of increase in demand and ship arrival delay. Three different industrial examples with 21, 21 and 42 periods are solved for above uncertainties with single and multiple objectives. In the single-objective formulation, profit is maximized whereas in multi-objective formulation an additional objective of inter-period deviation in crude flow to distillation units is minimized. The results obtained show the efficient handling of uncertainties with improved profitability and operability of the plant.",
     "keywords": ["Reactive scheduling", "Evolutionary algorithm", "Crude oil scheduling", "VLCC delay", "Demand increase"]},
    {"article name": "Reprint of: Expansion development planning of thermocracking-based bitumen upgrading plant under uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.10.014",
     "publication date": "08-2018",
     "abstract": "Expansion development of upgrading plants is an important decision to make for the oil sands industry. In this paper, we propose a multistage stochastic expansion development method to tackle uncertain synthetic crude oil (SCO) and CO2 tax prices. The linear decision rule based technique is applied to solve the proposed stochastic optimization model. Various analyses are conducted based on optimization results: (i) effects of the uncertainty set size, (ii) comparison of solutions for selected pessimistic, realistic, and optimistic scenarios, (iii) effects of different operating modes for an upgrading plant, and (iv) cost distribution. Results of this work demonstrate that the stochastic model provides a more flexible, economical, and robust solution compared to the deterministic solution. In addition, the CO2 tax price affects the optimal solution negligibly compared to the SCO price. Finally, expansion development of the studied upgrading plant is economically beneficial even at the current market state.",
     "keywords": ["Bitumen upgrading", "Capacity expansion", "Stochastic programming", "Linear decision rule"]},
    {"article name": "Modeling and solution for steelmaking scheduling with batching decisions and energy constraints",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.03.010",
     "publication date": "08-2018",
     "abstract": "This paper investigates a practical steelmaking scheduling problem with batching decisions and energy constraints, in which batching is used to decide how to group and sequence a number of jobs to form job groups so as to meet batch production mode. Incorporating energy consideration into the scheduling is motivated by practical demand and the potential to reduce the energy bill through optimal scheduling. Based upon our proposed energy expressions, an MINLP model is formulated and solved using the spatial branch and bound (B&B) algorithm, which is enhanced by the proposed decomposition strategy working as a node heuristic. The benefits of the integrated scheduling and some sensitivity analysis experiments are reported on an illustrative example. Experiments on randomly generated instances show that the energy expressions are superior to the one of Hadera et\u00a0al. (2015), the B&B outperforms the state-of-the-art commercial solvers and the decomposition strategy performs well running as an independent heuristic.",
     "keywords": ["Steelmaking scheduling", "Batching", "Energy optimization", "Mixed integer nonlinear program", "Spatial branch and bound", "Decomposition"]},
    {"article name": "Semantically-enabled repositories in multi-disciplinary domains: The case of biorefineries",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.04.022",
     "publication date": "08-2018",
     "abstract": "There is an increased use of problem representations (i.e. superstructures in synthesis problems; networks in route problems; graphs; ordered graphs in various systems representations) following on significant advances in optimization technologies that hold capabilities to solve, robustly, large-scale problems. In an attempt to systematically tackle disparate domains and build high-throughput functions, the paper contributes with a semantically-enabled approach systematized and engineered by ontologies. The aim is to develop an intelligent environment with capabilities to build and scale-up system representations, automatically. The work is demonstrated on problems akin to biorenewables and biorefineries; an identical approach is possible to the general problem. Using relations and rules defined among entities, semantics are deployed to model and expand domains (biorefinery pathways) whereas enabling extracting and creating knowledge. The repository, already on a web-based platform and available as open-source, essentially upgrades conventional representations with capabilities to share (import/export) and integrate its content externally.",
     "keywords": ["Ontology engineering", "Biorefineries", "Biorenewables", "Repository", "Synthesis of value chain"]},
    {"article name": "The design of optimal mixtures from atom groups using Generalized Disjunctive Programming",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.01.016",
     "publication date": "08-2018",
     "abstract": "A comprehensive computer-aided mixture/blend design methodology for formulating a general mixture design problem where the number, identity and composition of mixture constituents are optimized simultaneously is presented in this work. Within this approach, Generalized Disjunctive Programming (GDP) is employed to model the discrete decisions (number and identities of mixture ingredients) in the problems. The identities of the components are determined by designing molecules from UNIFAC groups. The sequential design of pure compounds and blends, and the arbitrary pre-selection of possible mixture ingredients can thus be avoided, making it possible to consider large design spaces with a broad variety of molecules and mixtures. The proposed methodology is first applied to the design of solvents and solvent mixtures for maximising the solubility of ibuprofen, often sought in crystallization processes; next, antisolvents and antisolvent mixtures are generated for minimising the solubility of the drug in drowning out crystallization; and finally, solvent and solvent mixtures are designed for liquid\u2013liquid extraction. The GDP problems are converted into mixed-integer form using the big-M approach. Integer cuts are included in the general models leading to lists of optimal solutions which often contain a combination of pure and mixed solvents.",
     "keywords": ["Mixture design", "Generalized Disjunctive Programming", "UNIFAC groups", "Crystallization", "Antisolvent", "Liquid\u2013liquid extraction"]},
    {"article name": "Reprint of: Heuristics with performance guarantees for the minimum number of matches problem in heat recovery network design",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.10.015",
     "publication date": "08-2018",
     "abstract": "Heat exchanger network synthesis exploits excess heat by integrating process hot and cold streams and improves energy efficiency by reducing utility usage. Determining provably good solutions to the minimum number of matches is a bottleneck of designing a heat recovery network using the sequential method. This subproblem is an NP -hard mixed-integer linear program exhibiting combinatorial explosion in the possible hot and cold stream configurations. We explore this challenging optimization problem from a graph theoretic perspective and correlate it with other special optimization problems such as cost flow network and packing problems. In the case of a single temperature interval, we develop a new optimization formulation without problematic big-M parameters. We develop heuristic methods with performance guarantees using three approaches: (i) relaxation rounding, (ii) water filling, and (iii) greedy packing. Numerical results from a collection of 51 instances substantiate the strength of the methods.",
     "keywords": ["Minimum number of matches", "Heat exchanger network design", "Heuristics", "Approximation algorithms", "Mixed-integer linear optimization"]},
    {"article name": "Generalized robust counterparts for constraints with bounded and unbounded uncertain parameters",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.09.007",
     "publication date": "08-2018",
     "abstract": "Robust optimization has emerged as a powerful and efficient methodology for incorporating uncertain parameters into optimization models. In robust optimization, robust counterparts for uncertain constraints are created by imposing a known set of uncertain parameter realizations onto the new robust constraint. For constraints with all bounded parameters, the interval\u00a0+\u00a0ellipsoidal and interval\u00a0+\u00a0polyhedral uncertainty sets are well-established in robust optimization literature, while box, ellipsoidal, or polyhedral sets may be used for unbounded parameters. However, there has yet to be any counterparts proposed for constraints that simultaneously contain both bounded and unbounded parameters. This is crucial, as using the traditional box, ellipsoidal, or polyhedral sets with bounded parameters may impose impossible parameter realizations outside of their bounds, unnecessarily increasing the conservatism of results. In this work, robust counterparts for uncertain constraints with both bounded and unbounded uncertain parameters are derived: the generalized interval\u00a0+\u00a0box, generalized interval\u00a0+\u00a0ellipsoidal, and generalized interval\u00a0+\u00a0polyhedral counterparts. These counterparts reduce to the traditional box, ellipsoidal, and polyhedral counterparts if all parameters are unbounded, and reduce to the traditional interval\u00a0+\u00a0ellipsoidal and interval\u00a0+\u00a0polyhedral counterparts if all parameters are bounded. It is proven that established a priori probabilistic bounds remain valid for these counterparts. The importance of these developments is demonstrated with computational examples, showing the reduction of conservatism that is gained by appropriately limiting the possible realizations of the bounded parameters. The developments increase the scope and applicability of robust optimization as a tool for optimization under uncertainty.",
     "keywords": ["Optimization under uncertainty", "Robust optimization", "Probabilistic bounds", "Mathematical modeling"]},
    {"article name": "Municipal solid waste to liquid transportation fuels \u2013 Part III: An optimization-based nationwide supply chain management framework",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.10.034",
     "publication date": "08-2018",
     "abstract": "An optimization-based supply chain management framework for municipal solid waste (MSW) to liquid transportation fuels (WTL) processes is presented. First, a thorough analysis of landfill operations and annual amounts of MSW that are deposited across the contiguous United States is conducted and compared with similar studies. A quantitative supply chain framework that simultaneously accounts for the upstream and downstream WTL value chain operations is then presented. A large-scale mixed-integer linear optimization model that captures the interactions among MSW feedstock availabilities and locations, WTL refinery locations, and product delivery locations and demand capacities is described. The model is solved for both the nationwide and statewide WTL supply chains across numerous case studies. The results of the framework yield insights into the strategic placement of WTL refineries in the United States as well as topological information on the feedstock and product flows. The results suggest that large-scale WTL supply chains can be competitive, with breakeven oil prices ranging between $64\u2013$77 per barrel.",
     "keywords": ["Supply chain management framework", "Waste-to-liquids process", "Mixed integer linear optimization", "Transportation fuels"]},
    {"article name": "Optimal design of energy systems using constrained grey-box multi-objective optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.02.017",
     "publication date": "08-2018",
     "abstract": "The (global) optimization of energy systems, commonly characterized by high-fidelity and large-scale complex models, poses a formidable challenge partially due to the high noise and/or computational expense associated with the calculation of derivatives. This complexity is further amplified in the presence of multiple conflicting objectives, for which the goal is to generate trade-off compromise solutions, commonly known as Pareto-optimal solutions. We have previously introduced the p-ARGONAUT system, parallel AlgoRithms for Global Optimization of coNstrAined grey-box compUTational problems, which is designed to optimize general constrained single-objective grey-box problems by postulating accurate and tractable surrogate formulations for all unknown equations in a computationally efficient manner. In this work, we extend p-ARGONAUT towards multi-objective optimization problems and test the performance of the framework, both in terms of accuracy and consistency, under many equality constraints. Computational results are reported for a number of benchmark multi-objective problems and a case study of an energy market design problem for a commercial building, while the performance of the framework is compared with other derivative-free optimization solvers.",
     "keywords": ["Derivative-free optimization", "Grey/black-box optimization", "Multi-objective optimization", "Energy systems engineering"]},
    {"article name": "Reprint of: Big data approach to batch process monitoring: Simultaneous fault detection and diagnosis using nonlinear support vector machine-based feature selection",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.10.016",
     "publication date": "08-2018",
     "abstract": "This paper presents a novel data-driven framework for process monitoring in batch processes, a critical task in industry to attain a safe operability and minimize loss of productivity and profit. We exploit high dimensional process data with nonlinear Support Vector Machine-based feature selection algorithm, where we aim to retrieve the most informative process measurements for accurate and simultaneous fault detection and diagnosis. The proposed framework is applied to an extensive benchmark data set which includes process data describing 22,200 batches with 15 faults. We train fault and time-specific models on the pre-aligned batch data trajectories via three distinct time horizon approaches: one-step rolling, two-step rolling, and evolving which varies the amount of data incorporation during modeling. The results show that two-step rolling and evolving time horizon approaches perform superior to the other. Regardless of the approach, proposed framework provides a promising decision support tool for online simultaneous fault detection and diagnosis for batch processes.",
     "keywords": ["Process monitoring", "Data-driven modeling", "Big data", "Feature selection", "Support vector machines"]},
    {"article name": "Reprint of: Enhancing natural gas-to-liquids (GTL) processes through chemical looping for syngas production: Process synthesis and global optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.10.017",
     "publication date": "08-2018",
     "abstract": "A process synthesis and global optimization framework is presented to determine the most profitable routes of producing liquid fuels from natural gas through competing technologies. Chemical looping is introduced into the framework for the first time as a natural gas conversion alternative. The underlying phenomena in chemical looping are complex and models from methods such as computational fluid dynamics are unsuitable for global optimization. Therefore, appropriate approximate models are required. Parameter estimation and disjunctive programming are described here for modeling two chemical looping processes. The first is a nickel oxide based process developed at CSIC in Spain; the second is a iron oxide based process developed at Ohio State University. These mathematical models are then incorporated into a comprehensive process superstructure to evaluate the performance of chemical looping against technologies such as autothermal reforming and steam reforming for syngas production. The rest of the superstructure consists of process alternatives for liquid fuels production from syngas and simultaneous heat, power, and water integration. Among the various case studies considered, it is shown that chemical looping can reduce the break-even oil prices for natural gas-to-liquids processes by as much as 40%, while satisfying production demands and obeying environmental constraints. For a natural gas price of $5/TSCF, the break-even price is as low as $32.10/bbl. Sensitivity analysis shows that these prices for chemical looping remain competitive even as natural gas cost rises. The findings suggest that chemical looping is a very promising option to enhance natural gas-to-liquids processes and their capabilities.",
     "keywords": ["Chemical looping", "GTL", "Process synthesis", "Global optimization", "Parameter estimation", "Disjunctive programming"]},
    {"article name": "Fault detection and diagnosis using empirical mode decomposition based principal component analysis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.03.022",
     "publication date": "07-2018",
     "abstract": "This paper presents a new algorithm to identify and diagnose stochastic faults in Tennessee Eastman (TE) process. The algorithm combines Ensemble Empirical Mode Decomposition (EEMD) with Principal Component Analysis (PCA) and Cumulative Sum (CUSUM) to diagnose a group of faults that could not be properly detected and/or diagnosed with previously reported techniques. This algorithm includes three steps: measurements pre-filtering, fault detection, and fault diagnosis. Measured variables are first decomposed into different scales using the EEMD-based PCA, from which fault signatures can be extracted for fault detection and diagnosis (FDD). The T2 and Q statistics-based CUSUMs are further applied to improve fault detection, where a set of PCA models are developed from historical data to characterize anomalous fingerprints that are correlated with each fault for accurate fault diagnosis. The algorithm developed in this paper can successfully identify and diagnose both individual and simultaneous occurrences of stochastic faults.",
     "keywords": ["Process monitoring and control", "Stochastic faults", "Uncertainty analysis", "System engineering", "Process data analytics"]},
    {"article name": "Distributed fault diagnosis for networked nonlinear uncertain systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.03.026",
     "publication date": "07-2018",
     "abstract": "In this work, we address the problem of simultaneous fault diagnosis in nonlinear uncertain networked systems utilizing a distributed fault detection and isolation (FDI) strategy. The key idea is to design a bank of local FDI (LFDI) schemes that communicate with each other for improved FDI. The proposed distributed FDI scheme is shown to be able to handle local faults as well as those that affect more than one subsystem. This is achieved via appropriate adaptation of the LFDI filters based on information exchange with other subsystems and using the proposed notion of detectability index. The detectability index and isolability conditions are rigorously derived for the distributed FDI scheme. Effectiveness of the proposed methodology is shown via application to a reactor-separator process subject to uncertainty and measurement noise.",
     "keywords": ["Distributed fault diagnosis", "Actuator faults", "Sensor faults", "Nonlinear systems", "Uncertainty", "High-gain observers,"]},
    {"article name": "Steady-state real-time optimization using transient measurements",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.03.021",
     "publication date": "07-2018",
     "abstract": "Real-time optimization (RTO) is an established technology, where the process economics are optimized using rigourous steady-state models. However, a fundamental limiting factor of current static RTO implementation is the steady-state wait time. We propose a \u201chybrid\u201d approach where the model adaptation is done using dynamic models and transient measurements and the optimization is performed using static models. Using an oil production network optimization as case study, we show that the Hybrid RTO can provide similar performance to dynamic optimization in terms of convergence rate to the optimal point, at computation times similar to static RTO. The paper also provides some discussions on static versus dynamic optimization problem formulations.",
     "keywords": ["Real-time optimization", "Steady-state optimization", "Dynamic models", "Production optimization", "Hybrid RTO"]},
    {"article name": "Big data approach to batch process monitoring: Simultaneous fault detection and diagnosis using nonlinear support vector machine-based feature selection",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.03.025",
     "publication date": "07-2018",
     "abstract": "This paper presents a novel data-driven framework for process monitoring in batch processes, a critical task in industry to attain a safe operability and minimize loss of productivity and profit. We exploit high dimensional process data with nonlinear Support Vector Machine-based feature selection algorithm, where we aim to retrieve the most informative process measurements for accurate and simultaneous fault detection and diagnosis. The proposed framework is applied to an extensive benchmark data set which includes process data describing 22,200 batches with 15 faults. We train fault and time-specific models on the pre-aligned batch data trajectories via three distinct time horizon approaches: one-step rolling, two-step rolling, and evolving which varies the amount of data incorporation during modeling. The results show that two-step rolling and evolving time horizon approaches perform superior to the other. Regardless of the approach, proposed framework provides a promising decision support tool for online simultaneous fault detection and diagnosis for batch processes.",
     "keywords": ["Process monitoring", "Data-driven modeling", "Big data", "Feature selection", "Support vector machines"]},
    {"article name": "A methodology to reduce the computational cost of transient multiphysics simulations for waste vitrification",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.03.027",
     "publication date": "07-2018",
     "abstract": "Legacy radioactive waste stored in tanks at the Hanford Site is scheduled to undergo vitrification in Joule-heated melters. A carefully calibrated computational fluid dynamics model has been developed to characterize fluid flow, chemistry and heat transfer in the melters. Bubbling is replaced by momentum source terms to approximate forced convection circulation patterns and reduce Courant number restrictions on the resolved liquid\u2013air interface. Void zones in the electrical field compensate for the removal of bubbles. The efficiency of the radiation solver is improved by reducing the update frequency of the discrete ordinates and using lower quadrature. A simple polynomial fit captures the waste-to-glass reactions in the cold cap. These simplifications reduce the turnaround time such that it is possible to simulate hundreds of seconds of physical time per day with the calibrated model versus only several seconds of physical time with the original, higher-fidelity model.",
     "keywords": ["Waste glass melter", "Vitrification", "Computational multiphase fluid dynamics", "Transient simulation"]},
    {"article name": "A multistream heat exchanger model with enthalpy feasibility",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.03.023",
     "publication date": "07-2018",
     "abstract": "A temperature feasibility constraint is an important part of multistream heat exchanger (MSHE) modeling. However, temperature feasibility of an MSHE model makes a numerical issue when a physical property package is used to obtain highly accurate temperature-enthalpy relationships in equation-oriented modeling environment. To resolve the issue, this study proposes a new MSHE model with enthalpy feasibility using the fact that enthalpy is a monotonically increasing function of temperature. A natural gas liquefaction process, called a single mixed refrigeration process, is optimized using the proposed MSHE model under an equation-oriented modeling environment with a physical property package as a case study.",
     "keywords": ["Pinch analysis", "Equation-oriented optimization", "LNG plant", "Single mixed refrigerant process", "PRICO process"]},
    {"article name": "Optimal synthesis of periodic sorption enhanced reaction processes with application to hydrogen production",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.04.004",
     "publication date": "07-2018",
     "abstract": "A systematic design and synthesis framework for multi-step, multi-mode and periodic sorption-enhanced reaction processes (SERP) is presented. The formulated nonlinear algebraic and partial differential equation (NAPDE)-based model simultaneously identifies optimal SERP cycle configurations, design specifications and operating conditions. Key modeling contributions include a generalized boundary-condition formulation and a representation that enables the selection of discrete operation modes and flow directions using continuous pressure variables. A simulation-based constrained grey-box optimization strategy is employed to obtain optimal cycles and design parameters. The framework has been used for designing two SERP systems, namely sorption-enhanced steam methane reforming (SE-SMR) and sorption-enhanced water gas shift reaction (SE-WGSR), for maximizing hydrogen productivity and minimizing hydrogen-production cost. Specifically, a cyclic SE-SMR process is designed that obtains 95% pure hydrogen from natural gas with 35% higher productivity and 10.86% lower cost compared to existing small-scale, distributed systems. The developed synthesis framework can also be applied for other applications.",
     "keywords": ["Sorption enhanced reaction process", "Cycle design", "Cyclic process synthesis", "Grey-Box optimization", "Process Intensification"]},
    {"article name": "A CFD simulation study of boiling mechanism and BOG generation in a full-scale LNG storage tank",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.04.003",
     "publication date": "07-2018",
     "abstract": "Despite heavy insulation, the unavoidable heat leak from the surroundings into an LNG (Liquefied Natural Gas) storage tank causes boil-off-gas (BOG) generation. A comprehensive dynamic CFD simulation of an onshore full-scale LNG tank in a regasification terminal is presented. LNG is approximated as pure methane, the axisymmetric VOF (Volume of Fluid) model is used to track the vapour-liquid interface, and the Lee model is employed to account for the phase change including the effect of static pressure. An extensive investigation of the heat ingress magnitude, internal flow dynamics, and convective heat transfer gives useful insights on the boiling phenomena and a reliable quantification of the BOG. Surface evaporation is the governing boiling mechanism and nucleate boiling is unlikely with proper insulation. The critical wall superheat marking the transition from surface evaporation to nucleate boiling is estimated as 2.5\u20132.8\u202fK for LNG.",
     "keywords": ["LNG", "Simulation", "Storage tank", "BOG", "CFD", "Boiling mechanism"]},
    {"article name": "Optimization-based approach for maximizing profitability of bioethanol supply chain in Brazil",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.04.001",
     "publication date": "07-2018",
     "abstract": "In this work, a mathematical approach for optimizing and planning the Brazilian bioethanol supply chains (SC) is presented. The optimization problem has an MILP formulation, aiming to maximize the net present value (NPV) of the entire SC of the sugar and bioethanol sector in Brazil. The model takes into account seven different production technologies, two types of warehouses, three types of transportation modes and seven exportation options, whose data were obtained from Brazilian industrial practices. The model aims to propose the optimal configuration of a bioethanol network, that is, the locations of the production and storage facilities, their capacity of expansion policy, the technology selected for manufacturing and materials storage and the flows of all feedstock and final products involved in the bioethanol SC in Brazil. A comparison between the current situation of Brazilian bioethanol SC and the optimal configuration achieved by the proposed model is also included.",
     "keywords": ["Mathematical programming", "Optimization", "MILP", "Supply chain management", "Bioethanol"]},
    {"article name": "A methodology to restructure a pipeline system for an oilfield in the mid to late stages of development",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.04.008",
     "publication date": "07-2018",
     "abstract": "One important issue in the mid to late development stages of oilfields is maintaining stable production, especially when the existing gathering pipeline system cannot fully satisfy the development of low pressures and low production rates. In these cases, it is necessary to restructure the original gathering pipeline system. In this study, an optimal design method is proposed to restructure a pipeline system in an oilfield in the mid to late development stages. Based on the demand of stable production and the existing structure of the pipeline system, a mixed-integer nonlinear programming (MINLP) model with an objective function that minimizes the total cost is developed. Hydraulic, technical and economic constraints are considered. The model is linearized based on a piecewise method and solved by the branch-and-bound algorithm. This method is applied to a real case study of a pipeline system in an oilfield.",
     "keywords": ["Restructuring", "Optimization", "Pipeline network", "Oilfield"]},
    {"article name": "Quality-relevant independent component regression model for virtual sensing application",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.04.006",
     "publication date": "07-2018",
     "abstract": "Independent component regression (ICR) is an efficient method for tackling non-Gaussian problems. In this work, the defects of the conventional ICR are analyzed, and a novel quality-relevant independent component regression (QR-ICR) method based on distance covariance and distance correlation is proposed. QR-ICR extracts independent components (ICs) using a quality-relevant independent component analysis (QR-ICA) algorithm, which simultaneously maximizes the non-Gaussianity of ICs and statistical dependency between ICs and quality variables. Meanwhile, two new types of statistical criteria, called cumulative percent relevance (CPR) and Max-Dependency (Max-Dep), are proposed to rank the order and determine the number of ICs according to their contributions to quality variables. The proposed QR-ICR(CPR) and QR-ICR(Max-Dep) methods were validated through a vinyl acetate monomer production process and a benchmark near-infrared spectral data. The results have demonstrated that the proposed QR-ICR(CPR) and QR-ICR(Max-Dep) provide simpler predictive models and give better prediction performances than PLS, ICR, ICR(CPR), and ICR(Max-Dep).",
     "keywords": ["Virtual sensing", "Independent component regression", "Distance covariance and distance correlation", "Quality-relevant independent component regression", "Vinyl acetate monomer process", "Near-infrared spectral analysis"]},
    {"article name": "Application of neural networks for optimal-setpoint design and MPC control in biological wastewater treatment",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.04.007",
     "publication date": "07-2018",
     "abstract": "This paper addresses both the design of an optimal variable setpoint and a setpoint-tracking control loop for the dissolved oxygen concentration in a biological wastewater treatment process. Although exact knowledge of influent changes during rain/storm events is unrealistic, we take advantage of the fact that during dry weather conditions the influent changes are periodic and thus predictable. Specifically, a nonlinear optimization procedure utilizes dry weather data to decide on a nominal fixed setpoint, or a weighting gain, or both; during weather events an algorithm uses the optimization solution(s) together with the ammonium predictions to adjust the setpoint dynamically (responding appropriately to significant changes in the influent). A constrained nonlinear neural-network model predictive control tracks the setpoint. Simulations with the BSM1 compare several variations of the proposed methods to a fixed-setpoint PI control, demonstrating improvement in effluent quality or reduction in energy use, or both.",
     "keywords": ["BSM1", "Neural network control", "Model predictive control", "Biological wastewater treatment"]},
    {"article name": "Optimization of dimethyl ether production process based on sustainability criteria using a homotopy continuation method",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.03.014",
     "publication date": "07-2018",
     "abstract": "Traditional criteria for designing processes that utilize only economic aspects can have a negative impact on the environment and society. In this study, a process for the production of dimethyl ether (DME) from methanol is evaluated by employing sustainability metrics. Operational conditions are optimized by implementing a rigorous global multi-objective optimization algorithm based on maximization of economic performance measured by the return on investment (ROI) and minimization of environmental and social impacts. The most efficient operational conditions for DME production based on sustainability criteria are obtained with a homotopy continuation method in conjunction with a process simulator. The resulting conditions indicate that the global warming metric of the DME process is decreased more than 10 times and the decrease of photochemical smog formation, mass intensity and energy intensity is 96%, 12% and 9%, respectively. The optimized, sustainable process shows an only an insignificant reduction in terms of economic aspect.",
     "keywords": ["Sustainable process", "Multi-objective optimization", "Homotopy continuation", "DME production"]},
    {"article name": "Integrating operations and control: A perspective and roadmap for future research",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.04.011",
     "publication date": "07-2018",
     "abstract": "This \u201cwhite paper\u201d is a concise perspective based on a session during FIPSE 3, held in Rhodes, Greece, June 20\u201323, 2016. This was the third conference in the series \u201cFuture Innovation in Process Systems Engineering\u201d (http://fi-in-pse.org), which takes place every other year in Greece, with a limited number of participants and just three topics/sessions whose objective is to pose and discuss open research challenges in Process Systems Engineering. This specific session comprised invited talks by Sigurd Skogestad and Iiro Harjunkoski, followed by short presentations by the participants and extensive discussions. The paper does not intend to provide a comprehensive review on the subject, or a detailed exposition of the concepts and problems. Its aim is to highlight open problems and directions for future research.",
     "keywords": ["Plant-wide control", "Operations", "Integration", "Scheduling", "Planning"]},
    {"article name": "Deep convolutional neural network model based chemical process fault diagnosis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.04.009",
     "publication date": "07-2018",
     "abstract": "Numerous accidents in chemical processes have caused emergency shutdowns, property losses, casualties and/or environmental disruptions in the chemical process industry. Fault detection and diagnosis (FDD) can help operators timely detect and diagnose abnormal situations, and take right actions to avoid adverse consequences. However, FDD is still far from widely practical applications. Over the past few years, deep convolutional neural network (DCNN) has shown excellent performance on machine-learning tasks. In this paper, a fault diagnosis method based on a DCNN model consisting of convolutional layers, pooling layers, dropout, fully connected layers is proposed for chemical process fault diagnosis. The benchmark Tennessee Eastman (TE) process is utilized to verify the outstanding performance of the fault diagnosis method.",
     "keywords": ["Fault diagnosis", "Deep convolutional neural network", "Alarm management", "Tennessee Eastman process"]},
    {"article name": "A sustainable process design to produce diethyl oxalate considering NOx elimination",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.04.002",
     "publication date": "07-2018",
     "abstract": "Diethyl oxalate (DEO) is widely used in fine chemical industry. In comparison with traditional esterification process, carbon monoxide coupling process is a novel routine for DEO production. This environmentally friendly process provides better selectivity and yield. Its unique feature is that a closed regeneration-coupling circulation is formed. Toxic byproduct-nitric oxide (NO) from coupling reaction is recycled to re-produce ethyl nitrite through regeneration reaction. This avoids significant amount of NOx emission. However, due to a few NOx emission, a contaminant handling system is applied for environmental protection. A systematical environmental analysis is also carried out to assess this process. Regeneration-coupling circulation brings interaction behaviors and some trade-offs including reactor size and recycle flowrate, regeneration and coupling reaction, loss of reactants and NO emission. Thus, a rigorous steady simulation is established to investigate these trade-offs. Then DEO process is optimized to obtain the optimal design. Finally a more economic flowsheet to produce DEO is proposed.",
     "keywords": ["Design", "Diethyl oxalate", "NOx emission", "Contaminant handling system", "Systematical environmental analysis"]},
    {"article name": "A multi-objective optimization approach for selection of energy storage systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.04.014",
     "publication date": "07-2018",
     "abstract": "Energy storage systems (ESS) are becoming an essential component of energy supply and demand matching. It is important yet complex to find preferable energy storage technologies for a specific application. In this paper, a decision support tool for energy storage selection is proposed; adopting a multi-objective optimization approach based on an augmented \u03b5-constraint method, to account technical constraints, economic and environmental objectives. A series of case studies on the optimal selection of energy storage technology for the general grid-scale applications in centralized energy systems and rising applications related to distributed energy systems are carried out. The result of the matching between the selected technologies and applications for the general centralized energy systems is consistent with the one suggested in the literature and technology roadmaps. For the emerging distributed energy systems, flow batteries, hydrogen energy storage, and Powerwall 2 turned out to be optimal energy storage alternatives for the applications.",
     "keywords": ["Energy storage", "Technology evaluation", "Optimization", "Techno-economic", "Environment", "Multi-criteria decision making"]},
    {"article name": "Synthesis of mass exchange networks: A novel mathematical programming approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.04.012",
     "publication date": "07-2018",
     "abstract": "Mass integration is an efficient tool to prevent pollution derived from chemical processes. In recent years, several advances have been reported in methodologies for the design of mass exchange networks (MENs), using approaches as the pinch point and mathematical programming. Recently, simultaneous design and optimization methodologies have been proposed. The design methods based on simultaneous optimization offer the possibility of synthetizing MENs in a single step, formulating the synthesis problem as a mixed-integer nonlinear programming problem (MINLP). In this work, a MINLP model for the synthesis of mass exchange networks is presented. The model is based on a superstructure represented with disjunctions. The proposed superstructure is obtained from a state-task-network representation and is applied to a process for copper removal from an etching process. The obtained mass exchange network showed a lower total annual cost than the networks previously reported for the same process.",
     "keywords": ["Mass exchange networks", "State-task-network representation", "Mathematical programming"]},
    {"article name": "A decision support platform for a bio-based supply chain: Application to the region of Lower Saxony and Bremen (Germany).",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.03.024",
     "publication date": "07-2018",
     "abstract": "In this work, a biomass supply chain model, for the region of Lower Saxony and Bremen in northern Germany, has been developed. Because of Germany's high demand for biofuels, the production and distribution of levulinic acid and bioethanol by wheat straw is studied. An illustrative bio-based supply chain model is developed and implemented in the Advanced Interactive Multidimensional Modeling (AIMMS) software. Then, this model is used to study the logistics, network optimization, transportation and inventory management, and the resulting environmental and economic impacts. In the end, a sensitivity analysis is conducted to evaluate the influence of key model parameters on these impacts. The results showed that a wheat straw supply chain network is profitable in the area of Bremen and Lower Saxony even though the bioproducts demand is not fully covered and that the transportation costs did not have a strong impact on the supply chain network.",
     "keywords": ["Wheat straw", "Bio-based supply chain", "Levulinic acid", "Bioethanol", "Optimization", "AIMMS"]},
    {"article name": "Errata: Heat exchanger network cleaning scheduling: From optimal control to mixed-integer decision making",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.03.028",
     "publication date": "07-2018",
     "abstract": "Errata to the article by Al Ismaili et\u00a0al. (2018) on the optimal scheduling of cleaning actions for Heat Exchanger Networks subject to fouling are presented. Errors present in the equations of the Pontryagin Minimum Principle analysis of the original article are indicated and rectified. It is noted that despite these errors, there is no change to the conclusions of the analysis given in Al Ismaili et\u00a0al. (2018).",
     "keywords": ["Optimal control problem", "Bang-bang control", "Fouling", "Optimisation", "Scheduling", "Heat exchanger networks"]},
    {"article name": "Active directional modifier adaptation for real-time optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.02.016",
     "publication date": "07-2018",
     "abstract": "Modifier adaptation is a real-time optimization (RTO) methodology that uses plant gradient estimates to correct model gradients, thereby driving the plant to optimality. However, obtaining accurate gradient estimates requires costly plant experiments at each RTO iteration. In directional modifier adaptation (DMA), the model gradients are corrected only in a small subspace of the input space, thus requiring fewer plant experiments. DMA selects the input subspace offline based on the local sensitivity of the Lagrangian gradient with respect to the uncertain model parameters. Here, we propose an extension, whereby the input subspace is selected at each RTO iteration via global sensitivity analysis, thus making the approach more reactive to changes and robust to large parametric uncertainties. Simulation results performed on the run-to-run optimization of two different semi-batch reactors show that the proposed approach finds a nice balance between experimental cost and optimality.",
     "keywords": ["Real-time optimization", "Plant-model mismatch", "Modifier adaptation", "Input dimension reduction"]},
    {"article name": "NARX modeling for real-time optimization of air and gas compression systems in chemical processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.04.026",
     "publication date": "07-2018",
     "abstract": "This study considers the Nonlinear Autoregressive eXogenous Neural Net model (NARX NN) based real-time optimization (RTO) for industrial-scale air & gas compression system in a commercial terephthalic acid manufacturing plant. NARX model is constructed to consider time-dependent system characteristics using actual plant operation data. The prediction performance is improved by extracting the thermodynamic characteristics of the chemical process as a feature of this model. And a systematic RTO method is suggested for calculating an optimal operating condition of compression system by recursively updating the NARX model. The performance of the proposed NARX model and RTO methodology is exemplified with a virtual plant that simulates the onsite commercial plant with 99.6% accuracy. NARX with feature extraction model reduces mean squared prediction error with the actual plant data 43.5% compared to that of the simple feed-forward multi-perceptron neural networks. The proposed RTO method suggests optimal operating conditions that reduce power consumption 4%.",
     "keywords": ["NARX", "NN", "Real time optimization", "Multi-stage compressor", "Industrial scale plant", "Process systems engineering"]},
    {"article name": "Iterative peptide synthesis in membrane cascades: Untangling operational decisions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.04.024",
     "publication date": "07-2018",
     "abstract": "Membrane enhanced peptide synthesis (MEPS) combines liquid-phase synthesis with membrane filtration, avoiding time-consuming separation steps such as precipitation and drying. Although performing MEPS in a multi-stage cascade is advantageous over a single-stage configuration in terms of overall yield, this is offset by the complex combination of operational variables such as the diavolume and recycle ratio in each diafiltration process. This research aims to tackle this problem using dynamic process simulation. The results suggest that the two-stage membrane cascade improves the overall yield of MEPS significantly from 72.2% to 95.3%, although more washing is required to remove impurities as the second-stage membrane retains impurities together with the anchored peptide. This clearly indicates a link between process configuration and operation. While the case study is based on the comparison of single-stage and two-stage MEPS, the results are transferable to other biopolymers such as oligonucleotides, and more complex system configurations (e.g. three-stage MEPS).",
     "keywords": ["Membrane enhanced peptide synthesis", "Biopolymer", "Membrane cascade", "Dynamic process model"]},
    {"article name": "Random Forests for mapping and analysis of microkinetics models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.04.019",
     "publication date": "07-2018",
     "abstract": "We introduce the application of an ensemble learning method known as Random Forests to microkinetics modeling and the computationally efficient integration of microkinetics into reaction engineering models. First, we show how Random Forests can be used for mapping pre-computed microkinetics data. Random Forests can be used to predict new datasets while keeping the prediction accuracy high and the computational load low. The method is also used to identify the important variables in the mechanism in regard to overall reaction rate and selectivity. The results are compared with results from a similar study using the Campbell's Degree of Rate Control approach and it is shown that the Random Forests method could be used to identify important features of the mechanism over a wide range of reacting conditions. Finally, the inclusion of the suggested method into reaction engineering models such as Computational Fluid Dynamics (CFD) resolved-particle simulations of fixed bed reactors is presented.",
     "keywords": ["Microkinetics", "Ensemble learning", "Random Forests", "Reaction engineering", "Computational fluid dynamics"]},
    {"article name": "A machine learning based computer-aided molecular design/screening methodology for fragrance molecules",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.04.018",
     "publication date": "07-2018",
     "abstract": "Although the business of flavors and fragrances has become a multibillion dollar market, the design/screening of fragrances still relies on the experience of specialists as well as available odor databases. Potentially better products, however, could be missed when employing this approach. Therefore, a computer-aided molecular design/screening method is developed in this work for the design and screening of fragrance molecules as an important first step. In this method, the odor of the molecules are predicted using a data driven machine learning approach, while a group contribution based method is employed for prediction of important physical properties, such as, vapor pressure, solubility parameter and viscosity. A MILP/MINLP model is established for the design and screening of fragrance molecules. Decomposition-based solution approach is used to obtain the optimal result. Finally, case studies are presented to highlight the application of the proposed fragrance design/screening method.",
     "keywords": ["Computer-aided molecular design", "Fragrance", "Machine learning", "Group contribution method", "Product property"]},
    {"article name": "A novel and systematic approach to identify the design space of pharmaceutical processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.04.021",
     "publication date": "07-2018",
     "abstract": "Feasibility analysis is a mathematical technique that can be used to assist the identification of the design space (DS) of a pharmaceutical process, given the availability of a process model. One of its main drawbacks is that it suffers from the curse of dimensionality, i.e. simulations can potentially become computationally extremely expensive and very cumbersome when the number of input factors is large. Additionally, giving a graphical and compact representation of the high-dimensional design space is difficult. In this study, we propose a novel and systematic methodology to exploit partial least-squares (PLS) regression modelling to reduce the dimensionality of a feasibility problem. We use PLS to obtain a linear transformation between the original multidimensional input space and a lower dimensional latent space. We then apply a Radial Basis Function (RBF) adaptive sampling feasibility analysis on this lower dimensional space to identify the feasible region of the process. We assess the accuracy and robustness of the results with three metrics, and we critically discuss the criteria that should be adopted for the choice of the number of latent variables. The performance of the methodology is tested on three simulated case studies, one of which involving the continuous direct compaction of a pharmaceutical powder. In all case studies, the methodology shows to be effective in reducing the computational burden while maintaining an accurate and robust identification of the design space.",
     "keywords": ["Feasibility analysis", "Partial least-squares regression", "Design space", "Continuous pharmaceutical manufacturing"]},
    {"article name": "MILP models for objective reduction in multi-objective optimization: Error measurement considerations and non-redundancy ratio",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.04.031",
     "publication date": "07-2018",
     "abstract": "A common approach in multi-objective optimization (MOO) consists of removing redundant objectives or reducing the set of objectives minimizing some metrics related with the loss of the dominance structure. In this paper, we comment some weakness related to the usual minimization of the maximum error (infinity norm or \u03b4-error) and the convenience of using a norm 1 instead. Besides, a new model accounting for the minimum number of Pareto solutions that are lost when reducing objectives is provided, which helps to further describe the effects of the objective reduction in the system. A comparison of the performance of these algorithms and its usefulness in objective reduction against principal component analysis\u202f+\u202fDeb & Saxena's algorithm (Deb & Saxena Kumar, 2005) is provided, and the ability of combining it with a principal component analysis in order to reduce the dimensionality of a system is also studied and commented.",
     "keywords": ["MOO objective reduction", "Non-Redundancy ratio", "PCA", "\u03b4-error", "Deb & Saxena algorithm"]},
    {"article name": "A real-time optimization framework for the time-varying economic environment",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.04.029",
     "publication date": "07-2018",
     "abstract": "In this paper, we propose a conceptual framework for nonlinear systems to integrate real-time optimization (RTO) and model predictive control (MPC) under time-varying economic environments. In the RTO layer, we introduce a lookup table including a large number of steady-state points of the nonlinear system, which are predetermined offline. Once the parameters of the economic cost function are varied, we are able to take a quick online search on the lookup table to find a point for satisfied economic performance and then send it to the MPC layer as a temporary control target. The temporary target is also employed as the initial solution for solving the optimization of the RTO layer. When the optimal target is calculated by RTO, it will replace the temporary one as the new control target of MPC. Compared to the two-layer framework which suffers from long waiting time to get the optimal operating points, the lookup-table-based RTO (LT-RTO) framework provides a quick-produced suboptimal target for MPC. It avoids unnecessary economic losses if MPC is still tracking outdated target even parameters of the cost function have already changed. We demonstrate the effectiveness through a chemical process model that the LT-RTO framework makes an improvement of the economic performance.",
     "keywords": ["Real-time optimization", "Model predictive control", "Nonlinear system", "Time-varying economic optimization"]},
    {"article name": "Data-driven rolling-horizon robust optimization for petrochemical scheduling using probability density contours",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.04.013",
     "publication date": "07-2018",
     "abstract": "In the process industry, uncertain factors, such as yield, can be quantified by analyzing industrial data generated from continuous sources. Traditional data-driven robust optimization models are mostly built on estimated probability distributions and convex uncertainty sets. As a result, the scheduling solution is only applicable to the limited sample of stochastic scenarios. We developed a rolling-horizon optimization approach to adapt the robust model to the changing environmental and operational conditions. First, a novel uncertainty set is defined by the probability density contours, covering scenarios with high possibility of occurrence. Then, we propose using new robust formulations induced by the outer-approximations of nonconvex uncertainty set. By implementing the raised model on a real-world ethylene production process using the available data, the fluctuation in fuel gas consumption can be controlled within 2%. Additionally, in agreement with our proof, the system\u2019s total profit and consumption of fuel gas stabilize in finite steps.",
     "keywords": ["Robust optimization", "Rolling-horizon", "Data-driven", "Probability density contour", "Petrochemical scheduling"]},
    {"article name": "Integrated scheduling and control in discrete-time with dynamic parameters and constraints",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.04.010",
     "publication date": "07-2018",
     "abstract": "Integrated scheduling and control (SC) seeks to unify the objectives of the various layers of optimization in manufacturing. This work investigates combining scheduling and control using a nonlinear discrete-time formulation, utilizing the full nonlinear process model throughout the entire horizon. This discrete-time form lends itself to optimization with time-dependent constraints and costs. An approach to combined SC is presented, along with sample pseudo-binary variable functions to ease the computational burden of this approach. An initialization strategy using feedback linearization, nonlinear model predictive control, and continuous-time scheduling optimization is presented. The formulation is applied with a generic continuous stirred tank reactor (CSTR) system in open-loop simulations over a 48-h horizon and a sample closed-loop implementation. The value of time-based parameters is demonstrated by applying cooling constraints and dynamic energy costs of a sample diurnal cycle, enabling demand response via combined scheduling and control.",
     "keywords": ["Scheduling", "Model predictive control", "Demand response"]},
    {"article name": "Multiproduct pipeline scheduling integrating for inbound and outbound inventory management",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.04.025",
     "publication date": "07-2018",
     "abstract": "This paper addresses the operation of a unidirectional multiproduct pipeline system integrated with inventory tanks management at the refinery and distribution center. A Mixed Integer Linear Programming (MILP) formulation is proposed to model the scheduling of a pipeline operation consisting of a single source node and a single destination node representing a refinery and a distribution center, respectively. Different operational aspects such as product availability, inventory limits, product quality, forbidden sequences, transition between different products injected into the pipeline and tanks inventory management are considered. Time synchronization among refinery, pipeline, and distribution center is guaranteed enhancing the model representation of real operations. Additionally, a solution procedure is developed to improve the model solution performance that explores the reduction of the number of variables and constraints. Different examples based on a real world case are solved showing the model's applicability, which proves to be effective in generating feasible solutions in reasonable computational times.",
     "keywords": ["Optimization", "Multiproduct pipeline", "Tanks management", "MILP formulation"]},
    {"article name": "Nonlinear dynamic analysis and control design of a solvent-based post-combustion CO2 capture process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.04.028",
     "publication date": "07-2018",
     "abstract": "A flexible operation of the solvent-based post-combustion CO2 capture (PCC) process is of great importance to make the technology widely used in the power industry. However, in case of a wide range of operation, the presence of process nonlinearity may degrade the performance of the pre-designed linear controller. This paper gives a comprehensive analysis of the dynamic behavior and nonlinearity distribution of the PCC process. Three cases are taken into account during the investigation: 1) capture rate change; 2) flue gas flowrate change; and 3) re-boiler temperature change. The investigations show that the CO2 capture process does have strong nonlinearity; however, by selecting a suitable control target and operating range, a single linear controller is possible to control the capture system within this range. Based on the analysis results, a linear model predictive controller is designed for the CO2 capture process. Simulations of the designed controller on an MEA based PCC plant demonstrate the effectiveness of the proposed control approach.",
     "keywords": ["Solvent-based post-combustion carbon capture", "Dynamic behavior variations", "Nonlinearity investigation", "Gap-metric", "Model predictve control"]},
    {"article name": "Comparison of flowsheets for THF/water separation using pressure-swing distillation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.04.023",
     "publication date": "07-2018",
     "abstract": "An eloquent optimization method was recently proposed and tested on the separation of tetrahydrofuran and water using pressure-swing distillation. The purpose of this paper is to compare the results of this rigorous optimum design with the design presented in a 1985 paper, which used heuristic engineering optimization. The new design uses more feed preheating in the low-pressure column and a higher pressure with more trays in the high-pressure column. Simulation results show that the old design has a 14% lower energy cost.",
     "keywords": ["Pressure-swing distillation", "THF", "Heat integration", "Heuristic optimization"]},
    {"article name": "Linguistic OWA and two time-windows based fault identification in wide plants",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.04.020",
     "publication date": "07-2018",
     "abstract": "Fault detection and diagnosis in industrial processes are challenging tasks that demand effective and timely decision making procedures. The multivariate statistical approaches for fault detection based on data have been very useful. However, they are known to be less powerful for fault diagnosis because they normally require prior knowledge of the problem involved. In this context, this proposal is based on an on-line, distributed fault isolation approach to provide a scored rank of variables considered as responsible for the faults in a more robust and earlier way than usual approaches. The fault isolation is carried out considering some top Fault Isolation (FI) methods, without prior knowledge regarding faults, in a distributed and collaborative way by a linguistic based decision making. The isolation of faulty variables provided by each FI approach is aggregated to provide a fault identification based on a scored ranking at two time points: after the fault detection and when the plant has recovered. In both cases, the final fault isolation is provided as a scored ranking obtained by Ordered Weighted Average operators (OWA) and Regular Increasing Monotone (RIM) aggregation functions, which permit the implementation of linguistic aggregation functions. The risk aversion during this multicriteria isolation is tuned by the user and can provide several strategies or policies. The fault isolation at two key times searches for the origin of faults and evaluates the evolution of the system after the fault\u2019s occurrence in the new working position of the plant. This is because faults in an industrial plant are propagated to different variables due to the actions of the process controllers. This method has been applied to two complex benchmark plants obtaining an earlier and more robust isolation.",
     "keywords": ["Fault identification", "Multicriteria decision making", "OWA operator", "Contribution plots"]},
    {"article name": "Efficient sampling algorithm for large-scale optimization under uncertainty problems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.05.007",
     "publication date": "07-2018",
     "abstract": "Uncertainty is part of the real-world optimization problems. The major bottleneck in solving large-scale stochastic optimization problems is the computational intensity of scenarios or samples. To this end, this research presents a novel sampling approach. This sampling called LHS-SOBOL combines one-dimensional uniformity of LHS and d-dimensional uniformity of Sobol. This paper analyzes existing and novel sampling techniques by conducting large-scale experiments with different functions. The sampling techniques which are analyzed are Monte Carlo Sampling (MCS), Latin Hypercube Sampling (LHS), Hammersley Sequence Sampling (HSS), Latin Hypercube-Hammersley Sequence Sampling (LHS-HSS), Sobol Sampling, and the proposed novel Latin Hypercube-Sobol Sampling (LHS-SOBOL). It was found that HSS performs better up to 40 uncertain variables, Sobol up to 100 variables, LHS-HSS up to 250 variables, and LHS-SOBOL for large-scale uncertainties for larger than 100 variables.",
     "keywords": ["LHS-Sobol", "Sampling techniques", "Stochastic optimization"]},
    {"article name": "Security constrained unit commitment scheduling: A new MILP formulation for solving transmission constraints",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.05.001",
     "publication date": "07-2018",
     "abstract": "This paper presents a new Mixed Integer Linear Programming model (MILP) to account for the Security-Constrained Unit Commitment Problem (SCUC). Transmission Constraints are introduced through bus balances, line power bound flows, and bus voltage angle differences. Line status is also considered. Binary variables regarding line status (active or inactive) are introduced for this purpose.These variables allow discrete decisions on the connectivity of buses, reducing the angle coupling between buses, with several advantages.Three examples are solved. The results indicate that this method can obtain feasible solutions with CPU times of 2.5\u202fs (for a 6-bus system) and 500\u202fs (for the IEEE 118-bus system), and they reached cost savings up to 4.9% of the total generating cost for one day of programming horizon, in comparison with classical models.Relations of the network are illustrated graphically, and an analysis of the results is presented through new evaluation indexes.",
     "keywords": ["Transmission constraints", "Mixed integer linear programming", "Power system scheduling", "Optimization"]},
    {"article name": "A distributed feasible-side convergent modifier-adaptation scheme for interconnected systems, with application to gas-compressor stations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.05.006",
     "publication date": "07-2018",
     "abstract": "Recently, a feasible-side globally convergent modifier-adaptation scheme has been proposed for the real-time optimization of uncertain systems. We show that this scheme is related to proximal-gradient algorithms in numerical optimization and we exploit this relationship to analyze its convergence in the case of inexact gradient information. We also make use of this relationship to propose a novel distributed modifier-adaptation algorithm for interconnected systems that uses a coordinator and knowledge of the interconnection topology. We then prove its feasible-side convergence to plant optimality. In addition, our distributed algorithm ensures confidentiality of local models and data. We finally demonstrate the applicability and effectiveness of this algorithm on a load-sharing optimization case study for serially connected gas compressors.",
     "keywords": ["Real-time optimization", "Interconnected systems", "Modifier adaptation", "Model uncertainty", "Proximal-gradient algorithm", "Gas compressors"]},
    {"article name": "Two methods of data reconciliation for pipeline networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.05.008",
     "publication date": "07-2018",
     "abstract": "The paper compares the two most common methods of data reconciliation for pipeline networks. The first method, an unscented Kalman filter (UKF), uses a system of nonlinear implicit ordinary differential equations derived from the governing partial differential equations. The second method, a quadratic program, relies on a transformation of the system of nonlinear ordinary differential equations into a set of linear difference equations, with the linearization optimized for known pressure and flow ranges using a novel linearization technique. Both the UKF and the quadratic programming approaches for data reconciliation in gas pipeline networks are viable for networks of small to moderate size. Given the reduced number of simplifying assumptions and the resulting improved accuracy, the UKF may be preferable when the computational problem is tractable. The quadratic programming approach is faster, accepts lower fidelity models, and provides acceptable accuracy.",
     "keywords": ["Kalman filter", "Data reconciliation", "Gas pipeline networks", "Quadratic program", "Linearization", "Ordinary differential equation"]},
    {"article name": "Feature extraction and reduced-order modelling of nitrogen plasma models using principal component analysis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.05.012",
     "publication date": "07-2018",
     "abstract": "Principal component analysis has been presented in recent research as an accurate and efficient method to reduce the complex chemistry and kinetics of large reacting mechanisms. Following the reduction, the original variables are transformed and projected onto a set of independent, orthogonal variables maximizing the total variance in the system: the principal components. However, these new variables are difficult to interpret physically and may introduce instabilities in the low dimensional representation of the manifold. In the present paper we will show the benefits of coupling PCA to a rotation method: the interpretation of the principal components can be related back to the physics. The advantages of rotation are demonstrated on a PCA reduced model for modelling dissociation and excitation processes in nitrogen shock flows.",
     "keywords": ["Principal component analysis", "Plasma flows", "Dynamical study", "Chemistry reduction", "Physical interpretation"]},
    {"article name": "Advanced optimization strategies for integrated dynamic process operations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.10.016",
     "publication date": "06-2018",
     "abstract": "Modern approaches for dynamic optimization trace their inception to Pontryagin's Maximum Principle 60 years ago. Since then the application of large-scale nonlinear programming strategies has been extended to deal with challenging real-world process optimization problems. This study discusses and demonstrates the effectiveness of dynamic optimization on three case studies on real-world chemical processes. In the first case, we consider the optimal design of runaway reactors, where simulation models may lead to unbounded profiles for many choices of design and operating conditions. As a result, optimization based on repeated simulations typically fails, and a simultaneous, equation-based approach must be applied. Second, we consider optimal operating policies for grade transitions in polymer processes. Modeled as an optimal control problem, we demonstrate how incorporation of product specification bands leads to multi-stage formulations that greatly improve process performance and significantly reduce off-grade product. Third, we consider an optimization strategy for the integration of scheduling and dynamic process operation for general continuous/batch processes. The method introduces a discrete time formulation for simultaneous optimization of scheduling and operating decisions. Finally, we provide a concise summary of directions and challenges for future extension of these optimization formulations and solution strategies.",
     "keywords": ["Dynamic optimization", "Nonlinear programming", "Direct transcription", "Simultaneous collocation", "Recipe optimization", "Grade transitions"]},
    {"article name": "Expanding scope and computational challenges in process scheduling",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.01.020",
     "publication date": "06-2018",
     "abstract": "In this paper, we present a brief overview of enterprise-wide optimization and challenges in multiscale temporal modeling and integration of different models for the levels of planning, scheduling and control. Next, we review Generalized Disjunctive Programming (GDP), as a new modeling paradigm for scheduling problems that are illustrated with the STN and RTN models. We then address scheduling problems that expand the scope of the area: simultaneous scheduling and heat integration, pipeline scheduling, crude oil and refined products blending, and demand side management. We illustrate the advantage of the GDP modeling framework, describe effective strategies for global optimization, and describe multistage affinely adjustable robust optimization for uncertain interruptible load. We address integration of planning and scheduling, for which several approaches are reviewed, including use of traveling salesman constraints for multiperiod refinery planning, and multisite planning and scheduling of multiproduct batch plants. We report computational results to highlight the challenges.",
     "keywords": ["Planning", "Scheduling", "Demand side management", "Mixed-integer programming", "Generalized disjunctive programming"]},
    {"article name": "Decomposing complex plants for distributed control: Perspectives from network theory",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.10.015",
     "publication date": "06-2018",
     "abstract": "This paper reviews recent research on the application of methods from the theory of networks for developing distributed control architectures for complex plants. The problem is defined as one of decomposing process networks into constituent subnetworks with strong intra-subnetwork and weak inter-subnetwork interactions. These interactions are quantified based on connectivity and response sensitivity information. This perspective is inspired by the community detection problem in networks. Several approaches are discussed based on hierarchical clustering and modularity optimization. The concepts and potential of these methods for developing control architectures for complex plants are illustrated through a case study. Future research directions are also discussed.",
     "keywords": ["Control architecture", "Network decomposition", "Community detection", "Distributed control"]},
    {"article name": "Nonsmooth differential-algebraic equations in chemical engineering",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.10.031",
     "publication date": "06-2018",
     "abstract": "This article advocates a nonsmooth differential-algebraic equations (DAEs) modeling paradigm for dynamic simulation and optimization of process operations. A variety of systems encountered in chemical engineering are traditionally viewed as exhibiting hybrid continuous and discrete behavior. In many cases such discrete behavior is nonsmooth (i.e. continuous but nondifferentiable) rather than discontinuous, and is appropriately modeled by nonsmooth DAEs. A computationally relevant theory of nonsmooth DAEs (i.e. well-posedness and sensitivity analysis) has recently been established (Stechlinski and Barton, 2016a, Stechlinski and Barton, 2017) which is suitable for numerical implementations that scale efficiently for large-scale dynamic optimization problems. Challenges posed by competing hybrid modeling approaches for process operations (e.g. hybrid automata) are highlighted as motivation for the nonsmooth DAEs approach. Several examples of process operations modeled as nonsmooth DAEs are given to illustrate their wide applicability before presenting the appropriate mathematical theory.",
     "keywords": ["Dynamic simulation", "Dynamic optimization", "Sensitivity analysis", "Hybrid systems", "Process systems engineering"]},
    {"article name": "Dynamic latent variable analytics for process operations and control",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.10.029",
     "publication date": "06-2018",
     "abstract": "After introducing process data analytics using latent variable methods and machine learning, this paper briefly review the essence and objectives of latent variable methods to distill desirable components from a set of measured variables. These latent variable methods are then extended to modeling high dimensional time series data to extract the most dynamic latent time series, of which the current values are best predicted from the past values of the extracted latent variables. We show with an industrial case study how real process data are efficiently and effectively modeled using these dynamic methods. The extracted features reveal hidden information in the data that is valuable for understanding process variability.",
     "keywords": ["Dynamic PCA", "Dynamic PLS", "Dynamic CCA", "Dynamic latent variable modeling", "Process monitoring", "Dynamic feature extraction"]},
    {"article name": "A framework for modeling and optimizing dynamic systems under uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.11.003",
     "publication date": "06-2018",
     "abstract": "Algebraic modeling languages (AMLs) have drastically simplified the implementation of algebraic optimization problems. However, there are still many classes of optimization problems that are not easily represented in most AMLs. These classes of problems are typically reformulated before implementation, which requires significant effort and time from the modeler and obscures the original problem structure or context. In this work we demonstrate how the Pyomo AML can be used to represent complex optimization problems using high-level modeling constructs. We focus on the operation of dynamic systems under uncertainty and demonstrate the combination of Pyomo extensions for dynamic optimization and stochastic programming. We use a dynamic semibatch reactor model and a large-scale bubbling fluidized bed adsorber model as test cases.",
     "keywords": ["Stochastic programming", "Dynamic optimization", "Optimal control", "Parameter estimation"]},
    {"article name": "Economic MPC and real-time decision making with application to large-scale HVAC energy systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.10.038",
     "publication date": "06-2018",
     "abstract": "With the potential to decrease operating costs and improve energy efficiency, model predictive control (MPC) has been proposed as a replacement for traditional heuristic, PID, and other conventional control strategies for heating, ventilation, and air conditioning (HVAC) systems in commercial buildings. Due to the size of large commercial HVAC systems, implementing MPC as a single monolithic optimization problem is not practical nor desirable given real-time operating requirements. In this paper, we present a hierarchical decomposition for economic MPC in large-scale commercial HVAC systems using a two-layer approach. We show a sample optimization for a campus of 25 buildings with 500 total zones and a central plant consisting of eight chillers. Then, we discuss an application of the ideas presented here in the recently completed $485-million replacement of the Stanford campus heating and cooling systems and conclude with some of the control theory challenges presented by this new class of applications.",
     "keywords": ["Model predictive control", "Commercial buildings", "Decomposition", "Optimization", "Scheduling"]},
    {"article name": "Global optimization of grey-box computational systems using surrogate functions and application to highly constrained oil-field operations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.01.005",
     "publication date": "06-2018",
     "abstract": "This work presents recent advances within the AlgoRithms for Global Optimization of coNstrAined grey-box compUTational problems (ARGONAUT) framework, developed for optimization of systems which lack analytical forms and derivatives. A new parallel version of ARGONAUT (p-ARGONAUT) is introduced to solve high dimensional problems with a large number of constraints. This development is motivated by a challenging case study, namely the operation of an oilfield using water-flooding. The objective of this case study is the maximization of the Net Present Value over a five-year time horizon by manipulating the well pressures, while satisfying a set of complicating constraints related to water-cut limitations and water handling and storage. Dimensionality reduction is performed via the parametrization of the pressure control domain, which is then followed by global optimization of the constrained grey-box system. Results are presented for multiple case studies and the performance of p-ARGONAUT is compared to existing derivative-free optimization methods.",
     "keywords": ["Derivative-free optimization", "Grey/black-box optimization", "Oilfield operations", "Oil-well control", "Waterflooding"]},
    {"article name": "Machine learning: Overview of the recent progresses and implications for the process systems engineering field",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.10.008",
     "publication date": "06-2018",
     "abstract": "Machine learning (ML) has recently gained in popularity, spurred by well-publicized advances like deep learning and widespread commercial interest in big data analytics. Despite the enthusiasm, some renowned experts of the field have expressed skepticism, which is justifiable given the disappointment with the previous wave of neural networks and other AI techniques. On the other hand, new fundamental advances like the ability to train neural networks with a large number of layers for hierarchical feature learning may present significant new technological and commercial opportunities. This paper critically examines the main advances in deep learning. In addition, connections with another ML branch of reinforcement learning are elucidated and its role in control and decision problems is discussed. Implications of these advances for the fields of process and energy systems engineering are also discussed.",
     "keywords": ["Machine learning", "Deep learning", "Reinforcement learning", "Process systems engineering", "Stochastic decision problems"]},
    {"article name": "The impact of digitalization on the future of control and operations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.10.037",
     "publication date": "06-2018",
     "abstract": "The notion of Internet of Things (IoT), as well as related topics such as Cyber-Physical Systems, Industrie 4.0 and Smart Manufacturing are currently attracting a lot of attention within the process and manufacturing industries. Clearly, IoT offers many potential applications for automation, ranging from engineering installation of new plants to production management and more intelligent maintenance schemes including novel sensor technologies. The focus of this paper is, however, on the control and operations. Most likely IoT leads to new system architectures where open standards play a significant role. Through better connectivity, information will be much more easily available, which could result in that previously isolated functions will become more closely integrated. Here modeling at the right level of fidelity will be absolutely key. It can be expected that the importance of optimization will increase and this paper discusses some aspects related to the opportunities, challenges and changes triggered by IoT.",
     "keywords": ["Digitalization", "Control", "Operations", "Optimization", "Scheduling", "Process automation"]},
    {"article name": "Smart manufacturing and energy systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.10.027",
     "publication date": "06-2018",
     "abstract": "While many U.S. manufacturing operations utilize optimization for individual unit processes, smart manufacturing (SM) systems that integrate manufacturing intelligence in real time across an entire production operation are not pervasive in industry. A vendor-agnostic SM platform is under development that integrates information technology, models, and simulations driven by real-time plant data and performance metrics. By utilizing existing process control and automation systems, manufacturing organizations can manage systems at a much lower cost, optimizing process knowledge and improving energy productivity. Three case studies are presented: steam methane reforming to make hydrogen, optimization of a heat treatment furnace for metals processing, and a fuel cell system, all of which utilize high fidelity models as a starting point for optimization and control. The Smart Manufacturing Leadership Coalition has led the national effort in SM, and the recently established National Manufacturing Innovation Institute funded by DOE, private industry, and state governments will be described.",
     "keywords": ["Smart manufacturing", "Advanced control", "Smart sensors", "Process modeling", "Energy efficiency", "Optimization"]},
    {"article name": "A multitree approach for global solution of ACOPF problems using piecewise outer approximations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.10.018",
     "publication date": "06-2018",
     "abstract": "Electricity markets rely on the rapid solution of the optimal power flow (OPF) problem to determine generator power levels and set nodal prices. Traditionally, the OPF problem has been formulated using linearized, approximate models, ignoring nonlinear alternating current (AC) physics. These approaches do not guarantee global optimality or even feasibility in the real ACOPF problem.We introduce an outer-approximation approach to solve the ACOPF problem to global optimality based on alternating solution of upper- and lower-bounding problems. The lower-bounding problem is a piecewise relaxation based on strong second-order cone relaxations of the ACOPF, and these piecewise relaxations are selectively refined at each major iteration through increased variable domain partitioning. Our approach is able to efficiently solve all but one of the test cases considered to an optimality gap below 0.1%. Furthermore, this approach opens the door for global solution of MINLP problems with AC power flow equations.",
     "keywords": ["Optimal power flow", "Outer-approximation", "Piecewise relaxation", "Global optimization", "Second-order cone relaxation", "ACOPF"]},
    {"article name": "Stochastic model predictive control \u2014 how does it work?",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.10.026",
     "publication date": "06-2018",
     "abstract": "Stochastic model predictive control (SMPC) provides a probabilistic framework for MPC of systems with stochastic uncertainty. A key feature of SMPC is the inclusion of chance constraints, which enables a systematic trade-off between attainable control performance and probability of state constraint violations in a stochastic setting. This paper presents an overview of core concepts in SMPC in relation to MPC and stochastic optimal control, with numerical illustrations on a typical chemical process. Estimation of stochastic disturbances as well as the impact of estimation quality of stochastic disturbances on the SMPC performance are discussed. Some avenues for future research in SMPC are suggested.",
     "keywords": ["Model predictive control", "Stochastic optimal control", "Chance constraints", "Disturbance modeling"]},
    {"article name": "Process operational safety via model predictive control: Recent results and future research directions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.10.006",
     "publication date": "06-2018",
     "abstract": "The concept of maintaining or enhancing chemical process safety encompasses a broad set of considerations which stem from management/company culture, operator procedures, and engineering designs, and are meant to prevent incidents at chemical plants. The features of a plant design that take action to prevent incidents on a moment-by-moment basis are the control system and the safety system (i.e., the alarm system, safety instrumented system, and safety relief system). Though the control and safety systems have a common goal in this regard, coordination between them has been minimal. One impediment to such an integrated control-safety system design is that the traditional industrial approach to safety focuses on root causes of incidents and on keeping individual measured variables within recommended ranges, rather than seeking to understand incidents from a more fundamental perspective as the result of the dynamic process state evolving to a value at which consequences to humans and the environment occur. This work reviews the state of the art in control system designs that incorporate explicit safety considerations in the sense that they have constraints designed to prevent the process state from taking values at which incidents can occur and in the sense that they are coordinated with the safety system. The intent of this tutorial is to unify recent developments in this area and to encourage further research by showcasing that the topic, though critical for safe operation of chemical processes particularly as we move to more tightly integrated and economics-focused operating strategies, is in its infancy and that many open questions remain.",
     "keywords": ["Process safety", "Process control", "Model predictive control", "Process operation", "Nonlinear processes", "Safety systems"]},
    {"article name": "Optimizing the Design of New and Existing Supply Chains at Dow AgroSciences",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.03.012",
     "publication date": "06-2018",
     "abstract": "In this paper, we discuss the design and optimization of supply chains at Dow AgroSciences. We start by introducing the design components of a typical supply chain. We then discuss the typical inputs required in a model and the type of outputs generated. Next we consider the strengths and weaknesses of the standard tools that we use as part of the model process.To show the breadth of problems addressed, three example models are presented. The first problem discusses the complexity of addressing a global supply chain for a new active ingredient. The second problem focused on a regional supply chain. The final problem showed a tactical model looking at rail fleet sizing.",
     "keywords": null},
    {"article name": "On improving the online performance of production scheduling: Application to air separation units",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.10.030",
     "publication date": "06-2018",
     "abstract": "In the operation of power-intensive Air Separation Units (ASUs) that produce storable liquid products, optimization opportunities exist at two time scales \u2212 week-ahead production scheduling to leverage fluctuations in electricity prices, and real-time decisions that optimize the entire plant operation and capture spot opportunities. In our previous work, we proposed a methodology based on flexibility analysis and robust optimization to ensure feasibility of real-time operational decisions at ASUs for future time periods within a scheduling horizon. In this paper, we build upon the methodology to propose approaches to improve the online performance of a production schedule for ASUs by using the real-time optimization layer. We compare several policies for real-time optimization and our studies on real plant data show interesting trade-offs between week-ahead scheduling and real-time optimization.",
     "keywords": ["Online scheduling", "Real-time optimization", "Decisions under uncertainty", "Industrial challenges"]},
    {"article name": "Application of formal verification and falsification to large-scale chemical plant automation systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.11.004",
     "publication date": "06-2018",
     "abstract": "In this paper, we apply formal verification and falsification of temporal logic specifications to analyze chemical plant automation systems. We present new results, obtained by applying a recently-developed approach to handle combined invariance and reachability requirements. In addition, we develop a set of tests that can be generated automatically for a given control system, some of which have the same form as those in the existing literature, and some of which combine invariance and reachability, to which we apply the new approach mentioned previously. In both cases, we work with abstractions of the automation systems in order to apply symbolic model checking to industrial-scale problems. We demonstrate the results using a series of small illustrative examples, and also report results from an industrial case study. The methods that we apply are implemented in a pair of open-source software tools, which we describe briefly.",
     "keywords": ["Formal methods", "Hybrid systems", "Programmable logic controllers", "Supervisory control", "Model checking", "Abstraction"]},
    {"article name": "Scheduling, optimization and control of power for industrial cogeneration plants",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.10.014",
     "publication date": "06-2018",
     "abstract": "Scheduling, optimization and control of power for three industrial cogeneration plants at one of Dow\u2019s Louisiana site is presented in this paper. A first principle mathematical model that includes mass and energy balances for gas turbines, heat recovery units, steam turbines, pressure relief valves and steam headers is used to formulate multiple optimization problems to recommend the best strategy to trade power. The model has detailed operational information that includes equipment status and control curves for different operating scenarios. The scheduled power offer curve is obtained by solving multiple optimization problems using the validated process model along with operational and equipment limitations. Adjustment of power schedule offer is done in the real-time market thirty minutes prior to the hour and implementation of the dispatched power schedule is done using a model predictive controller.",
     "keywords": ["Industrial cogeneration process", "Day-ahead scheduling", "Real-time optimization", "Model predictive control"]},
    {"article name": "Framework for a smart data analytics platform towards process monitoring and alarm management",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.10.010",
     "publication date": "06-2018",
     "abstract": "The fusion of information from disparate sources of data is the key step in devising strategies for a smart analytics platform. In the context of the application of analytics in the process industry, this paper provides a framework for seamless integration of information from process and alarm databases complimented with process connectivity information. The discovery of information from such diverse data sources can be subsequently used for process and performance monitoring including alarm rationalization, root cause diagnosis of process faults, hazard and operability analysis, safe and optimal process operation. The utility of the proposed framework is illustrated by several successful industrial case studies.",
     "keywords": ["Analytics", "Big data", "Performance monitoring", "Process monitoring", "Alarm systems", "Process data analytics", "Fault detection and diagnosis"]},
    {"article name": "Petroleum production optimization \u2013 A static or dynamic problem?",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.10.009",
     "publication date": "06-2018",
     "abstract": "This paper considers the upstream oil and gas domain, or more precisely the daily production optimization problem in which production engineers aim to utilize the production systems as efficiently as possible by for instance maximizing the revenue stream. This is done by adjusting control inputs like choke valves, artificial lift parameters and routing of well streams. It is well known that the daily production optimization problem is well suited for mathematical optimization. The contribution of this paper is a discussion on appropriate formulations, in particular the use of static models vs. dynamic models. We argue that many important problems can indeed be solved by repetitive use of static models while some problems, in particular related to shale gas systems, require dynamic models to capture key process characteristics. The reason for this is how reservoir dynamics interacts with the dynamics of the production system.",
     "keywords": ["Production optimization", "Oil and gas", "Shale gas", "Formulations", "Dynamic models"]},
    {"article name": "Economic opportunities for industrial systems from frequency regulation markets",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.09.018",
     "publication date": "06-2018",
     "abstract": "We analyze economic opportunities for industrial facilities provided by frequency regulation (FR) markets. We use classical frequency domain analysis techniques to characterize the harmonic content of FR signals and to analyze the impact of such harmonics on the response of dynamical systems. The analysis reveals that systems with slow dynamics, as those found in large industrial facilities, are suitable to provide FR capacity because they can naturally damp the dominant high-frequency harmonic content of FR signals. We also propose optimization formulations to quantify the maximum amount of FR capacity that can be provided by a system given its dynamic characteristics, its control architecture. A distillation case study demonstrates that significant economic potential exists for large industrial facilities.",
     "keywords": ["Electricity markets", "Frequency regulation", "Dynamics", "Manufacturing", "Control"]},
    {"article name": "A two-stage procedure for the optimal sizing and placement of grid-level energy storage",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.10.033",
     "publication date": "06-2018",
     "abstract": "The economic benefit realized from energy storage units on the electric grid is linked to the control policy selected to govern grid operations. Thus, the optimal sizing and placement (OSP) of such units is also dependent on the operating policy of the power network. In this work, we first introduce economic model predictive control (EMPC) as a viable economic dispatch policy for transmission networks with energy storage. However, the numeric basis of EMPC makes it ill-suited for the OSP problem. In contrast, the method of economic linear optimal control (ELOC) can be easily adapted to the OSP problem. However, the relaxation of point-wise-in-time constraints, inherent to ELOC, will introduce a systematic underestimation of operating costs. Thus, we introduce a 2-step OSP algorithm that begins with the ELOC-based approach to determine the placement of energy storage units. Then, an EMPC-based gradient search is used to determine optimal sizes.",
     "keywords": ["Smart grid", "Energy storage", "Economic MPC", "System design"]},
    {"article name": "A process systems approach for detailed rail planning and scheduling applications",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.11.022",
     "publication date": "06-2018",
     "abstract": "Value chain integration is an ongoing challenge: while computing power has improved, there is little modeling consistency across the system. This paper bridges this gap by proposing a novel formulation for train scheduling, a linking element of value chains, using the Unit-Operation-Port-State Superstructure (UOPSS). Train scheduling is a challenging problem: rail lines can be hundreds of kilometers long with train crossing strategies that are based on a train station level, while also requiring results with a minute-time scale resolution. In mixed-use rail systems with limited passing loop infrastructure, trains have different passing priorities and lengths, thus differing in their ability to use passing loops. The proposed model is the basis of the Hatch Rail Optimizer (HRO) software. In addition to small case studies, the power of HRO is demonstrated through a practical case study involving a 370\u00a0km rail corridor with five different train sizes over a week-long scheduling horizon.",
     "keywords": ["Value chain optimization", "Train scheduling", "Planning", "MILP", "Optimization", "Rail systems"]},
    {"article name": "An algorithm for gradient-based dynamic optimization of UV flash processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.10.007",
     "publication date": "06-2018",
     "abstract": "This paper presents a novel single-shooting algorithm for gradient-based solution of optimal control problems with vapor\u2013liquid equilibrium constraints. Such optimal control problems are important in several engineering applications, for instance in control of distillation columns, in certain two-phase flow problems, and in operation of oil reservoirs. The single-shooting algorithm uses an adjoint method for the computation of gradients. Furthermore, the algorithm uses either a simultaneous or a nested approach for the numerical solution of the dynamic vapor\u2013liquid equilibrium model equations. Two numerical examples illustrate that the simultaneous approach is faster than the nested approach and that the efficiency of the underlying thermodynamic computations is important for the overall performance of the single-shooting algorithm. We compare the performance of different optimization software as well as the performance of different compilers in a Linux operating system. These tests indicate that real-time nonlinear model predictive control of UV flash processes is computationally feasible.",
     "keywords": ["Dynamic optimization", "Optimal control", "Adjoint algorithm", "Single-shooting", "UV flash", "Vapor\u2013liquid equilibrium"]},
    {"article name": "On decoupling rate processes in chemical reaction systems \u2013 Methods and applications",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.09.021",
     "publication date": "06-2018",
     "abstract": "Models of chemical reaction systems can be complex as they need to include information regarding the reactions and the mass and heat transfers. The commonly used state variables, namely, concentrations and temperatures, express the interplay between many phenomena. As a consequence, each state variable is affected by several rate processes. On the other hand, it is well known that it is possible to partition the state space into a reaction invariant subspace and its orthogonal complement using a linear transformation involving the reaction stoichiometry. This paper uses a more sophisticated linear transformation to partition the state space into various subspaces, each one linked to a single rate process such as a particular reaction, mass transfer or heat transfer. The implications of this partitioning are discussed with respect to several applications related to data reconciliation, state and rate estimation, modeling, identification, control and optimization of reaction systems.",
     "keywords": ["Chemical reaction systems", "Reaction variants", "Vessel extents", "Data reconciliation", "State and rate estimation", "Modeling and identification", "Control and optimization"]},
    {"article name": "Modeling of hydraulic fracturing and designing of online pumping schedules to achieve uniform proppant concentration in conventional oil reservoirs",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.10.032",
     "publication date": "06-2018",
     "abstract": "We present a novel control framework for the closed-loop operation of a hydraulic fracturing process. Initially, we focus on the development of a first-principle model of a hydraulic fracturing process. Second, a novel numerical scheme is developed to efficiently solve the coupled partial differential equations defined over a time-dependent spatial domain. Third, a reduced-order model is constructed, which is used to design a Kalman filter to accurately estimate unmeasurable states. Lastly, model predictive control theory is applied for the design of a feedback control system to achieve uniform proppant concentration across the fracture at the end of pumping by explicitly taking into account the desired fracture geometry, total amount of proppant injected, actuator limitations, and safety considerations. We demonstrate that the proposed control scheme is able to generate a spatial concentration profile which is uniform and close to the target concentration compared to that of the benchmark, Nolte's pumping schedule.",
     "keywords": ["Hydraulic fracturing", "Optimal pumping schedule", "Model predictive control", "Unified fracture design", "Kalman filter"]},
    {"article name": "Analysis of the multiplicity of steady-state profiles of two tubular reactor models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.10.028",
     "publication date": "06-2018",
     "abstract": "This paper deals with the analysis of two tubular reactor models, the non-isothermal tubular reactor model and a biochemical reactor model. It is mathematically shown in particular that multiple equilibrium profiles can be exhibited if the diffusion coefficients are large enough by considering regular perturbation arguments.",
     "keywords": ["Reaction systems", "Multiple equilibrium profiles", "Tubular reactor", "Exothermic reaction", "Biochemical reaction"]},
    {"article name": "Analysis of the dynamics of an active control of the surface potential in metal oxide gas sensors",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.02.021",
     "publication date": "05-2018",
     "abstract": "Gas sensing is nowadays a key actor in pollution observation and detection of chemical toxic agents or explosives. All these applications require the shortest possible time response. Very recently, a control of the surface potential in gas sensors based on metal oxides has experimentally shown to dramatically improve the time response of metal-oxide gas sensors. The proposed control is inspired in sigma-delta modulators. This paper aims at studying the resulting dynamics in the sensor from a theoretical point of view. Using state space models, it is shown how the state variables, namely the concentrations of ionized species in the sensing layer, evolve with time in open and closed loop configuration. This analysis studies how it is possible to alter the dynamics of the overall system, while at the same time keeping some important characteristics of sigma-delta modulators, such as quantization noise-shaping. Numerical simulations validate the obtained results.",
     "keywords": ["Chemical sensors", "Surface potential", "Sliding mode control"]},
    {"article name": "Combining multi-attribute decision-making methods with multi-objective optimization in the design of biomass supply chains",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.02.010",
     "publication date": "05-2018",
     "abstract": "Multi-objective optimization (MOO) is widely applied in sustainability problems where several objectives must be accounted for in the analysis. Unfortunately, its complexity grows with the number of objectives, which hampers its practical use. In this paper, we simplify MOO problems via their combination with multi-attribute decision-making (MADM) methods. The approach identifies a unique Pareto solution of the MOO problem, which best reflects the decision-makers\u2019 preferences, by using weighting factors generated via four well-known MADM methods: SWING, SMART, AHP and TRADE OFF. The capabilities of this approach are illustrated through its application to the design and planning of a sugar/ethanol supply chain using questionnaires filled in by academic experts in the problem. We find that the weights obtained using MADM algorithms may well differ from the ones given by standard life-cycle assessment methods employed in systems engineering problems. Overall, our approach simplifies the MOO problem by identifying solutions consistent with the decision-makers\u2019 preferences and by providing valuable insight on how these preferences are articulated in practice.",
     "keywords": ["Sustainability", "Multi-criteria optimization", "Environmental impact", "Biorefinery design", "Decision-making"]},
    {"article name": "An integrated output space partition and optimal control method of multiple-model for nonlinear systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.02.018",
     "publication date": "05-2018",
     "abstract": "A systematic method for optimally partitioning operating range of each linear subsystem in output space under the criterion of closed-loop performance is initiated, when a multiple model approach is applied to nonlinear systems. As a result, an integrated output space partition and optimal control method is proposed. Firstly, linear input-output models are identified at given operating points and then reformulated as a hybrid model underlying each state having the same physical meaning. Secondly, the optimal state space partition is obtained according to a closed-loop control index. Finally, based on the obtained optimal state space partition, an optimal output space partition is achieved with the projection technique. Furthermore, a hybrid model-MPC strategy is designed according to the obtained multiple-model associated with its optimal output space partition. The integrated output space partition and optimal control method can improve the nonlinear system overall control performance and results of numerical simulation are provided.",
     "keywords": ["Nonlinear systems", "Output-space partition", "Optimal control", "Multilinear model", "Hybrid systems"]},
    {"article name": "CO2 water-alternating-gas injection for enhanced oil recovery: Optimal well controls and half-cycle lengths",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.03.006",
     "publication date": "05-2018",
     "abstract": "CO2 water-alternating-gas (WAG) injection is an enhanced oil recovery method designed to improve sweep efficiency during CO2 injection with the injected water to control the mobility of CO2 and to stabilize the gas front. Optimization of CO2 -WAG injection is widely regarded as a viable technique for controlling the CO2 and oil miscible process. Poor recovery from CO2 -WAG injection can be caused by inappropriately designed WAG parameters. In previous study (Chen and Reynolds, 2016), we proposed an algorithm to optimize the well controls which maximize the life-cycle net-present-value (NPV). However, the effect of injection half-cycle lengths for each injector on oil recovery or NPV has not been well investigated. In this paper, an optimization framework based on augmented Lagrangian method and the newly developed stochastic-simplex-approximate-gradient (StoSAG) algorithm is proposed to explore the possibility of simultaneous optimization of the WAG half-cycle lengths together with the well controls. The proposed framework is demonstrated with three reservoir examples.",
     "keywords": ["CO2-WAG", "Production optimization", "Optimal well controls", "Half-cycle length", "Enhanced oil recovery", "Ensemble-based methods"]},
    {"article name": "Heuristics with performance guarantees for the minimum number of matches problem in heat recovery network design",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.03.002",
     "publication date": "05-2018",
     "abstract": "Heat exchanger network synthesis exploits excess heat by integrating process hot and cold streams and improves energy efficiency by reducing utility usage. Determining provably good solutions to the minimum number of matches is a bottleneck of designing a heat recovery network using the sequential method. This subproblem is an NP -hard mixed-integer linear program exhibiting combinatorial explosion in the possible hot and cold stream configurations. We explore this challenging optimization problem from a graph theoretic perspective and correlate it with other special optimization problems such as cost flow network and packing problems. In the case of a single temperature interval, we develop a new optimization formulation without problematic big-M parameters. We develop heuristic methods with performance guarantees using three approaches: (i) relaxation rounding, (ii) water filling, and (iii) greedy packing. Numerical results from a collection of 51 instances substantiate the strength of the methods.",
     "keywords": ["Minimum number of matches", "Heat exchanger network design", "Heuristics", "Approximation algorithms", "Mixed-integer linear optimization"]},
    {"article name": "Formulation of the excess absorption in infrared spectra by numerical decomposition for effective process monitoring",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.01.025",
     "publication date": "05-2018",
     "abstract": "Iterative optimization technology (IOT), a method that predicts the component composition from only the infrared (IR) spectra of the pure components and mixtures by using Beer's law, has been proposed to reduce the number of calibration samples for process analytical technology in the pharmaceutical industry. However, IOT cannot be applied to mixtures that have wavelength regions where Beer's law does not hold, such as liquid mixtures. The objective of this study is to apply IOT to liquid mixtures to realize a calibration-minimum method. We propose a novel calibration-minimum method that formulates spectral changes by polynomials of the mole fractions considering reasonable boundary conditions for online monitoring. The prediction ability of the proposed method was verified by three case studies: two binary mixtures and one ternary mixture. The model selection strategy, conditions for calibration, and estimation of missing pure component spectra are also discussed. This research represents a step towards advanced calibration-minimum methods.",
     "keywords": ["Process analytical technology", "Calibration-minimum method", "Online process monitoring", "Concentration prediction", "Infrared spectroscopy", "Multivariate curve resolution"]},
    {"article name": "Satisfiability modulo theories for process systems engineering",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.03.004",
     "publication date": "05-2018",
     "abstract": "Process systems engineers have long recognized the importance of both logic and optimization for automated decision-making. But modern challenges in process systems engineering could strongly benefit from methodological contributions in computer science. In particular, we propose satisfiability modulo theories (SMT) for process systems engineering applications. We motivate SMT using a series of test beds and show the applicability of SMT algorithms and implementations on (i) two-dimensional bin packing, (ii) model explainers, and (iii) mixed-integer nonlinear optimization solvers.",
     "keywords": ["Satisfiability modulo theories", "Mixed-integer optimization", "Generalized disjunctive programming", "Mixed logical-linear programming"]},
    {"article name": "A physiologically-based diffusion-compartment model for transdermal administration \u2013 The melatonin case study",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.03.008",
     "publication date": "05-2018",
     "abstract": "There is a significant hype in the medical sector for the transdermal administration of drugs as it allows achieving a combination of multiple advantages: non-invasive procedure, pain avoidance, no first-pass hepatic metabolism, and induction of sustained plasma levels. This paper proposes a model for the study and prediction of drug transport through skin and the following distribution to human body. This is achieved by an innovative combination of the physiologically-based compartmental approach with Fick's laws of diffusion. The skin model features three strata: stratum corneum, viable epidermis, and dermis, which have a major impact on the absorption, distribution, and metabolism of transdermal drugs. The combined model accounts for skin transport via diffusion equations, and absorption and distribution in the rest of the body (i.e. organs/tissues) via material balances on homogeneous compartments. Experimental data of transdermal melatonin allow validation. Main applications are optimization of the dosage and study of skin transport.",
     "keywords": ["Transdermal delivery", "Pharmacokinetic simulation", "Model sensitivity", "Administration route", "ADME", "Melatonin"]},
    {"article name": "Simultaneous identification and optimization of biochemical processes under model-plant mismatch using output uncertainty bounds",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.03.001",
     "publication date": "05-2018",
     "abstract": "The method of simultaneous identification and optimization aims at satisfying the conditions of optimality while providing accurate predictions of the process outputs. The model parameters are updated in a run-to-run procedure as to account for changes in operating points and to correct for errors in the predicted gradients of the cost-function and constraints. To make this parameter updating step more robust, we propose a parameter identification objective that includes a ratio of the sum of squared errors to a parametric gradient sensitivity function. This results in an identified set of parameters which provide larger sensitivities for the subsequent gradient correction step thus leading to faster convergence to the optimum. Moreover, worst-case uncertainty bounds on the model outputs are utilized to enforce an adequate model fitting. This is especially valuable when identifying dynamic metabolic models with many parameters. The resulting improvements are illustrated using two simulated cell culture processes.",
     "keywords": ["Model-based optimization", "Model-plant mismatch", "Batch processes", "Biochemical processes", "Model correction", "Uncertainty bounds"]},
    {"article name": "A simulation-based optimization framework for integrating scheduling and model predictive control, and its application to air separation units",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.03.009",
     "publication date": "05-2018",
     "abstract": "The integration of dynamic process models in scheduling calculations has recently received significant attention as a mean to improve operational performance in increasingly dynamic markets. In this work, we propose a novel framework for the integration of scheduling and model predictive control (MPC), which is applicable to industrial size problems involving fast changing market conditions. The framework consists on identifying scheduling-relevant process variables, building low-order dynamic models to capture their evolution, and integrating scheduling and MPC by, (i) solving a simulation-optimization problem to define the optimal schedule and, (ii) tracking the schedule in closed-loop using the MPC controller. The efficacy of the framework is demonstrated via a case study that considers an air separation unit operating under real-time electricity pricing. The study shows that significant cost reductions can be achieved with reasonable computational times.",
     "keywords": ["Scheduling of production", "Process control", "Integrated scheduling and control", "Model predictive control", "Air separation units"]},
    {"article name": "CFD\u2013Aspen Plus interconnection method. Improving thermodynamic modeling in computational fluid dynamic simulations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.03.019",
     "publication date": "05-2018",
     "abstract": "Thermodynamic modeling in CFD is basically limited to the models available in the simulators. The method presented in this paper connects CFD simulators with Aspen Plus which instantaneously calculates and returns the value of any physical property required. Therefore, all the thermodynamic models and compounds available in Aspen Plus can be implemented in CFD simulations. The connection, created via Matlab and Excel-VBA, has been validated solving two identical CFD simulations first selecting a thermodynamic model available in the simulator and then connecting the simulator with Aspen Plus and selecting the same model. The maximum absolute average deviation between the density and viscosity values obtained in both simulations, for the two case studies analyzed, is lower than 0.7% which demonstrates the proper interconnection. The accuracy of the results obtained modeling multicomponent mixtures and supercritical fluids prove the applicability of the method to any scenarios.",
     "keywords": ["CAPE", "Multicomponent mixture", "Supercritical fluid", "Pseudocomponents"]},
    {"article name": "Large-scale DAE-constrained optimization applied to a modified spouted bed reactor for ethylene production from methane",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.03.017",
     "publication date": "05-2018",
     "abstract": "In this paper, a modified spouted bed reactor is proposed to enhance the yield of the oxidative coupling of methane (OCM). Optimization techniques are used to carry out a theoretical analysis of ethylene production via OCM and define some optimal operating conditions of the reacting system. A model-based DAE-constrained optimization strategy is proposed and applied to the OCM process to illustrate the computational capability of the proposed formulation, and the theoretical feasibility of the proposed reactor. The model developed for the reactor is a one-dimensional model composed of material, energy, and momentum balances. This model along with the kinetic model constitute a non-linear and differential-algebraic system, which is discretized using orthogonal collocation on finite elements with continuous profiles approximated by Lagrange polynomials. The resulting algebraic collocation equations are written as equality constraints in the optimization problem, which is solved with the IPOPT solver within the optimization-modeling platform. An initialization routine based on simulations was carried out to guarantee convergence in optimizations. Results from simulations and optimizations showed the potential of combining different reactor concepts to improve the ethylene production from natural gas via oxidative coupling of methane.",
     "keywords": ["Dynamic optimization", "Oxidative coupling of methane", "Spouted bed reactor", "Modeling", "Simulation", "Ethylene", "Methane"]},
    {"article name": "Multiscale three-dimensional CFD modeling for PECVD of amorphous silicon thin films",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.03.011",
     "publication date": "05-2018",
     "abstract": "The development of a three-dimensional, multiscale computational fluid dynamics (CFD) model is presented here which aims to capture the deposition of amorphous silicon thin films via plasma-enhanced chemical vapor deposition (PECVD). The macroscopic reactor scale and the microscopic thin film growth domains which define the multiscale model are linked using a dynamic boundary which is updated at the completion of each time step. A novel parallel processing scheme built around a message passing interface (MPI) structure, in conjunction with a distributed collection of kinetic Monte Carlo algorithms, is applied in order to allow for transient simulations to be conducted using a mesh with greater than 1.5 million cells. Due to the frequent issue of thickness non-uniformity in thin film production, an improved PECVD reactor design is proposed. The resulting geometry is shown to reduce the product offset from \u202f\u223c\u202f25\u202fnm to less than 13\u202fnm using identical deposition parameters.",
     "keywords": ["Multiscale modeling", "Computational fluid dynamics", "Thin film growth", "Parallel computing"]},
    {"article name": "In situ adaptive tabulation (ISAT) for combustion chemistry in a network of perfectly stirred reactors (PSRs) for the investigation of soot formation and growth",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.03.007",
     "publication date": "05-2018",
     "abstract": "This paper presents an efficient computational implementation of the in situ adaptive tabulation (ISAT) approach (Pope, 1997) for combustion chemistry in a network of perfectly stirred reactors (PSRs) for the investigation of soot formation and growth. This study, for the first time, extends the thermochemical composition vector to contain the soot moments, using the method of moments with interpolative closure (MOMIC) as a soot model with six concentration moments. A series of PSR calculations is carried out using the direct integration (DI) and ISAT approaches. Assessment of the accuracy of ISAT approach is conducted through direct comparisons with DI calculations. Moreover, complimentary cumulative distribution function (CCDF), sensitivity of ISAT calculations with respect to the absolute error tolerance values and speedup are analyzed for two different test cases of ethylene\u2013air using two different chemical kinetic mechanisms. A maximum speedup of 50\u202f\u00d7 was achieved for an error tolerance of 10 \u2212 4 .",
     "keywords": ["Perfectly stirred reactor", "ISAT", "Direct integration", "Reaction mapping gradient", "Complimentary cumulative distribution function"]},
    {"article name": "Challenges in process optimization for new feedstocks and energy sources",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.03.013",
     "publication date": "05-2018",
     "abstract": "Current and future challenges of optimization in the process industry are discussed. The gap between academic research and industrial workflow is analyzed. Moreover, issues arising from the shift from conventional fossil fuels as both feedstock and energy source to nonconventional feedstocks (shale gas, tar sands, CO2 and biomass) and penetration of intermittent renewable energy are discussed. This manuscript focuses mainly on offline model-based optimization of design and operation, including the generation and selection of promising process alternatives for new feedstocks in conceptual design, multi-objective optimization, the estimation of thermodynamic parameters of new intermediates and the optimization of process operation under the volatile availability of the new feedstocks and energy sources. Moreover, a number of opportunities and needs for research and development are identified, including the simultaneous optimization of feedstocks, processes and products and a production able to process a variety of feedstocks and to utilize energy when it is cheap.",
     "keywords": ["Multiobjective optimization", "Process design", "Optimal design and operation", "Renewable energy", "Nonconventional feedstock"]},
    {"article name": "Enhancing natural gas-to-liquids (GTL) processes through chemical looping for syngas production: Process synthesis and global optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.03.003",
     "publication date": "05-2018",
     "abstract": "A process synthesis and global optimization framework is presented to determine the most profitable routes of producing liquid fuels from natural gas through competing technologies. Chemical looping is introduced into the framework for the first time as a natural gas conversion alternative. The underlying phenomena in chemical looping are complex and models from methods such as computational fluid dynamics are unsuitable for global optimization. Therefore, appropriate approximate models are required. Parameter estimation and disjunctive programming are described here for modeling two chemical looping processes. The first is a nickel oxide based process developed at CSIC in Spain; the second is a iron oxide based process developed at Ohio State University. These mathematical models are then incorporated into a comprehensive process superstructure to evaluate the performance of chemical looping against technologies such as autothermal reforming and steam reforming for syngas production. The rest of the superstructure consists of process alternatives for liquid fuels production from syngas and simultaneous heat, power, and water integration. Among the various case studies considered, it is shown that chemical looping can reduce the break-even oil prices for natural gas-to-liquids processes by as much as 40%, while satisfying production demands and obeying environmental constraints. For a natural gas price of $5/TSCF, the break-even price is as low as $32.10/bbl. Sensitivity analysis shows that these prices for chemical looping remain competitive even as natural gas cost rises. The findings suggest that chemical looping is a very promising option to enhance natural gas-to-liquids processes and their capabilities.",
     "keywords": ["Chemical looping", "GTL", "Process synthesis", "Global optimization", "Parameter estimation", "Disjunctive programming"]},
    {"article name": "Decoupling the constraints for process simulation in large-scale flowsheet optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.03.018",
     "publication date": "05-2018",
     "abstract": "A distinct advantage of sequential quadratic programming (SQP) is global convergence that ensures convergence from a remote starting point. When the constraints are highly nonlinear such as in flowsheet optimization, however, locally convergent Newton's method used in SQP as the equation-solving tool may deteriorate the behavior of convergence. Our recognition that this issue remains to be resolved motivated us to study a two-tier SQP approach where the constraints for process simulation consisting of nonlinear equations are decoupled from the KKT system in order to block the adverse influence of nonlinearity on global convergence. Our equation oriented (EO) process simulator (Ishii and Otto, 2011) is employed to decouple the constraints and for maintaining feasibility of the decoupled constraints. The effectiveness and potential of the two-tier SQP approach for reliably and efficiently solving large-scale flowsheet optimization problems are numerically illustrated with fully thermally coupled distillation problems.",
     "keywords": ["Sequential quadratic programming", "Highly nonlinear constraints", "Reduced KKT system", "Flowsheet optimization", "Equation oriented process simulation", "Distillation"]},
    {"article name": "Life cycle aggregated sustainability index for the prioritization of industrial systems under data uncertainties",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.03.015",
     "publication date": "05-2018",
     "abstract": "This study aims at developing a generic method for measuring the sustainability of industrial systems and prioritizing industrial systems under uncertainties. The interval preference relation based goal programming model which can address vagueness and ambiguity existing in human's judgments was employed to determine the weights of the criteria for life cycle sustainability assessment. A life cycle aggregated sustainability index which incorporates both the data of industrial systems with respect to the evaluation criteria and the weights of the criteria was developed to prioritize the industrial systems. An illustrative case including four electricity generation systems were studied by the proposed method, and the results were also validated by another four multi-criteria decision making methods. The results reveal that the developed life cycle aggregated sustainability index can effectively prioritizing industrial systems under data uncertainties.",
     "keywords": ["Life cycle sustainability assessment", "Sustainability index", "Multi-criteria decision making", "Industrial systems", "uncertainties"]},
    {"article name": "Simulation of hybrid trickle bed reactor\u2013reverse osmosis process for the removal of phenol from wastewater",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.03.016",
     "publication date": "05-2018",
     "abstract": "Phenol and phenolic derivatives found in different industrial effluents are highly toxic and extremely harmful to human and the aquatic ecosystem. In the past, trickle bed reactor (TBR), reverse osmosis (RO) and other processes have been used to remove phenol from wastewater. However, each of these technologies has limitations in terms of the phenol concentration in the feed water and the efficiency of phenol rejection rate. In this work, an integrated hybrid TBR\u2013RO process for removing high concentration phenol from wastewater is suggested and model-based simulation of the process is presented to evaluate the performance of the process. The models for both TBR and RO processes were independently validated against experimental data from the literature before coupling together to make the hybrid process. The results clearly show that the combined process significantly improves the rejection rate of phenol compared to that obtained via the individual processes.",
     "keywords": ["Wastewater treatment", "Integrated process: Trickle bed reactor: Reverse osmosis (RO)", "Phenol rejection", "Modelling"]},
    {"article name": "Chromatography Analysis and Design Toolkit (CADET)",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.02.025",
     "publication date": "05-2018",
     "abstract": "CADET is an open source modeling and simulation framework for column liquid chromatography. The software is freely distributed to both academia and industry under the GPL license (http://github.com/modsim/cadet). CADET is based on a core simulator that is written in object oriented C++ and applies modern mathematical algorithms for efficiently solving a variety of customary chromatography models. This simulation engine is interfaced to a suite of MATLAB tools for setting up and executing the most common scientific workflows, e.g., model calibration, process design, robustness analysis, statistical analysis, and experimental design. The model library and numerical methods are continuously extended and improved. For instance, binding models with multiple bound states, pH and/or temperature dependence of binding parameters, surface diffusion, and arbitrary spacing of the radial discretization have been recently added. Moreover, numerical accuracy and computational speed of the code are comprehensively benchmarked using high precision reference solutions and realistic model problems. Versatility of the CADET modeling platform is demonstrated with several examples that are also published as open source code and can be freely adapted to specific use cases. In one of several case studies, sequential and simultaneous optimization of elution gradient shape and cut times are compared for a three component separation. This process is designed to achieve Pareto optimal purity and yield of the central fraction. Moreover, the robustness of these designs with respect to typical process variations is systematically studied. The last case study illustrates the optimal design of experiments for estimating model parameters with maximal accuracy.",
     "keywords": ["Column liquid chromatography", "General rate model", "Modeling and simulation platform", "Model calibration", "Process analysis and design", "Statistical analysis", "Experimental design"]},
    {"article name": "A linearization method for probability moment equations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.01.015",
     "publication date": "04-2018",
     "abstract": "We present a new method for calculating the time-transient behavior of stochastic reaction networks. We first derive the set of equations for the moments of the master probability distribution. We then linearize these equations calculating the Jacobian matrix around the stationary probability distribution. In order to demonstrate the method, we present examples of stochastic reaction networks and compute their dynamic behavior. We find that the calculations are accurate and significantly more efficient than stochastic simulation algorithms based on Gillespie\u2019s algorithms.",
     "keywords": ["Linearization", "Stochastic reaction networks", "Moment equations", "Chemical master equation", "Stationary Jacobian"]},
    {"article name": "Searching historical data segments for process identification in feedback control loops",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.01.018",
     "publication date": "04-2018",
     "abstract": "Mathematical models of dynamic processes are often required for assessment, diagnosis and improvement of control loop performances in process industries. Data samples collected in daily operations of feedback control loops may enclose data segments suitable for process identification to obtain the mathematical models. This paper proposes a new method to search such data segments. The searching criterion is that the reference and process output in a feedback control loop should experience significant magnitude changes. Hypothesis tests are exploited to find changing positions of data segments with different probability distributions and to verify whether the reference and process output make significant magnitude changes inside one data segment or between two adjacent segments. Simulation and industrial examples are provided to illustrate the effectiveness of the proposed method.",
     "keywords": ["Data segmentation", "Process identification", "Control loop performance"]},
    {"article name": "A Novel Approach for Linearization of a MINLP Stage-Wise Superstructure Formulation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.01.010",
     "publication date": "04-2018",
     "abstract": "Mathematical programming using superstructure formulations has been used for cost efficient heat exchanger network synthesis (HENS) for about three decades now and significant improvements have been achieved since then. One major problem is the combinatorial nature of the underlying superstructure formulations which means that the mathematical complexity of the HENS problem scales exponentially with problem size. In this paper a novel approach using convex linear approximations is presented for simultaneous HENS. The linearization is carried out prior to optimization and the original Mixed Integer Non-Linear Programming (MINLP) problem is reformulated into a Mixed Integer Linear Programming (MILP) problem. For the linearized problem a global optimum can be obtained much faster compared to the original MINLP formulation. For all presented case-studies feasible solutions could be obtained, which compare well with results from other authors.",
     "keywords": ["Mathematical programming", "Heat integration", "Linearization", "Heat exchanger network synthesis"]},
    {"article name": "Effect of cell heterogeneity on isogenic populations with the synthetic genetic toggle switch network: Bifurcation analysis of two-dimensional cell population balance models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.01.021",
     "publication date": "04-2018",
     "abstract": "The dynamics of gene regulatory networks are often modeled with the assumption of cellular homogeneity. However, this assumption contradicts the plethora of experimental results in a variety of systems, which designates that cell populations are heterogeneous systems in the sense that properties such as size, shape, and DNA/RNA content are unevenly distributed amongst their individuals. In order to address the implications of heterogeneity, we utilize the so-called cell population balance (CPB) models. Here, we solve numerically multivariable CPB models to study the effect of heterogeneity on populations carrying the toggle switch network, which features nonlinear behavior at the single-cell level. In order to answer whether this nonlinear behavior is inherited to the heterogeneous population level, we perform bifurcation analysis on the steady-state solutions of the CPB model. We show that bistability is present at the population level with the pertinent bistability region shrinking when the impact of heterogeneity is enhanced.",
     "keywords": ["Cell heterogeneity", "Toggle switch", "Bistability", "Broyden's algorithm"]},
    {"article name": "Automated reaction generation for polymer networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.01.022",
     "publication date": "04-2018",
     "abstract": "Most of the theoretical studies on polymer kinetics has been performed by manually reducing the chemical system to a few simple reaction mechanisms having a repeatable nature. Not being constrained by such reducibility, this work considers the polymerization as a product of a complex network of reactions that need not to be known in advance. Combining various ideas from graph theory, combinatorics and random graphs, we introduce a new modeling approach to complex polymerization that automatically constructs a reaction network, solves kinetic model, and retrieves such topological properties of the final polymer network as, for instance, distribution of molecular weight. In this way, the new approach acts as an intermediate layer that propagates the knowledge of the basic chemistry in order to capture and understand the complexity of the real world polymerizing systems.",
     "keywords": ["Rule-based network generation", "Polymerization", "Reaction network", "Kinetic modeling", "Macromolecular properties"]},
    {"article name": "How to tighten a commonly used MINLP superstructure formulation for simultaneous heat exchanger network synthesis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.01.011",
     "publication date": "04-2018",
     "abstract": "MINLP superstructures for heat exchanger network synthesis allow the simultaneous optimization of utility heat loads, the number of heat exchanger units and their area requirements. The algorithms used to solve these nonlinear non-convex optimization problems solve MILP and NLP sub-problems iteratively to find optimal solutions. If these sub-problems are tightened, which means that the solution space is reduced but still includes all feasible integer solutions, the algorithms can potentially go through the solution space faster as branches can be excluded earlier. In this work, tightening measures for a commonly used MINLP stage-wise superstructure formulation are proposed and the impact of tighter variable bounds and additional inequality constraints is investigated using various case-studies taken from literature. It is shown that tighter formulations help the solver to find global optimal solutions and that the duality gap can be reduced significantly if the test cases could not be solved to global optimality.",
     "keywords": ["Heat exchanger networks", "Tighter formulation", "Global optimization"]},
    {"article name": "Multi-level supervision and modification of artificial pancreas control system",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.02.002",
     "publication date": "04-2018",
     "abstract": "Artificial pancreas (AP) systems provide automated regulation of blood glucose concentration (BGC) for people with type 1 diabetes (T1D). An AP includes three components: a continuous glucose monitoring (CGM) sensor, a controller calculating insulin infusion rate based on the CGM signal, and a pump delivering the insulin amount calculated by the controller to the patient. The performance of the AP system depends on successful operation of these three components.Many APs use model predictive controllers that rely on models to predict BGC and to calculate the optimal insulin infusion rate. The performance of model-based controllers depends on the accuracy of the models that is affected by large dynamic changes in glucose-insulin metabolism or equipment performance that may move the operating conditions away from those used in developing the models and designing the control system. Sensor errors and missing signals will cause calculation of erroneous insulin infusion rates. And the performance of the controller may vary at each sampling step and each period (meal, exercise, and sleep), and from day to day.Here we describe a multi-level supervision and controller modification (ML-SCM) module is developed to supervise the performance of the AP system and retune the controller. It supervises AP performance in 3 time windows: sample level, period level, and day level. At sample level, an online controller performance assessment sub-module will generate controller performance indexes to evaluate various components of the AP system and conservatively modify the controller. A sensor error detection and signal reconciliation module will detect sensor error and reconcile the CGM sensor signal at each sample. At period level, the controller performance is evaluated with information collected during a certain time period and the controller is tuned more aggressively. At the day level, the daily CGM ranges are further analyzed to determine the adjustable range of controller parameters used for sample level and period level.Thirty subjects in the UVa/Padova metabolic simulator were used to evaluate the performance of the ML-SCM module and one clinical experiment is used to illustrate its performance in a clinical environment. The results indicate that the AP system with an ML-SCM module has a safer range of glucose concentration distribution and more appropriate insulin infusion rate suggestions than an AP system without the ML-SCM module.",
     "keywords": ["Artificial pancreas", "Sensor error detection", "Controller performance assessment", "Controller retuning", "Type 1 diabetes"]},
    {"article name": "Rigorous synthesis of energy systems by decomposition via time-series aggregation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.01.023",
     "publication date": "04-2018",
     "abstract": "The synthesis of complex energy systems usually involves large time series such that a direct optimization is computationally prohibitive. In this paper, we propose a decomposition method for synthesis problems using time-series aggregation. To initialize the method, the time series is aggregated to one time step. A lower bound is obtained by relaxing the energy balances and underestimating the energy demands leading to a relaxed synthesis problem, which is efficiently solvable. An upper bound is obtained by restricting the original problem with the full time series to an operation problem with a fixed structure obtained from the lower bound solution. If the bounds do not satisfy the specified optimality gap, the resolution of the time-series aggregation is iteratively increased. The decomposition method is applied to two real-world synthesis problems. The results show the fast convergence of the decomposition method outperforming commercial state-of-the-art optimization software.",
     "keywords": ["Optimal design", "Synthesis", "Energy system", "Decomposition", "Time-series aggregation", "Cluster method"]},
    {"article name": "Numerical analysis of accelerated degradation in large lithium-ion batteries",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.12.019",
     "publication date": "04-2018",
     "abstract": "The size effect on degradation in lithium-ion battery cells is investigated by simulations of lithium iron phosphate/graphite LIB cells with different size. An electrical-electrochemical-thermal model considering degradation phenomena is modeled for a 1Ah pouch cell and a 55Ah pouch cell with an identical electrode design. Numerical results in the large cell shows the additional voltage drops of 27\u202fmV and the mean temperature increase of 8\u00a0\u00b0C for 3C discharge due to overpotentials in metal current collectors and clear spatial imbalances of temperature, current density and electric potential. The capacity fade in the large cell is accelerated by about 33% for cycling operation due to the activated parasitic reactions at high temperature conditions. But even in an isothermal condition, the large cell still shows about 7% faster degradation than the small cell because it stays longer at high SOC in the charge process.",
     "keywords": ["Large lithium ion battery cell", "Accelerated degradation", "Size effect", "Multi-physics battery model"]},
    {"article name": "Model order reduction of nonlinear parabolic PDE systems with moving boundaries using sparse proper orthogonal decomposition: Application to hydraulic fracturing",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.02.004",
     "publication date": "04-2018",
     "abstract": "Developing reduced-order models for nonlinear parabolic partial differential equation (PDE) systems with time-varying spatial domains remains a key challenge as the dominant spatial patterns of the system change with time. To address this issue, there have been several studies where the time-varying spatial domain is transformed to the time-invariant spatial domain by using an analytical expression that describes how the spatial domain changes with time. However, this information is not available in many real-world applications, and therefore, the approach is not generally applicable. To overcome this challenge, we introduce sparse proper orthogonal decomposition (SPOD)-Galerkin methodology that exploits the key features of ridge and lasso regularization techniques for the model order reduction of such systems. This methodology is successfully applied to a hydraulic fracturing process, and a series of simulation results indicates that it is more accurate in approximating the original nonlinear system than the standard POD-Galerkin methodology.",
     "keywords": ["Proper orthogonal decomposition", "Galerkin\u2019s projection", "Moving boundary problems", "Nonlinear model order reduction", "Naive elastic net", "Hydraulic fracturing"]},
    {"article name": "Optimal cryogenic processes for nitrogen rejection from natural gas",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.02.006",
     "publication date": "04-2018",
     "abstract": "Nitrogen rejection processes are usually needed for two natural gas sources: sub-quality natural gas reserves and produced gas from enhanced oil/gas recovery technologies. The nitrogen content of natural gas in the former is usually constant during the project lifetime, but it varies from 5 to 70% during enhanced oil/gas recovery programs. This variation leads to different process flowsheets for nitrogen removal: single-column, double-column, three-column, and two-column processes. In order to determine which configuration is more suitable for a particular nitrogen content in a feed stream, we must minimize the energy requirement for each process. In this study, we merge all the four configurations into two categories: single-column and multi-column processes and then use the Particle Swarm Optimization algorithm to optimize process parameters for each process with the objective of energy consumption minimization. Finally, we use the exergy concept to analyze theoretically these different processes.",
     "keywords": ["Cryogenic distillation", "Cryogenic separation", "Natural gas processing", "Nitrogen\u2013methane separation", "Nitrogen removal"]},
    {"article name": "Model-based feedback control of oil production in oil-rim reservoirs under gas coning conditions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.02.001",
     "publication date": "04-2018",
     "abstract": "In oil-rim reservoirs, a thin oil layer lies between an aquifer layer and a large gas-cap. Traditionally, oil has been produced at constant rate from the oil layer. However, the constant-rate oil production strategy may lead to suboptimal performance, as it does not take into account the process dynamics near the wellbore. Motivated by this, first, we present a gas/oil flow model to describe the oil flow dynamics near the wellbore. Second, the high-fidelity simulation result is used to construct a reduced-order model for the design of a Luenberger observer for state estimation. Lastly, a model-based feedback control system is designed to compute the optimal oil production profile that maximizes the net present value (NPV) of oil produced from a horizontal well before gas breakthrough. We demonstrate that the proposed control scheme is able to achieve a high NPV compared to other state-of-the-art oil production strategies.",
     "keywords": ["Gas coning", "Model predictive control", "Boundary control problem", "Two-phase flow"]},
    {"article name": "A scheduling perspective on the monetary value of improving process control",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.01.019",
     "publication date": "04-2018",
     "abstract": "The goal of quantifying the monetary value of process control has been a target of much research since the inception of the field, and methods have been developed for quantifying the value of control in the case of predominantly steady-state processes. However, there has been no attempt to quantify the monetary value of control for predominantly transient processes. In this work, we utilize the general framework of integrated scheduling and control to develop novel performance functions that enable the quantification of the monetary value of control from a scheduling perspective for a predominantly transient process. Specifically, we posit that the transition time between one product and the next in a production sequence can be used as a performance metric over which the value of control can be quantified. We demonstrate the utility of the developed performance functions using a case study of the scheduling of a multi-product CSTR.",
     "keywords": ["Value of control", "Production scheduling", "Process economics", "Process control", "Economic benefit analysis"]},
    {"article name": "Utility network optimization in eco-industrial parks by a multi-leader follower game methodology",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.01.024",
     "publication date": "04-2018",
     "abstract": "A multi-leader-follower game (MLFG) model for the design of the utility network in an eco-industrial park (EIP) is studied and implemented by introducing the concept of an environmental authority. The methodology also considers the flowsheet simulation of each enterprise involved in the EIP in order to obtain utility consumption of each enterprise operating by itself. The approach is validated on a case study of a potential Norwegian EIP. In the latter, multi-leader-single-follower and single-leader-multi-follower game models are studied. Each enterprise's objective is to minimize the total annualized cost, while the EIP authority objective is to minimize the equivalent CO2 consumption related to utility consumption within the ecopark. The MLFG is transformed into a MOPEC and solved using GAMS\u00ae as an NLP. The methodology proposed is proven to be reliable in multi-criteria scenarios compared to traditional multiobjective optimization approaches, providing numerical Nash/Stackelberg equilibrium solutions and specifically in EIP planning and optimization.",
     "keywords": ["Eco-industrial parks", "Multi-leader-follower game", "Nash equilibrium", "Utility network", "MPCC", "Game theory", "Process simulation"]},
    {"article name": "A new framework of global sensitivity analysis for the chemical kinetic model using PSO-BPNN",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.02.003",
     "publication date": "04-2018",
     "abstract": "Global sensitivity analysis is a tool that primarily focuses on identifying the effects of uncertain input variables on the output and has been investigated widely in chemical kinetic studies. Conventional variance-based methods, such as Sobol\u2019 sensitivity estimation and high dimensional model representation (HDMR) methods, are computationally expensive. To accelerate global sensitivity analysis, a new framework that combines a variance-based (Wu's method) and two ANN-based sensitivity analysis methods (Weights and PaD) was proposed. In this framework, a back-propagation neural network (BPNN) methodology was applied, which was optimized by a particle swarm optimization (PSO) algorithm and trained with original samples. The Wu's method and Weights and PaD methods were employed to calculate sensitivity indices based on a well-trained PSO-BPNN. The convergence and accuracy of the new framework were compared with previous methods using a standard test case (Sobol\u2019 g-function) and a methane reaction kinetic model. The results showed that the new framework can greatly reduce the computational cost by two orders of magnitude, as well as guaranteeing accuracy. To take maximum advantage of the new framework, a four-step process combining the advantages of each method was proposed and applied to estimate the sensitivity indices of a C2H4 ignition model. The sensitivity indices of the more complex model could be implemented easily with good accuracy when the four-step process is followed",
     "keywords": ["Global sensitivity analysis", "High dimensional model representation (HDMR)", "Back-propagation neural network (BPNN)", "Particle swarm optimization (PSO)", "Garson method", "PaD method"]},
    {"article name": "An improved L-shaped method for two-stage convex 0\u20131 mixed integer nonlinear stochastic programs",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.01.017",
     "publication date": "04-2018",
     "abstract": "In this paper, we propose an improved L-shaped method to solve large-scale two-stage convex 0\u20131 mixed-integer nonlinear stochastic programs with mixed-integer variables in both first and second stage decisions and with relatively complete recourse. To address the difficulties in solving large problems, we propose a Benders-like decomposition algorithm that includes both (strengthened) Benders cuts and Lagrangean cuts in the Benders master problem. The proposed algorithm is applied to solve a batch plant design problem under demand uncertainty, and a planning problem under demand and price uncertainty. It is shown that the proposed algorithm outperforms the commercial solvers, DICOPT, SBB, Alpha-ECP, and BARON, for the problems with a large number of scenarios. Also, although the proposed algorithm cannot close the duality gap, it is proved that it can yield a lower bound that is at least as tight as the one from Lagrangean decomposition.",
     "keywords": ["Stochastic programming", "L-shaped method", "Convex MINLP", "Integer recourse"]},
    {"article name": "A survey of optimal process design capabilities and practices in the chemical and petrochemical industries",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.01.012",
     "publication date": "04-2018",
     "abstract": "To examine industrial capabilities and practices in using process design optimization software tools, we conducted a series of over one hundred interviews with practitioners and industry experts in optimal process design, focusing on current techniques, workflows, and challenges. In this article, we analyze the findings of these interviews, providing a perspective into the status of optimal process design in the petrochemical and chemical industries. We first present the findings categorized by company type and personnel function, followed by industry-specific insights.",
     "keywords": ["Optimal process design", "Process synthesis", "Process engineering", "Process simulation"]},
    {"article name": "Data-driven decision making under uncertainty integrating robust optimization with principal component analysis and kernel smoothing methods",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.02.007",
     "publication date": "04-2018",
     "abstract": "This paper proposes a novel data-driven robust optimization framework that leverages the power of machine learning and big data analytics for decision-making under uncertainty. By applying principal component analysis to uncertainty data, correlations between uncertain parameters are effectively captured, and latent uncertainty sources are identified. These data are then projected onto each principal component to facilitate extracting distributional information of latent uncertainties using kernel density estimation techniques. To explicitly account for asymmetric distributions, we introduce forward and backward deviation vectors into the data-driven uncertainty set, which are further incorporated into novel data-driven static and adaptive robust optimization models. The proposed framework not only significantly ameliorates the conservatism of robust optimization, but also enjoys computational efficiency and wide-ranging applicability. Three applications on optimization under uncertainty, including model predictive control, batch production scheduling, and process network planning, are presented to demonstrate the applicability of the proposed framework.",
     "keywords": ["Big data", "Robust optimization", "Principal component analysis", "Kernel smoothing methods", "Process control and operations"]},
    {"article name": "Integrating decisions of product and closed-loop supply chain design under uncertain return flows",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.02.011",
     "publication date": "04-2018",
     "abstract": "The shortage of natural resources, the need to take into account societal considerations, the emergence of new government regulations and the necessity to maintain and/or improve the economic benefit of the supply chain, have created a growing awareness on academia as well as industries towards the development of closed-loop supply chains (CLSCs), where explicitly products' life-cycles are accounted for. Concentrating on the problems of the product and network design for a multi-product, multi-echelon and multi-period CLSC, in this work a two-stage stochastic mixed integer linear model incorporating uncertainty on the quality and quantity of the return flows is proposed. In addition, risk management related to critical uncertain parameters is performed, where a conditional value at risk (CVaR) concept is applied to supply chain profits. The formulation considers decisions associated with the network design and, simultaneously, with the products to manufacture (new and remanufactured) and their associated raw materials (new and recovered). A network superstructure is considered accounting for two types of customers (first and second markets), raw material suppliers, factories, distribution centers, customer demands, recovery centers, recycle centers, final disposal locations and re-distribution centers. Optimal solutions with high economic and environmental benefits are obtained where the advantages of using the proposed approach are shown. A case study from a European consumer goods company is explored.",
     "keywords": ["Mathematical modeling", "Uncertainty", "Product design", "Closed-loop supply chain", "Stochastic approach"]},
    {"article name": "A POMDP framework for integrated scheduling of infrastructure maintenance and inspection",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.02.015",
     "publication date": "04-2018",
     "abstract": "This work presents an optimization scheme for maintenance and inspection scheduling of the infrastructure system whose states are nearly impossible or prohibitively expensive to estimate or measure online. The suggested framework describes state transition under the observation uncertainty as Partially Observable Markov Decision Process (POMDP) and can integrate heterogeneous scheduling jobs including maintenance, inspection, and sensor installation within a single model. The proposed approach performs survival analysis to obtain time-variant transition probabilities. A POMDP problem is then formulated via state augmentation. The resulting large-scale POMDP is solved by an approximate point-based solver. We exploit the idea of receding horizon control to the POMDP framework as a feedback rule for the online evaluation. Water distribution pipeline is analyzed as an illustrative example, and the results indicate that the proposed POMDP framework can improve the overall cost for maintenance tasks and thus the system\u2019s sustainability.",
     "keywords": ["Infrastructure scheduling", "Sensor scheduling", "POMDP", "Asset optimization", "Stochastic optimization", "Survival analysis"]},
    {"article name": "Modeling sex differences in the renin angiotensin system and the efficacy of antihypertensive therapies",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.02.009",
     "publication date": "04-2018",
     "abstract": "The renin angiotensin system is a major regulator of blood pressure and a target for many anti-hypertensive therapies; yet the efficacy of these treatments varies between the sexes. We use published data for systemic RAS hormones to build separate models for four groups of rats: male normotensive, male hypertensive, female normotensive, and female hypertensive rats. We found that plasma renin activity, angiotensinogen production rate, angiotensin converting enzyme activity, and neutral endopeptidase activity differ significantly among the four groups of rats. Model results indicate that angiotensin converting enzyme inhibitors and angiotensin receptor blockers induce similar percentage decreases in angiotensin I and II between groups, but substantially different absolute decreases. We further propose that a major difference between the male and female RAS may be the strength of the feedback mechanism, by which receptor bound angiotensin II impacts the production of renin.",
     "keywords": ["Renin angiotensin system", "Hypertension", "ACE Inhibitors", "Angiotensin receptor blockers", "Blood pressure"]},
    {"article name": "Simulation of a triple effect evaporator of a solution of caustic soda, sodium chloride, and sodium sulfate using Aspen Plus",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.02.005",
     "publication date": "04-2018",
     "abstract": "Worldwide, the Chlor-Alkali process is the most well-known method for the production of chlorine (Cl2) and sodium hydroxide (NaOH). NaOH, also known as caustic soda, is a very important alkali with many applications in the processing and production of paper, detergents, aluminum, petrochemicals, inorganics, and in the food industry. The aqueous solution of caustic soda, known as \u201ccell liquor,\" produced in this process must be concentrated from 11 to 50 percent weight, which is achieved through a multiple effect evaporator system. In some cases, the brine used as a raw material carries a few other components that cannot be separated before the feeding of the brine into the process. The presence of sulfates and chloride ions in addition to high NaOH concentrations (0.44 \u2013 0.5 mass fraction) and high temperatures (above 60\u00a0\u00b0C, at 86\u202fmmHg) causes the precipitation of a triple salt (Na2SO4\u202f\u00b7\u202fNaCl\u202f\u00b7\u202fNaOH). This work focuses on using and validating the model of a triple effect evaporator in Aspen Plus using plant data. According to our results, lower temperatures and the extraction of sulfates could reduce the proportion of triple salt that precipitates in the last stages of the evaporator.",
     "keywords": ["Triple effect evaporator", "Sodium hydroxide", "Sodium chloride", "Sodium sulfate Simulation", "Triple salt and precipitation"]},
    {"article name": "A stoichiometric method for reducing simulation cost of chemical kinetic models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.02.020",
     "publication date": "04-2018",
     "abstract": "Mathematical models for chemically reacting systems have high degrees of freedom (very large) and are computationally expensive to analyse. In this discussion, we present and analyse a model reduction method that is based on stoichiometry and mass balances. This method can significantly reduce the high degrees of freedom of such systems. Numerical simulations are undertaken to validate and establish efficiency of the method. A practical example of acid mine drainage is used as a test case to demonstrate the efficacy of the procedure. Analytical results show that the stoichiometrically-reduced model is consistent with the original large model, and numerical simulations demonstrate that the method can accelerate convergence of the numerical schemes in some cases.",
     "keywords": ["Chemically reacting systems", "Mathematical models", "Model reduction method", "Numerical simulation", "Stoichiometry", "92E20", "80A30", "92F05"]},
    {"article name": "Multiobjective optimization and experimental validation for batch cooling crystallization of citric acid anhydrate",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.02.019",
     "publication date": "04-2018",
     "abstract": "Multiobjective optimization (MOO) of crystallization systems is gaining importance due to its ability to handle multiple conflicting objectives together for finding optimal operating policies. The present study focuses on batch cooling crystallization of citric acid. Among the two forms of citric acid, citric acid anhydride (CAA) is chosen for experimentation as no such study is available. MOO is carried out to seek optimal cooling policy for unseeded cooling crystallization of CAA to maximize mean crystal size while minimizing variance in size. In this procedure, temperature is discretized using piecewise constant-control vector parameterization which is simple and convenient for practical implementation. The model reported in literature is suitably modified for solubility parameters which are verified experimentally, and employed for optimization. One of the optimal solutions from the Pareto solution set is implemented through experimentation successfully and the measured product crystal properties are comparable to the predicted results obtained through optimization.",
     "keywords": ["Multiobjective optimization", "Batch cooling crystallization", "Citric acid anhydrate", "Crystal size distribution", "Optimal cooling policy", "Experimental implementation"]},
    {"article name": "Modelling the effect of temperature on the gel-filtration chromatographic protein separation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.02.023",
     "publication date": "04-2018",
     "abstract": "In this study a mechanistic model for chromatography was taken and the solution to its Laplace transfer function was obtained using the Fast Fourier Transform method. Using previously developed correlations for modelling diffusion, both in solution and intraparticle, and estimating the mass transfer coefficient, the effect of temperature in gel fittration liquid chromatography was investigated. The effect of each individual parameter on the elution curve was systematically explored allowing for reasonable estimates for the different temperature cases. In this case, a system of bovine serum albumin and phenylalanine separated by gel filtration chromatography was simulated to demonstrate how the resolution and the selectivity of the separation will change with physical parameters. Decreasing the particle size and flow rate while increasing the temperature led to higher resolution, which is consistent with experimental literature data.",
     "keywords": ["Chromatography", "Temperature", "Diffusivity", "Simulation", "Separation"]},
    {"article name": "Identification of systems with slowly sampled outputs using LPV model",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.02.022",
     "publication date": "04-2018",
     "abstract": "Identification of systems with slowly sampled output is studied. A linear parameter varying (LPV) model with multi-model structure is used to solve the problem. The output error (OE) method is used to estimate model parameters. Firstly, the local models and weighting functions are estimated separately using optimization methods. Then, a relaxation iteration method is developed to refine the parameters of the total model. For LPV model structure determination, an engineering approach is proposed that combines process knowledge with the so-called final output error criteria (FOE). The method is verified using both simulation data and industrial data. In the industrial case study, the LPV models give more accurate prediction of product qualities than that of a linear dynamic model and that of a static nonlinear model; the result also indicates the necessity of using test signals in soft-sensor development.",
     "keywords": ["Identification", "Slowly sampled output", "LPV model", "OE method", "Relaxation method"]},
    {"article name": "Heat exchanger network cleaning scheduling: From optimal control to mixed-Integer decision making",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.12.004",
     "publication date": "03-2018",
     "abstract": "An approach for optimising the cleaning schedule in heat exchanger networks (HENs) subject to fouling is presented. This work focuses on HEN applications in crude oil preheat trains located in refineries. Previous approaches have focused on using mixed-integer nonlinear programming (MINLP) methods involving binary decision variables describing when and which unit to clean in a multi-period formulation. This work is based on the discovery that the HEN cleaning scheduling problem is in actuality a multistage optimal control problem (OCP), and further that cleaning actions are the controls which appear linearly in the system equations. The key feature is that these problems exhibit bang-bang behaviour, obviating the need for combinatorial optimisation methods. Several case studies are considered; ranging from a single unit up to 25 units. Results show that the feasible path approach adopted is stable and efficient in comparison to classical methods which sometimes suffer from failure in convergence.",
     "keywords": ["Optimal control problem", "Bang-bang control", "Fouling", "Optimisation", "Scheduling", "Heat exchanger networks"]},
    {"article name": "Rigorous design of reaction-separation processes using disjunctive programming models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.12.013",
     "publication date": "03-2018",
     "abstract": "A systematic and efficient method for the rigorous design of complex chemical processes is significant in the chemical industry. In this paper, a superstructure-based optimization approach for the rigorous and simultaneous design of reaction and separation processes using generalized disjunctive programming (GDP) models is presented. In the reactor network, disjunctions for conditional reactors are introduced where the balance and reaction kinetic equations are applied only if the reactor is selected. Based on the proposed reactor disjunctions, two different reactor superstructures are developed and employed. In addition, the GDP representation of distillation columns is used to model the separation network. The reliability and efficiency of the proposed optimization method are demonstrated on two case studies, i.e., one cyclohexane oxidation process and one benzene chlorination process. The flowsheet structure and process-unit operating conditions are simultaneously optimized to minimize the total annual cost of the processes.",
     "keywords": ["Reaction-separation process design", "Process optimization", "Generalized disjunctive programming", "Cyclohexane oxidation", "Benzene chlorination", "CSTR continuous stirred tank reactor", "continuous stirred tank reactor", "DDT dichloro-diphenyl-trichloroethane", "dichloro-diphenyl-trichloroethane", "GDP generalized disjunctive programming", "generalized disjunctive programming", "MINLP mixed-integer nonlinear programming", "mixed-integer nonlinear programming", "NLP nonlinear programming", "nonlinear programming", "PFR plug flow reactor", "plug flow reactor", "SEN state equipment network", "state equipment network", "STN state tank network", "state tank network", "TAC total annual cost", "total annual cost"]},
    {"article name": "A strategy for enhancing the operational agility of petroleum refinery plant using case based fuzzy reasoning method",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.12.021",
     "publication date": "03-2018",
     "abstract": "Operational agility, which represents the capability of the plant/facility regarding the fast detection and adaption to the new situations facing external/internal changes, is commonly regarded as one of central-properties for Smart Process Manufacturing. Clearly, operational agility significantly affects the plant/facility performance such as profit and safety. In this work, a strategy for enhancing the operational agility of petroleum refinery plants is proposed. For this strategy, the accumulated data sets from the industrial plants as well as the high-fidelity simulation activities are firstly processed to formulate the case base with a determined structure. Fuzzy matching is adopted to evaluate the similarity between the new coming case and the potential one in the formulated case base. A new criterion, namely stability number, is proposed as the performance metric for choosing an appropriate type of Fuzzy membership function (FMF). Furthermore, an optimization model is set to optimize parameters of the selected Fuzzy membership function. The application of the proposed strategy to an industrial Fluidized Catalytic Cracking Unit (FCCU) is performed to demonstrate the relevant effectiveness.",
     "keywords": ["Reformulated case structure", "Case base", "Fuzzy matching", "Fluidized catalytic cracking unit", "Smart process manufacturing"]},
    {"article name": "Chemical process systems analysis using thermodynamic balance equations with entropy generation. Revaluation and extension",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.12.020",
     "publication date": "03-2018",
     "abstract": "Modeling inconsistencies exhibited in previous work (O'Connell,\u00a02017) associated with post-combustion methanolamine (MEA) and ammonia (NH3) absorption processes have been revealed. The origin of the problem was that entropies for ionic reactions were not evaluated from input model equilibrium constants regressed from data. Revised calculations have been made using only properties of formation. Positive and consistent entropy generation rates ( S \u02d9 g e n ) are now found for all units and sections, and process convergence was achieved for multiple sections rather than for only single sections as before. Only minor changes in material, energy and overall S \u02d9 g e n appeared in the new simulations. Results for S \u02d9 g e n values show that the greatest irreversibilities for the MEA process are in the stripping section, though significant effects appear in the chiller, heat exchanger, and stripper flash. For the NH3 process, roughly equal and large contributions to S \u02d9 g e n are in the absorption, heat exchange, and stripping sections. Process improvements should focus on these sections. Applying the methodology to proposed changes would quantitatively reveal the amounts of increased efficiency.",
     "keywords": null},
    {"article name": "Optimal decomposition for distributed optimization in nonlinear model predictive control through community detection",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.12.010",
     "publication date": "03-2018",
     "abstract": "Distributed optimization, based on a decomposition of the entire optimization problem, has been applied to many complex decision making problems in process systems engineering, including nonlinear model predictive control. While decomposition techniques have been widely adopted, it remains an open problem how to optimally decompose an optimization problem into a distributed structure. In this work, we propose to use community detection in network representations of optimization problems as a systematic method of partitioning the optimization variables into groups, such that the variables in the same groups generally share more constraints than variables between different groups. The proposed method is applied to the decomposition of the optimal control problem involved in the nonlinear model predictive control of a reactor-separator process, and the quality of the resulting decomposition is examined by the resulting control performance and computational time. Our result suggests that community detection in network representations of the optimization problem generates decompositions with improvements in computational performance as well as a good optimality of the solution.",
     "keywords": ["Network decomposition", "Distributed optimization", "Community detection", "Nonlinear model predictive control"]},
    {"article name": "Integrated scheduling of rolling sector in steel production with consideration of energy consumption under time-of-use electricity prices",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.12.018",
     "publication date": "03-2018",
     "abstract": "Due to increasing load and penetration of renewables, the electric grid is using time-of-use pricing for industrial customers. Involving energy-intensive processes, steel companies can reduce their production cost by accounting for changes in electricity pricing. In particular, steel companies can take advantage of processing flexibility to make better use of electric power, and thus reduce the energy cost. In this paper, we address a new integrated scheduling problem of multi-stage production derived from the rolling sector of steel production, with consideration of campaign decisions and demand-side management. The problem is formulated as a continuous time mixed-integer nonlinear programming (MINLP) model with generalized disjunctive programming (GDP) constraints, which is then reformulated as a mixed-integer linear programming (MILP) model. Numerical results are presented to demonstrate that the model is computationally efficient and compact.",
     "keywords": ["Integrated scheduling", "Continuous time modeling", "MINLP", "GDP"]},
    {"article name": "A systematic approach for modeling of waterflooding process in the presence of geological uncertainties in oil reservoirs",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.12.012",
     "publication date": "03-2018",
     "abstract": "In this paper, a systematic approach which is able to consider different types of geological uncertainty is presented to model the waterflooding process. The proposed scheme, which is based on control and system theories, enables the experts to apply suitable techniques to optimize the production. By using the developed methodology, a reasonable mapping between defined system inputs and outputs in frequency domain and around a specific operating point is established. In addition, a nominal model for the process as well as a lumped representation for uncertainty effects are provided. Based on the proposed modeling mechanism, reservoir management goals can be pursued in the presence of uncertainty by utilization of complicated control and optimization strategies. The developed algorithm has been simulated on 10th SPE-model#2. Observed results have shown that the introduced methodology is able to effectively model the dynamics of waterflooding process, while taking into account the assumed induced geological uncertainty.",
     "keywords": ["Waterflooding process", "Uncertainty quantification", "System identification", "Robust control and optimization", "Geological realization of reservoir", "K-means clustering"]},
    {"article name": "State estimation of wastewater treatment plants based on model approximation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.01.003",
     "publication date": "03-2018",
     "abstract": "In this article, we consider state estimation of wastewater treatment plants based on model approximation. In particular, we consider a wastewater treatment plant described by the Benchmark Simulation Model No.1 which consists of a five-chamber reactor and a settler. We propose to use the proper orthogonal decomposition approach with re-identification of output equations to obtain a reduced-order model of the original system. Then, the reduced-order model is taken advantage of in state estimation. An approach on how to determine an appropriate minimum measurement set is also proposed based on degree of observability. A continuous-discrete extended Kalman filtering algorithm is used to design the estimator based on the reduced-order model. We show through extensive simulations under different weather conditions that the estimator based on the reduced-order model with re-identified output equations gives good state estimates of the actual process.",
     "keywords": ["State estimation", "Wastewater treatment plants", "Model approximation", "Proper orthogonal decomposition", "Observability"]},
    {"article name": "Bounded-error optimal experimental design via global solution of constrained min\u2013max program",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.12.016",
     "publication date": "03-2018",
     "abstract": "We present an improvement of existing methods for globally solving optimal experimental design (OED) for bounded-error estimation based on a bilevel formulation from Mukkala et\u00a0al. (2017). The proposed solution method for the min\u2013max program is based on our method for generalized semi-infinite programs (via restriction of the right-hand side). The algorithm employed has the advantage that it guarantees a global solution for the OED assuming the global solution of two subproblems. To obtain a feasible solution only the lower-level problem has to be solved globally. In case of a local solution of the upper-level problem, the solution is still feasible though it is an upper bound of the global solution. The min\u2013max method for OED is illustrated with four examples: two simple chemical reactions, BET-adsorption and a reformulated predator-prey system. The benefits of global methods are shown along with the limitations of state-of-the-art global solvers.",
     "keywords": ["Optimal experimental design", "Bounded-error estimation", "Set-membership estimation", "Global optimization", "Min\u2013max problems"]},
    {"article name": "Process modelling, simulation and technoeconomic evaluation of crystallisation antisolvents for the continuous pharmaceutical manufacturing of rufinamide",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.12.014",
     "publication date": "03-2018",
     "abstract": "Continuous Pharmaceutical Manufacturing (CPM) is a promising new paradigm to produce active pharmaceutical ingredients (APIs), allowing reduced equipment dimensions, lower waste production and energy consumption, and safer operation in comparison to the industrially dominant batch methods. Rufinamide is an antiepileptic agent whose demonstrated continuous flow synthesis (featuring three reactions in flow) circumvents the accumulation of toxic and explosive organoazide intermediates. To ascertain the feasibility and viability of this continuous synthetic route, systematic process modelling and costing is required. This paper presents a technoeconomic analysis of the upstream continuous flow synthesis of rufinamide via steady-state process modelling and plantwide simulation. Reaction kinetics and Arrhenius parameters are estimated from previously published experimental data, and plug flow reactor (PFR) volumes are calculated towards rigorous plant costing. Continuous reactor and separator units have been designed, and the CPM flowsheet is compared vs. the batch production method, with respect to technical efficiency and profitability. Plantwide costing via an established economic analysis methodology has been pursued to enable a detailed comparison of cost items towards process scale-up, as well as motivate the need for further systematic optimisation.",
     "keywords": ["Continuous pharmaceutical manufacturing (CPM)", "Rufinamide", "Process modelling and simulation", "Technoeconomic evaluation", "Green chemistry"]},
    {"article name": "Data-driven stochastic robust optimization: General computational framework and algorithm leveraging machine learning for optimization under uncertainty in the big data era",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.12.015",
     "publication date": "03-2018",
     "abstract": "A novel data-driven stochastic robust optimization (DDSRO) framework is proposed for optimization under uncertainty leveraging labeled multi-class uncertainty data. Uncertainty data in large datasets are often collected from various conditions, which are encoded by class labels. Machine learning methods including Dirichlet process mixture model and maximum likelihood estimation are employed for uncertainty modeling. A DDSRO framework is further proposed based on the data-driven uncertainty model through a bi-level optimization structure. The outer optimization problem follows a two-stage stochastic programming approach to optimize the expected objective across different data classes; adaptive robust optimization is nested as the inner problem to ensure the robustness of the solution while maintaining computational tractability. A decomposition-based algorithm is further developed to solve the resulting multi-level optimization problem efficiently. Case studies on process network design and planning are presented to demonstrate the applicability of the proposed framework and algorithm.",
     "keywords": ["Big data", "Optimization under uncertainty", "Bayesian model", "Machine learning", "Process design and operations"]},
    {"article name": "Collection of benchmark test problems for data reconciliation and gross error detection and identification",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.01.002",
     "publication date": "03-2018",
     "abstract": "In an industrial scenario, one can find measured data that do not satisfy the mass and energy laws of conservation. This problem can be approached by applying data reconciliation (DR) and gross error detection and identification (GEDI) techniques, however, authors generally validate their methods using a reduced set of problems, restricting the application of the proposed methods to them. The objective of this work is to present a collection of benchmark problems for DR and GEDI to help the evaluation of these methods in different types of flowsheets. First, challenges issues related with DR and GED are presented with examples. Then, a general overview of the benchmark collection set is presented. In conclusion, it can be observed that this challenging research area needs a common problem set for validating DR and GEDI and this paper fills this gap, helping the validation of the methods.",
     "keywords": ["Data reconciliation", "Gross error detection and identification", "Fault detection", "Benchmark problems"]},
    {"article name": "A novel approach to process operating mode diagnosis using conditional random fields in the presence of missing data",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.12.017",
     "publication date": "03-2018",
     "abstract": "Diagnosis of process operating modes is an important aspect of process monitoring. Due to its ability to model process transitions, the Hidden Markov Model (HMM) is widely used as a tool for operating mode diagnosis. However, it suffers from certain drawbacks due to its inherent assumptions. Hence, to address these issues and improve the operating mode diagnosis performance, we introduce the Conditional Random Field (CRF), which is a discriminiative probabilistic model based approach. Further, to deal with the missing measurement problem that commonly occurs in industrial datasets, a marginalized CRF framework is proposed in this paper and the related inference algorithms are developed under this newly designed framework. Validation studies performed on a simulated continuous stirred tank reactor (CSTR) system and an experimental hybrid tank system demonstrate that the proposed CRF based algorithms have superior performances compared to the existing approaches.",
     "keywords": ["Process monitoring", "Operating mode diagnosis", "Marginalized conditional random fields", "Hidden Markov Models", "Missing measurements"]},
    {"article name": "Optimal scheduling of interconnected power systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.01.004",
     "publication date": "03-2018",
     "abstract": "This paper presents an optimization-based approach to address the problem of the optimal daily energy scheduling of interconnected power systems in electricity markets. More specifically, a Mixed Integer Linear Programming model (MILP) has been developed to address the specific challenges of the underlying problem. The main focus of the proposed framework is to examine the importance and the impacts of electricity interconnections and cross-border electricity trade on the scheduling of power systems, both at a technical and economic level. The applicability of the proposed approach has been tested on an illustrative case study including five power systems which can be interconnected (with a certain interconnection structure) or not. The proposed model determines in a detailed and analytical way the optimal power generation mix, the electricity trade among the systems, the electricity flows (in case of interconnection options), the marginal price of each system, as well as it investigates through a sensitivity analysis the effects of the available interconnection capacity on the resulting power production mix. The work demonstrates that the proposed optimization approach is able to provide important insights into the appropriate energy strategies followed by the market participants, as well as on the strategic long-term decisions to be implemented by investors and/or policy makers at a national and/or regional level, underlining potential risks and providing appropriate price signals on critical energy infrastructure projects under real market operating conditions.",
     "keywords": ["Energy scheduling", "Electricity interconnections", "Unit commitment", "Electricity trade", "Day-ahead market"]},
    {"article name": "Efficient numerical simulation of simulated moving bed chromatography with a single-column solver",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.12.022",
     "publication date": "03-2018",
     "abstract": "We present four different numerical methods for the numerical simulation of simulated moving bed chromatography. Two approaches use fixed-point iteration for computing cyclic steady states, and two other approaches use operator splitting for computing complete system trajectories. All methods are based on weak coupling of individual column models and can easily be implemented using any existing single-column solver. Simulation software is implemented based on the CADET project and published as open source code.The numerical performance is compared using five case studies. For both fixed-point iteration and operator-splitting, an alternative approach is found to be more efficient than the standard approach. Namely, the one-column analog saves time in computing the cyclic steady state, while lag-aware operator-splitting yields more detailed information on the system trajectory. The presented methods can be combined with other models, for example to consider hold-up volumes, and have applications beyond simulated bed chromatography.",
     "keywords": ["Simulated moving bed chromatography", "One-column analog", "Operator-splitting", "Cyclic steady state", "System trajectory"]},
    {"article name": "Oil industry value chain simulation with learning agents",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.01.008",
     "publication date": "03-2018",
     "abstract": "Simulation is an important tool to evaluate many systems, but it often requires detailed knowledge of each specific system and a long time to generate useful results and insights. A large portion of the required time stems from the need to define operational rules and build valid models that represent them properly. To shorten this model construction time, a learning-agent-based model is proposed. This technique is recommended for cases where optimal policies are not known or hard and costly to unequivocally determine, as it enables the simulation agents to learn good policies \u201cby themselves\u201d. A model is built with this technique and a representative case study of oil industry value chain simulation is presented as a proof of concept.",
     "keywords": ["Simulation", "Oil", "Agent", "Machine learning"]},
    {"article name": "Optimal operation of parallel distillation systems with multiple product grades: An industrial case study",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.01.009",
     "publication date": "03-2018",
     "abstract": "In the fine chemical industry, customers often demand different grades with different purity specifications. To achieve the best performance, the production tasks should be assigned to different distillation columns at the most suitable operating conditions and time periods. In this paper, an optimal scheduling method is presented through an industrial case study with multiple products and parallel distillation columns. Rigorous nonlinear models are built for each distillation column and validated with plant data, based on which, a reduced-order model is obtained with data of optimal operating points at various conditions. The reduced-order model is then incorporated into a mode-based discrete-time mixed integer linear program (MILP) scheduling model, where transitions between different operating modes are specified based on plant data. The MILP-based scheduling is applied to a real-word industrial case study to demonstrate its computational efficiency and effectiveness in improving economic performance with comparison to two heuristic scheduling methods.",
     "keywords": ["Optimal scheduling", "Parallel distillation systems", "Multiple products", "Reduced-order model"]},
    {"article name": "Expansion development planning of thermocracking-based bitumen upgrading plant under uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.01.007",
     "publication date": "03-2018",
     "abstract": "Expansion development of upgrading plants is an important decision to make for the oil sands industry. In this paper, we propose a multistage stochastic expansion development method to tackle uncertain synthetic crude oil (SCO) and CO2 tax prices. The linear decision rule based technique is applied to solve the proposed stochastic optimization model. Various analyses are conducted based on optimization results: (i) effects of the uncertainty set size, (ii) comparison of solutions for selected pessimistic, realistic, and optimistic scenarios, (iii) effects of different operating modes for an upgrading plant, and (iv) cost distribution. Results of this work demonstrate that the stochastic model provides a more flexible, economical, and robust solution compared to the deterministic solution. In addition, the CO2 tax price affects the optimal solution negligibly compared to the SCO price. Finally, expansion development of the studied upgrading plant is economically beneficial even at the current market state.",
     "keywords": ["Bitumen upgrading", "Capacity expansion", "Stochastic programming", "Linear decision rule"]},
    {"article name": "Mathematical analysis of bioethanol production through continuous reactor with a settling unit",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.01.001",
     "publication date": "03-2018",
     "abstract": "Bioethanol generated from biomass and bioenergy crops has been promulgated as one of the most feasible alternatives to fossil fuel, as it is considered to be clean, green and of course renewable. In this paper, we develop a nonlinear chemostat model for bioethanol production. The growth rate is given by a modified Andrew expression with inhibition by ethanol, which is often used to model the growth of biomass. Then the uniqueness, invariance and dissipation of solutions of the model are established. Steady state solutions of the model and their stability are also determined as a function of the residence time. We also investigate the effects of the model parameters on a physically valid region.Finally, we compare the performance of a reactor, using a recycling process, against that of without recycling process. With a substrate feed concentration of 100\u202fg/L, there is a critical value of the residence time which identifies a turning point in the reactor performance for the removal of substrate.",
     "keywords": ["Mathematical model", "Bioethanol production", "The stability analysis", "Settling unit"]},
    {"article name": "Robust optimization for decision-making under endogenous uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.01.006",
     "publication date": "03-2018",
     "abstract": "This paper contemplates the use of robust optimization as a framework for addressing problems that involve endogenous uncertainty, i.e., uncertainty that is affected by the decision maker\u2019s strategy. To that end, we extend generic polyhedral uncertainty sets typically considered in robust optimization into sets that depend on the actual decisions. We present the derivation of robust counterpart models in this setting, and we discuss relevant algorithmic considerations for solving these models to guaranteed optimality. Besides capturing the functional changes in parameter correlations that may be induced by given decisions, we show how the use of our decision-dependent uncertainty sets allows us to also eradicate conservatism effects from parameters that become irrelevant in view of the optimal decisions. We quantify these benefits via a number of case studies, demonstrating our proposed framework\u2019s versatility to be utilized in the context of various applications.",
     "keywords": ["Robust optimization", "Endogenous uncertainty", "Decision-dependent uncertainty sets"]},
    {"article name": "Control of heat-integrated extractive distillation processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.12.008",
     "publication date": "03-2018",
     "abstract": "Pressure in many distillation columns is set such that cooling water can be used in the condenser. Pressure selection is more involved in some columns such as reactive distillation in which there is a trade-off between temperatures favorable for reaction kinetics and temperatures favorable for vapor-liquid equilibrium. In azeotropic systems, pressure selection is critical in achieving the desired separation by consideration of distillation boundaries and isovolatility curves. A recent paper presented a striking example of pressure selection in extractive distillation. Operation at 1\u00a0atm required a solvent-to-feed (S/F) ratio of 3.52 while operating at 10\u00a0atm cut the S/F to only 0.717.The purpose of this paper is to explore the dynamic controllability of this low S/F extractive distillation system. Results show that the control structure must be modified from the conventional configuration used in most extractive distillation processes.",
     "keywords": ["Extractive distillation", "Azeotropes", "Distillation control"]},
    {"article name": "Experimental and numerical assessment of the hydraulic behavior of a pilot-scale Periodic Anaerobic Baffled Reactor (PABR)",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.01.014",
     "publication date": "03-2018",
     "abstract": "A major factor that determines the mixing pattern inside a reactor is the Residence Time Distribution (RTD). The aim of this paper is the study of the hydraulic behavior of a pilot-scale Periodic Anaerobic Baffled Reactor (PABR) through experimental RTD tests and the development of a CFD model. The tank in series model was used for the determination of the equivalent number (NR) of Continuous Stirred Tank Reactors (CSTR) that simulates the mixing pattern. The hydraulic \u201cdead\u201d space (Vd) was also calculated. Both experimental and simulation results indicate the flexibility of the reactor to perform as a CSTR (NR=\u202f1, Vd=\u202f38.6%\u201339.6%), as a Plug Flow Reactor (PFR) (NR=\u202f10\u201316, Vd=\u202f25%\u201335.4%) or intermediate between the two, depending on the operating parameters. The CFD model can be used for the optimization of the reactor's geometry and for the selection of the optimal operating parameters depending on the type of wastewater to be treated.",
     "keywords": ["Pilot-scale PABR", "Modeling", "Hydraulic behavior", "Comsol Multiphysics", "Residence Time Distribution", "ABR Anaerobic Baffled Reactor", "Anaerobic Baffled Reactor", "CFD Computational Fluid Dynamics", "Computational Fluid Dynamics", "CSTR Continuous Stirred Tank Reactor", "Continuous Stirred Tank Reactor", "HRT Hydraulic Retention Time", "Hydraulic Retention Time", "NR equivalent number of CSTR in series", "equivalent number of CSTR in series", "OLR Organic Loading Rate", "Organic Loading Rate", "PABR Periodic Anaerobic Baffled Reactor", "Periodic Anaerobic Baffled Reactor", "PFR Plug Flow Reactor", "Plug Flow Reactor", "RTD Residence Time Distribution", "Residence Time Distribution", "SRT Solids Retention Time", "Solids Retention Time", "T Feedstock compartment switching period", "Feedstock compartment switching period", "UASB Upflow Anaerobic Sludge Blanket", "Upflow Anaerobic Sludge Blanket", "VFA Volatile Fatty Acids", "Volatile Fatty Acids", "Vd \u201cdead\u201d space percentage", "\u201cdead\u201d space percentage"]},
    {"article name": "Resilient solar photovoltaic supply chain network design under business-as-usual and hazard uncertainties",
     "doi": "https://doi.org/10.1016/j.compchemeng.2018.01.013",
     "publication date": "03-2018",
     "abstract": "Unlike their inherent advantageous features, photovoltaic systems have not yet penetrated the market adequately due to their high price against other electricity generation options. To propel this fledgling industry further towards commercialization, the efficient and effective design of its supply chain is of paramount importance. In this regard, this study proposes a hybrid robust-scenario based optimization model to design a resilient photovoltaic supply chain under both business-as-usual and hazard uncertainties. To capture business-as-usual uncertainty, a customized robust optimization method is developed, which is capable of tackling correlated uncertain parameters and adjusting the level of conservatism in the solutions. Likewise, a number of proactive and reactive resilience strategies are incorporated into the model to ameliorate the resilience level of the concerned supply chain in the presence of hazard uncertainty. The capabilities of the developed model are explored by discussing a real case study via which helpful managerial insights are gained.",
     "keywords": ["Solar energy", "Photovoltaic supply chain", "Robust optimization", "Resilience", "Business-as-usual uncertainty", "Hazard uncertainty"]},
    {"article name": "Efficient formulations for dynamic warehouse location under discrete transportation costs",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.05.011",
     "publication date": "03-2018",
     "abstract": "A Mixed-integer Linear Programming model is proposed to determine the optimal number, location and capacity of the warehouses required to support a long-term forecast with seasonal demand. Discrete transportation costs, dynamic warehouse contracting, and the handling of safety stock are the three main distinctive features of the problem. Four alternatives for addressing discrete transportation costs are compared. The most efficient formulation is obtained using integer variables to account for the number of units used of each transportation mode. Contracting policies constraints are derived to ensure use of warehouses for continuous periods. Similar constraints are included for the case when a warehouse is closed. Safety stock with risk-pooling effect is considered using a piecewise-linear representation. To solve large-scale problems, tightening constraints, and simplified formulations are proposed. These formulations are based on single-sourcing assumptions and yield near-optimal results with large reduction in the solution time.",
     "keywords": ["Supply chain design", "Facility location", "Discrete transportation cost", "Safety stock", "Mixed-integer Linear Programming"]},
    {"article name": "APT-MCMC, a C++/Python implementation of Markov Chain Monte Carlo for parameter identification",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.11.011",
     "publication date": "02-2018",
     "abstract": "The inverse problem associated with fitting parameters of an ordinary differential equation (ODE) system to data is nonlinear and multimodal, which is of great challenge to gradient-based optimizers. Markov Chain Monte Carlo (MCMC) techniques provide an alternative approach to solving these problems and can escape local minima by design. APT-MCMC was created to allow users to setup ODE simulations in Python and run as compiled C++ code. It combines affine-invariant ensemble of samplers and parallel tempering MCMC techniques to improve the simulation efficiency. Simulations use Bayesian inference to provide probability distributions of parameters, which enable analysis of multiple minima and parameter correlation.Benchmark tests result in a 20\u00d7\u201360\u00d7 speedup but 14% increase in memory usage against emcee, a similar MCMC package in Python. Several MCMC hyperparameters were analyzed: number of temperatures, ensemble size, step size, and swap attempt frequency. Heuristic tuning guidelines are provided for setting these hyperparameters.",
     "keywords": ["MCMC", "Simulation", "Bayesian inference"]},
    {"article name": "A novel robust optimization approach for an integrated municipal water distribution system design under uncertainty: A case study of Mashhad",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.11.017",
     "publication date": "02-2018",
     "abstract": "This paper proposes a novel robust optimization (RO) approach along with a two-stage scenario-based stochastic programming to optimize a municipal water distribution system (WDS) under demand and rainfall uncertainties. Firstly, we have proposed a new multi-period mixed-integer linear programming (MILP) formulation of a municipal WDS. The goal is to find solutions that are both cost-effective and completely fulfill potable and non-potable demand in an integrated system. Furthermore, a novel RO approach is developed which attempts to adjust protection level in a column what we call \u201cadjustable column-wise robust optimization\u201d. The interesting point of the proposed RO approach is its linear structure and being computationally tractable. The efficiency of the proposed models are evaluated through a real case study of Mashhad. The acquired results reveal the proposed WDS model have dramatically reduced the total costs. Simultaneously, the RO approach has risen robustness besides realization demonstrates its better performance than deterministic one.",
     "keywords": ["Robust optimization", "Water distribution system", "Two-stage scenario-based stochastic programming"]},
    {"article name": "An efficient MILP framework for integrating nonlinear process dynamics and control in optimal production scheduling calculations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.11.021",
     "publication date": "02-2018",
     "abstract": "The emphasis currently placed on enterprise-wide decision making and optimization has led to an increased need for methods of integrating nonlinear process dynamics and control information in scheduling calculations. The inevitable high dimensionality and nonlinearity of first-principles dynamic process models makes incorporating them in scheduling calculations challenging. In this work, we describe a general framework for deriving data-driven surrogate models of the closed-loop process dynamics. Focusing on Hammerstein\u2013Wiener and finite step response (FSR) model forms, we show that these models can be (exactly) linearized and embedded in production scheduling calculations. The resulting scheduling problems are mixed-integer linear programs with a special structure, which we exploit in a novel and efficient solution strategy. A polymerization reactor case study is utilized to demonstrate the merits of this method. Our framework compares favorably to existing approaches that embed dynamics in scheduling calculations, showing considerable reductions in computational effort.",
     "keywords": ["Integrated scheduling and control", "Nonlinear dynamics", "Surrogate models", "Hammerstein\u2013Wiener models", "Finite step response models", "Lagrangian relaxation"]},
    {"article name": "Distributionally robust optimization for planning and scheduling under uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.12.002",
     "publication date": "02-2018",
     "abstract": "Distributionally robust optimization (DRO) is an emerging and effective method to address the inexactness of probability distributions of uncertain parameters in decision-making under uncertainty. We propose an effective DRO framework for planning and scheduling under demand uncertainties. A novel data-driven approach is proposed to construct ambiguity sets based on principal component analysis and first-order deviation functions, which help excavating accurate and useful information from uncertainty data. Moreover, it leads to mixed-integer linear reformulations of planning and scheduling problems. To account for the multi-stage sequential decision-making structure in process operations, we further develop multi-stage DRO models and adopt affine decision rules to address the computational issue. Applications in industrial-scale process network planning and batch process scheduling demonstrate that, the proposed DRO approach can effectively leverage uncertainty data information, better hedge against distributional ambiguity, and yield more profits.",
     "keywords": ["Distributionally robust optimization", "Decision-making under uncertainty", "Multi-stage decision-making", "Process scheduling", "Process planning", "Big data"]},
    {"article name": "Efficient simulation of chromatographic separation processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.12.006",
     "publication date": "02-2018",
     "abstract": "This work presents the development and testing of an efficient, high resolution algorithm developed for the solution of equilibrium and non-equilibrium chromatographic problems as a means of simultaneously producing high fidelity predictions with a minimal increase in computational cost. The method involves the coupling of a high-order WENO scheme, adapted for use on non-uniform grids, with a piecewise adaptive grid (PAG) method to reduce runtime while accurately resolving the sharp gradients observed in the processes under investigation. Application of the method to a series of benchmark chromatographic test cases, within which an increasing number of components are included over short and long spatial domains and containing shocks, shows that the method is able to accurately resolve the discontinuities and that the use of the PAG method results in a reduction in the CPU runtime of up to 90%, without degradation of the solution, relative to an equivalent uniform grid.",
     "keywords": ["Column chromatography", "WENO scheme", "Adaptive mesh refinement"]},
    {"article name": "Global optimisation of multi-plant manganese alloy production",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.12.001",
     "publication date": "02-2018",
     "abstract": "This paper studies the problem of multi-plant manganese alloy production. The problem consists of finding the optimal furnace feed of ores, fluxes, coke, and slag that yields output products which meet customer specifications, and to optimally decide the volume, composition, and allocation of the slag. To solve the problem, a nonlinear pooling problem formulation is presented upon which the bilinear terms are reformulated using the Multiparametric Disaggregation Technique (MDT). This enables global optimisation by means of commercial software for mixed integer linear programs. We demonstrate the model and solution approach through case studies from a Norwegian manganese alloy producer. The computational study shows that the model and proposed optimisation approach can solve problem sizes of up to ten furnaces to a small optimality gap, that global optimization approach with MDT scales well with larger, real problem instances, and that the model outperforms the current operational practice.",
     "keywords": ["Manganese alloy production", "Pooling problem", "Multiparametric Disaggregation Technique", "Global optimisation", "Multi-plant production", "Mixed integer linear programming"]},
    {"article name": "A cost-effective retrofit of conventional distillation sequence to dividing-wall prefractionator configuration",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.12.009",
     "publication date": "02-2018",
     "abstract": "A dividing-wall prefractionator configuration was investigated for a safe and economic retrofit of the conventional sequence using simple distillation columns. In the proposed retrofit configuration, the first column was modified to a dividing wall column as a prefractionator to supply prefractionated multi-feeds to the subsequent column. To investigate the effectiveness of the proposed configuration, nine near-ideal feed mixtures were considered for analysis. The proposed configuration was then compared with several other alternative configurations. The proposed dividing-wall prefractionator efficiently generates prefractionated multi-feed streams avoiding feed mismatch and remixing effect with low modification cost. Moreover, because the proposed retrofit configuration allows for flexible switching between the dividing-wall prefractionator and the conventional operating mode, a safe retrofit is also ensured by reducing the operational risks. Several industrial retrofit cases were studied to validate the proposed dividing-wall prefractionator configuration.",
     "keywords": ["Distillation-process retrofit", "Dividing-wall column", "Cost effective retrofit", "Dividing-wall prefractionator", "Coordinate-descent methodology"]},
    {"article name": "Challenges and opportunities in biopharmaceutical manufacturing control",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.12.007",
     "publication date": "02-2018",
     "abstract": "This article provides a perspective on control and operations for biopharmaceutical manufacturing. Challenges and opportunities are described for (1) microscale technologies for high-speed continuous processing, (2) plug-and-play modular unit operations with integrated monitoring and control systems, (3) dynamic modeling of unit operations and entire biopharmaceutical manufacturing plants to support process development and plant-wide control, and (4) model-based control technologies for optimizing startup, changeover, and shutdown. A challenge is the ability to simultaneously address the uncertainties, nonlinearities, time delays, non-minimum phase behavior, constraints, spatial distributions, and mixed continuous-discrete operations that arise in biopharmaceutical operations. The design of adaptive and hybrid control strategies is discussed. Process data analytics and grey-box modeling methods are needed to deal with the heterogeneity and tensorial dimensionality of biopharmaceutical data. Novel bioseparations as discussed as a potential cost-effective unit operation, with a discussion of challenges for the widespread application of crystallization to therapeutic proteins.",
     "keywords": ["Biopharmaceuticals", "Biomanufacturing", "Biopharmaceutical manufacturing", "Continuous manufacturing", "Protein crystallization", "Process data analytics"]},
    {"article name": "A multifluid-PBE model for simulation of mass transfer limited processes operated in bubble columns",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.11.023",
     "publication date": "02-2018",
     "abstract": "Modeling of reactive dispersed flows with interfacial mass transfer limitations require an accurate description of the interfacial area, mass transfer coefficient and the driving force. The driving force is given by the difference in species composition between the continuous and dispersed phases and thus depends on bubble size. This paper shows the extension of the multifluid-PBE model to reactive and non-isothermal flows with novel transport equations for species mass and temperature which are continuous functions of bubble size. The model is demonstrated by simulating the Fischer-Tropsch synthesis operated in a slurry bubble column at industrial conditions. The simulation results show different composition and velocity for the smallest and largest bubbles. The temperature profile was independent of bubble size due to efficient heat exchange. The proposed model is particularly useful in investigating the effects of bubble size on strongly mass transfer limited processes operated in the heterogeneous flow regime.",
     "keywords": ["Kinetic theory of granular flow", "Multifluid model", "Population balance equation", "Dispersed phase flow", "Fischer-Tropsch", "Bubble column"]},
    {"article name": "Robust integrated production-maintenance scheduling for an evaporation network",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.12.005",
     "publication date": "02-2018",
     "abstract": "This work aims to reduce the global resource consumption in an industrial evaporation network by better tasks management and coordination. The network works in continuous, processing some products in several evaporation plants, so optimal load allocation and product-plant assignment problems appear. The plants have different features (capacity, equipment, etc.) and their performance is affected by fouling inside the heat exchangers and external factors. Hereby, the optimizer has to decide when maintenance operations have to be triggered. Therefore, a mixed production/maintenance scheduling problem arises. The plant behavior is approximated by surrogate linear models obtained experimentally, allowing thus the use of mixed-integer linear optimization routines to obtain solutions in acceptable time. Furthermore, uncertainty in the weather forecast and in the production plan is also considered via a two-stage stochastic programming approach. Finally, a trade-off analysis between other objectives of interest is given to support the decision maker.",
     "keywords": ["Production scheduling", "Stochastic optimization", "Integration", "Fouling", "Maintenance prediction", "Similarity index"]},
    {"article name": "Dynamic optimization of a cryogenic air separation unit using a derivative-free optimization approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.10.020",
     "publication date": "01-2018",
     "abstract": "Optimal dynamic product transition is a challenging and important issue in manufacturing plants. When a reliable dynamic model is available, gradient-based optimization algorithms can be used to achieve this aim. However, in some cases a first principles dynamic model may not be available. In this work, we will assume that only input-output information from a dynamic model embedded in a dynamic process simulator is available for optimal product transitions between products. We present a derivative-free optimization trust region approach to deal with the product dynamic optimization problem of an air separation unit used in several processing plants to obtain pure oxygen. High-purity oxygen is required in intensive energy applications such as steel plants and in combustion processes. A closed-loop model predictive control strategy is used where the system to be optimized is embedded in the ASPEN Dynamics simulation environment. The results demonstrate that black-box dynamic models can be dynamically optimized when model of the dynamic model and/or its gradient information are not available. We have successfully applied the non-linear model predictive control and derivate-free approach to several oxygen composition and productivity transition issues.",
     "keywords": ["Air separation unit", "Derivative-free optimization", "Model predictive control", "ASPEN simulation environment"]},
    {"article name": "Modelling and solving the integrated inventory-location-routing problem in a multi-period and multi-perishable product supply chain with uncertainty: Lagrangian relaxation algorithm",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.10.013",
     "publication date": "01-2018",
     "abstract": "In this article a three-echelon supply chain, consisting of a supplier, a number of distribution centers (DCs), and a number of retailers (customers) is modeled in form of the integrated inventory- location \u2013 routing problem (ILRP), in a way that perishable products are delivered to the customers in a limited time horizon, consisting of several time periods. The retailers\u2019 demand is stochastic and follows normal distribution with certain mean and standard deviation. The transportation fleet is heterogeneous, and distribution centers use a timetable, which will prevent interference of the vehicles operation and also allocation of a vehicle to more than one distribution center in each time period. Lagrangian Relaxation Method is used to solve the resulted model and determine the lower bound; and a heuristic algorithm is provided to feasibilize the result of the Lagrangian Relaxation Algorithm and determine the upper bound.",
     "keywords": ["Supply chain", "ILRP", "Perishable products", "Lagrangian relaxation algorithm"]},
    {"article name": "Identification in dynamic networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.10.005",
     "publication date": "01-2018",
     "abstract": "System identification is a common tool for estimating (linear) plant models as a basis for model-based predictive control and optimization. The current challenges in process industry, however, ask for data-driven modelling techniques that go beyond the single unit/plant models. While optimization and control problems become more and more structured in the form of decentralized and/or distributed solutions, the related modelling problems will need to address structured and interconnected systems. An introduction will be given to the current state of the art and related developments in the identification of linear dynamic networks. Starting from classical prediction error methods for open-loop and closed-loop systems, several consequences for the handling of network situations will be presented and new research questions will be highlighted.",
     "keywords": ["System identification", "Dynamic networks", "Identifiability", "Experiment design", "Model-based control", "Distributed control", "Closed-loop identification"]},
    {"article name": "Optimal scheduling and its Lyapunov stability for advanced load-following energy plants with CO2 capture",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.10.025",
     "publication date": "01-2018",
     "abstract": "In this study, a two-level control methodology consisting of an upper-level scheduler and a lower-level supervisory controller is proposed for an advanced load-following energy plant with CO2 capture. With the use of an economic objective function that considers fluctuation in electricity demand and price at the upper level, optimal scheduling of energy plant electricity production and carbon capture with respect to several carbon tax scenarios is implemented. The optimal operational profiles are then passed down to corresponding lower-level supervisory controllers designed using a methodological approach that balances control complexity with performance. Finally, it is shown how optimal carbon capture and electricity production rate profiles for an energy plant such as the integrated gasification combined cycle (IGCC) plant are affected by electricity demand and price fluctuations under different carbon tax scenarios. The paper also presents a Lyapunov stability analysis of the proposed scheme.",
     "keywords": ["CO2 capture", "Scheduling", "Predictive control", "Lyapunov stability", "Load-following", "IGCC"]},
    {"article name": "Data-driven robust optimization under correlated uncertainty: A case study of production scheduling in ethylene plant",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.10.024",
     "publication date": "01-2018",
     "abstract": "To hedge against the fluctuations generated from continuous production processes, practical solutions can be obtained through robust optimization induced by the classical uncertainty sets. However, uncertainties are sometimes correlated in industrial scheduling problems because of the connected process and various random factors. To capture and enrich the valid information of uncertainties, copulas are introduced to estimate the joint probability distribution and simulate mutual scenarios for uncertainties. Cutting planes are generated to remove unnecessary uncertain scenarios in the uncertainty sets, and then robust formulations induced by the cut set are proposed to reduce conservatism and improve the robustness of scheduling solutions. A real-world process of ethylene plant is introduced as the numerical case, and high-dimensional data-driven uncertainty sets are illustrated in detail. The proposed models are proved to control the fluctuation of consumed fuel gas below a lower level of conservatism.",
     "keywords": ["Data-driven", "Robust optimization", "Correlated uncertainty", "Production scheduling", "Copula", "Fuel gas"]},
    {"article name": "High-order approximation of chromatographic models using a nodal discontinuous Galerkin approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.10.023",
     "publication date": "01-2018",
     "abstract": "A nodal high-order discontinuous Galerkin finite element (DG-FE) method is presented to solve the equilibrium-dispersive model of chromatography with arbitrary high-order accuracy in space. The method can be considered a high-order extension to the total variation diminishing (TVD) framework used by Javeed et al., 2011a, Javeed et al., 2011b, Javeed et al., 2013 with an efficient quadrature-free implementation. The framework is used to simulate linear and non-linear multicomponent chromatographic systems. The results confirm arbitrary high-order accuracy and demonstrate the potential for accuracy and speed-up gains obtainable by switching from low-order methods to high-order methods. The results reproduce an analytical solution and are in excellent agreement with numerical reference solutions already published in the literature.",
     "keywords": ["High-order", "Discontinuous Galerkin finite element method", "Liquid chromatography", "Equilibrium-dispersive model", "Linear and nonlinear isotherm"]},
    {"article name": "Improved quadratic cuts for convex mixed-integer nonlinear programs",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.10.011",
     "publication date": "01-2018",
     "abstract": "This paper presents scaled quadratic cuts based on scaling the second-order Taylor expansion terms for the decomposition methods Outer Approximation and Partial Surrogate Cuts for solving convex Mixed Integer Nonlinear Programing problems. The scaled quadratic cut is proved to be a stricter and tighter underestimation for convex nonlinear functions than classical supporting hyperplanes, which results in the improvement of Outer Approximation and Partial Surrogate Cuts based solution methods. We integrate the strategies of scaled quadratic cuts with multi-generation cuts for Outer Approximation and Partial Surrogate Cuts and develop six types of Mixed Integer Nonlinear Programming solution methods with scaled quadratic cuts. These cuts are incorporated in the master problem of the decomposition methods leading to a Mixed Integer Quadratically Constrained Programming problem. Numerical results of benchmark Mixed Integer Nonlinear Programming problems demonstrate the effectiveness of the proposed Mixed Integer Nonlinear Programming solution methods with scaled quadratic cuts.",
     "keywords": ["MINLP", "Outer Approximation (OA)", "Quadratic cut", "MIQCP"]},
    {"article name": "Scenario tree reduction methods through clustering nodes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.10.017",
     "publication date": "01-2018",
     "abstract": "To develop practical and efficient scenario tree reduction methods, we introduce a new methodology which depends on clustering nodes, and thus an easy-to-handle distance function to measure the difference between two scenario trees is designed. On the basis of minimizing the new distance, we construct a multiperiod scenario tree reduction model which is supported theoretically by the stability results of stochastic programs. By solving the model, we design a stage-wise scenario tree reduction algorithm which is superior to the simultaneous backward reduction method in terms of both computational complexity and solution results of stochastic programming problems, the corresponding reduction algorithm especially for fan-liked trees is also presented. We further design a multiperiod scenario tree reduction algorithm with a pre-specified distance by utilizing the stability results of stochastic programs. A series of numerical experiments with real trading data and the application to multiperiod portfolio selection problem demonstrate the practicality, efficiency and robustness of proposed reduction model and algorithms.",
     "keywords": ["Stochastic programs", "Scenario tree reduction", "Clustering", "Stability", "Computational efficiency", "Portfolio selection"]},
    {"article name": "Life cycle analysis of coal based methanol-to-olefins processes in China",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.11.001",
     "publication date": "01-2018",
     "abstract": "In the present paper, life cycle analysis of coal based methanol-to-olefins processes in China is performed based on the detailed information of the china\u2019s largest project of its kind. The purpose of our analysis is to identify the reduction potentials of the project for the energy/water saving and the emission control. The details of the project are given together with the involved techniques. In our analysis, the water and energy consumptions, CO2/SO2/NOx emissions are all demonstrated in terms of six sub-processes with both the direct and indirect contributions. Based on the analysis, we identify that the coal-to-methanol process consumes a vast amount of water and energy with significant CO2/SO2/NOx emissions. For water/energy savings, methanol-to-olefins process is of litter potential because its consumptions are mainly the indirect ones. The negative effects of CCS should be noticed for the implement in the large-scale coal based chemical engineering due to its significant consumptions of the water and energy.",
     "keywords": ["Coal chemical engineering", "Coal-to-methanol", "Methanol-to-olefins", "Greenhouse gas", "Water saving", "CO2 emissions"]},
    {"article name": "A process simulator interface for multiobjective optimization of chemical processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.09.014",
     "publication date": "01-2018",
     "abstract": "The (bio)chemical process industry is under an increasing pressure due to smaller margins and increasing societal and legislative demands for a sustainable future. In this context model-based optimization contributes to the solution because it serves to improve the processes\u2019 performance. Furthermore, multiobjective optimization techniques provide the decision maker with a deeper insight in the tradeoffs when choosing an operating condition. However, an accurate process model is needed to apply these techniques efficiently. In this paper, a novel interface is developed between state-of-the-art gradient-based optimization techniques and the widely used process simulator Aspen Plus. Furthermore, specific challenges and solutions for overcoming the gap between process simulators and optimization tools are highlighted. The resulting interface allows gradient-based techniques to be exploited for optimization of complex industrial processes modeled in the advanced Aspen Plus environment. The interface ensures constraints satisfaction, and a higher computational performance than gradient free methods.",
     "keywords": ["Multiobjective optimization", "Gradient-based optimization", "Process optimization", "Aspen Plus"]},
    {"article name": "DMFA-based operation model for fermentation processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.11.008",
     "publication date": "01-2018",
     "abstract": "The existing dynamical fermentation models only reveal approximate macroscopic properties involving no internal mechanism, while dynamic metabolic flux analysis (DMFA) provides internal microscopic dynamic features, but without regulation from external operation conditions. This is the first attempt to bridge the mapping between macro operation variables and micro metabolic fluxes. Based on the macro-micro mapping relationship, a new operation model was constructed, which can use macro operation variables to regulate micro metabolic fluxes. Firstly, metabolic network was analyzed based on DMFA to derive flux distribution. Next, the fluxes defined as outputs were related to macro operation variables to establish the operation model. The complexity of cellular growth and diversity of flux distribution led to nonlinear and multi-stage characteristics of the mapping relationship, and thus a multi-model modeling method was employed as key algorithm. Finally, a simulation and a lab-scale experiment were conducted to demonstrate the application of the proposed method.",
     "keywords": ["Dynamic metabolic flux analysis", "Operation model", "Macro-micro mapping", "Multi-model modeling", "Fermentation process"]},
    {"article name": "Optimal operation of a Solar Membrane Distillation pilot plant via Nonlinear Model Predictive Control",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.11.012",
     "publication date": "01-2018",
     "abstract": "Solar Membrane Distillation (SMD) is an under-investigation desalination process suitable for developing self-sufficient small scale applications. The use of solar energy considerably reduces the operating costs, however, its intermittent nature requires a non-stationary optimal operation that can be achieved by means of advanced control strategies. In this paper, a hierarchical control system composed by two layers is used for optimizing the operation of a SMD pilot plant, in terms of thermal efficiency, distillate production and cost savings. The upper layer is formed by a Nonlinear Model Predictive Control (NMPC) scheme that allows us to obtain the optimal operation by optimizing the solar energy use. The lower layer includes a direct control system, in charge of attaining the variable references provided by the upper layer. Simulation and experimental tests are included and commented in order to demonstrate the benefits of the developed control system.",
     "keywords": ["Air-gap membrane distillation", "Hierarchical control", "Process control", "Solar energy", "Optimization"]},
    {"article name": "3D modeling of a CPOX-reformer including detailed chemistry and radiation effects with DUO",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.11.005",
     "publication date": "01-2018",
     "abstract": "The impact of radiation heat transfer and radial heat losses in small-scale monoliths, as often used for testing catalysts or qualifying process conditions, are an important consideration to design and predict performance of commercial size reactors. The paper presents the 3D modeling of a honeycomb CPOX (Catalytic Partial Oxidation) reformer, including detailed surface chemistry for the conversion of methane on rhodium. The calculation domain comprises the flow region and two monoliths (one of them coated) which are positioned in a glass tube. For the simulations the software tool DUO (coupling between OpenFOAM and DETCHEM\u2122) was used. The objective was to model the system without any boundary conditions for the temperature (aside from the inlet). As the temperature level is above 900\u00a0K solid body radiation has to be included. The comparison of the results with detailed experimental data shows that it is possible to reproduce the species concentrations and the temperature fields of the flow and solid structures well. The effect of radiation, leading to a heat transfer between the two monoliths, can clearly be indicated. However, this effect plays only a minor role with respect to the chemical conversion. The simulations capture the measured effect of radial heat removal on the conversion process in different channels inside the catalyst.",
     "keywords": ["CFD", "3D", "DUO", "OpenFOAM", "DETCHEM", "CPOX", "Simulation", "Modeling", "Monolith catalyst", "Honeycomb catalyst", "Reformer", "Detailed chemistry", "Solid body radiation"]},
    {"article name": "Maximal safe set computation for pressure swing adsorption processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.11.007",
     "publication date": "01-2018",
     "abstract": "In this paper we propose a method towards purity control of pressure swing adsorption (PSA) processes which is based on the use of hybrid systems formalism. Hybrid systems feature both continuous and discrete-event dynamics and hence are very suited to describe in detail PSA processes. Based on mechanistic model of the processes, a local reduced-order model (LROM) is developed for PSA processes. Then the processes are represented as hybrid systems whose continuous evolution is described by the LROM. We then perform an analysis of hybrid reachability properties of the hybrid system obtained, based on which the so-called maximal safe set is computed. The analysis is performed for a two-bed, six-step benchmark PSA process and the influence of the control inputs and external disturbances are investigated.",
     "keywords": ["Pressure swing adsorption", "Hybrid systems", "Process control", "Maximal safe set", "Reachability analysis", "Level set methods"]},
    {"article name": "Integration of Fuzzy Analytic Hierarchy Process into multi-objective Computer Aided Molecular Design",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.11.015",
     "publication date": "01-2018",
     "abstract": "In this paper, a novel Computer Aided Molecular Design (CAMD) framework is developed to solve multi-objective molecular design problems. CAMD can be formulated as a multi-objective optimisation problem when there are multiple target properties to be optimised simultaneously. A major obstacle faced by multi-objective CAMD problems is the difficulty in assigning weighting factors to the target properties, since the relative importance of these factors is not always defined. It is particularly difficult to compare target properties which belong to different categories, such as physicochemical, safety, health and environmental properties, on a common scale. This paper presents a systematic CAMD algorithm built on Fuzzy Analytic Hierarchy Process (FAHP) to deal with the ambiguity involved in evaluating the weights of target properties in multi-objective CAMD problem. Instead of using exact numerical values, FAHP approach expresses the pairwise comparison of target properties through triangular fuzzy numbers, which allow the degree of confidence of decision maker to be quantified. Hence, the proposed approach can address the uncertainties arising from ambiguity involved during value judgement elicitation in multi-objective CAMD problems. The solutions generated provide a better balance of performance for a set of identified target properties. The proposed methodology is illustrated through a case study on designing a better solvent for extracting residual oil from palm pressed fibre.",
     "keywords": ["Computer Aided Molecular Design (CAMD)", "Fuzzy Analytic Hierarchy Process (FAHP)", "Multi-objective optimisation", "Multi-objective molecular design"]},
    {"article name": "Modelling and optimization of a moving-bed adsorptive reactor for the reverse water-gas shift reaction",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.11.013",
     "publication date": "01-2018",
     "abstract": "In this work, a novel variant of the reverse water-gas shift reaction is proposed as a promising route to valorize CO2 as syngas. The reactor concept used is that of an adsorptive moving-bed in order to permit low-temperature operation with high conversions and to improve upon the fixed-bed adsorptive concept previously investigated. The reactor has been modelled for several configurations and subsequently optimized. The results show that an increase up to an order of magnitude in the space-time-yield (STY) is possible by using the moving-bed configuration in comparison to fixed-bed operation. Finally, a bi-objective optimization is carried out to identify the trade-off between operating at higher STY and higher adsorbent loadings.",
     "keywords": ["CO2-Utilization", "rWGS", "Syngas", "Moving-bed-reactor", "Adsorptive reactors", "NOMAD"]},
    {"article name": "Multi-criteria design of shale-gas-water supply chains and production systems towards optimal life cycle economics and greenhouse gas emissions under uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.11.014",
     "publication date": "01-2018",
     "abstract": "One of the critical problems in cooperative shale gas supply chains and production systems design is life cycle optimization of the economic and environmental performance under uncertainty. This study develops an inexact multi-criteria decision making (IMCDM) model with consideration of shale gas production profiles and recoverable reserves. The IMCDM framework is based on an integration of life cycle analysis, interval linear programming, multi-objective programming, and multi-criteria decision analysis approaches. An application to the Marcellus Shale supply chains is presented to demonstrate capabilities and effectiveness of the developed model, where the future spread in shale gas output follows from the variation in drilled well counts according to different scenarios. Design and operational decisions with respect to well drilling schedule, shale gas production, freshwater supply, wastewater disposal, and greenhouse gas (GHG) emissions are then generated. An optimal strategy is further provided for stakeholders after evaluation of the trade-off among multiple criteria.",
     "keywords": ["Shale gas", "Life cycle", "Uncertainty", "Multi-criteria decision making", "Production profiles", "Greenhouse gas emissions"]},
    {"article name": "Parameter estimation of models with limit cycle based on the reformulation of the objective function",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.11.009",
     "publication date": "01-2018",
     "abstract": "Many processes show limit cycles, meaning that the system presents oscillatory behavior. The parameter estimation of such kind of systems is not a simple task, due to the non-convexity of the optimization problem. This paper proposes the inclusion of a driving term based on the damping factor in the classical objective function formulation, reducing the non-convexity of the problem. This driving term is reduced after each iteration until its complete elimination, as the system starts to have oscillatory behavior close to the limit cycle. Two case studies illustrate the strengths of the proposed approach: the J\u00f6bses\u2019s dynamic model for ethanol fermentation with Zymomonas mobilis and the Di Meglio et al. (2009) model, which represents the slugging flow regime appearing in vertical risers. The results showed that the proposed approach was able to ensure the oscillatory behavior, forcing the dynamical behavior of the system to produce the limit cycle.",
     "keywords": ["Limit cycle", "Oscillatory behavior", "Parameter estimation"]},
    {"article name": "Model predictive control with closed-loop re-identification",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.11.016",
     "publication date": "01-2018",
     "abstract": "In this work, we address the problem of handling plant-model mismatch by designing a subspace identification based MPC framework that includes model monitoring and closed-loop identification components. In contrast to performance monitoring based approaches, the validity of the underlying model is monitored by proposing two indexes that compare model predictions with measured past output. In the event that the model monitoring threshold is breached, a new model is identified using an adapted closed-loop subspace identification method. To retain the knowledge of the nominal system dynamics, the proposed approach uses the past training data and current input, output and set-point as the training data for re-identification. A model validity mechanism then checks if the new model predictions are better than the existing model, and if they are then the new model is utilized within the MPC. The effectiveness of the proposed method is illustrated through simulations on a nonlinear polymerization reactor.",
     "keywords": ["System identification", "Subspace identification", "Closed-loop identification", "Model predictive control", "Re-identification"]},
    {"article name": "Monte-Carlo-simulation-based optimization for copolymerization processes with embedded chemical composition distribution",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.11.018",
     "publication date": "01-2018",
     "abstract": "As chemical composition distribution (CCD) is a crucial microstructural quality index of copolymers, optimization of operating policies using CCD is of great importance. Monte Carlo simulation is an efficient method to calculate the CCD that cannot be easily determined by traditional equation-based methods But this method is computationally expensive. In this project, we first propose a parallel technique to conduct the Monte Carlo simulation on the graphics processing unit (GPU) platform. Additionally, an adaptive simulation algorithm is proposed to reduce computational cost based on error estimation of the Monte Carlo simulation. Considering the uncertainties in the Monte Carlo simulation, derivative-free method is applied for the CCD-target optimization. A successive boundary shrinkage (SBS) formulation is developed to improve the convergence of problem solving. The above-mentioned methods are successfully integrated and implemented on the optimization of a copolymerization process with high efficiency and good performance.",
     "keywords": ["Chemical composition distribution", "Monte Carlo simulation", "Microstructural quality", "Derivative-free optimization", "Parallel computing", "GPU"]},
    {"article name": "Support vector regression modelling and optimization of energy consumption in carbon fiber production line",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.11.020",
     "publication date": "01-2018",
     "abstract": "The main chemical industrial efforts are to systematically and continuously explore innovative computing methods of optimizing manufacturing processes to provide better production quality with lowest cost. Carbon fiber industry is one of the industries seeks these methods as it provides high production quality while consuming a lot of energy and being costly. This is due to the fact that the thermal stabilization process consumes a considerable amount of energy. Hence, the aim of this study is to develop an intelligent predictive model for energy consumption in thermal stabilization process, considering production quality and controlling stochastic defects. The developed and optimized support vector regression (SVR) prediction model combined with genetic algorithm (GA) optimizer yielded a very satisfactory set-up, reducing the energy consumption by up to 43%, under both physical property and skin-core defect constraints. The developed stochastic-SVR-GA approach with limited training data-set offers reduction of energy consumption for similar chemical industries, including carbon fiber manufacturing.",
     "keywords": ["Thermal stabilization process", "Intelligent predictive models", "Energy optimization", "Carbon fiber industry", "Limited training data-set"]},
    {"article name": "Development of a recursive time series model for fed-batch mammalian cell culture",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.11.006",
     "publication date": "01-2018",
     "abstract": "Recursive time series models are developed in this work for a fed-batch mammalian cell culture producing monoclonal antibodies, with key culture variables measured at different sampling frequencies. Glucose and glutamine feed rates are considered as inputs. A composite of an autoregressive moving average with exogenous input model and a dual rate-autoregressive with exogenous input model is used. Appropriate parameter constraints are imposed in parameter estimation algorithms and stability of these is examined and ensured. The data required for parameter estimation are generated from simulated fed-batch experiments using a well-tested first principles model. The predictions for glucose, glutamine, and viable cell concentrations track very well the data for these, with the errors for the high prediction horizons considered being limited to 10% or less. The prediction accuracy can be increased further if data from prior experiments with dynamic similarities are available. The models can be used reliably for model predictive control.",
     "keywords": ["Mammalian cell culture", "Monoclonal antibody", "Recursive time series models", "Constrained parameter estimation", "Stability"]},
    {"article name": "On the effect of price policies in the design of formulated products",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.11.019",
     "publication date": "01-2018",
     "abstract": "In this work an extended pooling problem is formulated to select the optimal price policy for the ingredients of a formulated product such as a detergent. Various contracts for obtaining discounts as a function of the amount purchased are considered including fixed discount, linear, logit, a discount beyond a certain amount purchased and constant elasticity. The feature is that while certain ingredients are used in large amounts, others represent only a small fraction of the product. The problem becomes an MINLP that it is solved for the optimal product formulation selecting the contract on a year and on a multiperiod basis. Various mathematical reformulations are proposed. The results show the selection of the contract is ingredient dependent but it is quite robust with the environmental burden. However, this decision can change when actual contract conditions on the limiting amounts for the discount are provided to the model.",
     "keywords": ["Mathematical optimization", "Formulae performance", "Product and process design", "Price policies", "Pooling problem"]},
    {"article name": "Fault detection based on augmented kernel Mahalanobis distance for nonlinear dynamic processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.11.010",
     "publication date": "01-2018",
     "abstract": "This paper presents a fault detection method based on augmented kernel Mahalanobis distance (AKMD) for monitoring nonlinear dynamic processes. In order to reflect the information of dynamic correlations, the measurements are stacked into augmented vectors at adjacent sampling instants. The augmented kernel Mahalanobis distance serves as the detection index, and its control limit is determined by the empirical method with assigning a significance level. Contrary to the mainstream of process monitoring methods based on principal component analysis (PCA), dimensionality reduction is not used here. The disadvantage of dimensionality reduction and space partition is discussed, and the improvement of fault detectability via data augmentation is analyzed. In addition, the computational complexity of the proposed method is acceptable. For training dataset containing m variables and n samples, if n\u00a0\u226b\u00a0m, the online computational burden of the proposed method is about O ( n 2 ) . Simulations about a nonlinear dynamic process and the benchmark Tennessee Eastman process (TEP) both illustrate higher detection rates of the proposed method, compared with conventional multivariate statistical process monitoring (MSPM) methods such as PCA and its variants.",
     "keywords": ["Fault detection", "Nonlinear process monitoring", "Mahalanobis distance", "Data augmentation", "Kernel trick"]},
    {"article name": "Computational simulation of CO2 capture process in a fluidized-bed reactor",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.08.006",
     "publication date": "01-2018",
     "abstract": "A dry sorbent in a fluidized bed reactor system is nominated as an efficient method to remove CO2 from flue gases. In this study, a two-dimensional CFD simulation with the Euler\u2013Euler two-phase fluid flow model incorporating the kinetic theory of solid particles is used to validate a specific reported experimental data. The non-uniform distribution of the solid-phase volume fraction and CO2 concentration is captured in the carbonation reactor. CO2 removal increases by decreasing dry gas flow and enhancing solid sorbent flux. Furthermore, it has an ascending-descending behavior including a maximum point by increase in water vapor content of the feed gas. The CFD model is also implemented to optimize the reactor height based on operating conditions. By using optimized volume fraction of water vapor in the feed gas and enhancing solid sorbent rate to a minimum required value, the reactor height can be decreased to improve the cost and system size condition.",
     "keywords": ["CO2 removal", "Fluidized bed", "Carbonation reactor", "CFD simulation"]},
    {"article name": "A graph partitioning algorithm for leak detection in water distribution networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.08.007",
     "publication date": "01-2018",
     "abstract": "Urban water distribution networks (WDNs) are large scale complex systems with limited instrumentation. Due to aging and poor maintenance, significant loss of water can occur through leaks. We present a method for leak detection in WDNs using repeated water balance and minimal use of additional off-line flow measurements. A multi-stage graph partitioning approach is used to determine where the off-line flow measurements are to be made, with the objective of minimizing the measurement cost. The graph partitioning problem is formulated and solved as a multi-objective mixed integer linear program (MILP). We further derive an approximate method inspired by spectral graph bisection to solve the MILP, which is suitable for very large scale networks. The proposed methods are tested on large scale benchmark networks, and the results indicate that on average, flows in less than 3% of the pipes need to be measured to identify the leaky pipe or joint.",
     "keywords": ["Water distribution networks", "Leak detection", "Graph partitioning", "Multi-objective optimization"]},
    {"article name": "Utilizing null controllable regions to stabilize input-constrained nonlinear systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.08.002",
     "publication date": "01-2018",
     "abstract": "In this paper, we present a method for control of input-constrained nonlinear systems that offers guaranteed stabilization from the entire null controllable region (NCR). The controller achieves stabilization by using a constrained control Lyapunov function (CCLF) based on this NCR. Prior to online implementation, the level sets the CCLF are constructed using an iterative algorithm. The algorithm works by using an invariance principle to expand an initial quadratic Lyapunov function-based region of attraction. The level sets of this CCLF are then utilized in the control calculations, and in particular, an MPC is formulated that requires the system to go to lower level sets of the CCLF. The proposed MPC thus achieves stabilization from the entire NCR. The proposed approach is first corroborated against existing results for linear systems using two- and three-dimensional linear systems examples. Subsequently, the implementation is shown for two and three dimensional nonlinear systems.",
     "keywords": ["MPC", "Constrained control Lyapunov functions", "Reachable sets", "Nonlinear control", "Constrained control", "Null controllable region"]},
    {"article name": "Enhancement of modifier adaptation scheme via feedforward decision maker using historical disturbance data and deep machine learning",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.08.005",
     "publication date": "01-2018",
     "abstract": "Most advanced processes struggle to reduce the production cost under constraints. For this, an iterative optimization method called modifier adaptation has been utilized due to its ability to ensure the necessary conditions of optimality even under model-plant mismatch. However, the optimization performance may be degraded by the disturbance which may significantly change the true optimum. In this study, a feedforward decision maker is designed to deal with disturbances in advance and compensate the limitation of feedback scheme of the conventional modifier adaptation. It is constructed by historical data and deep machine learning, and combined with the modifier adaptation. When disturbances occur, the decision maker provides an initial point close to the true optimum by exploiting the historical data. As the information is accumulated, a better initial point for modifier adaptation is obtained. Constrained optimization of numerical example and run-to-run bioprocess are illustrated to validate the utility of the proposed method.",
     "keywords": ["Modifier adaptation", "Model-plant mismatch", "Deep neural network", "KKT conditions"]},
    {"article name": "NMPC using Pontryagin\u2019s Minimum Principle-Application to a two-phase semi-batch hydroformylation reactor under uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.08.010",
     "publication date": "01-2018",
     "abstract": "Nonlinear model predictive control (NMPC) is an important tool for the real-time optimization of batch and semi-batch processes. Direct methods are often the methods of choice to solve the corresponding optimal control problems, in particular for large-scale problems. However, the matrix factorizations associated with large prediction horizons can be computationally demanding. In contrast, indirect methods can be competitive for smaller-scale problems. Furthermore, the interplay between states and co-states in the context of Pontryagin\u2019s Minimum Principle (PMP) might turn out to be computationally quite efficient.This work proposes to use an indirect solution technique in the context of shrinking-horizon NMPC. In particular, the technique deals with path constraints via indirect adjoining, which allows meeting active path constraints explicitly at each iteration. Uncertainties are handled by the introduction of time-varying backoff terms for the path constraints. The resulting NMPC algorithm is applied to a two-phase semi-batch reactor for the hydroformylation of 1-dodecene in the presence of uncertainty, and its performance is compared to that of NMPC that uses a direct simultaneous optimization method. The results show that the proposed algorithm (i) can enforce feasible operation for different uncertainty realizations both within batch or from batch to batch, and (ii) is significantly faster than direct simultaneous NMPC, especially at the beginning of the batch. In addition, a modification of the PMP-based NMPC scheme is proposed to enforce active constraints via tracking.",
     "keywords": ["Nonlinear model predictive control", "Indirect optimization methods", "Indirect adjoining", "Semi-batch processes", "Pontryagin\u2019s Minimum Principle"]},
    {"article name": "Locating exchangers in an EIP-wide heat integration network",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.08.004",
     "publication date": "01-2018",
     "abstract": "Inter-plant heat integration offers an energy-saving opportunity in eco-industrial parks (EIPs) beyond the traditional intra-plant integration. The economic feasibility of this integration depends critically on the locations of heat exchangers. In this work, we generalize our previous study on centralized HENS (Heat Exchanger Network Synthesis) for EIPs to allow exchangers to be located at either plant or central sites facilitating both intra-plant and inter-plant heat integration in a seamless manner. We propose a mixed integer non-linear programming (MINLP) model that synthesizes a maximum-NPV (Net Present Value) EIP-wide HEN, while accounting for all the capital (e.g. heat exchangers, pumps, and pipelines) and operating (pumping costs, utility savings) cash flows along with the ambient heat gains/losses during transports. The model is tested on five examples from the literature and gives better HENs compared to the previous results. This work highlights and quantifies the impact of heat exchanger locations in an EIP-wide HEN.",
     "keywords": ["Eco-industrial park", "Heat integration", "Mixed-integer non-linear programming", "Heat exchanger network", "Heat exchanger"]},
    {"article name": "Understanding rare safety and reliability events using transition path sampling",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.06.016",
     "publication date": "01-2018",
     "abstract": "In the chemical and process industries, processes and their control systems are typically well-designed to mitigate abnormal events having potential adverse consequences to human health, environment, and/or property. Strong motivation exists to understand how these events develop and propagate. These events occur so rarely that statistical analyses of their occurrences alone are incapable of describing and characterizing them \u2212 especially when they have not yet occurred. Moreover, the use of process models to understand such rare events is hampered by the orders of magnitude separating the frequencies with which reliability and safety events (years to decades) occur and the duration over which they occur (minutes to hours). To address these challenges, we adapt a Monte-Carlo based, rare-event sampling technique, Transition Path Sampling (TPS), which was developed by the molecular simulation community. Important modifications to the TPS technique are needed to apply it to process dynamics, and are discussed herein.",
     "keywords": ["Risk analysis", "Abnormal events", "Transition path sampling"]},
    {"article name": "Online optimization for a plunger lift process in shale gas wells",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.09.001",
     "publication date": "01-2018",
     "abstract": "This paper presents a method for efficient optimization of a plunger lift process in shale gas wells. Plunger lift is a cyclic process consisting binary decision as well as continuous and discrete state variables. The time-series data comprising of surface measurements are converted into cycle-wise process-relevant performance outputs, while the binary manipulated variable is transformed into continuous threshold values. These transformed variables are used to develop a reduced order cycle-to-cycle model and corresponding receding horizon optimization problem that maximizes daily production while meeting operational constraints. The efficacy of the proposed algorithm is demonstrated on a simulated plunger lift process.",
     "keywords": ["Plunger lift", "Hybrid system", "Production optimization", "Multi-objective optimization", "Model predictive control", "Reduced order model"]},
    {"article name": "Cyclic operation as optimal control reflux policy of binary mixture batch distillation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.09.004",
     "publication date": "01-2018",
     "abstract": "We revisit the maximum distillate optimal control problem of batch distillation of non-ideal binary zeotropic mixtures. The direct method with full discretization is used. The problem formulation is based on full column dynamics and the distillate flow rate is used as control variable instead of the reflux. The purity constraint is handled as a new state variable, the purity deviation. Literature simulations showed that the cyclic reflux policy (bang-bang type control) performs better than variable reflux (singular type control) or constant reflux policy for small amount of light product in the load. For the first time, a cyclic reflux policy is found as the optimal control solution. The results are confirmed by rigorous simulation of the batch distillation, as the cyclic policy improves by 13% the product recovery over the variable reflux policy. Influence of the relative volatility, vapour flow rate, plate hold-up and initial load is discussed.",
     "keywords": ["Non-ideal binary mixture", "Batch distillation", "Optimal control", "Maximum distillate problem", "Direct method"]},
    {"article name": "Rapid phase stability calculations in fluid flow simulation using simple discriminating functions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.09.006",
     "publication date": "01-2018",
     "abstract": "This paper presents a new phase stability method that is applicable when repeated phase behavior calculations are needed as it is the case with multiphase fluid flow compositional simulation in upstream petroleum engineering. Two discriminating functions act as classifiers in such a way that a positive value of one of the two functions determines the stability state of the mixture. The two functions are generated off line, prior to the simulation, and their expressions are very simple so that they can be evaluated rapidly in a non-iterative way for every discretization block and at each timestep during the simulation. The CPU time required for phase stability calculations is dramatically reduced while still obtaining correct classification results corresponding to the global minimum of the system Gibbs energy function. The method can be applied to any chemical engineering problem where the class of several objects needs to be determined repeatedly and quickly.",
     "keywords": ["Multiphase fluid flow simulation", "Phase stability analysis", "Upstream petroleum engineering", "Classification algorithm"]},
    {"article name": "Online average-based system modelling method for batch process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.09.005",
     "publication date": "01-2018",
     "abstract": "Online system identification provides a powerful tool to process control engineers for controller synthesis, process dynamics monitoring, real-time optimization and other purposes at a low computational cost. Instead of processing data in time, this paper intends to propose processing data in a different \u201cdirection\u201d \u2013 in iteration/batch to improve the estimates tracking performance. However, directly changing the data processing direction gives rise to severe fluctuations on parameter estimates within a batch. To overcome this problem, two online identification methods with simple implementations are devised based on average. One method is applying average in the dual space, while the other in the primal space. The convergence of both approaches has been analyzed. An adaptive average strategy based on moving-window is also developed to track inter-batch dynamics drift. Finally, the simulation results on injection molding, two-tank system and continuous stirred tank reactor (CSTR) testify the improved performance of the methods proposed in this paper.",
     "keywords": ["Batch processes", "Dual average", "Online system identification", "Primal average", "Process modelling", "Two time-dimension"]},
    {"article name": "Heating, ventilation and air conditioning systems: Fault detection and isolation and safe parking",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.08.012",
     "publication date": "01-2018",
     "abstract": "This work presents an integrated framework for fault detection and isolation (FDI) and fault tolerant control (FTC) of variable air volume (VAV) boxes, a common component of heating, ventilation and air conditioning (HVAC) systems. To this end, first a statistical model based FDI framework is designed using existing techniques such as principal component analysis (PCA) and joint angle analysis as a benchmark for comparison. Then a novel linear causal model based framework for FDI of multiple actuator and multiple sensor faults is designed and implemented and shown to possess superior FDI capabilities compared to the statistical model based framework. Finally, a safe parking strategy is designed and the ensuing energy savings for the case of stuck dampers demonstrated.",
     "keywords": ["HVAC systems", "VAV boxes", "Fault diagnosis", "Actuator faults", "Sensor faults", "Safe parking"]},
    {"article name": "Sensor placement for classifier-based leak localization in water distribution networks using hybrid feature selection",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.09.002",
     "publication date": "01-2018",
     "abstract": "This paper presents a sensor placement approach for classifier-based leak localization in water distribution networks. The proposed method is based on a hybrid feature selection algorithm that combines the use of a filter based on relevancy and redundancy with a wrapper based on genetic algorithms. This algorithm is applied to data generated by hydraulic simulation of the considered water distribution network and it determines the optimal location of a prespecified number of pressure sensors to be used by a leak localization method based on pressure models and classifiers proposed in previous works by the authors. The method is applied to a small-size simplified network (Hanoi) to better analyze its computational performance and to a medium-size network (Limassol) to demonstrate its applicability to larger real-size networks.",
     "keywords": ["Sensor placement", "Leak localization", "Water distribution networks", "Feature selection", "Genetic algorithms", "Classifiers"]},
    {"article name": "Design optimization of oilfield subsea infrastructures with manifold placement and pipeline layout",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.08.009",
     "publication date": "01-2018",
     "abstract": "This paper presents a practical and effective optimization method to design subsea production networks, which accounts for the number of manifolds and platforms, their location, well assignment to these gathering systems, and pipeline diameter. It brings a fast solution that can be easily implemented as a tool for layout design optimization and simulation-based analysis. The proposed model comprises reservoir dynamics and multiphase flow, relying on multidimensional piecewise linearization to formulate the layout design problem as a MILP. Besides design validation, reservoir simulation serves the purpose of defining boundaries for optimization variables and parameters that characterize pressure decrease, reservoir dynamics and well production over time. Pressure drop in pipelines are modeled by piecewise-linear functions that approximate multiphase flow simulators. The resulting optimization model and approximation methodology were applied to a real oilfield with the aim of assessing their effectiveness.",
     "keywords": ["Subsea infrastructure", "Manifold placement", "Pipeline design", "Piecewise-linear approximation", "Mixed integer linear programming problem"]},
    {"article name": "Predicting the cradle-to-gate environmental impact of chemicals from molecular descriptors and thermodynamic properties via mixed-integer programming",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.09.010",
     "publication date": "01-2018",
     "abstract": "Life Cycle Assessment (LCA) has recently gained wide acceptance in the environmental impact evaluation of chemicals. Unfortunately, LCA studies require large amounts of data that are hard to gather in practice, a critical limitation when assessing the processes and value chains present in the chemical industry. We here develop an approach that predicts the cradle-to-gate life cycle production impact of organic chemicals from attributes related to their molecular structure and thermodynamic properties. This method is based on a mixed-integer programming (MIP) optimisation framework that systematically constructs short-cut predictive models of life cycle impact. On applying our approach to a data set containing 88 chemicals, 17 molecular descriptors and 15 thermodynamic properties, we estimate with enough accuracy (for the purposes of a standard LCA) several impact categories widely applied in LCA studies, including the cumulative energy demand, global warming potential and Eco-indicator 99. Our framework ultimately leads to linear models that can be easily integrated into existing modelling and optimisation software, thereby facilitating the design of more sustainable processes.",
     "keywords": null},
    {"article name": "Actuator network design to mitigate contamination effects in Water Distribution Networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.09.003",
     "publication date": "01-2018",
     "abstract": "Water Distribution Networks (WDNs) are vulnerable to accidental or deliberate contamination. Such contamination can be detected and identified by deploying a network of sensors. If the sensor network detects the presence of a contaminant, it is also very important to take corrective response actions to minimize the effects of contamination on the population being served. One possible mitigation option is to prevent the contaminated water from reaching any customer, by shutting down the distribution network using shut-off valves placed in the WDN. The design problem considered in this work is to determine pipes where the shut-off valves can be optimally located such that it is possible to prevent the contaminated water from reaching any demand point, regardless of the source node from where the contamination has originated. We refer to this problem as the actuator network design problem. We map the problem of actuator design into a graph partitioning problem and the minimal set of actuators are identified for ensuring the total shutdown of the network.",
     "keywords": ["Water Distribution Networks", "Observable sensor network", "Actuator network", "Graph partitioning"]},
    {"article name": "A comparative study on the performance of three treatment chamber designs for radio frequency electric field processing",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.09.009",
     "publication date": "01-2018",
     "abstract": "The performance of three different chamber designs: co-linear, Steinmetz and parallel-plate, for radio frequency electric field (RFEF) processing of liquid foods was evaluated and compared. The study was conducted via a computational model that predicted the electric field, flow, and temperature distribution in those three chambers. The parallel-plate, in spite of having the highest electric field peaks, exhibited not only the most uniform electric field distribution inside the treatment zone but also the most homogenously distributed velocity profile along with the lowest temperature increase and energy consumption. The model was validated by comparing the predicted and experimentally measured outlet temperatures. Experiments of E. coli inactivation were performed in all three chambers at a volumetrically averaged electric field strength of 13.2\u00a0kV\u00a0cm\u22121, a treatment time of 500\u00a0\u03bcs and outlet temperatures in the range of 20\u201350\u00a0\u00b0C showing equal inactivation given the uncertainties of microbial population quantification methodology.",
     "keywords": ["Multiphysics simulations", "Inactivation", "Homogeneity", "Radio frequency", "Pulsed electric fields", "Chamber design"]},
    {"article name": "Disjunctive model for the simultaneous optimization and heat integration with unclassified streams and area estimation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.09.013",
     "publication date": "01-2018",
     "abstract": "In this paper, we propose a disjunctive formulation for the simultaneous chemical process optimization and heat integration with unclassified process streams \u2013streams that cannot be classified a priori as hot or cold streams and whose final classification depend on the process operating conditions\u2013, variable inlet and outlet temperatures, variable flow rates, isothermal process streams, and the possibility of using different utilities.The paper also presents an extension to allow area estimation assuming vertical heat transfer. The model takes advantage of the disjunctive formulation of the \u2018max\u2019 operator to explicitly determine all the \u2018kink\u2019 points on the hot and cold balanced composite curves and uses an implicit ordering for determining adjacent points in the balanced composite curves for area estimation.The numerical performance of the proposed approach is illustrated with four case studies. Results show that the novel disjunctive model of the pinch location method has excellent numerical performance, even in large-scale models.",
     "keywords": ["Simultaneous optimization", "Heat integration", "Variable temperatures", "Disjunctive model", "Unclassified streams"]},
    {"article name": "Performance of different optimization concepts for reactive flow systems based on combined CFD and response surface methods",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.09.008",
     "publication date": "01-2018",
     "abstract": "CFD-based optimization provides an efficient tool to improve systems in which conversion processes depend on a strong interaction between flow field and chemical reactions. The present work develops an improved multi-parameter and multi-objective optimization concept for reactive flow systems and demonstrates this concept for a new quench reactor. Response surface methods (metamodels) are used within the metamodel-based optimization process to accelerate the optimization rapidly. Different metamodels such as polynomials, K-nearest, radial basis functions, anisotropic Kriging, smoothing spline analysis of variance, and artificial neural networks are applied and carefully analyzed. It can be shown that radial basis function metamodels in combination with a certain number of CFD calculations provide a robust and efficient optimization scheme. The computational effort can be reduced by a factor of 17, which provides a reliable basis to optimize more complex reactive flow systems, e.g. the high pressure partial oxidation of natural gas or crude oil.",
     "keywords": ["Metamodeling", "Response surface method", "Multi-objective optimization", "CFD", "Genetic algorithm", "Quench conversion reactor"]},
    {"article name": "Population balance model and experimental validation for reactive dissolution of particle agglomerates",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.09.019",
     "publication date": "01-2018",
     "abstract": "We propose a population balance model coupled with a mass transfer model to simulate the simultaneous shrinkage and breakage of particles during the reactive dissolution of particle agglomerates in stirred tank. The high-order moment-conserving method of classes is adopted to solve the population balance model. In the mass transfer model, the driving force is estimated by considering the physical constraints including electroneutrality, water dissociation and dissolution equilibrium. The simulation results, including the concentration and the particle size distribution of the final products, were validated by experiments carried out in a laboratory scale stirred tank. The unknown physical parameters in the particle breakage model were fitted against the experimental data. The results underline the importance of particle breakage in the reactive dissolution modeling under the investigated operating conditions. Several daughter size distributions functions found in literature were tested. Among them, the beta distribution provides the most flexible way to describe breakage of the particle agglomerates.",
     "keywords": ["Reactive dissolution", "Population balance", "Electroneutrality", "Particle breakage", "High order moment conserving method of classes"]},
    {"article name": "Advances in surrogate based modeling, feasibility analysis, and optimization: A review",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.09.017",
     "publication date": "01-2018",
     "abstract": "The idea of using a simpler surrogate to represent a complex phenomenon has gained increasing popularity over past three decades. Due to their ability to exploit the black-box nature of the problem and the attractive computational simplicity, surrogates have been studied by researchers in multiple scientific and engineering disciplines. Successful use of surrogates shall result in significant savings in terms of computational time and resources. However, with a wide variety of approaches available in the literature, the correct choice of surrogate is a difficult task. An important aspect of this choice is based on the type of problem at hand. This paper reviews recent advances in the area of surrogate models for problems in modeling, feasibility analysis, and optimization. Two of the frequently used surrogates, radial basis functions, and Kriging are tested on a variety of test problems. Finally, guidelines for the choice of appropriate surrogate model are discussed.",
     "keywords": ["Surrogate models", "Derivative-free optimization", "Feasibility analysis", "Sampling", "Model selection"]},
    {"article name": "Application and comparison of derivative-free optimization algorithms to control and optimize free radical polymerization simulated using the kinetic Monte Carlo method",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.09.015",
     "publication date": "01-2018",
     "abstract": "The diversity of the potential arrangements of multiple monomers along the length of polymer chains and their impact on polymer properties spark interest in the design of polymer sequence characteristics for particular applications. Kinetic Monte Carlo (KMC) is a technique that can track the explicit arrangement of monomers in the polymer chains, yet it is difficult to integrate with conventional gradient-based optimization algorithms that are typically invoked to design polymer properties. In this work, we applied and compared derivative-free optimization algorithms to incorporate KMC simulations and find synthesis conditions for achieving property targets and minimizing reaction time, advancing our ability to carry out the design of polymer microstructures and control polymerization processes.",
     "keywords": ["Kinetic Monte Carlo", "Derivative-free optimization", "Polymer", "Sequence"]},
    {"article name": "Evaluating smart sampling for constructing multidimensional surrogate models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.09.016",
     "publication date": "01-2018",
     "abstract": "In this article, we extensively evaluate the smart sampling algorithm (SSA) developed by Garud et al. (2017a) for constructing multidimensional surrogate models. Our numerical evaluation shows that SSA outperforms Sobol sampling (QS) for polynomial and kriging surrogates on a diverse test bed of 13 functions. Furthermore, we compare the robustness of SSA against QS by evaluating them over ranges of domain dimensions and edge length/s. SSA shows consistently better performance than QS making it viable for a broad spectrum of applications. Besides this, we show that SSA performs very well compared to the existing adaptive techniques, especially for the high dimensional case. Finally, we demonstrate the practicality of SSA by employing it for three case studies. Overall, SSA is a promising approach for constructing multidimensional surrogates at significantly reduced computational cost.",
     "keywords": ["Adaptive sampling", "Experimental design", "Surrogate model", "Process flowsheet"]},
    {"article name": "MPC-based reference governor control of a continuous stirred-tank reactor",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.09.020",
     "publication date": "01-2018",
     "abstract": "Optimal control of a CSTR represents a challenging task. The proposed paper discusses two issues. The first one addresses control of pH in a chemical vessel, where the reaction between sodium hydroxide and acetic acid occurs. The objective here is to improve control performance of a well tuned PI controller. It will be shown that this can be achieved by introducing a reference governor scheme. The second problem, elaborated in this paper, is the implementation of the reference governor paradigm. Concretely, we aim to design a fast and cheap MPC-based feedback controller. To achieve these goals, we exploit the region-less explicit technique, which efficiently reduces memory footprint issues of standard explicit MPC schemes. Such MPC-based reference governor was employed to control pH in the chemical vessel. Its control performance is compared with conventional PI controller. Finally, comparison of implementation requirements of region-less and region-based explicit techniques is investigated.",
     "keywords": ["Model predictive control", "Parametric programming", "Reference governor", "Continuous stirred-tank reactor"]},
    {"article name": "A novel data-driven leak detection and localization algorithm using the Kantorovich distance",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.09.022",
     "publication date": "01-2018",
     "abstract": "A novel data-driven leak detection and localization algorithm is proposed based on the Kantorovich distance concept. Mass flowrates and pressure measurements are used to identify possible change in the pipeline status. Based on the pipeline leak signature, leak is detected and the location is further inferred. The proposed method was applied successfully to a simulated pipeline in transient condition. The efficacy of the proposed method was also proven by applying it to an industrial pipeline network with controlled leak tests in real time. The method successfully detected both small (as small as 1% of the nominal flow rate) and large test leaks in the realistic pipeline. The time required to detect and localize leak with the proposed algorithm was much lower than the available commercial leak detection system. The accuracy of the proposed leak localization method was demonstrated to be better especially for the small leaks.",
     "keywords": ["Process faults", "Kantorovich distance", "Pipeline leak detection", "Leak localization", "Pipeline safety", "Industrial evaluation"]},
    {"article name": "Stochastic programming approach for the optimal tactical planning of the downstream oil supply chain",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.09.012",
     "publication date": "01-2018",
     "abstract": "This paper develops a multistage stochastic programming to optimally solve the distribution problem of refined products. The stochastic model relies on a time series analysis, as well as on a scenario tree analysis, in order to effectively deal and represent uncertainty in oil price and demand. The ARIMA methodology is explored to study the time series of the random parameters aiming to provide their future outcomes, which are then used in the scenario-based approach. As the designed methodology leads to a large scale optimization problem, a scenario reduction approach is employed to compress the problem size and improve its computational performance. A real-world example motivates the case study, based on the downstream oil supply chain of mainland Portugal, which is used to validate the applicability of the stochastic model. The results explicitly indicate the performance of the designed approach in tackling large and complex problems, where uncertainty is present.",
     "keywords": ["Uncertainty", "Stochastic programming", "Scenario-based approach", "Time series analysis", "Scenario tree reduction", "Downstream oil supply chain"]},
    {"article name": "Simultaneous optimization of complex distillation systems and heat integration using pseudo-transient continuation models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.10.004",
     "publication date": "01-2018",
     "abstract": "For a competitive design of a distillation system, heat integration is currently a necessary component in the chemical industry. In distillation systems with heat integration, the complex interaction among the column design variables and heat integration calls for a systematic optimization method. In this study, the development of a simultaneous optimization framework for distillation systems and heat integration is described. To achieve an accurate and reliable design, rigorous distillation column, heat exchanger and compressor models were used for the optimization. These models were reformulated using a pseudo-transient continuation (PTC) approach to improve their robustness. Because the framework is based on the equation-oriented environment, the automatic or symbolic differentiation is accessible for highly efficient gradient-based optimization algorithms. The results of two optimization cases of distillation systems with heat integration validated the proposed framework. In addition, the application of a bypass efficiency method while optimizing the number of stages is further discussed.",
     "keywords": ["Distillation systems", "Heat integration", "Simultaneous optimization", "Pseudo-transient continuation models", "Bypass efficiency method"]},
    {"article name": "A biobjective DC programming approach to optimization of rougher flotation process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.10.001",
     "publication date": "01-2018",
     "abstract": "New economic and environmental challenges have recently led to a growing interest in modern mathematical optimization and modeling techniques in order to improve the froth flotation performance. An important issue is that most of corresponding mathematical optimization models are non-convex and involve multiple criteria; hence, conventional convex optimization methods cannot guarantee finding global (Pareto) optimal solutions. We propose a deterministic biobjective mathematical programming framework, combined with experimental design and regression analysis, to optimizing flotation performance and determining the optimal operating conditions that meet specific needs. The framework aims at maximizing concentrate grade and recovery and is based on some well-known and advanced approaches including an epsilon-constraint method, DC programming, an exact penalty method, and the special global search strategy. To demonstrate the effectiveness of the proposed approach, we present a case study for the rougher flotation process of copper-molybdenum ores performed at the Erdenet Mining Corporation Mineral Processing Plant (Mongolia).",
     "keywords": ["DC programming", "Multiobjective optimization", "Froth flotation", "Mineral processing", "Global optimization", "Copper"]},
    {"article name": "Fault diagnosis based on dissipativity property",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.09.024",
     "publication date": "01-2018",
     "abstract": "In this paper, a novel fault diagnosis scheme for linear process systems using dissipativity theory is developed. Dissipativity (supply rate) of a process is an input/output property, which may not be valid when a fault occurs. For a given process, dissipativity is not a unique property, with different dissipative supply rates reflecting different aspects of its dynamics. In this approach, the dissipativity of a process is \u201cshaped\u201d such that it is fault-sensitive (i.e., no longer valid when faults occur) and fault-selective (i.e., no longer valid when one particular fault occurs). By adopting the storage functions and supply rates in the quadratic difference form (QdF), the dissipativity conditions are represented as quadratic functions of the input/output trajectories of the process, which captures much more detailed dynamical features compared to conventional dissipativity (e.g., QSR-type supply rates). These dissipativity properties are determined offline by solving an optimization problem with linear matrix inequality constraints. The online diagnosis algorithm involves checking of inequalities on input/output trajectories, which is much simpler compared to the diagnosis approaches based on observers or parameter estimation. The proposed approach is illustrated using a case study of fault diagnosis of a heat exchanger.",
     "keywords": ["Fault diagnosis", "Dissipativity", "Quadratic difference form"]},
    {"article name": "Model-based estimation and control of interface level in a two-phase vertical decanter: A case study of palm-oil/water system",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.10.022",
     "publication date": "01-2018",
     "abstract": "Separation of two immiscible liquids by a decanter is an important unit operation widely used in petrochemical processes. However, fluctuations in the feed stream create a difficulty in achieving the desired product by such a unit due to the limitations of interface detection and entrainment of heavy phase into a light phase. This research proposes techniques for estimating and controlling the interface position. The interface position is calculated by using information from an ultrasonic sensor and a pressure transducer. An input/output linearizing controller is applied to control the interface position and total liquid height by adjusting the outflows of each liquid phase. A real-time estimator predicts the feed fraction that cannot be measured directly. The proposed method is evaluated through servo and regulatory tests with a bench-scale palm\u2010oil/water decanter. The experimental results show that the developed method forces outputs to desired setpoints effectively despite the presence of feed disturbances.",
     "keywords": ["Two-phase vertical decanter", "Input/output linearization", "Interface estimation", "Model-based control", "Advanced process control", "Palm-oil/water separation"]},
    {"article name": "MILP method for objective reduction in multi-objective optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.10.021",
     "publication date": "01-2018",
     "abstract": "A procedure for reducing objectives in a multi-objective optimization problem given a set of Pareto solutions is presented. Three different models are detailed, which achieve three different degrees of objective reduction. These models are based on maintaining the dominance structure of the problem. To compare the performance of the proposed models, these are tested with pure mathematical cases and with actual data from previous works in the field of multi-objective optimization. The first model provides the reduced subset of objectives that do not alter the dominance structure of the problem at all. The second model determines the minimum subset of objectives that alters the dominance structure with an upper predefined limit for the error. The last model provides the subset of objectives with a previously defined cardinality, which achieves the minimum error. The possibility of different inputs introduces flexibility into the models, which accounts for the preferences of the decision-maker.",
     "keywords": ["Multi-objective optimization", "\u03b4-MOSS", "k-EMOSS", "Objective reduction", "Dominance structure", "Pareto frontier"]},
    {"article name": "Interference of the oscillating glycolysis with the oscillating tryptophan synthesis in the E. coli cells",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.10.003",
     "publication date": "01-2018",
     "abstract": "Autonomous oscillations of glycolytic intermediates\u2019 concentrations reflect the dynamics of the control and regulation of this major catabolic pathway in living cells. In spite of its major role in the cell central carbon metabolism (CCM), glycolysis interference with other metabolic pathways has seldom been investigated. However, the glycolysis model module is the \u2018core\u2019 part of any systematic and structured model-based analysis of the cell metabolism. On the other hand, tryptophan (TRP) synthesis is another oscillatory metabolic process of high practical interest in the biosynthesis industry, and in medicine. Because the TRP synthesis is strongly connected to the glycolysis through the PEP (phosphoenolpyruvate) node, based on two reduced kinetic models for glycolysis and TRP synthesis, this paper performs, for the first time, an in silico analysis of the way by which the two oscillatory processes interfere in the E. coli cells. The analysis allows guidance of tryptophan synthesis optimization.",
     "keywords": ["Reduced dynamic models", "Glycolysis", "Tryptophan synthesis", "Escherichia coli", "Oscillation occurrence"]},
    {"article name": "Heat-integrated water allocation network synthesis for industrial parks with sequential and simultaneous design",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.10.002",
     "publication date": "01-2018",
     "abstract": "This paper presents a methodology for the synthesis of inter-plant heat-integrated water allocation network (IHIWAN) in industrial parks, to promote the overall level of resource conservation by exploiting the cooperative utilization among multiple plants. Based on superstructure, the proposed method introduces inter-plant integration strategies specific to industrial parks, involving the coupling scheme of water allocation and heat exchange. And a non-linear programming model (NLP) is formulated containing water allocation subsystem and heat integration subsystem. Accordingly, economically optimal IHIWAN is synthesized via two optimization routes respectively: sequential design and simultaneous design. Results of application examples indicate that inter-plant integration of the overall water-heat system in the scale of the park can bring about significant savings compared to the summation of integration within each stand-alone plant. In addition, simultaneous design presents preferable result with regard to overall economic performance, yet sequential design features more targeting flexibility and lower requirement on solution process.",
     "keywords": ["Water allocation network", "Heat integration", "Industrial park", "Sequential design", "Simultaneous design", "NLP model"]},
    {"article name": "Adaptive robust optimization with minimax regret criterion: Multiobjective optimization framework and computational algorithm for planning and scheduling under uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.09.026",
     "publication date": "01-2018",
     "abstract": "Regret is defined as the deviation of objective value from the perfect information solution, and serves as an important evaluation metric for decision-making under uncertainty. This paper proposes a novel framework that effectively incorporates the minimax regret criterion into two-stage adaptive robust optimization (ARO). In addition to the conventional robustness criterion, this ARO framework also simultaneously optimizes the worst-case regret to push the performance of the resulting solution towards the utopia one under perfect information. By using a data-driven uncertainty set, we formulate a multiobjective ARO problem that generates a set of Pareto-optimal solutions to reveal the systematic trade-offs between the conventional robustness and minimax regret criteria. The resulting multi-level mixed-integer programming problem cannot be solved directly by any off-the-shelf optimization solvers, so we further propose tailored column-and-constraint generation algorithms to address the computational challenge. Two applications on process network planning and batch process scheduling are presented to demonstrate the applicability of the proposed framework and the efficiency of the proposed solution algorithms.",
     "keywords": ["Adaptive robust optimization", "Minimax regret", "Big data", "Multiobjective optimization", "Planning and scheduling"]},
    {"article name": "On the application of a nature-inspired stochastic evolutionary algorithm to constrained multi-objective beer fermentation optimisation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.10.019",
     "publication date": "01-2018",
     "abstract": "Fermentation is an essential step in beer brewing, often acting as the system bottleneck due to the time-consuming nature of the process stage (duration >120\u00a0h), where a trade-off exists between attainable ethanol concentration and required batch time. To explore this trade-off we employ a multi-objective plant propagation algorithm (the Strawberry algorithm), for identifying temperature manipulations for improved fermentation performance. The methodology employed successfully produces families of favourable temperature profiles which exist along the Pareto front. A subset of these output profiles can simultaneously reduce batch time and increase product ethanol concentration while satisfying constraints on by-products produced in the fermenters, representing significant improvements in comparison with current industrial practice. A potential batch time reduction of over 12\u00a0h has been highlighted, coupled with a moderate improvement in ethanol content.",
     "keywords": ["Dynamic optimisation", "Nature-inspired optimisation", "Multi-objective optimisation", "Stochastic optimisation", "Solution representation", "Beer fermentation"]},
    {"article name": "Chemical process systems analysis using thermodynamic balance equations with entropy generation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.03.003",
     "publication date": "12-2017",
     "abstract": "Systematic analyses of chemical process systems with steady-state balance equations for Material, Energy and Entropy can yield insights about energy efficiency and property model consistency, with entropy generation being the indicator quantity. Calculations of different sets of independent overall and subsection variables can reveal inefficient locations as well as the impacts of changes in process configuration and conditions. Demonstration is also made of how the analysis tests properties models because values of entropy generation must be positive, and the sum of heat rejections and entropy generations from sections must equal those from an overall analysis. Examples focus on chemical processes with internal circulation of complex mixtures, including thermochemical decomposition of water, and three different carbon-capture processes having essentially the same flue gas feed and gaseous products. Ranking of the CO2 capture processes is made, while negative entropy generation is encountered with some property models.",
     "keywords": ["Chemical process systems", "Thermodynamics", "Entropy generation", "Post-combustion carbon capture", "Property models"]},
    {"article name": "Multi-scale equation of state computations for confined fluids",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.05.028",
     "publication date": "12-2017",
     "abstract": "Fluid properties of five binary mixtures relevant to shale gas and light tight oil in confined nano-channels are studied. Canonical (NVT) Monte Carlo simulations are used to determine internal energies of departure of pure fluids using the RASPA software system (Dubbeldam et al., 2015). The linear mixing rule proposed by Lucia et al. (2012) is used to determine internal energies of departure for mixtures, U M D , in confined spaces and compared to U M D from direct NVT Monte Carlo simulation. The sensitivity of the mixture energy parameter, aM, for the Gibbs-Helmholtz constrained (GHC) equation, confined fluid molar volume, VM, and bubble point pressure are studied as a function of uncertainty in U M D . Results show that the sensitivity of confined fluid molar volume to 5% uncertainty in U M D is less than 1% and that the GHC equation predicts physically meaningful reductions in bubble point pressure for light tight oils.",
     "keywords": ["Confined fluids", "Monte Carlo simulation", "GHC equation of state", "Sensitivity analysis", "Bubble point pressure"]},
    {"article name": "Mixture formulation through multivariate statistical analysis of process data in property cluster space",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.06.017",
     "publication date": "12-2017",
     "abstract": "Data-driven modeling approaches are suitable for representing complex processes and phenomena in cases where cause-and-effect cannot be easily described from first-principles. Chemical product formulation in industrial research and development is an area where the analysis of mixture data could be utilized more effectively. Correlation, either partial or complete, is inherent in such mixture data and requires the use of multivariate statistical tools for visualization and identification of important relationships in the data. In this paper, a systematic methodology is developed by integrating data-driven chemometric techniques and property based visualization and optimization tools to solve mixture formulation problems involving multi-block data structures. Effort has been focused on: the development of mathematical models by utilizing multivariate understanding of process and product data, visually identifying design targets a priori, and decomposition of the design problem by incorporating the concept of reverse problem formulation and property clustering techniques. A case study in industrial thermo-plastic development is presented to illustrate the methodology developed in this paper.",
     "keywords": ["Mixture formulation", "Model reduction", "Optimization", "Chemical product design", "Visualization", "Systems engineering"]},
    {"article name": "Thermodynamic analysis of the driving force approach: Non-reactive systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.06.021",
     "publication date": "12-2017",
     "abstract": "A thermodynamic analysis of the driving force approach proposed by Gani and Bek-Pedersen (2000) has been performed. Through the thermodynamic analysis, the fundamental relationships between the driving force definition and the energy involved in the vapor-liquid phase equilibrium are revealed. It is clearly demonstrated that the driving force approach is not a heuristic relationship, rather, it may be considered as a powerful thermodynamics-based approach for the design of many non-reactive processes: i.e., distillation, liquid extraction, crystallization, etc. It is shown that the driving force is explicitly related to: the vapor and liquid enthalpies, the heats of vaporization of the different species and the energy associated to the non-idealities. Also, the connection between the maximum driving force and the minimum separation energy is shown through the thermodynamic separation efficiency concept proposed by Robinson and Gilliland (1950). The analysis is highlighted with two representative separation problems involving an ideal and a non-ideal binary mixtures.",
     "keywords": ["Driving force approach", "Thermodynamic analysis", "Minimum energy"]},
    {"article name": "Hybrid ontology-learning materials engineering system for pharmaceutical products: Multi-label entity recognition and concept detection",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.03.012",
     "publication date": "12-2017",
     "abstract": "The dawn of a new era in knowledge management due to information explosion is making old habits of modeling knowledge and decision-making inadequate. In the search for new modeling paradigms, we expect ontologies to play a big role. One of the critical challenges we face is the scarcity of semantically rich, properly populated, ontologies in most application domains in chemical and materials engineering. Developing such ontologies is a very challenging task requiring considerable investment in time, effort, and expert knowledge. One needs automation tools that can assist an ontology engineer to quickly develop and curate domain-specific ontologies. We consider our conceptual framework in this paper, a general approach for populating scientific ontologies, and its implementation as the prototype HOLMES, as an early attempt towards such an automated knowledge management environment. Our approach integrates a variety of machine learning and natural language processing methods to extract information from journal articles and store them semantically in an ontology. In this work, identification of key terms (such as chemicals, drugs, processes, anatomical entities, etc.) from abstracts, and the classification of these terms into 25 classes are presented. Two methods, a multi-class classifier (SVM) and a multi-label classifier (HOMER), were tested on an annotated data set for the pharmaceutical industry. The test was done using two different versions of the same data set, one using the BIO notation and the other not. The F1 scores for HOMER, were better in the BIO notation (63.6% vs 48.5%) while SVM performed better in the non-BIO version (54.1% vs 53.2%). However, the standard metrics did not consider the effect of the multiple answers that the multi-label classifier is allowed to obtain. As the results of our computational experiments show, while the performance of multi-label classifier is encouraging, much more remains to be done in order to develop a practically viable automated ontology-based knowledge management system.",
     "keywords": ["Natural language processing", "Entity recognition", "Ontology", "Machine learning", "Concept detection"]},
    {"article name": "An integrated framework for designing formulated products",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.05.014",
     "publication date": "12-2017",
     "abstract": "Formulated products are created by mixing various ingredients together. They are commercially produced for drugs, cosmetics, cleaning agents, lubricants and many others. The design of formulated products is diverse and multidisciplinary in nature; physico-chemical properties, product microstructure, product quality, and all other related areas have to be considered integratively. Various types of data, methods and tools are needed to tackle formulated product design problems to achieve an optimal design. In this study, an integrated framework as well as the design steps are proposed for the design of all kinds of formulated products including liquids, solids, emulsions, etc. Strategies for the selection of different modelling methods such as rule-based methods, model-based methods, experiments and databases are also presented. A multi-layer knowledge library is proposed to store all the information needed in the design of formulated products. Case studies of battery electrolyte and laundry detergent are given to illustrate the design framework.",
     "keywords": ["Product design", "Formulated products", "Product ingredients", "Rule-based methods", "Model-based methods"]},
    {"article name": "Process analysis and optimization of continuous pharmaceutical manufacturing using flowsheet models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.030",
     "publication date": "12-2017",
     "abstract": "Continuous manufacturing has attracted increasing research attention in the pharmaceutical industry within the last decade. Based on the extensive experimental studies, numerous modeling and computational approaches have been developed to capture the process information and make predictions. Moreover, flowsheet models have been built to simulate the dynamic behaviors of a plant-wide manufacturing process with respect to different process input factors. However, there still lacks a systematic way to make the best use of flowsheet models in pharmaceutical processes. In this work, we propose a framework of process analysis and optimization for the continuous pharmaceutical manufacturing process where flowsheet models are available. Specifically, sensitivity analysis is conducted to identify the input factors that are most influential on the output; feasibility analysis is then implemented to characterize the design space in the high-dimensional space. Finally, process optimization is performed to find the optimal operation conditions that result in the minimum cost.",
     "keywords": ["Flowsheet modeling", "Sensitivity analysis", "Design space", "Continuous pharmaceutical manufacturing", "Optimization"]},
    {"article name": "Building pharmacokinetic compartmental models using a superstructure approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.05.027",
     "publication date": "12-2017",
     "abstract": "An optimization framework is presented to support the model builder in elucidating compartmental models that plausibly describe data obtained during experimentation. Here, one specifies a priori the maximum number of compartments and type of flows to contemplate during the optimization. The mathematical model follows a \u2018superstructure\u2019 approach, which inherently considers the different feasible flows between any pair of compartments. The model activates those flows/compartments that provide the optimal fit for a given set of experimental data. A regularized log-likelihood function is formulated as the performance metric. To deal with the resulting set of differential equations orthogonal collocation on finite elements is employed. A case study related to pharmacokinetics of an oncological agent demonstrates the advantages and limitations of the proposed approach. Numerical results show that the proposed approach can provide 33% smaller mean square prediction error in comparison with a compartmental model previously suggested in the literature.",
     "keywords": ["Compartmental modeling", "NLP", "Pharmacokinetics"]},
    {"article name": "On the analysis of complex biological supply chains: From process systems engineering to quantitative systems pharmacology",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.06.003",
     "publication date": "12-2017",
     "abstract": "The use of models in biology has become particularly relevant as it enables investigators to develop a mechanistic framework for understanding the operating principles of living systems as well as in quantitatively predicting their response to both pathological perturbations and pharmacological interventions. This application has resulted in a synergistic convergence of systems biology and pharmacokinetic-pharmacodynamic modeling techniques that has led to the emergence of quantitative systems pharmacology (QSP). In this review, we discuss how the foundational principles of chemical process systems engineering inform the progressive development of more physiologically-based systems biology models.",
     "keywords": ["Pharmacokinetics", "Pharmacodynamics", "Multi-scale systems biology", "Quantitative systems pharmacology"]},
    {"article name": "Production and maintenance planning optimisation in biopharmaceutical processes under performance decay using a continuous-time formulation: A multi-objective approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.03.008",
     "publication date": "12-2017",
     "abstract": "This work addresses the optimal planning of biopharmaceutical continuous manufacturing processes under performance decay, modelling its operational constraints in a mixed integer linear programing (MILP) model, based on a Resource Task Network (RTN) continuous single-time grid formulation. The model assesses the manufacturing constraints for the determination of the production schedule while defining the appropriate maintenance planning timing in downstream units to assure optimal process performance. An improved model approach is discussed and compared with the application results to literature-based industrial cases. In order to evaluate different solutions towards the multiple decision maker\u2019s strategic and operational objectives, the definition of the Pareto sets for three bi-objective analyses is performed with the augmented \u03b5-constraint method, using the profit maximisation: with the minimisation of the number of intermediate maintenance operations, the maximisation of average service level, and the maximisation of the utilisation rate of processing units.",
     "keywords": ["Biopharmaceutical plants", "Production and maintenance planning", "Performance decay", "Multi-objective optimisation"]},
    {"article name": "Analysis of the fermentation strategy and its impact on the economics of the production process of PHB (polyhydroxybutyrate)",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.03.009",
     "publication date": "12-2017",
     "abstract": "PHB (polyhydroxybutyrate) is a biodegradable polymer that can replace petrochemical polymers due to its properties and applications. However, its commercial use remains limited because of its high production cost. In this work, factors affecting the PHB fermentation reactor and its impact on the production cost are studied. The production process considers the sucrose fermentation with Azohydromonas australica, followed by the PHB purification and drying. The proposed methodology is divided into two stages: firstly, the analysis of the reactor where it is necessary to define the operation mode and the operating conditions to achieve high product yield and productivity; and secondly, the analysis of production process using technical, economic and environmental criteria to evaluate the process conceptual design. The proposed methodology can reduce complex efforts to achieve an efficient fermentation and validate the design of PHB production process, thereby enabling a biopolymer production with a competitive cost and low environmental impact. The best operation mode for the fermentation reactor was fed-batch, whose feeding strategy allowed to achieve satisfactory technical-economic-environmental results for the PHB production plant: product yield of 0.36\u00a0g PHB/g sucrose, productivity of 1.6\u00a0g/L/h, production cost of 2.6\u00a0USD/kg PHB, return on investment of 34.2%, payback period of 2.9\u00a0yr, emissions of 1.7\u00a0kg CO2/kg PHB, energy consumption of 10.1\u00a0MJ/kg PHB and process requirement of 16.9\u00a0kg water/kg PHB.",
     "keywords": ["Fermentation", "Modeling", "Dynamic analysis", "Operation strategy", "Process simulation", "Techno-economic evaluation"]},
    {"article name": "Agent-based modeling \u2013 Proof of concept application to membrane separation and hydrogen storage in a MOF",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.03.004",
     "publication date": "12-2017",
     "abstract": "Modeling techniques such as molecular dynamics are often complicated and computationally expensive. This paper discusses agent-based modeling (ABM) as an alternative modeling approach that allows for any level of modeling complexity at a reasonable computational effort. The ABM framework is generally applicable for modeling complex systems. More specifically, ABM makes it possible to impose simple behaviors on individual \u201cagents\u201d which interact to produce an overall complex system behavior, thus facilitating the modeling of complex systems. To demonstrate the feasibility of ABM, we apply the approach to two problems: gas separation in an inorganic-organic membrane and hydrogen capture in a Metal-Organic Framework (MOF). The data collected from the models is consistent with experimental data suggesting that ABM is an effective modeling tool.",
     "keywords": ["Agent-based modeling", "Molecular dynamics", "Metal-organic framework", "Membrane", "Separation"]},
    {"article name": "Parallelization methods for efficient simulation of high dimensional population balance models of granulation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.043",
     "publication date": "12-2017",
     "abstract": "In order to solve high resolution PBMs to simulate real systems, with high accuracy and speed, a comprehensive and robust parallelization framework is needed. In this work, parallelization using just Message Passing Interface (MPI) and a more advanced method using a hybrid MPI\u00a0+\u00a0OpenMP (Open Multi-Processing) technique, have been applied to simulate high resolution PBMs on the computing clusters, SOEHPC and Stampede. We study the speed up and the scale up of these parallelization techniques for different system sizes and different computer architectures to come up with one of the fastest ways to solve a PBM to date. Parallel PBMs ran approximately 50\u201360 times faster, when using 128 cores, than the serial PBMs ran. In this work it is found that hybrid MPI\u00a0+\u00a0OMP methods which account for socket affinities led to the fastest PBM compute times and about 80% less memory than a purely MPI approach.",
     "keywords": ["MPI", "OpenMP", "Parallel computing", "Population balance model", "Granulation", "Pharmaceutical process design"]},
    {"article name": "Discrete-time mixed-integer programming models for short-term scheduling in multipurpose environments",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.06.013",
     "publication date": "12-2017",
     "abstract": "We present new discrete-time mixed-integer linear programming formulations for short-term scheduling in multi-purpose batch plants, the most general sequential production environment. We first discuss how multi-purpose batch plants can be expressed using State-Task Network and Resource-Task Network representations through batch-based definition of states (resources) and tasks. We then develop two models based on each representation that account for limited intermediate storage, and discuss extensions such as limited shared resources and time-varying resource availability/cost. Finally, we present several case studies to illustrate the applicability and performance of the proposed models.",
     "keywords": ["Process operations", "Sequential production environment"]},
    {"article name": "A computational framework for integrating campaign scheduling, dynamic optimization and optimal control in multi-unit batch processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.05.024",
     "publication date": "12-2017",
     "abstract": "This contribution presents a framework for addressing the campaign scheduling, dynamic optimization and optimal control of batch processes in an integrated fashion. The strategy is comprised of an offline and an online phase. The first involves solving a conventional campaign scheduling problem and serves to generate key information needed in the second. The latter consists of a modified dynamic optimization/optimal control algorithm and serves to update the offline campaign schedule in real time as well as to provide the batch process with optimal control actions to achieve maximum campaign profit/performance. As a result of this two-phase architecture, the algorithm avoids the solution of a mixed-integer optimization problem online and can support virtually any process recipe structure including any type of recycle. To demonstrate its potential, we test the proposed methodology to solve the integrated campaign scheduling, dynamic optimization and optimal control of a batch plant for the production of nopol.",
     "keywords": ["Model predictive control", "Batch process scheduling", "Dynamic optimization"]},
    {"article name": "Modeling framework and computational algorithm for hedging against uncertainty in sustainable supply chain design using functional-unit-based life cycle optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.05.021",
     "publication date": "12-2017",
     "abstract": "In this work, we address the life cycle economic and environmental optimization of a supply chain network considering both design and operational decisions under uncertainty. A modeling framework is proposed that integrates the functional-unit-based life cycle optimization methodology and the two-stage stochastic programming approach for sustainable supply chain optimization under uncertainty. We develop a stochastic mixed-integer linear fractional programming (SMILFP) model to tackle multiple uncertainties regarding feedstock supply and product demand. To address the computational challenge of solving the resulting large-scale SMILFP problems, an efficient solution algorithm is developed that takes advantage of the efficiency of parametric algorithm and the decomposition-based multi-cut L-shaped method. We present a case study based on a spatially explicit model for the optimal design and operations of a county-level hydrocarbon biofuel supply chain in Illinois to demonstrate the applicability of the proposed modeling framework and the efficiency of the solution algorithm.",
     "keywords": ["Sustainable supply chain", "Life cycle optimization", "Uncertainty", "Mixed-integer fractional programming", "Biofuels"]},
    {"article name": "Design, simulation and techno-economic analysis of two processes for the conversion of shale gas to ethylene",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.05.023",
     "publication date": "12-2017",
     "abstract": "Shale gas is being considered as feedstock for the production of major petrochemicals. One such chemical is ethylene. Although the typical production for ethylene is carried out via thermal cracking, alternative processes have been gaining importance recently. Among such alternatives are the Oxidative Coupling of Methane (OCM) and the Methanol to Olefins (MTO) processes. The first one is a direct conversion process while the second one involves several stages. The aim of this work is to carry out an economic, energy and environmental assessment for these two processes. The results show that the MTO process is more profitable under the economic and technical scenario considered here. A sensitivity analysis was conducted to show the shale gas and ethylene prices under which the OCM process would be economically attractive. An analysis on catalyst improvement needed for the OCM process to be profitable is also reported.",
     "keywords": ["Shale gas", "Ethylene", "Oxidative coupling of methane", "Methanol to olefins", "Techno-economic analysis", "CO2 emissions"]},
    {"article name": "A systems approach to quantifying the value of power generation and energy storage technologies in future electricity networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.05.012",
     "publication date": "12-2017",
     "abstract": "A new approach is required to determine a technology's value to the power systems of the 21st century. Conventional cost-based metrics are incapable of accounting for the indirect system costs associated with intermittent electricity generation, in addition to environmental and security constraints. In this work, we formalise a new concept for power generation and storage technology valuation which explicitly accounts for system conditions, integration challenges, and the level of technology penetration. The centrepiece of the system value (SV) concept is a whole electricity systems model on a national scale, which simultaneously determines the ideal power system design and unit-wise operational strategy. It brings typical Process Systems Engineering thinking into the analysis of power systems. The model formulation is a mixed-integer linear optimisation and can be understood as hybrid between a generation expansion and a unit commitment model. We present an analysis of the future UK electricity system and investigate the SV of carbon capture and storage equipped power plants (CCS), onshore wind power plants, and grid-level energy storage capacity. We show how the availability of different low-carbon technologies impact the optimal capacity mix and generation patterns. We find that the SV in the year 2035 of grid-level energy storage is an order of magnitude greater than that of CCS and wind power plants. However, CCS and wind capacity provide a more consistent value to the system as their level of deployment increases. Ultimately, the incremental system value of a power technology is a function of the prevalent system design and constraints.",
     "keywords": ["Power system modeling", "Unit commitment", "Future electricity system", "Technology value", "Energy storage", "Intermittent renewables", "Carbon capture and storage"]},
    {"article name": "Process integration and superstructure optimization of Organic Rankine Cycles (ORCs) with heat exchanger network synthesis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.05.013",
     "publication date": "12-2017",
     "abstract": "Low and medium temperature energy utilization is one way to alleviate the energy crisis and environmental pollution problems. In the past decades, Organic Rankine Cycles (ORCs) have become a very promising technology for low and medium temperature energy utilization. When an ORC is used to recover waste heat in chemical plants, heat integration between the ORC and the process streams should be performed to save more utilities and generate more power. This study aims to integrate an ORC into a background process to generate maximum electricity without increasing the hot utility usage. We propose a two-step method to integrate an ORC to a background process, optimally considering the modifications of the ORC to increase the thermal efficiency and heat recovered by the working fluid simultaneously. The first step is to determine the configuration (turbine bleeding, regeneration, superheating) and operating conditions (working fluid flowrate, evaporation and condensation temperatures, turbine bleed ratio, degree of superheat, bleeding pressure). The second step is to synthesize the heat exchanger network by minimizing the number of heat exchangers that keep the hot utility unchanged. A well-studied example from the literature is solved to demonstrate the effectiveness of the proposed model for industrial waste heat recovery. The net power output in this paper is improved by 13% compared with the best known previous literature design for this system. The proposed method is also useful for quickly screening working fluids while considering integration potential. Screening of several working fluids revealed that using R601 (n-pentane) in place of the original working fluid (n-hexane) can increase the power output of the example system by an additional 14%.",
     "keywords": ["Organic Rankine Cycle", "Optimization", "Process integration", "Duran\u2013Grossmann model", "Heat exchanger network synthesis"]},
    {"article name": "Multi-resolution model of an industrial hydrogen plant for plantwide operational optimization with non-uniform steam-methane reformer temperature field",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.040",
     "publication date": "12-2017",
     "abstract": "Hydrogen is consumed in large quantities in the chemical industry. The most common industrial process for hydrogen production is steam-methane reforming, which is carried out using an energy-intensive furnace. The plant energy efficiency depends strongly on the spatial temperature distribution within the furnace; the narrower the distribution, the higher the efficiency that can be achieved. However, currently available studies on plantwide optimization of hydrogen plants ignore this crucial aspect. Adequate resolution of the spatial temperature distribution is necessary to determine the furnace operating temperature, which, in turn, determines the plant efficiency. In this work, a multi-resolution model of a hydrogen plant is developed. It includes a high-resolution model of the furnace, and low-resolution models, adequate for the purpose of plantwide optimization, of other unit operations. The developed model is used to determine the optimal process conditions after furnace temperature homogenization as part of a plant start-up or setpoint changeover procedure.",
     "keywords": ["Hydrogen plant", "Steam-methane reformer", "Plantwide optimization", "Multi-resolution modeling", "Smart manufacturing"]},
    {"article name": "On modeling and optimization of micro-photosynthetic power cells",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.015",
     "publication date": "12-2017",
     "abstract": "In this paper, we describe a model for a micro photosynthetic power cell (\u03bcPSC). \u03bcPSC is an electrochemical device which harnesses electrons and protons from the electron transport chains of photosynthesis (in the presence of light), and respiration (in the absence of light) occurring inside an oxygenic photosynthetic organism to generate electricity. \u03bcPSC has been identified as a potential power generating device for low power applications. A one dimensional model for a \u03bcPSC based on mass balances of species in the anode and cathode chambers considering unsteady one dimensional diffusion is developed. The governing equations are solved to obtain the v \u2212 i characteristics of a \u03bcPSC device. The unknown parameters in the model are estimated from experimental data. The model with its estimated parameters is validated using test v \u2212 i characteristic data. Simulation analysis is performed to understand performance improvement possibilities. Explicit optimization formulations to identify promising device designs are also explored.",
     "keywords": ["Micro photosynthetic power cell", "Mathematical modeling", "1-D model", "Design optimization"]},
    {"article name": "Closed-loop reactor network synthesis with guaranteed robustness",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.06.010",
     "publication date": "12-2017",
     "abstract": "This paper presents a systematic methodology to design closed-loop reactor networks with guaranteed robustness. The methodology is based on the superstructure approach for reactor network synthesis and extends our previous work (Zhao and Marquardt, 2016) to simultaneously design the process and the control system structure. The spectral abscissa of the Jacobian matrix of the closed-loop reactor network is chosen to measure the response speed of the designed process. A mixed-integer nonlinear program (MINLP) with complementarity constraints and a robust eigenvalue constraint is formulated and solved sequentially by a two-step solution strategy. Structural alternatives of the process and the control system as well as parametric uncertainties are considered in an integrated framework. A case study involving continuous stirred-tank (CSTR) and plug flow (PFR) reactors is presented to illustrate the novel approach and compared with an established two-step design approach.",
     "keywords": ["Reactor network synthesis", "Decentralized control structure selection", "Superstructure approach", "Integration of process and control system design", "Normal vector approach", "Robust optimization"]},
    {"article name": "Modeling, estimation and control of the anaesthesia process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.016",
     "publication date": "12-2017",
     "abstract": "In this work we present different design strategies towards model based simultaneous multiparametric model predictive control and state estimation for intravenous anaesthesia. We first present a detailed compartmental mathematical model featuring a pharmacokinetic and a pharmacodynamics part. Due to unavailability of data and information, different estimation techniques are formulated and implemented. Furthermore these estimation techniques are implemented simultaneously with multiparametric model predictive controllers and tested for real patient data under the assumption that the output is either noise free or corrupted by noise. The derived control schemes are able to deal with two of the main challenges in controlling the depth of anaesthesia: (i) model nonlinearity and (ii) inter- and intra- patient variability.",
     "keywords": ["Intravenous anaesthesia", "Compartmental modeling", "Mutiparameric/explicit model predictive control", "Estimation", "-Inter/-intra patient variability"]},
    {"article name": "Input estimation as a qualitative trend analysis problem",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.04.011",
     "publication date": "12-2017",
     "abstract": "The study of techniques for qualitative trend analysis (QTA) has been a popular approach to address challenges in fault diagnosis of engineered processes. Such challenges include the lack of reliable extrapolation of available models and lack of representative data describing previously unseen circumstances. Many of these challenges appear in biological systems even when normal operation can be assumed. It is for this reason that QTA techniques have also been proposed for the purpose of fault detection, automation, and dynamic modeling. In this work, we adopt a shape-constrained spline function method for the purpose of unknown input estimation. Thanks to data collected at laboratory-scale in a biological reactor for urine nitrification, this novel approach has been demonstrated successfully for the first time.",
     "keywords": ["DO dissolved oxygen", "dissolved oxygen", "LTI linear time-invariant", "linear time-invariant", "MHE moving horizon estimation", "moving horizon estimation", "OUR oxygen uptake rate", "oxygen uptake rate", "SCS shape-constrained splines", "shape-constrained splines", "QTA qualitative trend analysis", "qualitative trend analysis", "Global optimization", "Input estimation", "Oxygen uptake rate", "Qualitative trend analysis", "Wastewater treatment"]},
    {"article name": "Global optimisation for dynamic systems using interval analysis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.028",
     "publication date": "12-2017",
     "abstract": "Engineers seek optimal solutions when designing dynamic systems but a crucial element is to ensure bounded performance over time. Finding a globally optimal bounded trajectory requires the solution of the ordinary differential equation (ODE) systems in a verified way. To date these methods are only able to address low dimensional problems and for larger systems are unable to prevent gross overestimation of the bounds. In this paper we show how interval contractors can be used to obtain tightly bounded optima. A verified solver constructs tight upper and lower bounds on the dynamic variables using contractors for initial value problems (IVP) for ODEs within a global optimisation method. The solver provides guaranteed bound on the objective function and on the first order sensitivity equations in a branch and bound framework. The method is compared with three previously published methods on three examples from process engineering.",
     "keywords": ["Global optimisation", "Dynamic systems", "Verified integration", "Interval analysis", "Interval contractors"]},
    {"article name": "Safety Analysis Embedded in Heat Exchanger Network Synthesis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.04.009",
     "publication date": "12-2017",
     "abstract": "Optimization of Heat Exchanger Networks (HEN) has received considerable attention in last decades, but a few studies on inherent safety. In this paper, risk assessment is considered simultaneously during the synthesis of HENs. As risks depend on the equipment selected, a superstructure enabling selection of direct and indirect heat transfer between hot and cold streams and different types of heat exchangers (HEs) was tested. The individual heat transfer and the overall HEN risk were analyzed. Different individual risk limits have been introduced for certain types of heat transfer, e.g. between two process streams or between utility and process streams. The sensitivity analyses were performed first, considering only toxicity as a risk, but later flammability and explosiveness were also simultaneously tested, in order to consider the most important aspects of safety. The results obtained indicate that rather significant changes in HEN designs can increase safety, while still exhibiting similar economic efficiency.",
     "keywords": ["heat exchanger network", "synthesis", "safety analysis", "risk assessment", "simultaneous risk assessment"]},
    {"article name": "Risk analysis of turnaround reschedule planning in integrated chemical sites",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.04.017",
     "publication date": "12-2017",
     "abstract": "Plant maintenance turnarounds constitute a large fraction of all maintenance activities in the process industries. We consider turnaround planning problems over large networks of interconnected plants. The network interactions provide an opportunity to plan and coordinate the different turnaround activities to save on annual downtime and recover the associated revenue. We propose a stochastic optimization model to quantify the risk of loss in rescheduling maintenance turnarounds that have been previously planned and compare the proposed approach to alternative approaches incorporating different production planning strategies under uncertainty. The model also provides simultaneous hedging strategies using inventories for unplanned outages. Thus, our model offers additional flexibility to previous approaches that address long- and medium-term turnaround planning problems, and explicitly incorporates plant reliability in the planning process. Through extensive computational studies, we show that proactive planning strategies that take uncertainty over multiple time periods into account offer substantial benefits over a reactive strategy.",
     "keywords": ["Maintenance planning", "Turnaround planning", "Enterprise-wide optimization", "Risk quantification", "Stochastic programming"]},
    {"article name": "A deep belief network based fault diagnosis model for complex chemical processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.041",
     "publication date": "12-2017",
     "abstract": "Data-driven methods have been regarded as desirable methods for fault detection and diagnosis (FDD) of practical chemical processes. However, with the big data era coming, how to effectively extract and present fault features is one of the keys to successful industrial applications of FDD technologies. In this paper, an extensible deep belief network (DBN) based fault diagnosis model is proposed. Individual fault features in both spatial and temporal domains are extracted by DBN sub-networks, aided by the mutual information technology. A global two-layer back-propagation network is trained and used for fault classification. In the final part of this paper, the benchmarked Tennessee Eastman process is utilized to illustrate the performance of the DBN based fault diagnosis model.",
     "keywords": ["Fault diagnosis", "Deep belief network", "Feature extraction", "Early warning", "Alarm management"]},
    {"article name": "A superstructure-based design of experiments framework for simultaneous domain-restricted model identification and parameter estimation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.014",
     "publication date": "12-2017",
     "abstract": "We present a novel design of experiments (DOE) approach to incorporate model identification into optimal experimental designs based on a postulated model superstructure and an associated relaxation strategy. We show that an adaptive online design of experiments allows for the accurate estimation of the parameters of a domain-restricted model, as well as the model structure and domain on which that model is valid. We further show that previous attempts at combining model identification and parameter estimation are a special case of this framework (when the objective function is formulated in terms of the trace of the Fisher information matrix), and thus the proposed formulation provides the option to use alternate or more complex objective functions. The efficacy of the proposed framework is shown through two case studies: a batch reactor with Arrhenius-type reactions and a carbon dioxide adsorption system.",
     "keywords": ["Design of experiments", "Dynamic model identification", "Parameter estimation"]},
    {"article name": "Refined convergent cross-mapping for disturbance propagation analysis of chemical processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.03.026",
     "publication date": "11-2017",
     "abstract": "In chemical processes, determining the root causes of faults and disturbances is important for improving process safety and economic profit. Recently proposed convergent cross-mapping (CCM) is suitable for both linear and nonlinear systems. However, it cannot be directly applied to chemical processes. This is because chemical processes possess characteristics, such as nonlinearity, high dimensions, continuity, and time delay, that are significantly different from ecosystems, where CCM is applicable. In this paper, we propose refined CCM (RCCM), which integrates fast embedding dimension determinations with a modified procedure to improve the speed of the calculation and accuracy of the analysis. Examples show that RCCM fits chemical processes with the above characteristics. In addition, it shows better performance than other methods in finding the propagation paths of disturbances.",
     "keywords": ["Convergent cross-mapping", "Root cause", "Nonlinearity", "Time delay", "Chemical process"]},
    {"article name": "Improving the conventional reactor/separation/recycle DME process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.05.008",
     "publication date": "11-2017",
     "abstract": "The conventional process to produce dimethyl ether (DME) by the dehydration of methanol is a classical example of a chemical process with a reaction section, a separation section and recycles of material and energy connecting the two. Alternative processes have been explored in attempts to increase process intensification. A recent paper claims that reactive distillation can reduce capital investment and energy costs when compared to a conventional reactor/separation/recycle process.The purpose of this paper is to demonstrate that the conventional process can be significantly improved to the point that it is much superior to the published conventional process and is even more economical than the published reactive distillation process. The proposed flowsheet requires more capital investment (53%) than the reactive distillation process but saves 38% in energy costs, which produces an attractive 61% return on the larger capital investment.",
     "keywords": ["DME process", "Cooled tubular reactor", "Vaporizer"]},
    {"article name": "Sensitivity analysis and stochastic modelling of lignocellulosic feedstock pretreatment and hydrolysis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.05.015",
     "publication date": "11-2017",
     "abstract": "Pretreatment and hydrolysis of lignocellulosic biomass are affected by several uncertainties, which must be systematically considered for a robust process design. In this work, stochastic simulations for expected uncertainties in feedstock composition, kinetic parameter values, and operational parameter values for these two steps were performed. The results indicated that these uncertainties significantly impacted the concentration profiles, which could also affect the optimal batch time. Global sensitivity analysis was then used to identify the critical uncertain parameters. In the feedstock components, cellulose and xylan fractions for acid pretreatment and cellulose fraction for enzymatic hydrolysis were important. Temperature was the most sensitive operating parameter for both acid pretreatment and hydrolysis. The activation energies for different reactions were ranked in terms of their impact on process output. The selected parameters were used to develop stochastic process models using Ito process and mean reverting process for feed composition and kinetic parameter uncertainty.",
     "keywords": ["Lignocellulosic biomass acid pretreatment", "Enzymatic hydrolysis", "Global sensitivity analysis", "Ito process", "Mean reverting process"]},
    {"article name": "Multi-objective optimization of a reactive batch distillation process using reduced order model",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.05.017",
     "publication date": "11-2017",
     "abstract": "Reactive batch distillation (RBD) processes are highly nonlinear in nature and use of first principles model into the optimization problem demands high computational time. Therefore, a reduced order model for RBD process is proposed which is based on selection of states of prime importance, identification of input state variables, definition of reference trajectories for nominal case, and definition of additional variable transformation to facilitate adaptation of the model to variations in operating conditions. Further, the work focuses on development of a multi-objective optimization formulation for arriving at an optimal operation policy for RBD, and a modified version of Non-dominated sorting Genetic algorithm (NSGA-II) is developed for its solution. The proposed approach is evaluated by application to a simulation case study for production of butyl acetate. The optimization results obtained using the reduced order model are comparable to those obtained with first principles model, while requiring only 1/20th of the time.",
     "keywords": ["Reactive batch distillation", "First principles model", "Reduced order model", "Input parameterizations", "Multi-objective optimization", "Modified version of non-dominated sorting genetic algorithm (NSGA-II)"]},
    {"article name": "Comparison of stochastic fault detection and classification algorithms for nonlinear chemical processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.05.016",
     "publication date": "11-2017",
     "abstract": "This paper presents a comparative study of two methods to identify and classify intermittent stochastic faults occurring in a dynamic nonlinear chemical process. The methods are based on two popular stochastic modelling techniques, i.e., generalized polynomial chaos expansion (gPC) and Gaussian Process (GP). The goal is to assess which method is more efficient for fault detection and diagnosis (FDD) when using models with parametric uncertainty, and to show the capabilities and drawbacks of each method. The first method is based on a first-principle model combined with a gPC expansion to represent the uncertainty. Resulting statistics such as probability density functions (PDFs) of the measured variables is further used to infer the intermittent faults. For the second method, a GP model is used to project multiple inputs into a univariate model response from which the fault can be identified based on a minimum distance criterion. The performance of the proposed FDD algorithms is illustrated through two examples: (i) a chemical process involving two continuous, stirred tank reactors (CSTRs) and a flash tank separator, and (ii) the Tennessee Eastman benchmark problem.",
     "keywords": ["Uncertainty analysis", "Generalized polynomial chaos", "Gaussian process", "Model adjustment", "Process monitoring"]},
    {"article name": "Design of computer experiments: A review",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.05.010",
     "publication date": "11-2017",
     "abstract": "In this article, we present a detailed overview of the literature on the design of computer experiments. We classify the existing literature broadly into two categories, viz. static and adaptive design of experiments (DoE). We begin with the abundant literature available on static DoE, its chronological evolution, and its pros and cons. Our discussion naturally points to the challenges that are faced by the static techniques. The adaptive DoE techniques employ intelligent and iterative strategies to address these challenges by combining system knowledge with space-filling for sample placement. We critically analyze the adaptive DoE literature based on the key features of placement strategies. Our numerical and visual analyses of the static DoE techniques reveal the excellent performance of Sobol sampling (SOB3) for higher dimensions; and that of Hammersley (HAM) and Halton (HAL) sampling for lower dimensions. Finally, we provide several potential opportunities for the future modern DoE research.",
     "keywords": ["Design of experiments", "Computer experiments", "Adaptive sampling", "Space-filling", "Surrogate development"]},
    {"article name": "Multi-mode operation of principal component analysis with k-nearest neighbor algorithm to monitor compressors for liquefied natural gas mixed refrigerant processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.05.029",
     "publication date": "11-2017",
     "abstract": "LNG mixed refrigeration (MR) process is usually used for liquefying natural gas. The compressors for refrigerant compression are associated with the high-speed rotating parts to create a high-pressure. However, any malfunction in the compressors can lead to significant process downtime, catastrophic damage to equipment and potential safety consequences. The existing methodology assumes that the process has a single mode of operation, which makes it difficult to distinguish between a malfunction of the process and a change in mode of operation. Therefore, k-nearest neighbor algorithm (k-NN) is employed to classify the operation modes, which is integrated into multi-mode principal component analysis (MPCA) for process monitoring and fault detection. When the fault detection performance is evaluated with real LNG MR process data, the proposed methodology shows more accurate and early detection capability than conventional PCA. Consequently, proposed k-NN integrated multi-mode PCA methodology will play an important role in monitoring the LNG process.",
     "keywords": ["Principal component analysis", "k-nearest neighbor", "Liquefied natural gas", "Mixed refrigeration process", "Multi-mode operation"]},
    {"article name": "Optimal design of bitumen upgrading facility with CO2 reduction",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.06.001",
     "publication date": "11-2017",
     "abstract": "CO2 emissions from bitumen upgrading represent a major source of greenhouse gas emission in the oil sands industry of Canada. In this paper, optimal design of bitumen upgrading plant was studied with the aim of CO2 reduction. Various CO2 capture techniques including oxyfuel combustion, pre\u2013combustion and post\u2013combustion are modeled and incorporated into an integrated optimization model for simultaneous design of bitumen upgrading plant and the associated utility plant. To solve the resulting large-scale mixed integer nonlinear programming (MINLP) problem, augmented Lagrangian decomposition method is used. Optimal configurations under different scenarios (including plant capacity, natural gas, electricity and crude oil prices, and carbon tax) are discussed. The key findings of this study include: (i) hydrocracking for bitumen upgrading is favored, (ii) producing extra power and high\u2013quality duty with a gas turbine burning natural gas is beneficial, and (iii) carbon capture with pre\u2013combustion technology is suggested.",
     "keywords": ["Carbon capture", "Oil sands upgrading", "Utility system", "Integrated optimization model", "Decomposition algorithm"]},
    {"article name": "A new integrated heat pump option for heat upgrading in Cu-Cl cycle for hydrogen production",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.05.009",
     "publication date": "11-2017",
     "abstract": "A potential cascaded vapor compression heat pump is proposed to address the high temperature heat demand in the copper chlorine (Cu-Cl) thermochemical cycle for hydrogen production. The configuration studied is a cuprous chloride CuCl vapor compression heat pump cascaded with a biphenyl (C6H5)2 heat pump. Such cascaded heat pumps is meant to upgrade heat from nuclear power plants with a heat input temperature of approximately 300\u00a0\u00b0C or industrial waste heat to meet the Cu2OCl2 decomposition reactor heat demand. Energy and exergy analyses are performed to understand the performance of the heat pump. It is determined that the CuCl-biphenyl heat pump exhibits a high coefficient of performance for certain operating conditions relating to the compressor isentropic efficiency and the excess CuCl feed temperature. The base energetic and exergetic coefficient of performances of the CuCl-biphenyl heat pump are 1.76 and 1.15 respectively.",
     "keywords": ["Cascaded heat pump", "Hydrogen production", "Biphenyl", "CuCl", "Energy", "Exergy", "Coefficient of performance"]},
    {"article name": "An improved Lagrangian relaxation approach to scheduling steelmaking-continuous casting process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.05.026",
     "publication date": "11-2017",
     "abstract": "In the steelmaking continuous-casting (SCC) process, scheduling problem is a key issue for the iron and steel production. To improve the productivity and reduce material consumption, optimal models and approaches are the most useful tools for production scheduling problems. In this paper, we firstly develop a mixed integer nonlinear mathematical model for the SCC scheduling problem. Due to its combinatorial nature and complex practical constraints, it is extremely difficult to cope with this problem. In order to obtain a near-optimal schedule in a reasonable computational time, Lagrangian relaxation approach is developed to solve this SCC scheduling problem by relaxing some complicated constraints. Owing to the existence of the nonseparability coming from the product of two binary variables, it is still hard to deal with this relaxed problem. By making use of their characteristics, the subproblems of the relaxed problem can be converted into different difference of convex (DC) programming problems, which can be solved effectively by using the concave\u2013convex procedure. Under some reasonable assumptions, the convergence of the concave\u2013convex procedure can be established. Furthermore, we introduce an improved conditional surrogate subgradient algorithm to solve the Lagrangian dual (LD) problem and analyze its convergence under some appropriate assumptions. In addition, we present a simple heuristic algorithm to construct a feasible schedule by adjusting the solutions of the relaxed problem. Lastly, some numerical results are reported to illustrate the efficiency and effectiveness of the proposed method.",
     "keywords": ["Scheduling problem", "Steelmaking continuous casting", "Surrogate subgradient method", "Lagrangian relaxation", "Heuristic method"]},
    {"article name": "A dynamic optimization framework for integration of design, control and scheduling of multi-product chemical processes under disturbance and uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.05.007",
     "publication date": "11-2017",
     "abstract": "A novel dynamic optimization framework is presented for integration of design, control, and scheduling for multi-product processes in the presence of disturbances and parameter uncertainty. This framework proposes an iterative algorithm that decomposes the overall problem into flexibility and feasibility analyses. The flexibility problem is solved under a critical (worst-case) set of disturbance and uncertainty realizations, whereas the feasibility problem evaluates the dynamic feasibility of each realization, and updates the critical set accordingly. The algorithm terminates when a robust solution is found, which is feasible under all identified scenarios. To account for the importance of grade transitions in multiproduct processes, the proposed framework integrates scheduling into the dynamic model by the use of flexible finite elements. This framework is applied to a multi-product continuous stirred-tank reactor (CSTR) system subject to disturbance and parameter uncertainty. The proposed method is shown to return robust solutions that are of higher quality than the traditional sequential method. The results indicate that scheduling decisions are affected by design and control decisions, thus motivating the need for integration of these three aspects.",
     "keywords": ["Dynamic optimization", "Optimal design and control", "Process scheduling", "Decomposition algorithm"]},
    {"article name": "One-layer gradient-based MPC\u00a0+\u00a0RTO of a propylene/propane splitter",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.06.006",
     "publication date": "11-2017",
     "abstract": "Here, the implementation of the gradient-based Economic MPC (Model Predictive Control) in an industrial distillation system is studied. The approach is an alternative to overcome the conflict between the MPC and RTO (Real Time Optimization) layers in the conventional control structure. The study is based on the rigorous dynamic simulation software (SimSciDynsim\u00ae) that reproduces the real system very closely and is able to communicate with Matlab. The gradient of the economic function, is obtained through the sensitivity tool of the real-time optimization package (SimSciROMeo\u00ae). In order to study the pros and cons of the new strategy, a propylene distillation system is simulated with both, the proposed approach (one-layer MPC\u00a0+\u00a0RTO) and the conventional two-layer hierarchical structure of control and optimization. The results show that, for this particular system, from the performance, stability and disturbance rejection viewpoint, the proposed gradient-based extended control method is equivalent or better than the conventional approach.",
     "keywords": ["Economic model predictive control", "Real time optimization", "Dynamic simulation", "Propylene production unit"]},
    {"article name": "An adaptive sampling approach for Kriging metamodeling by maximizing expected prediction error",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.05.025",
     "publication date": "11-2017",
     "abstract": "As a well-known approximation method, Kriging is widely used in process engineering design and optimization for saving computational budget. The Kriging model for a target function is fitted to a set of sample points, the responses of which are expensive to obtain in practice and the sample distribution of which has a great impact on the model prediction quality. Therefore, a main task in adaptive sampling for Kriging metamodeling is to gather informative points in order to build an accurate model with as few points as possible. To this end, we propose an adaptive sampling approach under the bias-variance decomposition framework. This novel sampling approach sequentially selects new points by maximizing an expected prediction error criterion that considers both the bias and variance information. Particularly, it presents an adaptive balance strategy to dynamically balance the local exploitation and global exploration via the error information from the previous iteration. Four benchmark cases and four engineering cases from low to high dimensions are used to assess the performance of the proposed approach. Numerical results reveal that this adaptive sampling approach is very promising for constructing accurate Kriging models for problems with diverse characteristics.",
     "keywords": ["Adaptive sampling", "Kriging metamodeling", "Expected prediction error", "Adaptive balance strategy"]},
    {"article name": "Subspace-based model identification of a hydrogen plant startup dynamics",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.05.020",
     "publication date": "11-2017",
     "abstract": "This work addresses the problem of determining a data-driven model for the startup of a hydrogen production unit, and demonstrates the approach both on a detailed first principles simulation model and by application to real data. To this end, first a detailed first principles model of the hydrogen plant is developed in Honeywell's UniSim design by adapting the plant standard operating procedure (SOP). Illustrative simulations are next presented to establish the meaningfulness of approximating process nonlinearity with a (higher order) linear time invariant (LTI) model. Then an LTI data-driven model of the hydrogen unit startup process using subspace identification based methods is identified. The framework is then implemented and successfully validated data on simulated data and on data from an industrial hydrogen unit.",
     "keywords": ["Hydrogen plant", "Steam-methane reforming", "UniSim design", "Subspace identification", "Batch processes", "Startup"]},
    {"article name": "Quantifying situation awareness of control room operators using eye-gaze behavior",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.06.004",
     "publication date": "11-2017",
     "abstract": "In an attempt to improve process safety, today\u2019s plants deploy sophisticated automation and control strategies. Despite these, accidents continue to occur. Statistics indicate that human error is the predominant contributor to accidents today. Traditionally, human error is only considered during process hazard analysis. However, this discounts the role of operators in abnormal situation management. Recently, with the goal to develop proactive strategies to prevent human error, we utilized eye tracking to understand the situation awareness of control room operators. Our previous studies reveal the existence of specific eye gaze patterns that reveal operators\u2019 cognitive processes. This paper further develops this cognitive engineering based approach and proposes novel quantitative measures of operators\u2019 situation awareness. The proposed measures are based on eye gaze dynamics and have been evaluated using experimental studies. Results demonstrate that the proposed measures reliably identify the situation awareness of the participants during various phases of abnormal situation management.",
     "keywords": ["Control room", "Process safety", "Monitoring", "Human error", "Cognitive engineering"]},
    {"article name": "Characterization of influent wastewater with periodic variation and snow melting effect in cold climate area",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.06.009",
     "publication date": "11-2017",
     "abstract": "The daily, weekly and seasonal variation of influent characteristics of wastewater treatment plants (WWTPs) highly affects the performance of wastewater treatment. In cold climate area, snow melting happens frequently in cold season and affects wastewater characteristics significantly. The dilution effect of snow melting in cold season makes it impossible to compare cold season influent and warm season influent fairly. To enable the study of influent seasonal variation, a stepwise approach was developed to determine whether the WWTP influent wastewater contains snowmelt (wet climate) or not (dry climate). This study investigated the daily, weekly and seasonal variation of WWTP influent, and provided evidence of climate effect on influent characteristics by analyzing the correlation of climatic information and wastewater characteristics. A classification model was developed to further discriminate climate conditions of influent, which will be applied to develop scenario-based soft sensor as well as support WWTP surveillance and control.",
     "keywords": ["Climate", "Cluster analysis", "Discriminant analysis", "Principal component analysis", "Snow melting", "Wastewater characteristics"]},
    {"article name": "An analytical method for diseases prediction using machine learning techniques",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.06.011",
     "publication date": "11-2017",
     "abstract": "The use of medical datasets has attracted the attention of researchers worldwide. Data mining techniques have been widely used in developing decision support systems for diseases prediction through a set of medical datasets. In this paper, we propose a new knowledge-based system for diseases prediction using clustering, noise removal, and prediction techniques. We use Classification and Regression Trees (CART) to generate the fuzzy rules to be used in the knowledge-based system. We test our proposed method on several public medical datasets. Results on Pima Indian Diabetes, Mesothelioma, WDBC, StatLog, Cleveland and Parkinson\u2019s telemonitoring datasets show that proposed method remarkably improves the diseases prediction accuracy. The results showed that the combination of fuzzy rule-based, CART with noise removal and clustering techniques can be effective in diseases prediction from real-world medical datasets. The knowledge-based system can assist medical practitioners in the healthcare practice as a clinical analytical method.",
     "keywords": ["Machine learning", "Diseases classification", "Fuzzy logic", "Analytical method"]},
    {"article name": "Process plant layout optimization with uncertainty and considering risk",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.05.022",
     "publication date": "11-2017",
     "abstract": "Facility layout is an important factor in designing of process plants such as petrochemical units and refineries. An appropriate design of layout ensures proper performance of corresponding departments as well as providing an equilibrium between large number and sometimes inconsistent structural design constraints. In this research, a mixed integer non-linear programming (MINLP) formulation is proposed for process plant layout problem, considering the toxic release risk and possible scenarios of fire, explosion and the domino effects of them. Then the Bat metaheuristic algorithm was employed to solve it. Finally, the model was implemented on 6th refinery, unit 101 of Iran's South Pars gas field, and the results showed the effectiveness of the model, in costs reduction and safety improvement.",
     "keywords": ["Process plant layout", "Toxic release", "Fire and explosion scenarios", "Domino effect", "Risk assessment"]},
    {"article name": "Optimal retrofit of heat exchanger networks: A stepwise approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.06.008",
     "publication date": "11-2017",
     "abstract": "This paper introduces a new stepwise approach for the optimal retrofit of heat exchanger networks. At each iteration, the proposed method involves superstructures which embed retrofit alternatives as well as numerical optimization to minimize the project\u2019s total annualized cost. In order to reduce calculation times, new algorithms are proposed to thin out superstructures. These are based on thermodynamic criteria, namely on the concept of heat path, and on new graph theoretical results. To carry out the numerical optimization tasks, novel mixed integer nonlinear models are developed. These models support any combination of constant, polynomial and (continuous) piecewise linear enthalpy functions of temperature. Numerical examples are presented for the retrofit of the preheat train of an atmospheric crude unit and for the retrofit of a fluid catalytic cracking plant. The new method is shown to reduce calculation times and total annualized costs reported in the literature.",
     "keywords": ["Heat exchanger networks", "Optimization", "MINLP", "Retrofit", "Heat path"]},
    {"article name": "Heat integration of multipurpose batch plants through multiple heat storage vessels",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.06.007",
     "publication date": "11-2017",
     "abstract": "Energy minimisation in batch plants has garnered popularity over the past few decades, leading to direct and indirect heat integration techniques being formulated for multipurpose batch plants through the utilisation of mathematical formulations and insight-based methods Some mathematical formulations utilise predetermined scheduling frameworks which may result in suboptimal results, whilst other formulations only use one heat storage vessel which may cause limitations in the plant. The work presented in this manuscript is aimed at minimising energy consumption in multipurpose batch plants by exploring both direct and indirect heat integration through multiple heat storage vessels. It investigates the optimal number of heat storage vessels as well as design parameters, i.e. size and initial temperature of vessels. The cost of the heat storage vessels is considered within the model. The model is applied to two case studies resulting in significant increase in profits.",
     "keywords": ["Batch plants", "Heat integration", "Energy", "Minimisation", "Heat storage", "Optimisation"]},
    {"article name": "Steady-state optimization of biochemical systems by bi-level programming",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.06.019",
     "publication date": "11-2017",
     "abstract": "A new method is proposed for the steady-state optimization of biochemical systems described by Generalized Mass Action (GMA) models. In this method, a bi-level programming with a two-layer nested structure is established. In this bi-level problem, the upper-level objective is to maximize a flux or a metabolite concentration, and the lower-level objective is to minimize the total sum of metabolite concentrations of biochemical systems. The biological significance of the presented bi-level programming problem is to maximize the production rate or concentration of the desired product under a minimal metabolic cost to the biochemical system. To efficiently solve the above NP-hard, non-convex and nonlinear bi-level programming problem, we reformulate it into a single-level optimization problem by using appropriate transformation strategies. The proposed framework is applied to four case studies and has shown the tractability and effectiveness of the method. A comparison of our proposed method and other methods is also given.",
     "keywords": ["Optimization", "Bi-level programming", "Algorithm", "Generalized mass action", "Biochemical systems"]},
    {"article name": "SPREAD \u2013 Exploring the decision space in energy systems synthesis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.06.002",
     "publication date": "11-2017",
     "abstract": "A method is presented to systematically analyze the decision space in the synthesis of energy supply systems. Commonly, synthesis problems are solved by mathematical optimization yielding a single optimal design. However, optimization is based on a model which never represents reality to perfection. Thus, the designer will be forced to revise parts of the optimal solution. We therefore support the design process by automatically identifying important features of good solutions. For this purpose, we analyze near-optimal solutions. To explore the decision space, we minimize and maximize both the number and the capacity of units while keeping the costs within a specified range. From this analysis, we derive insight into correlations between decisions. To support the decision maker, we represent the range of good design decisions and their correlations in the flowsheet of the energy system. The method is illustrated for the synthesis of an energy system in the pharmaceutical industry.",
     "keywords": ["Decision support", "Synthesis of energy supply systems", "Integer cuts", "MILP", "Near-optimal solutions"]},
    {"article name": "A piecewise McCormick relaxation-based strategy for scheduling operations in a crude oil terminal",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.06.012",
     "publication date": "11-2017",
     "abstract": "The petroleum supply chain can be divided into three segments: upstream, midstream and downstream. This work studies the scheduling of operations in a crude oil terminal within the midstream segment. The first challenge consists in deciding how the crude oil that arrives in vessels should be uploaded to the storage tanks. At the same time, the operations engineer must decide which storage tanks will feed the pipeline connected to the refinery in order to satisfy its demand. This work concerns the crude oil terminal of the national refinery of Uruguay. To schedule terminal operations, this work proposes an iterative two-step MILP-NLP algorithm based on piecewise McCormick relaxation and a domain-reduction strategy for handling bilinear terms. For small instances for which an optimal solution is known, the proposed strategy consistently finds optimal or near-optimal solutions. It also solves larger instances which are, in some cases, intractable by a global optimization solver.",
     "keywords": ["Bilinear terms", "MINLP", "Piecewise McCormick envelopes", "Blending", "Crude oil scheduling"]},
    {"article name": "A novel factorial design search to determine realizable constant sets for a multi-mechanism model of mixing sensitive precipitation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.06.014",
     "publication date": "11-2017",
     "abstract": "A mechanistic model for the reactive precipitation of amine mono- and di-hydrochloride (AHC) salts was developed in the first stage of this work (Komrakova and Maluta, 2016). In a companion paper, an extensive experimental investigation of the reaction products is reported (Ershad, 2013). The experimental work shows a sudden shift from mono- to di- production as the blend strength of 4, 4\u2032-methylene dianiline (MDA) decreases, and a strong dependence of di-concentration on excess HCl in the feed for the high blend strength experiments. Due to the complexity of the reaction-precipitation system, eleven constants related to physical properties and rates were identified in the model. An extensive search of the literature returned only realistic ranges for most of these constants. In this work, a novel method for reducing the uncertainty in the space defined by these eleven constants is presented. The trends in the experimental data were successfully reproduced and the model results provide a number of important insights into the two complex reaction outcomes. A factorial design search for similar loosely bounded parameter spaces is proposed.",
     "keywords": ["Amine hydrochloride salts", "Reactive precipitation", "Full factorial design", "Mechanistic model", "Mixing", "Optimization"]},
    {"article name": "Real-time management of the waterflooding process using proxy reservoir modeling and data fusion theory",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.06.018",
     "publication date": "11-2017",
     "abstract": "Waterflooding is the use of water injection to enhance the oil recovery in mature oil reservoirs. In this paper an adaptive algorithm has been introduced for waterflooding management in oil reservoirs based on proxy modeling technique. The presented approach is capable to handle the time-varying nature and the inherent nonlinearity of the complex process. In addition, any variation either in market prices or in operational costs is compensated by the designed adaptive controller to fix the obtained profit (here, the net present value: npv) at a desired achievable value. The observed outcomes on 10th SPE-Model#2 benchmark case study have shown that by using this algorithm, any feasible desired trajectory for the expected benefit can be satisfied during the waterflooding-based production. Since the suggested controller has adaptive structure, it can be re-adjusted continuously in each time-step, using available operational data, to take into account the reservoir dynamical variations as well as the external disturbances to present an acceptable performance. By including a monitoring module in the algorithm structure based on data fusion technique, the updated profitability/productivity status of the reservoir is estimated. By using this information the npv setpoint induced to the closed-loop system can be automatically re-adjusted such that it always remains in an acceptable and reasonable range. In conclusion, the proposed methodology is an applicable solution for fairly profit-sharing in different kinds of contracts. In other words, the gained profit can be appropriately allocated to the shareholders according to the contractual obligations or a defined npv trajectory while considering the current condition of the reservoir. This strategy helps to prevent from ultra-production in a specific period of time by the clients or contractors which may lead to an unexpected reduction in the share of other parties in the reservoir life-cycle.",
     "keywords": ["Waterflooding process", "Adaptive control", "Proxy reservoir modeling", "Data fusion", "Self-optimizing-control", "Reservoir management"]},
    {"article name": "Raising quality and safety of platelet transfusion services in a patient-based integrated supply chain under uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.06.015",
     "publication date": "11-2017",
     "abstract": "This paper develops a stochastic multi-period mixed-integer model for collection, production, storage, and distribution of platelet in Blood Transfusion Organizations ranging from blood collection centers to clinical points. In this model, the age of platelet and ABO-Rh priority matching rules are incorporated based on the type of patient to raise the quality and safety of platelet transfusion services. At first, a discrete Markov Chain Process is applied to predict the number of donors. Afterwards, the uncertain demand is captured using a two-stage stochastic programming. A challenging aspect of applying stochastic programming in a dynamic setting is to construct an appropriate set of discrete scenarios. Therefore, we introduce an improved approach for scenario reduction which well represents multivariate stochastic processes for uncertain parameters. To manage risk, a straightforward approach to reduce the expected value and variance of cost is proposed. Finally, management strategies inspired from a real case study are presented.",
     "keywords": ["Blood platelet supply chain", "ABO-Rh priority matching rules", "Donor prediction", "Tow-stage stochastic programming", "Scenario reduction"]},
    {"article name": "A decomposition framework for managing inventory and distribution of fluid products by an order-based-resupply methodology",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.06.022",
     "publication date": "11-2017",
     "abstract": "Fluid chemicals are usually distributed according two main methodologies; the vendor managed inventory modality and the order-based re-supply modality. In this paper, the problem of optimizing the delivery of fluids by tanker trucks on a daily basis according the order-basedresupply methodology is addressed. According to this modality, replenishment orders are triggered by customers specifying the quantity of fluids and time-windows within which the delivery must be fulfilled. The objective is to minimize the replenishment cost while meeting customers orders over the pre-defined time-horizon. An Integer Program modelling the problem is proposed and used to develop an incomplete branch & price procedure with the purpose of finding near optimal solutions to instances arising from realistic examples. A computational study on realistic examples with different topologies demonstrates that the method is effective and able to provide solutions with integrality gaps below the 16% threshold for instances with 120 orders.",
     "keywords": ["Distribution of fluids", "Order-based-resupply", "Incomplete branch-and-price", "Inventory constraints"]},
    {"article name": "A framework for developing a structure-based lumping kinetic model for the design and simulation of refinery reactors",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.06.025",
     "publication date": "11-2017",
     "abstract": "This study represents a systematic framework supporting the development of a structure-based lumping kinetic model useful for designing, simulating, and optimizing refinery reactors. Utilizing the wealth of the available data and information provided by petroleomics, the framework comprises strategies for analyzing and reducing a large and complicated elementary kinetic reaction model as well as generating lumps in light of the structure and reactivity of the species included in the feedstock and product mixtures. A structure-based model developed by applying the framework cannot only predict yield but can also provide the structure-based composition of the product mixture. A case study on the hydrodesulfurization of light gas oil (LGO-HDS) was used to illustrate the applicability of the framework. The developed LGO-HDS structure-based kinetic model can describe reaction behavior and predict product distribution with high accuracy.",
     "keywords": ["Systematic framework", "Structure-based lumping kinetic model", "Product yield", "Product molecule-based information"]},
    {"article name": "Internal rate of return: Good and bad features, and a new way of interpreting the historic measure",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.06.005",
     "publication date": "11-2017",
     "abstract": "IRR, a widely used profitability measure, is the Discount Rate that yields Net Present Value (NPV)\u00a0=\u00a00 for a stream of positive and negative cash flows, at least one of each sign and with no explicit financing payments. A big disadvantage is lack of parameters, such as a project finance rate or the enterprise rate (ER), i.e., Return on Investment of the overarching investment group to serve as a measure of opportunity cost. The coupled metrics proposed earlier by the author\u2014 NPVproject and NPV%\u2013do not suffer these disadvantages, so IRR is analyzed in terms of NPV%. Useful information can be obtained from a projection of IRR values onto the NPV%, ER plane revealing the sensitivity of IRR to risk under meaningful operating conditions.",
     "keywords": ["Internal rate of return", "Hurdle rate", "Profitability measure", "Enterprise rate of return", "Modified internal rate of return", "Business risk"]},
    {"article name": "Thermodynamic analysis of formulations to discriminate multiple roots of cubic equations of state in process models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.06.023",
     "publication date": "11-2017",
     "abstract": "Several discrimination criteria have been proposed to select the correct root in the presence of multiple real roots to a cubic equation of state. Herein, a criterion which exploits the well-known method of Cardan to solve cubic equations is presented. The criterion is straightforward, continuous and covers different root scenarios. As a reference, minimization of Gibbs free energy is formulated for const. (P, T). From its KKT conditions, it follows that, in case a phase vanishes, relaxation of cubic equality is an alternative to relaxation of root discrimination as proposed by Kamath et al. (2010). The proposed criterion and those from literature satisfy at most a necessary condition, while minimization of Gibbs free energy obviously is a necessary and sufficient condition for thermodynamic stability of the selected root. In certain cases, formulations from literature are found to exclude valid solutions, whereas the proposed criterion overcomes this deficiency.",
     "keywords": ["CEOS cubic equation of state", "cubic equation of state", "CQ constraint qualification", "constraint qualification", "EC equality constraint", "equality constraint", "IC inequality constraint", "inequality constraint", "KKT Karush\u2013Kuhn\u2013Tucker", "Karush\u2013Kuhn\u2013Tucker", "LBD lower bound", "lower bound", "LICQ linear independence constraint qualification", "linear independence constraint qualification", "MFCQ Mangasarian Fromovitz constraint qualification", "Mangasarian Fromovitz constraint qualification", "MPCC Mathematical Program with Complementarity Constraints", "Mathematical Program with Complementarity Constraints", "n. appl. not applicable", "not applicable", "optca absolute optimality criterion", "absolute optimality criterion", "optcr relative optimality criterion", "relative optimality criterion", "PR Peng\u2013Robinson", "Peng\u2013Robinson", "SRK Soave\u2013Redlich\u2013Kwong", "Soave\u2013Redlich\u2013Kwong", "UBD upper bound", "upper bound", "VLE vapor\u2013liquid equilibrium", "vapor\u2013liquid equilibrium", "Cubic equation of state", "Root discrimination", "Thermodynamic stability", "Helmholtz energy", "Gibbs free energy", "Tangent plane"]},
    {"article name": "Rapid impedance measurement using chirp signals for electrochemical system analysis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.05.018",
     "publication date": "11-2017",
     "abstract": "Energy storage (batteries) and conversion devices (fuel cells) operate based on electrochemical principles. Electrochemical impedance spectroscopy (EIS) is an important experimental technique that can be used to optimize the performance of these devices at the design stage. Further, EIS can also be used for non-invasive, in-situ diagnostics of device performance, while operational. While EIS is a powerful technique, the instrumentation required for implementation can be bulky and the time for analysis could also become unacceptable, particularly for online applications. The proposed work is an investigation and development of a rapid impedance measurement technology using large bandwidth and short duration diagnostic signals. The key concept behind the approach is the use of a particular definition of instantaneous frequency in tandem with chirp signals for testing. The instantaneous frequency definition allows one-to-one time frequency mapping and the chirp signal incorporates a wide bandwidth of frequencies. As a result, a novel impedance plot qualitatively, and largely quantitatively, matching the corresponding EIS plot is generated. Consequently, any diagnostic approach based on EIS can potentially be realized using the proposed approach. The impedance plots are generated in a very short time making the approach amenable for online applications. In-silico validation of the proposed method on few equivalent circuits of electrochemical systems is presented in this work; future work will include experimental validation of the technique on real electrochemical systems.",
     "keywords": ["Impedance", "Chirp analysis", "Batteries", "Diagnosis"]},
    {"article name": "New computational tool to evaluate experimental VLE and VLLE data of multicomponent systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.07.003",
     "publication date": "11-2017",
     "abstract": "This work presents a rigorous method to analyze the thermodynamic consistency of VLE and VLLE data of multicomponent systems, as an extension of a method previously proposed for binary solutions. The method or proposed test verifies the coherence between the Gibbs-Duhem equation and experimental data, using the same number of equations as degrees of freedom of the system. Resolution is achieved by means of two methods called the integral-form and differential-form, and for each of these, characteristic parameters that qualify the quality of the data are generated. The integration of the equation above mentioned between data pairs generates the residuals \u03b4\u03c8 and constitutes the integral-form. This form verifies the consistency when the value of these residuals is lower than the maximum value, calculated as \u03b5 \u03c8 M = \u03ba \u03c8 \u03b5 \u03c8 M,0 , where \u03b5 \u03c8 M,0 is the error associated with \u03c8 at each point and \u03ba \u03c8 =\u00a05; it should occurs that \u03b5 \u03c8 M \u2264\u00a04. In the application of the differential-form each partial derivative of \u03c8 is verified and can be used to verify the coherence between the compositions of each component in all the phases by the residual \u03b4\u03b6i. The maximum values of these residuals are established by \u03b5 \u03b6 i M = \u03ba \u03b6 i \u03b5 \u03b6 i M,0 , where \u03b5 \u03b6 i M,0 is the maximum permissible error and \u03ba \u03b6 i = 5 , it should occurs that \u03b5 \u03b6 i M \u2264 | 0.1 [ max ( \u03b6 i ) -min ( \u03b6 i ) ] | . The limits of the parameters for the proposed test are established after applying the method to several systems generated artificially. The test was applied to a set of real systems, 50 ternaries and 2 quaternaries, verifying the degree of consistency/inconsistency according to the parameters defined. The behavior of the test is compared with that of Wisniak-Tamir in multicomponent systems. In summary, the proposed test is shown to be a useful tool to assess the quality of VLE and VLLE data of multicomponent systems.",
     "keywords": ["Consistency-test", "VLE", "VLLE", "Modeling", "Multicomponent", "Programming"]},
    {"article name": "Data-driven robust optimization based on kernel learning",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.07.004",
     "publication date": "11-2017",
     "abstract": "We propose piecewise linear kernel-based support vector clustering (SVC) as a new approach tailored to data-driven robust optimization. By solving a quadratic program, the distributional geometry of massive uncertain data can be effectively captured as a compact convex uncertainty set, which considerably reduces conservatism of robust optimization problems. The induced robust counterpart problem retains the same type as the deterministic problem, which provides significant computational benefits. In addition, by exploiting statistical properties of SVC, the fraction of data coverage of the data-driven uncertainty set can be easily selected by adjusting only one parameter, which furnishes an interpretable and pragmatic way to control conservatism and exclude outliers. Numerical studies and an industrial application of process network planning demonstrate that, the proposed data-driven approach can effectively utilize useful information with massive data, and better hedge against uncertainties and yield less conservative solutions.",
     "keywords": ["Robust optimization", "Uncertainty set", "Data-driven methods", "Support vector clustering", "Piecewise linear modeling"]},
    {"article name": "Modifier Adaptation methodology based on transient and static measurements for RTO to cope with structural uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.07.001",
     "publication date": "11-2017",
     "abstract": "Optimal process operation is carried out by a Real-Time Optimization (RTO) layer which is not always able to achieve its targets due to the presence of plant-model mismatch. To overcome this issue, the economic optimization problem solved in the RTO is changed following the Modifier Adaptation methodology (MA), which uses plant measurements to find a point that satisfies the necessary optimality conditions (NCO) of an uncertain process. MA proceeds by iteratively adjusting the optimization problem with first and zeroth order corrections, calculated from steady-state information at each RTO execution. This implies a long convergence time. This paper presents a new method based on a recursive identification algorithm to estimate process gradients from transient measurements to speed up the convergence of MA. The proposed approach is implemented in a simulated depropanizer column that incorporates a simplified model in the RTO, reducing by 8 the convergence time compared with traditional MA.",
     "keywords": ["Real-time optimization", "Modifier Adaptation", "Uncertainty", "Distillation columns", "MPC"]},
    {"article name": "Development of local dynamic mode decomposition with control: Application to model predictive control of hydraulic fracturing",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.07.002",
     "publication date": "11-2017",
     "abstract": "Dynamic mode decomposition with control (DMDc) is a modal decomposition method that extracts dynamically relevant spatial structures disambiguating between the underlying dynamics and the effects of actuation. In this work, we extend the concepts of DMDc to better capture the local dynamics associated with highly nonlinear processes and develop temporally local reduced-order models that accurately describe the fully-resolved data. In this context, we first partition the data into clusters using a Mixed Integer Nonlinear Programming based optimization algorithm, the Global Optimum Search, which incorporates an added feature of predicting the optimal number of clusters. Next, we compute the reduced-order models tailored to each cluster by applying DMDc within each cluster. The developed models are subsequently used to compute approximate solutions to the original high-dimensional system and to design a feedback control system of hydraulic fracturing processes for the computation of optimal pumping schedules.",
     "keywords": ["Distributed parameter systems", "Reduced-order model", "Temporal clustering", "Model predictive control", "Hydraulic fracturing", "Dynamic mode decomposition"]},
    {"article name": "Enhancing the value of detailed kinetic models through the development of interrogative software applications",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.07.009",
     "publication date": "11-2017",
     "abstract": "A suite of user-friendly software tools was developed to help increase the accessibility of detailed kinetic models to a wide range of users from modelers to research collaborators to process engineers. The aims of these users were the basis for the software development. The tools are illustrated for development, analysis and usage of a detailed catalytic naphtha reforming model. After initial model construction, the Reaction Network Visualizer and KME (Kinetic Model Editor) Results Analyzer helped in understanding the characteristics of the reaction pathways, molecular profile and also assisted in tuning the kinetic parameters. The I/O (Input/Output) Converter permits execution of a molecular-level model in a manner which focuses on only measurable inputs and outputs. The KME Reactor Flowsheet tool increased the core kinetics model capabilities by allowing the model building to be based on data from a pilot or commercial reactor, including schemes with split feed streams and reactor bypasses.",
     "keywords": ["Software applications", "Detailed kinetic model", "Catalytic reforming", "Naphtha"]},
    {"article name": "Comment on solution of differential-algebraic equations through gradient flow embedding",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.07.008",
     "publication date": "11-2017",
     "abstract": "The paper \u201cOn the solution of differential-algebraic equations through gradient flow embedding,\u201d Computers and Chemical Engineering, 103 (2017), 165\u2013173, presents an algorithm that is designed to integrate some index one DAEs. This letter reviews index one DAEs, and clarifies how the assumptions of the algorithm fit within a more general framework. The types of index one DAEs for which the new algorithm are not appropriate are discussed.",
     "keywords": ["Differential-algebraic equations", "Gradient flows", "Index", "Ordinary differential equations"]},
    {"article name": "Graph representation and decomposition of ODE/hyperbolic PDE systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.07.005",
     "publication date": "11-2017",
     "abstract": "This paper deals with the decomposition of process networks consisting of distributed parameter systems modeled by first-order hyperbolic partial differential equations (PDEs) and lumped parameter systems modeled by ordinary differential equations (ODEs) into compact, weakly interacting subsystems. A structural interaction parameter (SIP) generalizing the concept of relative degree in ODE systems to first-order hyperbolic PDE systems is defined. An equation graph representation of these systems is developed for efficient calculation of SIPs. An agglomerative (bottom-up) hierarchical clustering algorithm and a divisive (top-down) algorithm are used to obtain hierarchical decompositions based on the SIPs. Modularity maximization is used to select the optimal decomposition. A network of two absorbers and two desorbers serves as a case study. The optimal decompositions of this network obtained from both the algorithms illustrate the effectiveness of the graph-based procedure in capturing key structural connectivity properties of the process network.",
     "keywords": ["Distributed control", "Distributed parameter system", "Graph theory", "Process network decomposition"]},
    {"article name": "A novel algorithm for fast representation of a Pareto front with adaptive resolution: Application to multi-objective optimization of a chemical reactor",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.06.020",
     "publication date": "11-2017",
     "abstract": "Solving a multi-objective optimization problem yields an infinite set of points in which no objective can be improved without worsening at least another objective. This set is called the Pareto front. A Pareto front with adaptive resolution is a representation where the number of points at any segment of the Pareto front is directly proportional to the curvature of this segment. Such representations are attractive since steep segments, i.e., knees, are more significant to the decision maker as they have high trade-off level compared to the more flat segments of the solution curve. A simple way to obtain such representation is the a posteriori analysis of a dense Pareto front by a smart filter to keep only the points with significant trade-offs among them. However, this method suffers from the production of a large overhead of insignificant points as well as the absence of a clear criterion for determining the required density of the initial dense representation of the Pareto front. This paper's contribution is a novel algorithm for obtaining a Pareto front with adaptive resolution. The algorithm overcomes the pitfalls of the smart filter strategy by obtaining the Pareto points recursively while calculating the trade-off level between the obtained points before moving to a deeper recursive call. By using this approach, once a segment of trade-offs insignificant to the decision maker's needs is identified, the algorithm stops exploring it further. The improved speed of the proposed algorithm along with its intuitively simple solution process make it a more attractive route to solve multi-objective optimization problems in a way that better suits the decision maker's needs.",
     "keywords": ["Multi-objective optimization", "Pareto front representation", "Divide and conquer strategy", "Dynamic optimization"]},
    {"article name": "Design and control of entrainer-assisted reactive distillation for N-propyl propionate production",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.08.003",
     "publication date": "11-2017",
     "abstract": "An entrainer-assisted reactive distillation process is proposed to produce high-purity N-propyl propionate from propionic acid and N-propanol. The E-RD process can take advantages of both the heterogeneous azeotropic distillation (HAD) and reactive distillation (RD). Cyclohexane is selected as the proper entrainer in the E-RD process. And the E-RD process is optimized by calculating the minimum total annual cost (TAC). The optimal results reveal that the E-RD process can save 46.11% of TAC and 41.40% of reboiler duty compared with the two-column process. Furthermore, two control structures for the E-RD process are considered. The dynamic performances demonstrate that the improved control structure (CS2) can solve the problem of disturbances and maintain the product purities close to the set points with small deviations and short settling times.",
     "keywords": ["Esterification", "Propyl propionate", "Entrainer-assisted reactive distillation", "Temperature control"]},
    {"article name": "On anti-aliasing filtering and over-sampling scheme in system identification",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.07.010",
     "publication date": "11-2017",
     "abstract": "In this work, two fundamental issues in system identification, sampling frequency and anti-aliasing filtering are revisited, questions like, \u201cCan higher sampling frequency help increase model accuracy?\u201d, and, \u201cCan we do better than anti-aliasing filtering?\u201d will be answered. First it is shown that, the traditional anti-aliasing filtering procedure does not give a consistent estimate of the desired model. Then an identification method is proposed based on the over-sampling scheme where a high frequency model is first identified and then converted to the (lower) control sampling frequency. It is shown that when the output noise contains energy beyond the bandwidth of the plant, the proposed method performs anti-aliasing in both open-loop and closed-loop identification; and in closed-loop identification, extra excitation is achieved from the high frequency noise. Simulation studies are used to illustrate the findings. The result has important implications in control applications.",
     "keywords": ["System identification", "Sampling frequency", "Anti-aliasing filtering", "Variance", "Over-sampling scheme"]},
    {"article name": "Dynamic optimization of beer fermentation: Sensitivity analysis of attainable performance vs. product flavour constraints",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.06.024",
     "publication date": "11-2017",
     "abstract": "The declining alcohol industry in the UK and the concurrent surge in supply and variety of beer products has created extremely competitive environment for breweries, many of which are pursuing the benefits of process intensification and optimization. To gain insight into the brewing process, an investigation into the influence of by-product threshold levels on obtainable fermentation performance has been performed, by computing optimal operating temperature profiles for a range of constraint levels on by-product concentrations in the final product. The DynOpt software package has been used, converting the continuous control vector optimization problem into nonlinear programming (NLP) form via collocation on finite elements, which has then been solved with an interior point algorithm. This has been performed for increasing levels of time discretization, by means of a range of initializing solution profiles, for a wide spectrum of imposed by-product flavour constraints. Each by-product flavour threshold affects process performance in a unique way. Results indicate that the maximum allowable diacetyl concentration in the final product has very strong influence on batch duration, with lower limits requiring considerably longer batches. The maximum allowable ethyl acetate concentration is shown to dictate the attainable ethanol concentration, and lower limits adversely affect the desired high alcohol content in the final product.",
     "keywords": ["Beer fermentation", "Dynamic optimization", "Multi-objective optimization", "Orthogonal collocation on finite elements", "Sensitivity analysis", "Flavour constraints"]},
    {"article name": "Rapid and accurate reachability analysis for nonlinear dynamic systems by exploiting model redundancy",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.08.001",
     "publication date": "11-2017",
     "abstract": "A new method is presented for enclosing the reachable sets of nonlinear ordinary differential equations subject to a range of inputs. Reachable set enclosures are used for uncertainty propagation, robust control, and global optimization of dynamic systems arising in a variety of applications. However, existing methods often provide an unworkable compromise between cost and accuracy. For example, fast interval methods often produce divergent bounds, while methods based on more complex sets scale poorly with problem size. To overcome this, a novel method is introduced for reducing the conservatism of fast interval methods through the select addition of redundant model equations that can be exploited in the bounding procedure. Several case studies demonstrate that such redundancy can dramatically reduce conservatism. The additional cost is modest in most cases, but does become significant when many redundant equations are used.",
     "keywords": ["Reachability", "Interval analysis", "Differential inequalities", "Global optimization", "Uncertainty propagation"]},
    {"article name": "Probabilistic uncertainty based simultaneous process design and control with iterative expected improvement model",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.07.011",
     "publication date": "11-2017",
     "abstract": "The simultaneous design and control aims to achieve economic profits and smooth operation of the process even under uncertainties. However, the over-estimation of the uncertainties leads to conservative design decisions. Because of the disturbance inputs, the cost is not easily evaluated. Unlike the past work of design and control, the proposed probabilistic approach framework directly uses the Gaussian process (GP) model to represent the uncertainty in the input. The GP model that acts as the cost function model is trained by an iterative approach. The variability can be evaluated statistically by the GP model. In addition, the expected improvement optimization is employed to select the representative data, so no redundant data are used in the modeling. The expected improvement searches for the most probable operating condition for improvement based on the predictive distribution from the GP model. The applicability of the proposed method is tested on a mixing tank.",
     "keywords": ["Design and control", "Expected improvement", "Gaussian process", "Probabilistic modeling"]},
    {"article name": "Mixed-integer programming models for simultaneous batching and scheduling in multipurpose batch plants",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.07.007",
     "publication date": "11-2017",
     "abstract": "We propose two novel discrete-time mixed-integer programming models for simultaneous batching and scheduling in multipurpose batch plants with storage constraints. The proposed models adopt two different modeling approaches. The first is based on explicit labeling of batches, while the second is based on identifying possible batch size intervals for each order and the corresponding unit routings. We also present extensions that allow us to consider limited shared utilities (with both fixed and time-varying availability and cost), storage with capacity limits and stage-dependent batch sizes. Finally, we study how instance characteristics (e.g. expected number of batches per order, uniformity in unit capacities) impact the effectiveness of the proposed models. We show that by carefully selecting the model allows us to effectively solve large-scale instances.",
     "keywords": ["Sequential production environment", "Storage policies"]},
    {"article name": "A multiparametric CFD analysis of multiphase annular flows for oil and gas drilling applications",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.08.011",
     "publication date": "11-2017",
     "abstract": "The increasing global energy demand has pushed the oil industry towards developing more innovative and advanced methods of enhancing oil recovery even under unfavourable technical and environmental conditions. The severity of many operational problems affecting the drilling and production of oil and gas wells is worsened by inaccessibility; hence, remedial efforts must be implemented from afar. One of these problems is ensuring efficient removal of formation rock cuttings with a suitable fluid whose rheology is often complicated. Furthermore, pressure losses along the annular geometry involved, and decreased rates of penetration (ROP) due to accumulated drill cuttings downhole, constitute significant portions of the total energy to be supplied. Thus, the application of sophisticated modelling techniques with credible elucidation of the phase distributions (solids, liquids and gas) and prevalent flow regimes becomes essential, if adequate and economic design of a drilling program is desired. The advent of computational fluid dynamics (CFD) and the growth in the available computational power to support it have provided an unprecedented opportunity to simulate and understand complex real flows, especially when experimental methods become extremely demanding. The present study employs the tool of computational fluid dynamics to simulate a two-phase solid\u2013liquid flow in an annulus based on the analysis of cuttings concentration, pressure drop profiles, axial fluid, and solid velocities as a function of several drilling parameters: drill pipe eccentricity, inclination, drill pipe rotation, ROP and fluid rheology. Special emphasis is however placed on the impact of changing hole eccentricity on cuttings transport efficiency. The suitability of the Eulerian\u2013Eulerian (EE) multiphase tracking scheme in modelling systems of high volume fractions is fully utilised in this work. A non-Newtonian (power law) fluid model with well-described flow parameters is implemented, considering a uniform cuttings size distribution (3\u00a0mm). A commercial CFD software (ANSYS FLUENT\u2122 17.1) has been used; the descriptive and predictive potential of the CFD software has been confirmed on account of the reasonable agreement with previously published experimental data (a relative error of less than 11% is achieved), as illustrated by the corresponding sensitivity plots. This multi-parametric CFD analysis study of multiphase cutting transport during drilling applications has confirmed that fluid velocity, hole inclination and annular eccentricity are the most influential factors governing the cuttings transport efficiency.",
     "keywords": ["Oil and gas drilling engineering", "Multiphase flow", "Computational fluid dynamics (CFD)", "Cuttings concentration", "Pressure drop", "Cuttings velocity"]},
    {"article name": "MINLP model and two-stage algorithm for the simultaneous synthesis of heat exchanger networks, utility systems and heat recovery cycles",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.043",
     "publication date": "11-2017",
     "abstract": "This work proposes a novel approach for the simultaneous synthesis of Heat Exchanger Networks (HEN) and Utility Systems of chemical processes and energy systems. Given a set of hot and cold process streams and a set of available utility systems, the method determines the optimal selection, arrangement and design of utility systems and the heat exchanger network aiming to rigorously consider the trade-off between efficiency and capital costs. The mathematical formulation uses the SYNHEAT superstructure for the HEN, and ad hoc superstructures and nonlinear models to represent the utility systems. The challenging nonconvex MINLP is solved with a two-stage algorithm. A sequential synthesis algorithm is specifically developed to generate a good starting solution. The algorithm is tested on a literature test problem and two industrial problems, the optimization of the Heat Recovery Steam Cycle of a Natural Gas Combined Cycle and the heat recovery system of an Integrated Gasification Combined Cycle.",
     "keywords": ["HEN synthesis", "Heat integration", "Steam cycles", "Utility synthesis", "MINLP", "MILP"]},
    {"article name": "Hybrid genetic algorithm-decision tree approach for rate constant prediction using structures of reactants and solvent for Diels-Alder reaction",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.022",
     "publication date": "11-2017",
     "abstract": "In recent years, Computer-Aided Molecular Design (CAMD) has been extensively used for defining and designing reactions at their maximal potential. In all of these contributions, either the structures of reactants/products have been considered to be unchanging or the solvent structure. Developing a QSPR model which not only captures the influence of reactant structures but also the solvent effect on reaction rate, is essential. Since the structures of reactants and products are related, such QSPR models will serve as a prerequisite for the simultaneous CAMD of reactants, products and solvents. They will also provide a useful tool for predicting the rate constant without relying on experiments. To develop such a QSPR, in our work, the Diels-Alder reaction with different sets of reactants and solvents was investigated. Connectivity indices were used to represent the structures of the members of each set. Principal Component Analysis (PCA) was applied to identify principal components (PCs) corresponding to the structures of reactants and solvent of each set. Linear models expressed in terms of PCs were then generated using a Decision Tree (DT) algorithm such that the R2 value was maximized. These models formed the initial population on which the GA performed operations such as crossover and mutation to obtain model(s) with best rate constant prediction. Thus, the novelty of our approach is that after feature extraction using PCA, a DT algorithm generates an ensemble of linear models, which through the GA is transformed into a model with best fit. Our approach required much lesser generations to provide a model with highest R2ext value as compared to the case where the DT did not initialize the population of models.",
     "keywords": ["Structure-rate constant relationships", "QSPR", "Genetic algorithm-decision tree", "Reactants", "Solvents", "Hybrid algorithm", "Divide and conquer"]},
    {"article name": "Design and optimization of shale gas energy systems: Overview, research challenges, and future directions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.032",
     "publication date": "11-2017",
     "abstract": "The \u201cshale revolution\u201d has been a game changer in the global energy market and raised the importance of optimal design and operations of shale gas energy systems. This article highlights key challenges and identifies potential research opportunities in the corresponding area. A brief introduction of shale gas energy systems and their significant impacts are first presented, followed by a comprehensive overview and classification of relevant publications. Based on the literature review, we further investigate and discuss the research challenges of developing integrated, sustainable shale gas energy systems under the guidance of \u201ctriple bottom line\u201d and integrated approaches of material flow analysis and life cycle optimization. Leveraging the large amount of data from shale gas industry, data-driven optimization methods could open up new research opportunities for hedging against uncertainty in design and operations of shale gas energy systems. Moreover, potential opportunities are explored by introducing game theory into modeling and optimization of decentralized shale gas systems with multiple stakeholders. The benefits of enabling emerging technologies/operations are discussed as well.",
     "keywords": ["Shale gas", "Sustainable design and operations", "Optimization under uncertainty", "Game theory", "Multi-scale decisions"]},
    {"article name": "Integrated reactor staging and plant optimization of a Biomass-To-Liquid technology",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.03.028",
     "publication date": "11-2017",
     "abstract": "In this work an industrial Biomass-To-Liquid (BTL) plant simulation and optimization are presented. Biomass is first gasified with oxygen and steam, and the produced syngas is fed to a multi-tubular fixed bed reactor for Fischer-Tropsch (FT) synthesis, obtaining a distribution of hydrocarbons with different molecular weight. A simplified model for the biomass gasification section is implemented in HYSYS\u00ae V8.4, while the Fischer-Tropsch reactor is simulated using MATLAB\u00ae R2013a. The kinetic parameters of the FT reaction have been determined by using a non-liner regression performed with the experimenta data obtained with a bench-scale FT-rig. The model developed for the Fischer-Tropsch reactor takes into account the catalytic pellet's effectiveness factor and the eventual formation of a liquid phase in each point along reactor\u2019s axial coordinate. The whole BTL plant is simulated connecting MATLAB and HYSYS. Moreover, the staging of the Fischer-Tropsch reactor is studied performing a techno-economic analysis of three different plant configurations and evaluating the corresponding pay-back time.",
     "keywords": ["Gasification", "Biomass-To-Liquid", "Fischer-Tropsch synthesis", "Reactor staging", "Techno-economic analysis"]},
    {"article name": "Modelling and simulation of an industrial riser in fluid catalytic cracking process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.013",
     "publication date": "11-2017",
     "abstract": "Fluid Catalytic Cracking (FCC) unit is an important unit of modern refineries and any improvement in the unit\u2019s operations and design to increase yield and meet the ever increasing demand for fuel brings about the overall profitability of the FCC. In this work, simulation of an FCC riser of varied diameter was carried out to improve the unit\u2019s operations and design, and the results are compared with risers of different diameters. The riser with varied diameter produces 53.4\u00a0wt%, a 3.18% increased yield of gasoline at low catalyst to oil (C/O) ratio of 1.27 compared to 51.7\u00a0wt% from a 1\u00a0m diameter riser. At increased C/O ratio, more gases and coke are produced in the varied diameter riser. Larger diameter demands more catalyst but yields more gases. Process variables can be directly correlated with yield of gasoline, which can aid process design.",
     "keywords": ["FCC unit", "Riser", "Variable diameter", "Simulation", "Modelling"]},
    {"article name": "Simulation and optimization of polymer molecular weight distribution with nonideal reactors",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.03.017",
     "publication date": "11-2017",
     "abstract": "Molecular weight distribution (MWD) is essential for describing the microstructural quality of a polymer. However, most of the studies on MWD are limited to ideal reactors. Computational fluid dynamics (CFD) methods is a useful tool to deal with the nonideal reactors. Few studies on CFD have been extended to the simulation and optimization of polymer MWD due to the computational difficulties. In this study, a new strategy is proposed to simulate the spatial MWD for a type of nonideal reactors using the method of moments with interfacing to the CFD software. Subsequently, given a target MWD curve, process optimization is proposed to achieve the optimal operating conditions. The tubular and autoclave reactors of the low-density polyethylene process are demonstrated for the simulations and optimizations. The influences of the decision variables on the feasibility and objective function are also discussed.",
     "keywords": ["Molecular weight distribution", "Computational fluid dynamics", "Polymerization", "Process optimization", "Non-ideal reactors"]},
    {"article name": "Dynamic kriging based fault detection and diagnosis approach for nonlinear noisy dynamic processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.03.016",
     "publication date": "11-2017",
     "abstract": "This paper presents a hybrid approach to improve data-based Fault Detection and Diagnosis (FDD). It is applicable to nonlinear dynamic noisy processes, operated under time-varying inputs. The method is based on the combination of kriging models and Pattern Recognition Techniques. A set of Multivariate Dynamic Kriging-based predictors (MDKs) is built and used to estimate the process dynamic behavior, while static kriging models are used to smooth the eventually noisy process outputs. The estimated and the actual smoothed outputs are compared, taking advantage of the higher capacity of the residual patterns generated in this way to characterize the process state. The performance of the method is illustrated through its application to a well-known benchmark case study, for which the FDD performance has been significantly improved. This improvement is consistently maintained in different dynamic operating conditions and faulty situations, including scenarios with modified fault severities and fault styles.",
     "keywords": ["Fault detection and diagnosis", "Pattern classification techniques", "Data-Based predictor", "Multivariate dynamic kriging", "Dynamic modelling"]},
    {"article name": "POD-DEIM for efficient reduction of a dynamic 2D catalytic reactor model",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.032",
     "publication date": "11-2017",
     "abstract": "Many computational difficulties in dealing with chemical process models often result from spatially distributed states as well as nonlinear correlations (e.g., for transport coefficients or reaction kinetics). Surrogate models with sufficient accuracy represent one remedy to this problem. Featuring a lower number of states, model order reduction (MOR) generates considerably less complex models and leads to faster model evaluations. Especially for nonlinear systems, snapshot-based MOR techniques are considered to be one of the most promising methods. In this study, we apply proper orthogonal decomposition together with the discrete empirical interpolation method (POD-DEIM) to a dynamic, two-dimensional reactor model for catalytic carbon dioxide methanation. Motivated by renewable energy integration, we consider this reactor in two different dynamic scenarios: Disturbed continuous operation and start-up. It can be shown that the reduced order model (ROM) is accurate and, furthermore, the solution of the FOM is accelerated at least by one order of magnitude.",
     "keywords": ["Nonlinear model reduction", "Proper orthogonal decomposition", "Empirical interpolation methods", "Catalytic reactor", "Methanation"]},
    {"article name": "The ALAMO approach to machine learning",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.010",
     "publication date": "11-2017",
     "abstract": "ALAMO is a computational methodology for learning algebraic functions from data. Given a data set, the approach begins by building a low-complexity, linear model composed of explicit non-linear transformations of the independent variables. Linear combinations of these non-linear transformations allow a linear model to better approximate complex behavior observed in real processes. The model is refined, as additional data are obtained in an adaptive fashion through error maximization sampling using derivative-free optimization. Models built using ALAMO can enforce constraints on the response variables to incorporate first-principles knowledge. The ability of ALAMO to generate simple and accurate models for a number of reaction problems is demonstrated. The error maximization sampling is compared with Latin hypercube designs to demonstrate its sampling efficiency. ALAMO's constrained regression methodology is used to further refine concentration models, resulting in models that perform better on validation data and satisfy upper and lower bounds placed on model outputs.",
     "keywords": ["Model selection", "Parametric regression", "Feature selection", "Mixed-integer optimization"]},
    {"article name": "A simulation-optimization approach to integrate process design and planning decisions under technical and market uncertainties: A case from the chemical-pharmaceutical industry",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.04.008",
     "publication date": "11-2017",
     "abstract": "This study addresses the product-launch planning problem in the chemical-pharmaceutical industry under technical and market uncertainties, and considering resource limitations associated to the need of processing in the same plant products under development and products in commercialization. A novel approach is developed by combining a mixed integer linear programming (MILP) model and a Monte Carlo simulation (MCS) procedure, to deal with the integrated process design and production planning decisions during the New Product Development (NPD) phase. The Monte Carlo simulation framework was designed as a two-step sampling procedure based on Bernoulli and Normal distributions. Results show the unquestionable influence of the uncertainty parameters on the decision variables and objective function, thus highlighting the inherent risks associated to the deterministic models. Process designs and scale-ups that maximize expected profit were determined, providing a valuable knowledge frame to support the long-term decision-making process, and enabling earlier and better decisions during NPD.",
     "keywords": ["Process design", "Capacity planning", "Scale-ups", "Mixed integer linear programing", "Monte carlo simulation", "Uncertainty"]},
    {"article name": "Explicit hybrid model predictive control strategies for intravenous anaesthesia",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.033",
     "publication date": "11-2017",
     "abstract": "In this work, we first present a piece-wise affine model for intravenous anaesthesia, based on which a hybrid explicit/multiparametric model predictive control strategy is developed. To deal with the inter- and intra-patient variability, an estimation strategy, the multiparametric moving horizon estimator, and different robust algorithms such as Offset Correction, State-Output Correction and Prediction Output Correction are further designed and implemented simultaneously with the hybrid multiparametric model predictive control. Simulation results for a set of 12 virtually generated patients for the regulation of the depth of anaesthesia by means of the Bispectral Index with Propofol as the anaesthetic, demonstrate the validity and usefulness of the proposed advanced control and estimation strategies.",
     "keywords": ["Intravenous anaesthesia", "Mutiparameric/explicit model predictive control", "Hybrid systems", "Robust model predictive control", "Moving horizon estimation"]},
    {"article name": "From process control to supply chain management: An overview of integrated decision making strategies",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.006",
     "publication date": "11-2017",
     "abstract": "Optimal decision-making in a process industry is fundamental in order to guarantee optimality of operations and increase profits and performance of a company. Decision-making occurs at different levels, from process control to supply chain management. Traditionally, these decisions have been considered individually, with little or no interaction between each other. However, an integrated decision-making framework can guarantee solutions closer to optimality. Such integration usually results in complex and large scale problems that are difficult to solve. We provide an overview of integrated decision-making strategies and review recent advances in the area, highlighting promising works as well as the main challenges that have yet to be overcome.",
     "keywords": ["Process control", "Production scheduling", "Production planning", "Supply-chain management", "Integrated decision making"]},
    {"article name": "Dynamic real-time optimization under uncertainty of a hydroformylation mini-plant",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.041",
     "publication date": "11-2017",
     "abstract": "In this contribution, the chance constrained optimization approach is applied for the dynamic real-time optimization under uncertainty of a hydroformylation mini-plant. For this purpose, a dynamic model of the plant is designed in MOSAIC consisting of 23 units, 12 components, and 25 streams. Three uncertain parameters were identified: the activation energy of the main reaction, a reaction inhibition factor, and the feed stream flows. The goal of the optimization is the maximization of the product content in the outlet stream, while minimizing the catalyst loss for economic reasons. The problem is solved with IPOPT every 5\u00a0h whereby set-points for seven controls are calculated. A fully automated mini-plant is operated for 200\u00a0h, during which offline gas chromatographs analyse samples on an hourly basis to provide information on the simulation and optimization runs. Overall, a stable mini-plant operation with a constant yield and efficient phase separation was achieved.",
     "keywords": ["D-RTO", "Optimization under uncertainty", "Chance constraints", "Hydroformylation", "Mini-plant"]},
    {"article name": "Optimisation approaches for the synthesis of water treatment plants",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.12.018",
     "publication date": "11-2017",
     "abstract": "Efficient water treatment design has progressively been growing in importance as the usage of water resources increases with population rise and industrial development. Their availability has been reduced with the more evident effects of climate change. Addressing this challenge necessitates more and efficient purification plants which can be realised by optimal design at conceptual stage. In this work, a mixed integer nonlinear programming (MINLP) model for the synthesis and optimisation of water treatment processes is proposed. Due to its numerous non-linearities and consequently, its non-stability, various linearisation, approximation and reformulation techniques have been implemented. Consequently, two improved formulations are derived, i.e. a partially linearised MINLP (plMINLP) and a mixed integer linear fractional programming (MILFP) models. The applicability of the mathematical formulations are investigated in case studies of seawater desalination and surface water treatment for the production of potable water. Finally, the models performance is analysed and compared against each other.",
     "keywords": ["MINLP", "Reformulation techniques", "MILFP", "Surface water treatment", "Seawater desalination"]},
    {"article name": "P-graph and Monte Carlo simulation approach to planning carbon management networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.047",
     "publication date": "11-2017",
     "abstract": "A P-graph and Monte Carlo simulation approach to planning carbon management networks is proposed. These networks are generalized systems for minimizing emissions of CO2. Application of the P-graph framework to such problems has the added advantage of being able to rigorously identify both optimal and near-optimal solutions, which is a feature that is useful for practical decision-making; Monte Carlo simulation can then be used to evaluate the robustness of a network to variations in system parameters. Two literature case studies are used to demonstrate this methodology. The first example is a carbon-constrained energy sector planning problem, while the second example is a CO2 capture and storage planning problem. In both cases, it is demonstrated that multiple solutions generated using P-graph methodology allow identification of robust, near-optimal carbon management networks.",
     "keywords": ["P-graph", "Process network synthesis", "Carbon management network", "Carbon emissions pinch analysis", "CO2 capture and storage"]},
    {"article name": "Industrial scheduling solution based on flexible heuristics",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.018",
     "publication date": "11-2017",
     "abstract": "This paper presents a generic heuristic-based scheduling solution. It highlights the flexibility that a simple heuristic method can offer and shows that using the ISA-95 standard it is possible to express the most relevant problem requirements. In order to illustrate the possible benefits, the paper also compares the solution quality of a smaller scale example scheduling problem to a rigorous mixed-integer linear programming (MILP) approach and shows how a heuristic approach scales towards large-size industrial problems. The paper concludes with a discussion of the advantages and disadvantages of both approaches, showing that for certain types of problems, the heuristic approach is fully sufficient, even if it cannot be expected to result in optimal solutions.",
     "keywords": ["Scheduling", "Standards", "Heuristics", "Large-scale problems"]},
    {"article name": "A generic methodology for processing route synthesis and design based on superstructure optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.030",
     "publication date": "11-2017",
     "abstract": "In this paper, a systematic framework for novel and sustainable synthesis-design of processing routes is presented along with the associated computer-aided methods and tools. In Stage 1, superstructure optimization is used to determine the optimal processing route(s). In Stage 2, the design issues are resolved and targets for improvement are identified through the use of integrated tools. In Stage 3, new alternatives are generated using the selected route and the previously identified targets. In addition to the various computer-aided tools, two special tools are presented: (1) a database employing a specially developed knowledge representation system, and (2) Super-O, a software interface that guides users through the formulation and solution of synthesis problems. Super-O transfers data between the different tools, including a library of generic models, representing a wide range of processing options. Application of the synthesis and design stages is highlighted through two case studies (biorefinery and carbon capture-utilization).",
     "keywords": ["BR Brazil", "Brazil", "CA Canada", "Canada", "CN China", "China", "DMC dimethyl carbonate", "dimethyl carbonate", "DME dimethyl ether", "dimethyl ether", "EC ethylene carbonate", "ethylene carbonate", "EG ethylene glycol", "ethylene glycol", "EO ethylene oxide", "ethylene oxide", "IN India", "India", "MEA monoethyl amine", "monoethyl amine", "MeOH methanol", "methanol", "MILP mixed-integer liner programming", "mixed-integer liner programming", "MINLP mixed-integer nonlinear programming", "mixed-integer nonlinear programming", "MX Mexico", "Mexico", "NC number of components", "number of components", "NDV number of discrete variables", "number of discrete variables", "NEQ number of equations", "number of equations", "NF number of feedstocks/raw materials", "number of feedstocks/raw materials", "NI number of processing intervals", "number of processing intervals", "NL Number of geographic locations", "Number of geographic locations", "NP number of products", "number of products", "NR number of reactions", "number of reactions", "NS number of processing steps", "number of processing steps", "NU number of utilities", "number of utilities", "NV number of variables", "number of variables", "PC propylene carbonate", "propylene carbonate", "PG propylene glycol", "propylene glycol", "PO propylene oxide", "propylene oxide", "PSIN Processing Step-Interval Network", "Processing Step-Interval Network", "TH Thailand", "Thailand", "US United States", "United States", "Process synthesis", "Processing networks", "Superstructure optimization", "Database", "Sustainable process design", "Process synthesis tool", "Computer-aided tools"]},
    {"article name": "Management of \u00abSystematic Innovation\u00bb: A kind of quest for the Holy Grail!",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.019",
     "publication date": "11-2017",
     "abstract": "In this paper, authors propose a contribution for improving the open innovation processes. It shows the necessity to get an efficient methodology for open innovation in order to build a computer aided tool for inventive design in Process Systems Engineering (PSE). The proposed methodology will be evocated to be fully used in the context of the \u201crevolutionary\u201d concepts around the so-called factory for the future, also called integrated digital factory, innovative factory\u2026 As a result the main contribution of this paper is to propose a software prototype for an Open Computer Aided Innovation 2.0. By definition this open innovation relies on collaboration. This collaboration should enable a community, with a very broad spectrum of skills, to share data, information, knowledge and ideas. As a consequence, a first sub objective is to create a methodological framework that takes advantages of collaboration and collective intelligence (with its capacity to join intelligence and knowledge). Furthermore, the raise of the digital company and more particularly the breakthroughs in information technologies is a powerful enabler to extend and improve the potential of collective intelligence. The second sub objective is to propose a problem resolution process to impel creativity of expert but also to develop, validate and select innovative solutions. After dealing with the importance of Process Innovation and Problem solving investigation in PSE, the proposed approach originally based on an extension of the TRIZ theory (Russian acronym for Theory of Inventive Problem Solving), has been improved by using approach such as case-based reasoning, in order to tackle and revisit problems encountered in the PSE. A case study on biomass is used to illustrate the capabilities of the methodology and the tool.",
     "keywords": ["Open computer aided innovation", "TRIZ", "Factory for the future", "Knowledge management system"]},
    {"article name": "An ontological approach to chemical engineering curriculum development",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.021",
     "publication date": "11-2017",
     "abstract": "Continuous reflection and evolution of curricula in chemical engineering is beneficial for adaptation to evolving industries and technologies and for improving student experience. To this end it was necessary to develop a method to enable a holistic reflection on the curriculum and to examine potential areas of improvement and change. The curriculum was modelled using knowledge modelling through the development of an ontology, Chemical Engineering Education Ontology (ChEEdO) in the Prot\u00e9g\u00e9 3.5 environment. ChEEdO models topics, taught modules and the learning outcomes of the modules within the domain of chemical engineering. The learning outcomes were related to the topics using verb properties from Bloom\u2019s taxonomy and the context of each learning outcome. The functionality of semantic reasoning via the ontology was demonstrated with a case study. The modelling results showed that the ontology could be successfully utilised for curriculum development, horizontal and vertical integration and to identify appropriate pre-requisite learning.",
     "keywords": ["Knowledge modelling", "Curriculum development", "Ontology", "Chemical engineering", "Education"]},
    {"article name": "Systematic process intensification using building blocks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.044",
     "publication date": "10-2017",
     "abstract": "We present a novel method for systematic process design and intensification. We depart from the classical unit operation-based representation of process units, flowsheets and superstructures and propose a new representation using fundamental building blocks. These building blocks can be associated with different process phenomena, tasks and unit operations. An assembly of blocks of the same type obtains a classical unit, while an assembly of blocks with different types results in an intensified unit. This allows to systematically identify and incorporate many intensification pathways using a general block-based superstructure. We design an intensified process by optimizing a performance metric for given raw materials and product specifications, material properties and bounds on flow rates. The overall problem is formulated using a single mixed-integer nonlinear optimization (MINLP) model that can be solved using commercial solvers. We show the applicability of our approach using several design and intensification case studies.",
     "keywords": ["Process intensification", "Process design", "Process synthesis", "Optimization", "Building blocks"]},
    {"article name": "Process intensification of reactive separator networks through the IDEAS conceptual framework",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.12.006",
     "publication date": "10-2017",
     "abstract": "A method to rigorously identify the performance limits of a reactive separator network is presented in this paper. The quantification of the enhancement potential for a given technology can greatly benefit process intensification studies in the pursuit of radical improvements. The Infinite DimEnsionAl State-Space (IDEAS) conceptual framework is first reviewed and then shown to be capable of assessing the potential for process intensification of reactive separation processes. The IDEAS framework is employed to formulate an infinite linear program (ILP) that can synthesize optimal reactive separation networks, and establish rigorous tradeoffs between total network reactive holdup, and total network capacity. The proposed reactive separation process intensification method is demonstrated on a case study involving the metathesis of 2-pentene through reactive distillation. Significant intensification over prior designs is demonstrated.",
     "keywords": ["Reactive separation", "Distillation", "Process intensification", "Process network synthesis", "Performance limits", "Tradeoffs"]},
    {"article name": "A computer-aided approach for achieving sustainable process design by process intensification",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.025",
     "publication date": "10-2017",
     "abstract": "Process intensification can be applied to achieve sustainable process design. In this paper, a systematic, 3-stage synthesis-intensification framework is applied to achieve more sustainable design. In stage 1, the synthesis stage, an objective function and design constraints are defined and a base case is synthesized. In stage 2, the design and analysis stage, the base case is analyzed using economic and environmental analyses to identify process hot-spots that are translated into design targets. In stage 3, the innovation design stage, phenomena-based process intensification is performed to generate flowsheet alternatives that satisfy the design targets thereby, minimizing and/or eliminating the process hot-spots. The application of the framework is highlighted through the production of para-xylene via toluene methylation where more sustainable flowsheet alternatives that consist of hybrid/intensified unit operations are generated from the application of phenomena-based process intensification.",
     "keywords": ["Process intensification", "Phenomena synthesis", "Sustainable design", "Toluene methylation", "Simulation"]},
    {"article name": "A computer-aided software-tool for sustainable process synthesis-intensification",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.001",
     "publication date": "10-2017",
     "abstract": "Currently, the process industry is moving towards the design of innovative, more sustainable processes that show improvements in both economic and environmental factors. The design space of unit operations that can be combined to generate process flowsheet alternatives considering known unit operations as well as reported hybrid/intensified unit operations is large and can be difficult to manually navigate in order to determine the best process flowsheet for the production of a desired chemical product. Therefore, it is beneficial to utilize computer-aided methods and tools to enumerate, analyze and determine within the design space, the more sustainable processes. In this paper, an integrated computer-aided software-tool that searches the design space for hybrid/intensified more sustainable process options is presented. Embedded within the software architecture are process synthesis and intensification methods that operate at multiple scales, namely, unit operation, task and phenomena. First a base case process flowsheet (if it is not already available) is generated through process synthesis considering only known unit operations. The generated or supplied base case is then analyzed in order to identify process bottlenecks/limitations (hot-spots) that are translated into design targets. Next, phenomena-based synthesis is performed to identify process flowsheets that match the design targets through the use of hybrid/intensified unit operations. As these process flowsheets satisfy all process constraints while also matching the design targets, they are therefore more sustainable than the base case. The application of the software-tool to the production of biodiesel is presented, highlighting the main features of the computer-aided, multi-stage, multi-scale methods that are able to determine more sustainable designs.",
     "keywords": ["2phM two phase mixing", "two phase mixing", "C cooling", "cooling", "CAMD computer-aided molecular design", "computer-aided molecular design", "CP closed path", "closed path", "De decanter process-group", "decanter process-group", "DM dipole moment", "dipole moment", "DW divided wall column", "divided wall column", "ed extractive distillation process-group", "extractive distillation process-group", "ESA energy separating agent", "energy separating agent", "FAME fatty-acid methyl ester", "fatty-acid methyl ester", "fl flash process-group", "flash process-group", "GJ giga Joules", "giga Joules", "H heating", "heating", "I initiator", "initiator", "ICAS integrated computer-aided system", "integrated computer-aided system", "kg kilogram", "kilogram", "L liquid", "liquid", "LCA life cycle assessment", "life cycle assessment", "LL liquid-liquid", "liquid-liquid", "M mixing", "mixing", "MINLP mixed integer non-linear programming", "mixed integer non-linear programming", "Min minimize", "minimize", "MSA mass separating agent", "mass separating agent", "Mv vapor mixing", "vapor mixing", "Mw molecular weight", "molecular weight", "NLP non-linear programming", "non-linear programming", "OP open path", "open path", "PBB phenomena building block", "phenomena building block", "PC phase contact", "phase contact", "PS phase separation", "phase separation", "PT phase transition", "phase transition", "pv pervaporation process-group", "pervaporation process-group", "PVL separation by permeability/affinity", "separation by permeability/affinity", "R reaction", "reaction", "R-task reaction task", "reaction task", "RG radius of gyration", "radius of gyration", "RD reactive distillation", "reactive distillation", "RDWC reactive divided wall column", "reactive divided wall column", "re reactor process-group", "reactor process-group", "R-S Task reaction-separation task", "reaction-separation task", "S-Task separation task", "separation task", "SFILE simplified flowsheet-input line-entry system", "simplified flowsheet-input line-entry system", "SL solid-liquid", "solid-liquid", "SP solubility parameter", "solubility parameter", "SPB simultaneous phenomena building block", "simultaneous phenomena building block", "Tb normal boiling point", "normal boiling point", "Tm normal melting point", "normal melting point", "V vapor", "vapor", "VdWV van der Waals volume", "van der Waals volume", "VL vapor-liquid", "vapor-liquid", "VP vapor permeation", "vapor permeation", "VV vapor-vapor", "vapor-vapor", "Process synthesis", "Process intensification"]},
    {"article name": "Sustainable process design & analysis of hybrid separations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.031",
     "publication date": "10-2017",
     "abstract": "Distillation is an energy intensive operation in chemical process industries. There are around 40,000 distillation columns in operation in the US, requiring approximately 40% of the total energy consumption in US chemical process industries. However, analysis of separations by distillation has shown that more than 50% of energy is spent in purifying the last 5\u201310% of the distillate product. Membrane modules on the other hand can achieve high purity separations at lower energy costs, but if the flux is high, it requires large membrane area. A hybrid scheme where distillation and membrane modules are combined such that each operates at its highest efficiency, has the potential for significant energy reduction without significant increase of capital costs. This paper presents a method for sustainable design of hybrid distillation-membrane schemes with guaranteed reduction of energy consumption together with two illustrative examples.",
     "keywords": ["Process intensification", "Membrane separation", "Hybrid distillation"]},
    {"article name": "A reactive distillation process for co-hydrotreating of non-edible vegetable oils and petro-diesel blends to produce green diesel fuel",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.018",
     "publication date": "10-2017",
     "abstract": "A reactive distillation (RD) process for the hydrotreating (HDT) of vegetable oils and sulphured petro-diesel to produce green diesel is developed. Process intensification (PI) of a reactive separation unit that combines both, the hydrodeoxigenation (HDO) of triglycerides and free fatty acids and the hydrodesulfurization (HDS) of petroleum-diesel reactions, is carried out. PI considers the thermodynamic analysis of model mixtures of vegetable oils with hexadecane and hydrogen to determine the appropriate operating conditions (temperature, pressure and composition blends) of the RD process. Two different hydrotreating RD column configurations are proposed. The simulation of the hydrotreating RD processes is performed with the Aspen Plus environment using the PC-SAFT option to modelling the phase equilibria. Simulation results show that the performance of the hydrotreating RD process is more energy efficient and higher yields are attained when blends of vegetable oils with high free fatty acids content and petro-diesel are premixed and further hydrotreated.",
     "keywords": ["Process intensification", "Reactive distillation", "Vegetable oils", "Green diesel", "Hydrotreating"]},
    {"article name": "Techno-economic evaluation of an ultrasound-assisted Enzymatic Reactive Distillation process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.042",
     "publication date": "10-2017",
     "abstract": "Enzymatic Reactive Distillation (ERD) is a bioreactive process, in which enzymes are immobilized on column internal surface, and helps to overcome chemical reaction and phase equilibrium limitations. The activation of enzymes by ultrasound (US) leads to ultrasound-assisted ERD (US-ERD) which might display more eco-efficiency than standard processing of valuable chemicals. Reaction rate improvements of more than 50% could be achieved by the assistance of US.An US-ERD process for the synthesis of butyl butyrate (10\u00a0kilotons per year, 99 wt% purity) was designed. A techno-economic evaluation via process optimization was carried out to minimize the annual costs, by using an evolutionary algorithm. The techno-economic evaluation shows that the US-ERD process and the ERD share nearly equal costs. Installation costs of the US equipment are high but they are compensated by a 12% lower reactive section height and a 7% lower total height of the US-ERD column in comparison to ERD.",
     "keywords": ["Biocatalysis", "Optimization", "Process intensification", "Sonochemistry"]},
    {"article name": "Optimal design of a multi-product reactive distillation system for silanes production",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.014",
     "publication date": "10-2017",
     "abstract": "Silane has found recent applications in the manufacture of solar photovoltaic cells, which provide solar power economically. One method for the production of silane takes trichlorosilane as a starting molecule, with a reaction mechanism that involves two other intermediate valuable products, dichlorosilane and monochlorsilane. The production system involves a series of reactors and separation steps that can be intensified via reactive distillation. In this work, optimal designs of reactive distillation systems for the production of trichlorosilane, dichlorosilane and silane are developed. Furthermore, a multi-product reactive distillation system for the production of any of these types of silane products is designed. It is shown that an intensified design can be obtained for the production of the three silane products using the same column. The operating conditions at which the columns should be adjusted for the production of each product are reported.",
     "keywords": ["Silane", "Dichlorosilane", "Monochlorosilane", "Reactive distillation column", "Genetic algorithms"]},
    {"article name": "Optimal design of intensified processes for DME synthesis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.004",
     "publication date": "10-2017",
     "abstract": "Dimethyl ether (DME) is widely used as green aerosol propellant, precursor to other organic compounds, or as a clean fuel for diesel engines or in combustion cells. The classic method for producing DME is by dehydration of methanol in a catalytic gas-phase reactor, and purification in a direct sequence of two distillation columns. Reactive distillation (RD) is a much better alternative for DME synthesis, based on process intensification principles.This paper presents the optimal design of novel DME processes based on reactive distillation, and makes a fair comparison with the classic reactor-separation-recycle process (for a plant capacity of 100 ktpy DME). The new RD processes were optimized in terms of minimizing the total annual costs, leading to savings of 30% in CapEx and 6% in energy requirements for the RD process. The results indicate that a RD column is recommended for new DME plants, while a combination of gas-phase reactor and RD is suitable for revamping existing plants.",
     "keywords": ["Dimethyl ether", "Gas-phase reactor", "Reactive distillation", "Process design", "Process optimization"]},
    {"article name": "Optimal synthesis of rotating packed bed reactor",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.026",
     "publication date": "10-2017",
     "abstract": "The rotating packed bed (RPB), a novel apparatus involving process intensification (PI), has been successfully adopted in the chemical industry in recent decades. This study demonstrates a superstructure synthesis approach for evaluating PI equipment through a case study involving an RPB and a packed bed (PB). A multiscale model links the micro-mechanisms of intensification in the RPB with the macro-level decisions of equipment selection and interconnection. The proposed superstructure considers configurations of an RPB and PB in series, in parallel, and in between, allowing quantification of their synergy. The overall model is formulated as a nonlinear programming problem and is demonstrated with the case of H2S removal from refinery tail gas.",
     "keywords": ["CNY China Yuan", "China Yuan", "RPB rotating packed bed", "rotating packed bed", "PB packed bed", "packed bed", "PI process intensification", "process intensification", "PSE process system engineering", "process system engineering", "Process intensification", "Process synthesis", "Rotating packed", "Bed", "Nonlinear porogramming"]},
    {"article name": "Pseudo-transient models for multiscale, multiresolution simulation and optimization of intensified reaction/separation/recycle processes: Framework and a dimethyl ether production case study",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.12.019",
     "publication date": "10-2017",
     "abstract": "Including detailed models of processing equipment in the process flowsheet model is required in the case of \u201cnon-standard\u201d unit operations, including, e.g., intensified equipment and unconventional reactor designs. We provide a unified multiscale framework for including such models in equation-oriented process flowsheet modeling, simulation and optimization. Relying on the reaction/separation/recycle process prototype, we propose a multiresolution paradigm, whereby detailed, distributed-parameter representations of reacting systems and rigorous models of (intensified) separation units are embedded in process flowsheet models. We develop an equation-oriented modeling approach based on a pseudo-transient reformulation of the balance equations, enabling the reliable and robust simulation of the process flowsheet. We also describe a companion design optimization routine. We illustrate these concepts with an extensive case study on dimethyl ether production using an intensified process featuring a dividing-wall distillation column and an adiabatic packed bed reactor with intermediate quenching.",
     "keywords": ["Process intensification", "Multiscale modeling", "Process design optimization", "Dividing-wall distillation column", "Dimethyl ether (DME) production"]},
    {"article name": "Intelligent, model-based control towards the intensification of downstream processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.005",
     "publication date": "10-2017",
     "abstract": "Process Intensification (PI) has been gaining increasing interest as industrial trends urge a shift towards more eco-efficient processes of significantly decreased operation and capital costs. In this direction we focus on the development of advanced control strategies of the Multicolumn Countercurrent Solvent Gradient Purification Process (MCSGP), an industrial, semi-continuous, chromatographic process, used for the purification of several biomolecules. We present a novel control approach that manages to drive the process towards continuous, sustainable operation. The presented controllers are designed within the PARametric Optimization and Control (PAROC) framework/software platform that enables the development of intelligent, model-based controllers through a step-by-step approach. The controllers are successfully tested against various disturbance profiles and they manage to track the predefined setpoints without significant offset.",
     "keywords": ["Process intensification", "Downstream processing", "Periodic systems", "Advanced control strategies", "Multi-parametric control"]},
    {"article name": "Multi-objective optimization involving cost and control properties in reactive distillation processes to produce diphenyl carbonate",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.022",
     "publication date": "10-2017",
     "abstract": "Polycarbonate has a very extensive commercial application thanks to its multiple physical properties. The environmental problems associated with the conventional process to obtain polycarbonate by the phosgene route has stimulated the research of intensified schemes as is the case of the reactive distillation for the synthesis of Diphenyl Carbonate (DPC). These intensified processes are complex in their structure because of the several degrees of freedom and are subject to be optimized in terms of crucial criteria such as the cost and the control properties. This work presents the multi-objective optimization involving cost and control properties of four intensified systems configurations to produce DPC: a conventional reactive distillation (CRD), a thermally coupled reactive distillation (TCRD), and two novel configurations which uses vapor recompression technology, a reactive distillation with heat integration (RDHI) through vapor recompression, and a reactive distillation with thermally coupling and heat integration (THRD) through vapor recompression. The results of the multi-objective optimization show that the tray holdups and diameter values of the columns strongly influence the control properties; thus, for large values of the tray holdups and diameters, the control properties were better. The control properties also were influenced by the presence of the interlinking streams in the reactive and separation columns, in particular, the presence of one interlinking stream tend to favor the control properties of a configuration. To close it is important to remark that the TCRD configuration was the most attractive arrangement to be implemented to produce DPC, mainly because the optimized designs showed the best control properties, and also the optimized designs achieved significant energy savings because they did not require electric power.",
     "keywords": ["Multi-objective optimization", "Reactive distillation", "Cost and control properties", "Diphenyl carbonate synthesis"]},
    {"article name": "Space-constrained purification of dimethyl ether through process intensification using semicontinuous dividing wall columns",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.037",
     "publication date": "10-2017",
     "abstract": "In this work, a distillation system is designed to purify dimethyl ether (DME) from its reaction by-products in the conversion of flare gas into a useful energy product. The distillation equipment has a size constraint for easy transportation, making process intensification the best strategy to efficiently separate the mixture. The process intensification distillation techniques explored include the dividing wall column (DWC) and a novel semicontinuous dividing wall column (S-DWC). The DWC and the S-DWC both purify DME to fuel grade purity along with producing high purity waste streams. An economic comparison is made between the two systems. The DWC is a cheaper method of producing DME however the purity of methanol, a reaction intermediate, is not as high as the S-DWC. Overall, this research shows that it is possible to purify DME and its reaction by-products in a 40-foot distillation column at a cost that is competitive with Diesel.",
     "keywords": ["ACCE Aspen Capital Cost Estimator", "Aspen Capital Cost Estimator", "CCA capital cost allowance", "capital cost allowance", "CO2 carbon dioxide", "carbon dioxide", "DME dimethyl ether", "dimethyl ether", "DWC dividing wall column", "dividing wall column", "H2O water", "water", "MeOH methanol", "methanol", "MV middle vessel", "middle vessel", "PPDP product and process design principles", "product and process design principles", "S-DWC semicontinuous dividing wall column", "semicontinuous dividing wall column", "SwoMV semicontinuous without middle vessel", "semicontinuous without middle vessel", "Dimethyl ether", "Process intensification", "Dividing wall column", "Semicontinuous distillation", "Plant-on-a-truck", "Mobile plant", "Chemical process simulation", "Aspen plus dynamics"]},
    {"article name": "Integrated reaction\u2013extraction process for the hydroformylation of long-chain alkenes with a homogeneous catalyst",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.019",
     "publication date": "10-2017",
     "abstract": "A lingering issue with the hydroformylation of long-chain alkenes is the cost of catalyst leaching. One effective method to recover homogeneous catalysts is the use of thermomorphic solvent systems (TMS). However, catalyst leaching is still too high using the current solvents DMF and decane, limiting economic feasibility. This work presents extraction as a possible method for intensifying catalyst recovery when using a TMS for the hydroformylation of 1-dodecene. A thermodynamic model for determining the LLE of the solvent system and for catalyst leaching is developed for implementation within a process-wide optimization problem. Using this model, the optimal reactor design with an integrated downstream separation including the catalyst loss can be investigated in more detail. It is shown that in this process the reactor design strongly depends on catalyst recovery and that by using the proposed extraction cascade the process becomes economically viable and more robust in regards to reactor performance.",
     "keywords": ["Thermomorphic solvents", "Homogeneous catalysis", "Hydroformylation", "Process optimization", "Process intensification"]},
    {"article name": "A physics-based model for industrial steam-methane reformer optimization with non-uniform temperature field",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.002",
     "publication date": "10-2017",
     "abstract": "In an industrial hydrogen production facility, steam-methane reforming reactions take place inside hundreds of catalyst-filled tubes placed in a large scale, high temperature furnace. Process efficiency depends strongly on the wall temperature distribution of the ensemble of reformer tubes; a narrower distribution has a process intensification effect, by providing similar processing experience to every feedstock molecule. Such process intensification efforts require a furnace model that can predict the temperature distribution as a function of operating conditions. Currently available furnace modeling solutions are either computationally intensive, making them unsuitable for (online) optimization calculations, or empirical, having limited accuracy when wide changes in operating conditions are required. In this work, a physics-based furnace model is presented that overcomes these limitations. Empirical perturbations in a Hottel zone radiation model are proposed to capture the spatially non-symmetrical temperature distribution. The low computational time makes the model suitable for operational intensification based on reduction of temperature distribution non-uniformity.",
     "keywords": ["Steam-methane reformer", "Furnace balancing", "Process intensification", "Smart manufacturing", "Process optimization"]},
    {"article name": "Simulation of intensified process of sorption enhanced chemical-looping reforming of methane: Comparison with conventional processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.031",
     "publication date": "10-2017",
     "abstract": "Intensified process of sorption enhanced chemical-looping reforming (SECLR) for hydrogen production was studied. This process was modified by recycling a portion of solid NiO and CaO from air reactor (AR) to reforming reactor (RR) and employing the exhaust CO2 as sweep gas at the calcination reactor (CR) under energy self-sufficient operation. By comparing the process performances among SECLR, conventional steam reforming (SR) and sorption enhanced steam reforming (SESR) at their optimum conditions, the intensified SECLR under adiabatic operation showed the best performance with hydrogen productivity of 3.95\u00a0kmol/h, CH4 conversion of 98% and H2 purity of 98.37%. Additionally, the optimized SECLR process operated adiabatically required the solid ratio from CR to AR of 0.945 and solid ratio from AR to RR of 0.008, leading to the minimum of heat requirement. Furthermore, the influence of CO2 content in feed stream on adiabatic operation of the SECLR process was investigated.",
     "keywords": ["Hydrogen production", "Sorption enhanced steam reforming", "Chemical-looping", "Process simulation"]},
    {"article name": "An optimization-based operability framework for process design and intensification of modular natural gas utilization systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.12.010",
     "publication date": "10-2017",
     "abstract": "This paper introduces a novel operability-based framework for process design and intensification of modular systems. This framework is based on nonlinear programming concepts, including bilevel optimization, and extends the application domain of classical operability approaches to process simulator runs (Aspen Plus) for obtaining the relationship between input and output spaces, in addition to first-principles models. As potential modular applications of the proposed approach, a catalytic membrane reactor (MR) for the direct methane aromatization (DMA) conversion to fuels and chemicals, and a natural gas combined cycle (NGCC) system for power generation, are addressed. These applications are especially motivated by the advent of the shale gas revolution that allowed the recent discovery of stranded shale/natural gas deposits at different shale formations in the U.S. The utilization of such deposits could be capitalized by employing modular technologies for the production of power, value-added chemicals and fuels at the site. However, the design and intensification of modular energy systems is a challenging task as these processes are represented by complex and nonlinear models. The obtained results indicate that the proposed framework has the potential to accelerate the realization of the concept of modular manufacturing with reduced footprint and maximized efficiency, resulting in future cost savings. In particular for the NGCC process, the results provide the gas and steam turbine cycle conditions that could enable modular plants that are about three orders of magnitude smaller in terms of power generation than typical operating processes.",
     "keywords": ["Process intensification", "Bilevel optimization", "Operability", "Modular systems", "Natural gas utilization"]},
    {"article name": "A systematic simulation-based process intensification method for shale gas processing and NGLs recovery process systems under uncertain feedstock compositions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.010",
     "publication date": "10-2017",
     "abstract": "Handling uncertainty in feedstock compositions is an important challenge for shale gas processing and natural gas liquids (NGL) recovery process systems. If the process system is designed without considering uncertain feedstock compositions, the product specifications could be easily violated. To address this challenge, we develop a systematic simulation-based process intensification method. This method consists of three steps, namely process simulation, capacity-oriented process intensification, and design validation. An iterative feature of the proposed method guarantees the intensified design hedged against uncertain feedstock compositions. The proposed method is illustrated on a conventional process system and a novel condensation-based system. In the novel system, a condensation process is considered in the gas dehydration section and it is integrated with a turboexpander process to improve the overall energy utilization efficiency. The intensified design of the novel system shows a lower total annualized cost than that of the conventional system.",
     "keywords": ["Shale gas processing", "NGLs recovery", "Feedstock uncertainty", "Process intensification", "Techno-economic analysis"]},
    {"article name": "Multi-scale approaches for gas-to-liquids process intensification: CFD modeling, process synthesis, and global optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.016",
     "publication date": "10-2017",
     "abstract": "This paper presents a multi-scale framework for the intensification of small scale gas-to-liquids (GTL) processes. As the process intensification tool, a radial microchannel reactor is used to facilitate the catalytic steam reforming of methane. Due to the endothermicity of this reaction, a microchannel reactor serves as a promising alternative because of its enhanced heat transfer characteristics. However, the underlying mathematical model for the microchannel reforming process is complex. Since our aim is to elucidate the optimal process topology from a plethora of alternatives through a global optimization framework, we built a surrogate mathematical model to bridge this gap. Through a rigorous model identification, parameter estimation, and cross-validation analysis we have developed an accurate mathematical model that can predict the microchannel reactor output within 0.43%. We then implemented this mathematical model into a process superstructure that considers several novel and competing process alternatives for the production of liquid fuels from natural gas. Across different case studies ranging from 500 to 5000 barrels per day of total production, we have observed that the microchannel process can improve break-even oil prices (BEOP) by as much as $10/bbl. Since the small scale GTL process aims to utilize stranded natural gas with almost zero value, a parametric analysis is performed to evaluate the BEOP at different feedstock prices. We have observed that the microchannel reforming alternative is the superior process at all the scales investigated when a natural gas price of $1/TSCF is considered. The topological findings suggest that process intensification through microchannel steam reforming is a viable approach to monetize stranded natural gas.",
     "keywords": ["Process intensification", "Stranded natural gas", "GTL", "Model identification", "Process synthesis", "Global optimization"]},
    {"article name": "Bioprocess intensification for the effective production of chemical products",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.015",
     "publication date": "10-2017",
     "abstract": "The further implementation of new bioprocesses, using biocatalysts in various formats, for the synthesis of chemicals is highly dependent upon effective process intensification. The need for process intensification reflects the fact that the conditions under which a biocatalyst carries out a reaction in nature are far from those which are optimal for industrial processes. In this paper the rationale for intensification will be discussed, as well as the four complementary approaches used today to achieve bioprocess intensification. Two of these four approaches are based on alteration of the biocatalyst (either by protein engineering or metabolic engineering), resulting in an extra degree of freedom in the process design. To date, biocatalyst engineering has been developed independently from the conventional process engineering methodology to intensification. Although the integration of these two methodologies has now started, in the future synergistic integration should enable many new opportunities for bioprocesses for the production of chemicals.",
     "keywords": ["Bioprocess", "Biocatalysis", "Fermentation", "Process intensification", "Protein engineering", "Metabolic engineering"]},
    {"article name": "Towards zero CO2 emissions in the production of methanol from switchgrass. CO2 to methanol",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.030",
     "publication date": "10-2017",
     "abstract": "In this work an integrated facility is proposed that produces methanol from switchgrass and uses the captured CO2 to enhance the production capacity by 50% via CO2 hydrogenation. The process consists of two sections, biomass processing to syngas and its conversion to methanol, and the electrolytic section where hydrogen is produced to hydrogenate the CO2 that has been captured during syngas cleaning. The integrated facility produces up to 207\u00a0Mgal/yr of methanol and 318\u00a0kt/yr of oxygen, but requires a large amount of electricity to generate the hydrogen. Thus, it can only be used in regions where wind velocity is above 8\u00a0m/s and solar radiation is above 5\u00a0kWh/m2/day such as in the Midwest of US, certain regions in China or in the South of Europe. The investment is high, around 1000\u00a0M\u20ac, but the production cost of methanol is promising, 0.25\u20130.35\u00a0\u20ac/kg with a high production capacity.",
     "keywords": ["Solar energy", "Biomass", "Wind power", "Methanol", "Hydrogen"]},
    {"article name": "Optimal dynamic operation of microalgae cultivation coupled with recovery of flue gas CO2 and waste heat",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.011",
     "publication date": "10-2017",
     "abstract": "This work addresses integration of microalgae cultivation with power production for the recovery of flue gas waste heat and CO2. The economics of supplying CO2 to an outdoor open pond for cultivating microalgae are compared for pure CO2 gas and flue gas injection scenarios. Cooling of the flue gas is modeled for direct-contact heat transfer arrangement. The economic advantage of employing waste heat recovery using indirect-contact heating of the open pond water is investigated as well. An optimization problem is formulated to determine the optimal monthly operations of the open pond and heat exchangers under each scenario. The flue gas injection scenario is found to be more economical with an improved annual harvest compared to the pure CO2 gas injection case. Waste heat recovery is shown to slightly increase the annual harvest, but is also uneconomical due to the high capital cost of the heat exchanger.",
     "keywords": ["Microalgae cultivation", "Waste heat recovery", "Process optimization", "Dynamic modeling"]},
    {"article name": "Strategy to synthesize integrated solar energy coproduction processes with optimal process intensification. Case study: Efficient solar thermal hydrogen production",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.038",
     "publication date": "10-2017",
     "abstract": "The development and implementation of alternative energy conversion techniques using renewable energy sources is critical for a sustainable economy. Among renewable energy sources, solar energy is prominent due to its abundance. Towards a sustainable economy, this paper presents a process design concept to synthesize Solar Electricity, Water, Food and Chemical (SEWFAC) processes. The proposed approach entails systematic synthesis of energy efficient, synergistic processes incorporating process intensification for optimal utilization of resources. The objective is the development of coproduction processes around the clock on an as-needed basis. A general strategy and detailed analysis to synthesize efficient solar thermal hydrogen production processes through solar thermal power cogeneration. Process simulations and optimizations are performed using an integrated MATLAB and Aspen Plus modeling environment. The proposed process designs are evaluated based on the various metrics introduced. Process designs are estimated to achieve 11% higher exergy efficiency compared to the traditional processes.",
     "keywords": ["Solar energy conversion", "Conceptual process design", "Hydrogen production", "Electricity generation", "Process integration"]},
    {"article name": "A new neural network for convex quadratic minimax problems with box and equality constraints",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.03.022",
     "publication date": "09-2017",
     "abstract": "This paper presents a new neural network for solving a class of convex quadratic minimax problems with equality and box constraints by means of the sufficient and necessary conditions of the saddle point of the underlying function. By defining a proper energy function, the proposed model is proved to be stable in the sense of Lyapunov and converges to an exact solution of the original problem for any starting point under the condition that the objective function is convex-concave on the linear equation sets. Compared with the existing neural networks for the same convex quadratic minimax problem, the proposed neural network has the fewest neurons and lower complexity, and requires weaker stability conditions. The validity and transient behavior of the proposed neural network are demonstrated by some numerical results.",
     "keywords": ["Neural network", "Minimax problem", "Stability", "Convergence", "Saddle point"]},
    {"article name": "Investigation of bio-augmentation of overloaded activated sludge plant operation by computer simulation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.04.004",
     "publication date": "09-2017",
     "abstract": "Based on the essential role of nitrifiers in nitrification process, bio-augmentation of nitrifiers in activated sludge process via two configurations is proposed to improve nutrient removal under overloaded conditions. In the first configuration a bio-augmentation batch enhanced reactor was applied to activated sludge. To improve nitrogen removal in overloaded plant, side-stream bio-augmented sequential batch reactor was examined as the second configuration. The simulation of overloaded plant without additional aeration tanks as base-case was performed by ASM1 model. The operation of bio-augmentation configurations and overloaded plant with additional aeration tanks were also simulated. The results reveal that nitrification efficiency can be improved up to 88% under 30% overloading condition compared to the base-case by applying the second configuration to the overloaded plant. Second configuration shows a significant potential for nitrification improvement as a result of N-load reduction compared to the main process as well as augmentation of the endogenous nitrifiers.",
     "keywords": ["Bio-augmentation", "Nitrification", "Autotrophic nitrifiers", "Simulation", "ASM models", "Overloaded wastewater treatment plant"]},
    {"article name": "Integrated real-time production scheduling of a multiple cryogenic air separation unit and compressor plant",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.04.001",
     "publication date": "09-2017",
     "abstract": "The development and application of an integrated real-time production scheduling and control strategy for a multiple cryogenic air separation unit (ASU) and compressor plant is discussed. Using a top-down optimisation approach, the operational targets for ASU production and compressor configuration are obtained for a given customer demand and subsequently managed using a real-time optimisation strategy. This is integrated with existing control to implement the steady-state configuration targets subject to process disturbances, power price fluctuations and against network change penalty weightings. Network material balance and network component operating constraints are met while simultaneously minimising plant reconfiguration costs during transient operation which occurs as a result of changing demands. Implemented using mixed integer linear programming, it is demonstrated that the two-stage optimisation strategy improves site operating costs by an average of 5% over the considered trial period (which would translate into substantial cost savings for such an energy intensive process).",
     "keywords": ["Production scheduling", "Site-wide optimisation", "Real-time optimisation", "Process control", "Power consumption", "Mixed integer linear programming"]},
    {"article name": "An engine oil closed-loop supply chain design considering collection risk",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.04.005",
     "publication date": "09-2017",
     "abstract": "Manufacturers are devising new methods to make their production systems more efficient and effective. Designing an optimized supply chain can support the corresponding processes to integrate the resources. However, one of the most important obstacles is the resource limitation. Recycling the second-hand products is one of the approaches to cope with this issue. Reverse logistics is a system of collecting products from end-users to the manufacturing centers for obtaining values from collected materials. In this research, the collection and distribution process of engine oil, which is derived from one of the most valuable natural resources, is considered. A mixed-integer linear programming model for a closed-loop supply chain of used engine oil is proposed and a case study of an oil refinery company is proposed to explore the applicability of the model. Due to uncertainty in the amounts of collected oil in the engine oil reverse logistics, the robust optimization approach is chosen to deal with uncertainty. Two objective functions of maximizing profit and minimizing the risk of the collection are considered. To solve the bi-objective model, augmented \u03b5-constraint approach is utilized. The results depict the merit of the current research.",
     "keywords": ["Closed-loop supply chain", "Bi-objective optimization", "Robust optimization", "\u03b5-Constraint", "Used engine oil", "Collection risk"]},
    {"article name": "Application of machine learning to pyrolysis reaction networks: Reducing model solution time to enable process optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.04.012",
     "publication date": "09-2017",
     "abstract": "Comprehensive models of biomass pyrolysis are needed to develop renewable fuels and chemicals from biomass. Unfortunately, the detailed kinetic schemes required to optimize industrial biomass pyrolysis processes are too computationally expensive to include in models that account for both kinetics and transport within reacting particles. Here we present a machine learning approach using artificial neural networks and decision trees to reduce the computational expense of detailed kinetic models by four orders of magnitude, enabling their use in comprehensive models. The trained neural networks generalize very well, predicting the outputs of the detailed kinetic model with over 99.9% accuracy on new data. The machine learning approach we outline is not specific to kinetic modeling and can be applied to any set of input and output data, even if the underlying relationship between inputs and outputs is unknown.",
     "keywords": ["Neural network", "Kinetic modeling", "Pyrolysis", "Biomass"]},
    {"article name": "Globally optimal dynamic real time optimization without model mismatch between optimization and control layer",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.04.006",
     "publication date": "09-2017",
     "abstract": "Global optimality of dynamic operations ensures maximum economic benefits over time. We introduce Dynamic Real Time Optimization (D-RTO) method which uses the same model for D-RTO and for Nonlinear Model Predictive Controller (NMPC), thereby eliminating the model mismatch between the D-RTO and the NMPC. An integration framework based on two different predictive horizons for time-scale decomposition is proposed. Fast updates and reoptimization of the dynamic trajectories ensure that the dynamic operation is optimal over time. Proposed global optimization algorithm is a variation of normalized multi-parametric disaggregation (NMDT) where NMDT is modified by the use of a bivariate partitioning to solve the D-RTO problem. Elimination of the mismatch between D-RTO and NMPC models significantly reduces the effort required to eliminate the model errors between D-RTO, NMPC and the plant. Three chemical process examples are included to illustrate the proposed framework.",
     "keywords": ["Global optimal dynamic real time optimization", "Normalized multi-parametric disaggregation", "Integrated dynamic real time optimization and nonlinear model predictive control"]},
    {"article name": "Process operational safety using model predictive control based on a process Safeness Index",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.04.010",
     "publication date": "09-2017",
     "abstract": "It has been repeatedly suggested that the common cause-and-effect approach to evaluating process safety has deficiencies that could be addressed by a systems engineering perspective. A systems approach should consider safety as a system-wide property and thus would be required to integrate all aspects of the process involved with monitoring or manipulating the process dynamics, including the control, alarm, and emergency shut-down systems while operating them independently for redundancy. In this work, we propose initial steps in the first systems safety approach that coordinates the control and safety systems through a common metric (a Safeness Index) and develop a controller formulation that incorporates this index. Specifically, this work presents an economic model predictive control (EMPC) scheme that utilizes a Safeness Index function as a hard constraint to define a safe region of operation termed the safety zone. Under the proposed EMPC design, the closed-loop state of a nonlinear process is guaranteed to enter the safety zone in finite time in the presence of uncertainty while maximizing a stage cost that reflects the economics of the process. Closed-loop stability is established for a nonlinear process under the proposed implementation strategy.",
     "keywords": ["Economic model predictive control", "Process control", "Process safety", "Safeness Index", "Process operation"]},
    {"article name": "Effect of reactor configuration on performance of vacuum gas oil (VGO) hydrotreater: Modelling studies",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.04.007",
     "publication date": "09-2017",
     "abstract": "While kinetics is independent of either scale or configuration of hydrotreating reactor, hydrodynamics of reactor depend on both. The hydrodynamics of reactor which comprise of phase mass-transfer, catalyst wetting and pressure drop affect its performance significantly and should be addressed adequately while deciding on configuration or scale up issues. This study evaluates and compares the performance of different configurations of commercial VGO hydrotreater by employing mathematical model encompassing kinetics and hydrodynamics. Two configurations have been studied:\u2022 Conventional trickle-bed reactor, subdivided into (a) beds in parallel and (b) beds in series. \u2022 Pre-saturated one liquid flow (POLF) reactor, subdivided into (a) reactor with single pre-saturator (POLF-SP) and (b) multiple reactors in series with intermittent pre-saturators (POLF-MP).The performance of conventional reactor in series/parallel is found to be superior to POLF configurations. The inferior performance of POLF configurations is attributed to mixed flow behaviour due to recycle as against plug flow behaviour in conventional trickle-bed reactors.",
     "keywords": ["Trickle-bed reactor", "Pre-saturated one liquid flow (POLF)", "Hydrotreating", "Vacuum gas oil", "Reactor configurations"]},
    {"article name": "A stochastic programming approach to integrated water supply and wastewater collection network design problem",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.04.003",
     "publication date": "09-2017",
     "abstract": "In this paper, a mixed scenario-based and probabilistic two-stage stochastic programming model is proposed for the design of integrated water supply and wastewater collection systems. None of the existing models simultaneously takes into account both business-as-usual and hazard uncertainties. To explore suitable solutions in a reasonable time, a solving procedure comprised of the (1) sample average approximation method, (2) Bezdek fuzzy clustering method and (3) Benders decomposition algorithm is developed. In order to expedite the convergence of the applied Benders decomposition algorithm, different acceleration techniques especially the local branching method are utilized. The performance of the proposed mathematical model and solution procedure is analyzed computationally through a real case study which the results show the usefulness of the developed stochastic programming model as well as the efficiency of the solution approach.",
     "keywords": ["Water supply network", "Wastewater collection system", "Uncertainty", "Sample average approximation", "Accelerated Benders decomposition algorithm", "Local branching"]},
    {"article name": "Modeling and simulated design: A novel model and software of a solar-biomass hybrid dryer",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.04.002",
     "publication date": "09-2017",
     "abstract": "Solar-biomass hybrid dryers are widely used as environmentally friendly alternatives to fossil fuel dryers for the preservation of agricultural food products. However, the design and optimization of hybrid dryers often involve construction of expensive prototype systems and time-consuming studies. This study presents a novel hybrid dryer model and a dryer design and simulation software, which can be used to improve hybrid dryer design efficiency. The methodologies adopted are those of functional analysis, mathematical modeling and virtual prototyping, with SolidWorks employed for CAD modeling, and Matlab for numerical simulations and implementation of mathematical models into a user interface. The software package has as main functions (a) dimensioning the hybrid dryer, which comprises a solar collector, combustion reactor and tunnel drying chamber, (b) cost analysis and financial appraisal, and (c) temperature dynamic simulation in the system. The novel software has been successfully employed to dimension a hybrid dryer for drying of green pepper.",
     "keywords": ["Modeling", "Simulated design", "Solar-biomass dryer", "Design software", "Food products"]},
    {"article name": "Mechanistic modelling of industrial-scale roller compactor \u2018Freund TF-MINI model\u2019",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.04.018",
     "publication date": "09-2017",
     "abstract": "A pharmaceutical roller compaction process was modelled using the Johanson powder mechanics model, which may be employed to achieve desired process performance. Mathematical modelling of the compaction of microcrystalline cellulose (PH102) was used to determine optimal process conditions for an industrial scale roll compactor (Freund, Model TF-MINI). The critical process parameters investigated were screw speed and roll pressure, while the rolls were kept at constant speed and the roll gap as variable. The roll-compacted ribbon density was considered the quality attribute of interest, as it is directly linked to the granule properties, particle size distribution and tablet mechanical properties. Experimental process data were used for model calibration and validation. This was followed by utilisation of the predictive capability of the model to achieve desired process performance of the compactor. The model findings indicated that the developed mechanistic model for the roller compactor can provide a design space to correlate the process parameters and materials properties to the critical quality attributes of granules which would be useful for implementation of Quality-by-Design paradigm in pharmaceutical manufacturing.",
     "keywords": ["Johanson model", "Mechanistic modelling", "Relative density", "Ribbon", "Roll compaction"]},
    {"article name": "Iterative modeling and optimization of biomass production using experimental feedback",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.04.020",
     "publication date": "09-2017",
     "abstract": "Models of cultures of microorganisms are widely used for analysis, control and optimization of bioreactors in order to enhance productivity and performance. Typically, model-based optimization approaches may have acceptable convergence rates to a local optimum, but they are negatively affected by modeling errors when extrapolating to unknown operating conditions. In this work, a model-based optimization methodology that uses experimental feedback is applied to a fed-batch bioreactor. Experimental feedback is used to solve the extrapolation problem. After the model has been (re)parameterized, an optimized experiment is designed to maximize the performance of the bioprocess. Data gathered in this experiment is used to correct the model, and the cycle continues until no further improvement is found. The method is tested in the production of baker\u2019s yeast biomass. Results obtained demonstrate the capability of the proposed approach to find an improved feeding profile that leads to better performance with minimum experimental effort.",
     "keywords": ["Model-based optimization", "Experimental design", "Biomass production", "Fed-batch reactor", "Baker\u2019s yeast"]},
    {"article name": "Locally weighted kernel partial least squares regression based on sparse nonlinear features for virtual sensing of nonlinear time-varying processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.04.014",
     "publication date": "09-2017",
     "abstract": "Virtual sensing technology is crucial for monitoring product quality when real-time measurement is not available. To deal with both strong nonlinearity and time-varying dynamics of industrial processes, we propose a novel locally weighted kernel PLS (LW-KPLS) based on sparse nonlinear features in this research. Unlike the conventional locally weighted PLS (LW-PLS), the proposed method weights the training samples by using sparse kernel feature characterization factors (SKFCFs), which take account of the strength of nonlinear dependency between samples in the Hilbert feature space. By integrating the nonlinear features into the locally weighted regression framework, LW-KPLS not only can cope with the time-varying characteristics but also is more suitable for highly nonlinear processes. The proposed method was validated through a numerical example, a penicillin fermentation process, and a real industrial cleaning process for residual drug substances. The results have demonstrated that the proposed LW-KPLS outperforms the conventional PLS, KPLS, LW-PLS, and eLW-KPLS in the prediction performance.",
     "keywords": ["Soft-sensor", "Virtual sensing", "Nonlinear time-varying processes", "SKFCFs", "Instance-wise kernelized elastic net learning", "Locally weighted kernel partial least squares"]},
    {"article name": "Multi-period water network management for industrial parks considering predictable variations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.04.015",
     "publication date": "09-2017",
     "abstract": "Designing processes in industrial parks by breaking plant boundaries will produce more resource and economic benefits than just focusing on every single plant. This article addresses the water management problem in industrial parks with considering two major points: (1) the water network synthesis problem towards park scenario in each time period, (2) the multi-period problem resulted from predictable development and adjustment variations of park. To achieve the optimal network configurations and water management plans for the operational scenarios in the future, an optimization mathematical model is formulated based on the proposed network superstructure including indirect water reuse and the planning strategies for multi-period concern. At last, three design cases are illustrated to demonstrate the application of the methodology.",
     "keywords": ["Water network", "Multi-period", "Industrial park", "Inter-plant", "Optimization"]},
    {"article name": "Temperature balancing in steam methane reforming furnace via an integrated CFD/data-based optimization approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.04.013",
     "publication date": "09-2017",
     "abstract": "In this work, we introduce a furnace-balancing scheme that generates an optimal furnace-side feed distribution that has the potential to improve the thermal efficiency of a reformer. The furnace-balancing scheme is composed of four major components: data generation, model identification, a model-based furnace-balancing optimizer and a termination checker. Initially, a computational fluid dynamics (CFD) model of an industrial-scale reformer, developed in our previous work, is used for the data generation as the model has been confirmed to simulate the typical transport and chemical reaction phenomena observed during reformer operation, and the CFD simulation data is in good agreement with various sources in literature. Then, we propose a model identification process in which the algorithm is formulated based on the least squares regression method, basic knowledge of radiative heat transfer and the existing furnace-side flow pattern. Subsequently, we propose a model-based furnace-balancing optimizer that is formulated as an optimization problem within which the valve position distribution is the decision variable, and minimizing the sum of the weighted squared deviations of the outer reforming tube wall temperatures from a set-point value for all reforming tubes with a penalty term on the deviation of the valve positions from their fully open positions is the objective function. CFD simulation results provide evidence that the optimized furnace-side feed distribution created by the furnace-balancing scheme can reduce the severity of nonuniformity in the spatial distribution of furnace-side temperature in the combustion chamber even when the reformer is under the influence of common valve-related disturbances.",
     "keywords": ["Methane reforming", "Furnace-balancing", "Process operation", "Process optimization", "Data-based modeling", "Valve defects"]},
    {"article name": "Significant cost and energy savings opportunities in industrial three phase reactor for phenol oxidation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.04.016",
     "publication date": "09-2017",
     "abstract": "Energy saving is an important consideration in process design for low cost sustainable production with reduced environmental impacts (carbon footprint). In our earlier laboratory scale pilot plant study of catalytic wet air oxidation (CWAO) of phenol (a typical compound found in wastewater), the energy recovery was not an issue due to small amount of energy usage. However, this cannot be ignored for a large scale reactor operating around 140\u2013160\u00a0\u00b0C due to high total energy requirement. In this work, energy savings in a large scale CWAO process is explored. The hot and cold streams of the process are paired up using 3 heat exchangers recovering significant amount of energy from the hot streams to be re-used in the process leading to over 40% less external energy consumption. In addition, overall cost (capital and operating) savings of the proposed process is more than 20% compared to that without energy recovery option.",
     "keywords": ["CWAO", "Phenol", "Trickle bed reactor", "Energy recovery", "Modelling", "Optimisation"]},
    {"article name": "Robust synthesis of the pressure-swing distillation process under azeotropic feed composition disturbance\u2014Study of the tetrahydrofuran/methanol system",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.04.022",
     "publication date": "09-2017",
     "abstract": "Inferential control strategies on the basis of the temperature measurements in the separation process of the tetrahydrofuran/methanol azeotropic mixture using the pressure-swing distillation cannot guarantee the specification of the products. Accordingly, tetrahydrofuran mol% in the feed stream is assumed to be a Gaussian variable with known mean and standard deviation, and a stochastic mixed-integer nonlinear programming optimization framework is developed that is on the basis of the steady-state model of the plant. Proportional-integral controllers are implemented using the \u201cDesign Spec/Vary\u201d utility in Aspen Plus and the optimization problem is formulated taking the set-points of the controllers together with the design parameters as decision variables. This leads to a closed-loop stochastic optimization problem in which unscented transform is used as the uncertainty propagation tool. The optimal solution shows more robustness against the imposed feed composition disturbances and handles them more effectively while the desired purity of the products can be maintained.",
     "keywords": ["Azeotrope", "Pressure swing distillation", "Stochastic optimization", "Uncertainty", "Unscented transform"]},
    {"article name": "Implementing discrete element method for large-scale simulation of particles on multiple GPUs",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.04.019",
     "publication date": "09-2017",
     "abstract": "Recently, the use of graphics processing units (GPUs) has become popular in scientific computations due to their low cost, impressive floating-point capabilities, high memory bandwidth, and low electrical power requirements. Discrete element method (DEM), closely related to molecular dynamics (MD), is a widely-accepted method for simulating particle systems, such as powders, bubbles, grains, etc. In this work, we present an efficient implementation of DEM on multiple GPUs. Linked-cell list method is used to accelerate recognition of contacting particles and is beneficial for communications among multiple GPUs. Moreover, asynchronous communication is used to reduce the time costs of communications. Physical validation is performed and parallel efficiency is evaluated, showing that our implementation has high computational efficiency and good scalability. The simulation of a large-scale dense particle system (up to 128,000,000) is achieved with a good parallel efficiency. The speed-up ratio in 16 GPUs vs 128 CPU cores is 10.39.",
     "keywords": ["Multiple GPUs", "DEM", "Contact model", "Linked-cell method", "Large-scale simulation"]},
    {"article name": "Optimized energy use through systematic short-term management of industrial waste incineration",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.03.023",
     "publication date": "09-2017",
     "abstract": "The introduced work aims to systematic optimize detailed short-term scheduling for industrial waste incineration to support daily decision-making processes. The overall objective is to reduce the utilization of auxiliary fuel used to overcome waste related energy in the incineration process. The reduction is achieved through adequate mixing and scheduling of waste streams with different energy content and improved logistic strategies. With it, regulatory and technical constraints are fulfilled and holdups in production processes derived from bottlenecks in the waste treatment are avoided.The considered system consists of storage tanks, a piping network, tank wagons for waste transportation, unloading pumps and incineration lances. The optimization model is formulated as a mixed-integer linear programming (MILP) problem with a discrete time representation based on a single uniform grid. It consists of two blocks: i) waste transfer and mixing, and ii) the incineration process itself including energy calculations and constraints.The industrial case study of this work shows a saving potential of auxiliary fuel of up to 90%, leading to a reduction of CO2 emissions of up to 13%. Hence, the systematic scheduling of industrial waste incineration leads to an important reduction of auxiliary fuel consumption, resulting in economic and environmental benefits.",
     "keywords": ["EES energy-efficient scheduling", "energy-efficient scheduling", "EWO enterprise-wide optimization", "enterprise-wide optimization", "MILP mixed-integer linear programming", "mixed-integer linear programming", "MINLP mixed-integer nonlinear programming", "mixed-integer nonlinear programming", "MSW municipal solid waste", "municipal solid waste", "MTOE million tons of oil equivalents", "million tons of oil equivalents", "MU monetary unit", "monetary unit", "NLP nonlinear programming", "nonlinear programming", "WtE waste-to-energy", "waste-to-energy", "Waste-to-energy", "Industrial waste", "Waste incineration", "Scheduling", "Mathematical optimization", "Energy efficiency"]},
    {"article name": "Assessing energetic and available fuel demands from a soybean biorefinery producing refined oil, biodiesel, defatted meal and power",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.03.025",
     "publication date": "09-2017",
     "abstract": "Soybean processing facilities can produce various products, readily fitting in a biorefinery concept. In addition, they can also produce their own energetic needs by using their own residues and/or products. The aim of this study was to assess the relationship among the availability of fuel, process energetic demands and different market situations in a soybean processing facility. It was observed that soybean straw or a combination of straw, hulls and biodiesel could supply all the thermal and electricity demands of the process and even produce electricity surplus. Using an extraction-condenser turbine operating at 60\u00a0bar and 480\u00a0\u00b0C to fulfil energetic demands from the process using only one fuel type, 18.33%\u201319.41% of the total straw brought from the field would be necessary, depending on the amount of degummed oil diverted to biodiesel production. Furthermore, the maximum electricity surplus this facility could produce is 0.95\u00a0kWh/kg of processed soybeans.",
     "keywords": ["Soybean", "Biorefinery", "Cogeneration", "Bioenergy"]},
    {"article name": "Combined model predictive control and scheduling with dominant time constant compensation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.04.024",
     "publication date": "09-2017",
     "abstract": "Linear model predictive control is extended to both control and optimize a product grade schedule. The proposed methods are time-scaling of the linear dynamics based on throughput rates and grade-based objectives for product scheduling based on a mathematical program with complementarity constraints. The linear model is adjusted with a residence time approximation to time-scale the dynamics based on throughput. Although nonlinear models directly account for changing dynamics, the model form is restricted to linear differential equations to enable fast online cycle times for large-scale and real-time systems. This method of extending a linear time-invariant model for scheduling is designed for many advanced control applications that currently use linear models. Simultaneous product switching and grade target management is demonstrated on a reactor benchmark application. The objective is a continuous form of discrete ranges for product targets and economic terms that maximize overall profitability.",
     "keywords": ["Scheduling", "Model predictive control", "Dynamic pricing", "Time scaling", "Complementarity constraints"]},
    {"article name": "Product-centric continuous-time formulation for pipeline scheduling",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.04.023",
     "publication date": "09-2017",
     "abstract": "Continuous-time scheduling models for multiproduct pipelines typically view the contents of a pipeline as a set of batches. It facilitates tracking batch coordinates but prevents rigorously enforcing forbidden product sequences when dealing with intermediate dual-purpose nodes. Such models require the initial characterization of the pipeline and total number of batches as inputs, decisions that can be non-trivial. To overcome these limitations, this paper presents a new mixed-integer linear programming (MILP) formulation for straight pipelines with a single tuning parameter, the number of event points in the grid. It is derived from Generalized Disjunctive Programming (GDP) followed by a convex hull reformulation. Compared to a product-centric formulation based on the Resource-Task Network (RTN), it is much simpler, smaller in size and can be up to two orders of magnitude faster. Comparison to its batch-centric counterpart is not as favorable, the highlight being an 8% improvement in makespan for a benchmark problem.",
     "keywords": ["Mathematical modeling", "Optimization", "Process operations", "Computational logistics"]},
    {"article name": "Optimization formulations for multi-product supply chain networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.04.021",
     "publication date": "09-2017",
     "abstract": "We present optimization formulations for multi-product supply chain networks. The formulations use a general graph representation that captures dependencies between an arbitrary number of products, technologies, and transportation paths. We discuss how to use the framework to compute compromise solutions that resolve geographical and stakeholder conflicts. We present case studies in which we seek to design supply chains to collect and process organic waste from a large number of farms in the State of Wisconsin to mitigate point phosphorus and methane emissions.",
     "keywords": ["Multi-product", "Graph", "Supply chain", "Priorities", "Multi-stakeholders", "Organic waste"]},
    {"article name": "A pharmacokinetic/pharmacodynamic model of ACE inhibition of the renin-angiotensin system for normal and impaired renal function",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.03.027",
     "publication date": "09-2017",
     "abstract": "Angiotensin II (Ang II) is a hormone that regulates blood pressure and is produced by the renin-angiotensin system (RAS). Angiotensin converting enzyme (ACE) inhibitor drugs inhibit the production of Ang II. ACE inhibitors are well-characterized for use in hypertension, but they are not as well understood for use in chronic kidney disease. Existing models for ACE inhibitors have only been applied to normal renal function. Here, published experimental data for two ACE inhibitors (benazepril and cilazapril) in patients with normal and impaired kidneys are used to construct a pharmacokinetic model of the fate of drug doses in the circulatory system. A pharmacodynamic model connecting drug concentration to the inhibition of the RAS and production of Ang II is proposed and parameterized for the two drugs in the cases of normal and impaired renal function. The model is packaged as a MATLAB app to facilitate reuse for research and educational purposes.",
     "keywords": ["Nonlinear dynamic modeling", "Parameter estimation", "Systems pharmacology", "Hypertension", "Chronic kidney disease"]},
    {"article name": "The effect of Sales and Operations Planning (S&OP) on supply chain\u2019s total performance: A case study in an Iranian dairy company",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.05.002",
     "publication date": "09-2017",
     "abstract": "Despite Sales and Operations Planning (S&OP) is widely known in today\u2019s supply chain literature, its impact on supply chain\u2019s performance has been neglected. Furthermore, no research exists on mathematical modeling of S&OP in dairy industry. This study is going to investigate the benefits of S&OP using a mathematical modeling approach. Three novel Mixed Integer Programming models are presented to do so: a Fully Integrated S&OP (FI-S&OP) model, a Partially Integrated S&OP (PI-S&OP) model, and a decoupled planning (DP) model. All these models are developed for a multi-site manufacturing company, which is coping with various raw material suppliers and Third Party Logistics (3PLs), Distribution Centers (DCs), and customers with wide range of product families. Finally, all the models are applied in a real case in a dairy manufacturing company in Iran. The results demonstrate a slight superiority of FI-S&OP over PI-S&OP model and a strong domination over DP model.",
     "keywords": ["Sales and Operations Planning (S&OP)", "Make to Stock (MTS)", "Dairy industry", "Mixed Integer Programming (MIP)", "Real world case study"]},
    {"article name": "An energy-efficient multi-objective optimization for flexible job-shop scheduling problem",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.05.004",
     "publication date": "09-2017",
     "abstract": "In recent years there has been increased concern on energy efficiency of industries. Since the scheduling problems in the shop floors are directly related to the energy consumption, an appropriate way to improve energy efficiency in industrial plants is to develop effective scheduling strategies. Hence, the aim of this paper is to design an energy-efficient scheduling in a shop floor industrial environment, i.e., flexible job-shop scheduling problem (FJSP). To this end, a multi-objective optimization model is developed with three objective functions: (i) minimizing total completion time, (ii) maximizing the total availability of the system, and (iii) minimizing total energy cost of both production and maintenance operations in the FJSP. To cope with this multi-objective optimization problem, an enhanced evolutionary algorithm incorporated with the global criterion, as a multi-objective handling technique, is proposed and then performance evaluation is performed based on an extensive numerical analysis.",
     "keywords": ["Energy consumption", "Energy-efficient scheduling", "Industrial processes", "Scheduling problems", "Multi-Objective optimization"]},
    {"article name": "Run-To-Run control of the Czochralski process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.05.001",
     "publication date": "09-2017",
     "abstract": "Commercially, the Czochralski process plays a key role in production of monocrystalline silicon for semiconductor and solar cell applications. However, it is a highly complex batch process which requires careful control throughout the whole crystal production. In the present paper, an iterative method based on model predictive control (MPC) for calculating a new and improved trajectory from a growth run to the next is explored. The method uses the results of the previous growth run in combination with an underlying model which incorporates the complex dynamic effect of the heater temperature on the pulling rate. The motivation behind this choice of strategy is to enhance the quality of the fully grown ingot from one run to the next by applying the most recent estimates of the unknown parameters. The results show that combining MPC, estimation and Run-To-Run control has enabled simulation of effective control of the Czochralski crystallization process.",
     "keywords": ["Run-To-Run control", "Model predictive control", "Czochralski process"]},
    {"article name": "Simultaneous parameter identification and discrimination of the nonparametric structure of hybrid semi-parametric models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.05.005",
     "publication date": "09-2017",
     "abstract": "In this work, a hybrid semi-parametric modelling framework implemented using mixed integer linear programming (MILP) is used to extract (coupled) nonlinear ordinary differential equations (ODEs) from process data. Applied to fed-batch (bio) chemical reaction systems, unknown (or partially known) system connectivity and/or reaction kinetics are represented using a multivariate rational function (MRF) superstructure. The MRF\u2019s are embedded within an ODE framework which is used to incorporate known system model characteristics. Using derivative estimation, the ODEs are decoupled and a MILP algorithm is then used to identify appropriate constitutive model terms using sparse regression. Superstructure sparsity is promoted using a L0 \u2013 pseudo norm penalty, i.e. the cardinality of the model parameter vector, enabling the simultaneous yet decoupled identification of the parameters and model structure discrimination. Using simulated data, two case studies demonstrate a principled approach to hybrid model development, distilling unknown elements of (bio) chemical model structures from process data.",
     "keywords": ["Hybrid semi-parametric modelling", "Sparse regression", "Mixed integer linear programming", "Fed-batch (bio) chemical reactors"]},
    {"article name": "Machine learning model and optimization of a PSA unit for methane-nitrogen separation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.05.006",
     "publication date": "09-2017",
     "abstract": "In this work we study the separation of N2/CH4 in a bed packed with silicalite. Pressure swing adsorption (PSA) is a competitive technology for this task. Predicting PSA performance is a time consuming computational intensive problem. Direct optimization of the system of differential algebraic equations (DAE) describing the phenomena takes an impractical amount of time. We then analyze the suitability of using artificial neural networks (ANN) as a surrogate model to predict and optimize the PSA performance. Using the ANN surrogate model, optimization time decreased from 15.7\u00a0h to 50\u00a0s. We demonstrate that the PSA cycle proposed can achieve an optimized 99.5% nitrogen purity stream from an 85% inlet stream and a 50% purity stream from a 10% inlet stream. We also show that nitrogen recovery can be at most 90%. We further carry out a multi-objective optimization to demonstrate the tradeoff curve between nitrogen purity and recovery.",
     "keywords": ["Pressure swing adsorption", "Neural networks", "Surrogate model", "Optimization"]},
    {"article name": "A new optimization method: Electro-Search algorithm",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.046",
     "publication date": "08-2017",
     "abstract": "Natural phenomena have been the inspiration for proposing various optimization algorithms such as genetic algorithms (GA), particle swarm optimization (PSO) and simulated annealing (SA) methods. The main contribution of this study is to propose a novel optimization method, Electro-Search algorithm, based on the movement of electrons through the orbits around the nucleus of an atom. Electro-Search (ES) algorithm incorporates some physical principals such as Bohr model and Rydberg formula, adopting a three-phase scheme. In the atom spreading phase, the atoms (i.e., candidate solutions) are randomly spread all over the molecular space (i.e., search space). In the orbital transition phase, the electrons jump to larger orbits, aiming for orbits with higher energy levels (i.e., better fitness value). The atoms are then relocated towards the global optimum point in the atom relocation phase, navigated by other atoms\u2019 trajectory. Besides, the ES tuning parameters are progressively updated through successive iterations via a self-tuning approach developed, namely Orbital-Tuner method (OTM). The efficiency of ES algorithm is examined in various optimization problems and compared with other well-known optimization methods. The effectiveness and robustness of ES algorithm is then tested in achieving the optimal design of an industrial problem. The results demonstrated the superiority of the new ES algorithm over other optimization algorithms tested, and outperforms current optimization algorithms in real-life industrial optimization problems.",
     "keywords": ["Optimization methods", "Meta-heuristics", "Electro-Search algorithm", "Process optimization", "Acetic-acid dehydration"]},
    {"article name": "Molecular simulation and Monte Carlo study of structural-transport-properties of PEBA-MFI zeolite mixed matrix membranes for CO2, CH4 and N2 separation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.03.002",
     "publication date": "08-2017",
     "abstract": "In this communication, structural-transport-properties of polyether block amide (PEBA)-MFI zeolite mixed matrix membranes (MMMs) have been investigated with molecular simulation (MS) and Monte Carlo (MC) technique. The density profile, RDF, FFV and XRD characterization have been calculated to investigate the aforementioned MMMs microstructure and also sorption and diffusion structural dependence of CO2\u2013CH4 and CO2\u2013N2 pairs. Sorption, diffusion, permeability and selectivity of natural gases including CO2, CH4 and N2 by PEBA polymeric membrane filled with nano-zeolite (MFI type) have been examined to study the influence of MFI nanomaterial and nanomaterial loading content on the performance of the MMMs. By increase of MFI loading from 10 to 20\u00a0wt.%, permeability for CO2, CH4 and N2 increases from 106.5 to 123.6, 4.75\u20134.74 and 1.28 to1.31, respectively. Structural analysis shows very good behavior for simulated membranes. Moreover, experiments show that simulation results for XRD and diffusivity are in good agreement with experiment results.",
     "keywords": ["Molecular simulation", "MFI zeolite", "Nanocomposite PEBA membranes", "Gas transport", "Monte carlo technique"]},
    {"article name": "Approximation of closed-loop prediction for dynamic real-time optimization calculations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.037",
     "publication date": "08-2017",
     "abstract": "Dynamic real-time optimization (DRTO) is an extension of the traditional steady-state RTO paradigm to account for process dynamics in the RTO calculations. This paper presents methods for approximating closed-loop dynamic predictions within DRTO calculations for processes regulated under constrained model predictive control (MPC). Three approximation approaches are formulated and analyzed \u2013 hybrid, bilevel and input clipping formulations. The hybrid formulation involves application of rigorous closed-loop prediction over a limited DRTO horizon, followed by open-loop optimal control. In the bilevel formulation, only a single MPC optimization subproblem is embedded, whereas the input clipping approach is formulated using an unconstrained MPC algorithm with an input saturation mechanism applied over the DRTO horizon. The relative performance of the proposed approximation approaches is illustrated through two case study applications, the second of which involves economically optimal polymer grade transitions. Excellent closed-loop approximation is achieved without significant loss of prediction accuracy.",
     "keywords": ["Dynamic real-time optimization", "DRTO", "Closed-loop prediction", "Complementarity constraints", "MPC"]},
    {"article name": "Subspace model identification and model predictive control based cost analysis of a semicontinuous distillation process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.03.011",
     "publication date": "08-2017",
     "abstract": "Semicontinuous distillation is a process intensification technique for purification of multicomponent mixtures. The system is control-driven and thus the control structure and its tuning parameters have crucial importance in the operation and the economics of the process. In this study, for the first time, a model predictive control (MPC) formulation is implemented on a semicontinuous process to evaluate the associated closed-loop cost. A cascade configuration of MPC and PI controllers is designed in which the setpoints of the PI controllers are determined via a shrinking-horizon MPC. The objective is to reduce the operating cost of a cycle while simultaneously maintaining the required product qualities. A subspace identification method is adopted to identify a linear, state-space model to be used in the MPC. The first-principals model of the process is then simulated in gPROMS. Simulation results demonstrate that the MPC has reduced the operational cost of a semicontinuous process by about 11%.",
     "keywords": ["Semicontinuous distillation", "Model predictive control (MPC)", "Cascade MPC with PI", "Subspace identification", "Dynamic distillation", "gPROMS"]},
    {"article name": "Multi-scale computational fluid dynamics of impregnation die for thermoplastic carbon fiber prepreg production",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.03.007",
     "publication date": "08-2017",
     "abstract": "A multi-scale computational fluid dynamics (CFD) model of a pultrusion process was proposed for unidirectional carbon fiber (UD-CF) prepreg production. Polyamide 6 (PA6) and polyacrylonitrile-based CF were used as the thermoplastic polymer matrix and reinforcement, respectively. The non-Newtonian viscosity of PA6 was expressed by Carreau's model. A micro-scale CFD model was constructed to obtain a proper resin permeability to CF filaments, while the tow domain was treated as sliding porous media in the macro-scale CFD. The resin velocity profile showed a similar shape to the relative resin amount experimentally measured in the UD-CF prepreg. The uniformity index of the resin velocity (UIv) on the outlet surface was calculated for 45 case studies with several tow speeds and resin flow rates. The tow speed showing a maximum UIv was remarkably well expressed as a linear function of the slip velocity, which is the difference between the tow speed and resin velocity.",
     "keywords": ["Unidirectional carbon fiber prepreg (UD-CF prepreg)", "Polyamide 6 (PA6)", "Thermoplastic pultrusion", "Computational fluid dynamics (CFD)", "Multi-scale simulation"]},
    {"article name": "Generalized disjunctive programming model for the multi-period production planning optimization: An application in a polyurethane foam manufacturing plant",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.03.006",
     "publication date": "08-2017",
     "abstract": "A Generalized Disjunctive Programming (GDP) model for the optimal multi-period production planning and stock management is proposed in this work. The formulation is applied to a polyurethane foam manufacturing plant that comprises three stages: a first step that produces pieces with certain characteristics, a second process that involves the location of these pieces in a limited area and a third stage where pieces are stored in dedicated spaces. This article shows the GDP capabilities to provide a qualitative framework for representing the problem issues and their connections in a natural way, especially in a context where decisions integration is required. Due to the multi-period nature of the planning problem, a rolling horizon approach is suitable for solving it in reasonable computing time. It serves as a tool for analyzing the trade-offs among the different costs. Through the examples, the capabilities of the formulation and the proposed resolution method are highlighted.",
     "keywords": ["Generalized disjunctive programming", "Production planning", "Stock", "Optimization", "Rolling horizon", "Mattress industry"]},
    {"article name": "Dynamic plunger lift model for deliquification of shale gas wells",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.03.005",
     "publication date": "08-2017",
     "abstract": "This paper presents first principles model for operation of a plunger lift system in natural gas wells. The model consists of pressure and flow dynamics of fluids in annulus and the central tubing sections of the well, and dynamics of plunger fall and rise in the tubing. System dynamics switch on shutting or opening the production valve, and with autonomous events related to plunger motion. A nonlinear hybrid model is realized with nine states, switching though six stages (modes) of operation. This is the first instance of modeling complex system of plunger lift using a standard hybrid system model (HSM) framework. The resulting model is used to present insight into plunger lift operation, including an efficient simulation of multiple plunger cycles and analysis of effect of uncertainties on the well behavior.",
     "keywords": ["Hybrid state model", "Artificial lift", "Plunger lift", "Upstream oil and gas", "Simulation", "Periodic process"]},
    {"article name": "Optimal sensor placement with mitigation strategy for water network systems under uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.03.014",
     "publication date": "08-2017",
     "abstract": "Contamination of water in a distributed water network can cause serious public health disaster. The contamination can occur at one part of the network and may spread to different region depending of the flow pattern of water. It is essential to monitor contamination using distributed water networks and a practical way to do so, is through the application of sensor networks. Sensors are used for timely detection of the presence of various contaminants in water. To accurately and optimally map the contamination distribution, it is quintessential to place sensors in optimal positions in the network whereby the goal is to minimize the net effect from contamination \u2013 inadvertent or intentional. The demand of water at different locations in the network can be uncertain leading to various flow patterns. The location of contamination within the network can also be uncertain. With these uncertainties, the propagation of the contaminant in the network varies accordingly, thus affecting the impact from contamination. An optimal placement of the sensors should account for these uncertainties. A mitigation response of contamination, as detected by a sensor network is manifested in a decreased demand due to public information as a function of distance from the sensor. The overall impact from contamination is estimated by including impacts, as found in the downstream of the sensor in the network. In this work, the most efficient network of sensors is designed by including demand and location uncertainty as well as response from downstream from the sensor. The problem is solved by formulating a nonlinear, stochastic mixed integer problem. Generally stochastic formulation requires repeated function calculations. In our research, a reweighting approach is used instead of repeated function simulation to estimate an expected value of the impact. The reweighting scheme along with a novel sampling technique has been implemented through a Better Optimization algorithm for Nonlinear Uncertain Systems (BONUS). The results are compared with that from TEVA SPOT, which neither takes into account demand uncertainty nor downstream response from the source of contamination (attack). Inclusion of downstream response and demand uncertainty on optimal sensor placement in water network systems show better sensor layouts with minimum impacts from contamination.",
     "keywords": ["Sensor placement", "Water network", "Stochastic optimization", "Impact assessment", "Uncertainty Principle"]},
    {"article name": "On the design of complex energy systems: Accounting for renewables variability in systems sizing",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.03.010",
     "publication date": "08-2017",
     "abstract": "The variability challenge inherent in the design and sizing of stand-alone renewables-based energy systems incorporating storage is addressed at the design stage. The framework developed for reliability evaluation combines the stochastic modelling of renewable resources with chronological simulation of energy system performance for the evaluation of system reliability. The effect of inter-year variability is quantified by using a modified form of the loss of power supply probability as the reliability objective. A bi-criteria problem of capital cost minimization and reliability maximization is solved for two cases of remotely-located mining operations in Chile and Canada to demonstrate the capabilities of the methodology. Approximations to the Pareto-optimal fronts generated using a multi-objective genetic algorithm (NSGA-II). The performances of the minimum-cost designs generated are investigated in each case. The methodology provides the decision maker with necessary information about a number of alternative designs based on which sizing decisions may be made.",
     "keywords": ["Energy storage", "Multi-objective optimization", "Reliability", "Renewables variability", "Systems design"]},
    {"article name": "New a priori and a posteriori probabilistic bounds for robust counterpart optimization: III. Exact and near-exact a posteriori expressions for known probability distributions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.03.001",
     "publication date": "08-2017",
     "abstract": "The performance of robust optimization is closely connected with probabilistic bounds that determine the probability of constraint violation due to uncertain parameter realizations. In Part I of this work, new a priori and a posteriori probabilistic bounds were developed for cases when robust optimization is applied to uncertain optimization problems with parameters whose probability distributions were unknown. In Part II, the focus shifted to known probability distributions and a priori bounds. In this paper, new, tight a posteriori expressions are developed for constraints containing parameters with specific known distributions, that is, those attributed normal, uniform, discrete, gamma, chi-squared, Erlang, or exponential distributions. The nature of some of the expressions requires efficient implementations, and new algorithmic methods are discussed which greatly improve applicability. These new expressions are much tighter than existing bounds and greatly reduce the conservatism of robust solutions. The theoretical and algorithmic results of Parts I, II, and III allow for wider usage of robust optimization in process synthesis and operations research applications.",
     "keywords": ["Robust counterpart optimization", "Optimization under uncertainty", "Probabilistic bounds", "Mathematical modeling"]},
    {"article name": "Estimating refrigeration costs at cryogenic temperatures",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.03.013",
     "publication date": "08-2017",
     "abstract": "Conceptual process design makes extensive use of typical costs for energy sources that supply heat at various high temperatures and for energy sinks that remove heat at various low temperatures. Many books provide extensive data for energy sources such as steam, dowtherm, natural gas, coal, Bunker C and electricity over a broad range of temperatures, with energy cost naturally increasing with increasing temperature.Much less information is available about the cost of refrigeration at various temperature levels. The purpose of this paper is to greatly expand and update the information for estimating cryogenic refrigeration costs. Process source temperatures from \u221225\u00a0\u00b0C to \u2212190\u00a0\u00b0C are considered using single-stage and up to four-stage compression refrigeration systems with different refrigerants used at each stage. Refrigeration costs vary from $18 per GJ at \u221225\u00a0\u00b0C to $360 per GJ at \u2212190\u00a0\u00b0C.",
     "keywords": ["Compression refrigeration", "Multi-stage cycles", "Conceptual design"]},
    {"article name": "A parallel structure exploiting nonlinear programming algorithm for multiperiod dynamic optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.03.021",
     "publication date": "08-2017",
     "abstract": "This article develops a sequential quadratic programming (SQP) algorithm that utilizes a parallel interior-point method (IPM) for the QP subproblems. Our approach is able to efficiently decompose and solve large-scale multiperiod nonlinear programming (NLP) formulations with embedded dynamic model representations, through the use of an explicit Schur-complement decomposition within the IPM. The algorithm implementation makes use of a computing environment that uses the parallel distributed computing message passing interface (MPI) and specialized vector-matrix class representations, as implemented in the third-party software package, OOPS. The proposed approach is assessed, with a focus on computational speedup, using several benchmark examples involving applications of parameter estimation and design under uncertainty which utilize static and dynamic models. Results indicate significant improvements in the NLP solution speedup when moving from a serial full-space direct factorization approach, to a serial Schur-complement decomposition, to a parallelized Schur-complement decomposition for the primal-dual linear system solution within the IPM.",
     "keywords": ["Multiperiod dynamic optimization", "Multiple-shooting", "Sequential quadratic programming", "Interior-point methods", "Parallel computing"]},
    {"article name": "On the solution of differential-algebraic equations through gradient flow embedding",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.03.020",
     "publication date": "08-2017",
     "abstract": "In this paper Gradient Flow methods are used to solve systems of differential-algebraic equations via a novel reformulation strategy, focusing on the solution of index-1 differential-algebraic equation systems. A reformulation is first effected on semi-explicit index-1 differential-algebraic equation systems, which casts them as pure ordinary differential equation systems subject to an embedded pointwise least-squares problem. This is then formulated as a gradient flow optimization problem. Rigorous proofs for this novel scheme are provided for asymptotic and epsilon convergence. The computational results validate the predictions of the effectiveness of the proposed approach, with efficient and accurate solutions obtained for the case studies considered. Beyond the theoretical and practical value for the solution of DAE systems as pure ODE ones, the methodology is expected to have an impact in similar cases where an ODE system is subjected to algebraic constraints, such as the Hamiltonian necessary conditions of optimality in optimal control problems.",
     "keywords": ["Differential-algebraic equations", "Gradient flow", "Semi-explicit index-1", "DAE", "Ordinary differential equations"]},
    {"article name": "Mixture semisupervised probabilistic principal component regression model with missing inputs",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.03.015",
     "publication date": "08-2017",
     "abstract": "Principal component regression (PCR) has been widely used as a multivariate method for data-based soft sensor design. In order to take advantage of probabilistic features, it has been extended to probabilistic PCR (PPCR). Commonly, industrial processes operate in multiple operating modes. Moreover, in most cases, outputs are measured at a slower rate than inputs, and for each sample of input variable, its corresponding output may not always exist. These two issues have been solved by developing the mixture semi-supervised PPCR (MSPPCR) method. In this paper, we extend this developed model to the case of simultaneous missing data in both input and output. Missing data in multidimensional input space constitutes a significantly more challenging problem. Missing input data occurs frequently in industrial plants because of sensor failure and other problems. We develop and solve the MSPPCR model by using the expectation-maximization (EM) algorithm to deal with missing inputs, in addition to missing outputs and multi-mode conditions. Finally, we present two case studies to demonstrate its performance.",
     "keywords": ["Probabilistic principal component regression", "Missing data", "Mixture semisupervised modeling", "Soft sensor design"]},
    {"article name": "Comparative analysis of extractive and pressure swing distillation for separation of THF-water separation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.03.019",
     "publication date": "08-2017",
     "abstract": "In the present work, extractive and pressure-swing distillation methods are analyzed in detail through steady state ASPEN Plus simulations to propose the most economic method for separation of equimolar mixture of THF-Water. Various solvents were evaluated and DMSO was identified as the most appropriate solvent, as it gave minimum Total Annual Cost (TAC) for desired purity. In case of pressure swing distillation, various pressure ranges were explored to achieve minimum TAC. Configurations for extractive and pressure swing distillation with heat integration were also worked out. The optimum designs of extractive and pressure swing distillation with and without heat integration were compared on a common basis of feed conditions and purity constraints. Results indicate that TAC of extractive distillation with heat integration is 5.2% less than that of PSD with partial heat integration.",
     "keywords": ["Azeotrope", "Extractive distillation", "Pressure swing distillation", "THF-water", "TAC", "Optimization"]},
    {"article name": "Thermodynamic optimization of atmospheric distillation unit",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.03.024",
     "publication date": "08-2017",
     "abstract": "This paper presents a methodology for optimising the exergy efficiency of atmospheric distillation unit without trading off the products qualities and process throughput. The presented method incorporates the second law of thermodynamics in data driven models. Bootstrap aggregated neural networks (BANN) are used for enhanced model accuracy and reliability. The standard error of the individual neural network predictions is taken as the indication of model prediction reliability and is incorporated in the optimization objective function. The economic analysis of the recoverable energy (sum of internal and external exergy losses) reveals the energy saving potential of the proposed method, which will aid the design and operation of energy efficient atmospheric distillation columns.",
     "keywords": ["Atmospheric distillation", "Exergy", "Optimization", "Neural networks"]},
    {"article name": "Estimation of aggregation kernels based on Laurent polynomial approximation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.03.018",
     "publication date": "08-2017",
     "abstract": "The dynamics of particulate processes can be described by population balance equations which are governed by the phenomena of growth, nucleation, aggregation and breakage. Estimating the kinetics of the latter phenomena is a major challenge particularly for particle aggregation because first principle models are rarely available and the kernel estimation from measured population density data constitutes an ill-conditioned problem. In this work we demonstrate the estimation of aggregation kernels from experimental data using an inverse problem approach. This approach is based on the approximation of the aggregation kernel by use of Laurent polynomials. We show that the aggregation kernel can be well estimated from in silico data and that the estimation results are robust against substantial measurement noise. The method is demonstrated for three different aggregation kernels. Good agreement between true and estimated kernels was found in all investigated cases.",
     "keywords": ["Aggregation", "Aggregation kernel", "Inverse methods", "Polynomial approximation"]},
    {"article name": "Technoeconomic optimisation and comparative environmental impact evaluation of continuous crystallisation and antisolvent selection for artemisinin recovery",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.046",
     "publication date": "08-2017",
     "abstract": "Systematic nonlinear optimisation is a valuable tool towards evaluating the performance of conceptual Continuous Pharmaceutical Manufacturing (CPM) flowsheets. This study considers total cost minimisation of multiple plausible design choices and eight candidate antisolvents for the continuous recovery of artemisinin (a potent antimalarial Active Pharmaceutical Ingredient/API) via continuous crystallisation, with simultaneous evaluation of process mass and environmental efficiency via the E-factor (an established green chemistry metric). Essential design variables include the crystallisation cooling temperature, the antisolvent requirements and the use of multiple crystallisers in series. Acetonitrile achieves the minimum total cost for one crystalliser (761\u00a0\u00b7\u00a0103\u00a0GBP, for a crystallisation at 5\u00a0\u00b0C, with 80% antisolvent addition and an E-factor of 29.1). The use of a second crystalliser in series allows for further total cost savings for all antisolvents; E-factors continue to decrease accordingly, albeit with very limited scope for each successive crystalliser due to negligible productivity improvements.",
     "keywords": ["Continuous Pharmaceutical Manufacturing (CPM)", "Nonlinear Programming (NLP)", "Optimisation", "Crystallisation", "Solvent selection", "Artemisinin"]},
    {"article name": "Models and computational strategies for multistage stochastic programming under endogenous and exogenous uncertainties",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.011",
     "publication date": "08-2017",
     "abstract": "In this work, we address the modeling and solution of mixed-integer linear multistage stochastic programming problems involving both endogenous and exogenous uncertain parameters. We first propose a composite scenario tree that captures both types of uncertainty, and we exploit its unique structure to derive new theoretical properties that can drastically reduce the number of non-anticipativity constraints (NACs). Since the reduced model is often still intractable, we discuss two special solution approaches. The first is a sequential scenario decomposition heuristic in which we sequentially solve endogenous MILP subproblems to determine the binary investment decisions, fix these decisions to satisfy the first-period and exogenous NACs, and then solve the resulting model to obtain a feasible solution. The second is Lagrangean decomposition. We present numerical results for a process network and an oilfield development planning problem. The results clearly demonstrate the efficiency of the special solution methods over solving the reduced model directly.",
     "keywords": ["Multistage stochastic programming", "Endogenous uncertainty", "Exogenous uncertainty", "Non-anticipativity constraints", "Lagrangean decomposition", "Oilfield planning"]},
    {"article name": "Modeling and performance assessment of a new integrated gasification combined cycle with a water gas shift membrane reactor for hydrogen production",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.024",
     "publication date": "08-2017",
     "abstract": "This paper investigates the effects of flow rate of gasification oxidant, gasification agent and coal type on the energy efficiency and hydrogen production rate of a proposed system. The present system consists of a pressurized entrained flow gasifier integrated with a cryogenic air separation unit, a water gas shift membrane reactor and a combined cycle. Aspen Plus software is used to develop and simulate the integrated system. Three different types of coal are fed to the gasifier. A Gibbs free energy based model of the gasification process is developed and validated. The Gibbs free energy based model is then used to assess the effect of gasification parameters since it is more straightforward regarding changing input flow rates. It is found that gasifying low grade coal is preferable in terms of energy efficiency to gasifying high grade coals, which are more advantageous for combustion regarding energy efficiency.",
     "keywords": ["Hydrogen production", "Coal", "Gasification", "Energy", "Efficiency", "Combined cycle"]},
    {"article name": "Improved approximation for the Butler-Volmer equation in fuel cell modelling",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.10.018",
     "publication date": "07-2017",
     "abstract": "This work presents an improved approximation for the explicit form of the Butler-Volmer (BV) equation, which is used in modelling the activation phenomena in fuel cells. Three representations of the BV-equation in the form x\u00a0=\u00a0f(x) are presented in this paper, out of which, two forms are reducible to the high field approximation and one is reducible to the hyperbolic sine approximation under certain conditions. It is found that one of the forms offer an excellent approximation for the BV-equation throughout the applicable range. The detailed analysis of the convergence properties, the applicability ranges and comparative studies offer insights into the ranges of relevance. The proposed approximation will be accurate and applicable for a wide range of operation enabling its adaptation into model based algorithms for fuel cell operation and control. The proposed fixed point based iterative method can offer an alternative to the traditional methods that require the Jacobian information.",
     "keywords": ["Fuel cell Modelling", "Activation polarisation", "Butler-Volmer equation", "Approximation", "Fixed point iteration"]},
    {"article name": "Production of oxygen-enriched air via desorption from water: Experimental data, simulations and economic assessment",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.07.031",
     "publication date": "07-2017",
     "abstract": "Oxygen enriched air with a composition of oxygen till 35% can be produced by water desorption by simple water degassing. This work reports the simulation of a small bench-scale plant for the continuous production of oxygen-enriched air by simple water degassing. The basic thermodynamic principle involved is the higher water solubility of oxygen compared to the one of nitrogen. Different experiments were performed in a continuous small bench-scale plant changing the main operating parameters, i.e. water temperature, degasser pressure and the water flowrate in order to develop rigorous and reliable simulations with suitable software (PRO/II 9.3 by SIMSCI-Schneider Electric). The results obtained showed a good fitting between the experiments and the simulations and demonstrated the possibility for the production of enriched air using this new technology. Moreover, the calculation of the economic potentials of this new process were carried out, and the results compared to the already existing technologies.",
     "keywords": ["Oxygen enriched air", "Water desorption", "Henry\u2019s law", "Simulation", "Economic assessment"]},
    {"article name": "Design of an energy-efficient side-stream extractive distillation system",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.12.001",
     "publication date": "07-2017",
     "abstract": "A novel extractive distillation system is presented. The proposed separation system evolves from an extractive thermally coupled system with a side rectifier. Specifically, the modification involves substituting the thermal coupling, i.e., the vapor-liquid interconnection, by a liquid side stream fed to a modified second column of the arrangement. The design procedure and performance on energy consumption and CO2 emissions of the proposed structure is illustrated with three case studies, which involve bioethanol dehydration and the separation of acetone-methanol and heptane-toluene mixtures. The results show that the proposed system can be more energy efficient and provide a more sustainable option when compared to conventional extractive distillation or thermally coupled sequences. We also show how the implementation of heat integration between columns of the proposed structure can be considered for additional savings on energy and CO2 emissions.",
     "keywords": ["Side-stream distillation", "Thermally coupled distillation", "Extractive distillation", "Energy consumption", "CO2 emissions"]},
    {"article name": "Robust multi-objective optimization for sustainable design of distributed energy supply systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.038",
     "publication date": "07-2017",
     "abstract": "Sustainable design of distributed energy supply systems involves multiple aims. Therefore, multi-objective optimization is the appropriate concept for sustainable design. However, input parameters are in general uncertain. If uncertainties are disregarded in the optimization, solutions usually become infeasible in practice. To incorporate uncertain parameters, we apply the concept of minmax robust multi-objective optimization for designing sustainable energy supply systems. We propose a mixed-integer linear problem formulation. The proposed formulation allows to identify robust sustainable designs easily guaranteeing security of energy supply. Energy systems are shown to typically exhibit objective-wise uncertainties. Thus, a Pareto front can still be derived.In a real-world case study, robust designs are identified with a good trade-off between economic and ecologic criteria. The robust designs perform remarkably well in the nominal scenario. The presented problem formulation transfers the important theoretical concept of minmax robust multi-objective optimization into engineering practice for the design of sustainable energy systems.",
     "keywords": ["Sustainable energy supply systems", "Robust optimization", "Multi-objective optimization", "Robust multi-objective optimization", "Mixed-integer linear programming (MILP)", "Energy system design"]},
    {"article name": "Development and implementation of supply chain optimization framework for CO2 capture and storage in the Netherlands",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.08.011",
     "publication date": "07-2017",
     "abstract": "In this work, we develop a mixed integer linear optimization model that can be used to select appropriate sources, capture technologies, transportation network and CO2 storage sites and optimize for a minimum overall cost for a nationwide CO2 emission reduction in the Netherlands. Five different scenarios are formulated by varying the location of source and storage sites available in the Netherlands. The results show that the minimum overall cost of all scenarios is \u20ac47.8 billion for 25 years of operation and 54\u00a0Mtpa capture of CO2. Based on the investigated technologies, this work identifies Pressure Swing Adsorption (PSA) as the most efficient for post-combustion CO2 capture in the Netherlands. The foremost outcome of this study is that the capture and compression is the dominant force contributing to a majority of the cost.",
     "keywords": ["Carbon capture", "CO2 reduction", "Supply chain", "Optimization", "CCS", "Mathematical model"]},
    {"article name": "A long-term multi-region load-dispatch model based on grid structures for the optimal planning of China\u2019s power sector",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.06.017",
     "publication date": "07-2017",
     "abstract": "China's power sector has experienced rapid development over the past decade. With the clean energy targets and carbon mitigation objectives proposed by the government as well as rapid development of power transmission infrastructure, future potential pathways for the expansion of China's power sector are worth assessing. In this paper, a mathematical model, named the Long-term Multi-region Load-dispatch Grid-structure-based (LoMLoG) has been developed and a \u201cmost-likely\u201d scenario has been created that delivers insights into optimal regional power generation, transmission and emissions profiles. The results confirm the importance of clean energy targets in driving deployment of renewable energy and maximizing its contribution to carbon mitigation of China\u2019s power sector. In addition, the development of power transmission infrastructure will significantly influence regional power generation and transmission profiles. Last but not least, carbon mitigation approaches in the long term are discussed, and rational methods of allocating future carbon caps are presented.",
     "keywords": ["Power generation", "Power transmission", "Grid structures", "Multi-region modelling", "Load dispatch", "Clean energy", "Carbon dioxide mitigation", "Least cost optimisation"]},
    {"article name": "MINLP-based Analytic Hierarchy Process to simplify multi-objective problems: Application to the design of biofuels supply chains using on field surveys",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.10.014",
     "publication date": "07-2017",
     "abstract": "Multi-objective optimization (MOO) is widely used in engineering systems design and planning. The solution of a MOO problem leads to a set of efficient points (Pareto set) from which decision-makers should identify the one that best fits their preferences. Generating this set requires large computational efforts, and the post-optimal analysis of the solutions becomes difficult as the number of objectives increases. This work introduces an approach based on the Analytic Hierarchy Process (AHP) to overcome these limitations. Through the definition of an aggregated objective function calculated using the AHP algorithm, a single-objective model is constructed that provides a unique Pareto solution of the original MOO model. The AHP is combined with a mixed-integer non-linear programming (MINLP) formulation that simplifies its application and is particularly suited to deal with many objectives (like those arising in sustainable engineering problems). The capabilities of the approach are demonstrated through a case study addressing the sustainable sugar/ethanol supply chain design problem.",
     "keywords": ["Optimization", "Sustainability", "Multi-criteria decision-making", "Weighting"]},
    {"article name": "Stackelberg-game-based modeling and optimization for supply chain design and operations: A mixed integer bilevel programming framework",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.07.026",
     "publication date": "07-2017",
     "abstract": "While Stackelberg leader\u2013follower games and bilevel programming have become increasingly prevalent in game-theoretic modeling and optimization of decentralized supply chains, existing models can only handle linear programming or quadratic programming followers\u2019 problems. When discrete decisions are involved in the follower's problem, the resulting lower-level mixed-integer program prohibits direct transformation of the bilevel program into a single-level mathematical program using the KKT conditions. To address this challenge, we propose a mixed-integer bilevel programming (MIBP) modeling framework and solution algorithm for optimal supply chain design and operations, where the follower is allowed to have discrete decisions, e.g., facility location, technology selection, and opening/shutting-down of production lines. A reformulation-and-decomposition algorithm is developed for global optimization of the MIBP problems. A case study on an integrated forestry and biofuel supply chain is presented to demonstrate the application, along with comparisons to conventional centralized modeling and optimization methods.",
     "keywords": ["Supply chain optimization", "Game theory", "Mixed-integer bilevel programming", "Reformulation and decomposition algorithm", "Biofuel"]},
    {"article name": "A systematic approach to the optimal design of chemical plants with waste reduction and market uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.032",
     "publication date": "07-2017",
     "abstract": "The paper presents a methodology for the quantitative assessment of sustainability applied to the design of chemical plants. Specifically, we focus on the economic and environmental sustainability. The methodology implemented for the economic assessment is the predictive conceptual design (PCD) that uses as indicator the cumulated dynamic economic potential over a long-term horizon. PCD accounts for both CAPEX and OPEX terms, which on their turn depend on dynamic econometric models of commodities and utilities. The environmental assessment is based on the waste reduction algorithm and on the evaluation of the potential environmental impact. The benefit of PCD consists in accounting for market uncertainty and prices/costs volatility of OPEX terms. The optimal solutions of the economic and environmental assessment lay on the Pareto line produced by the multi-objective-optimization (MOO) problem. The MOO of a cumene plant allows discussing various optimal solutions in terms of economic and environmental concerns/criteria.",
     "keywords": ["ADL Autoregressive Distributed Lag", "Autoregressive Distributed Lag", "AP Acidification Potential", "Acidification Potential", "ARMAX AutoRegressive Moving Average with eXogenous inputs", "AutoRegressive Moving Average with eXogenous inputs", "ARX AutoRegressive model with eXogenous inputs", "AutoRegressive model with eXogenous inputs", "ATP Aquatic Toxicity Potential", "Aquatic Toxicity Potential", "CAPE Computer Aided Process Engineering", "Computer Aided Process Engineering", "CAPEX CAPital EXpenses", "CAPital EXpenses", "CD Conceptual Design", "Conceptual Design", "CO Crude Oil", "Crude Oil", "COCO CAPE-OPEN to CAPE-OPEN", "CAPE-OPEN to CAPE-OPEN", "DCD Dynamic Conceptual Design", "Dynamic Conceptual Design", "DEP Dynamic Economic Potential", "Dynamic Economic Potential", "DOE Department Of Energy", "Department Of Energy", "EA Economic Assessment", "Economic Assessment", "EE Electric Energy", "Electric Energy", "EIA Energy Information Administration", "Energy Information Administration", "EM Econometric Model", "Econometric Model", "EP Economic Potential", "Economic Potential", "EPA Environmental Protection Agency", "Environmental Protection Agency", "FEHE Feed Effluent Heat Exchanger", "Feed Effluent Heat Exchanger", "GWP Global Warming Potential", "Global Warming Potential", "HDA HydroDeAlkylation", "HydroDeAlkylation", "HTPE Human Toxicity Potential by Exposure", "Human Toxicity Potential by Exposure", "HTPI Human Toxicity Potential by Ingestion", "Human Toxicity Potential by Ingestion", "ICIS Independent Chemical Information Service", "Independent Chemical Information Service", "IEA International Energy Agency", "International Energy Agency", "IRR Internal Rate of Return", "Internal Rate of Return", "LC50 Lethal Concentration to 50% of organisms", "Lethal Concentration to 50% of organisms", "LCA Life Cycle Assessment", "Life Cycle Assessment", "LD50 Lethal Dose to 50% of organisms", "Lethal Dose to 50% of organisms", "M&S Marshall and Swift", "Marshall and Swift", "MOO Multi-Objective Optimization", "Multi-Objective Optimization", "NARMAX Non-linear AutoRegressive Moving Average model with eXogeneous inputs", "Non-linear AutoRegressive Moving Average model with eXogeneous inputs", "NARX Non-linear AutoRegressive model with eXogeneous inputs", "Non-linear AutoRegressive model with eXogeneous inputs", "NG Natural Gas", "Natural Gas", "NPV Net Present Value", "Net Present Value", "ODP Ozone Depletion Potential", "Ozone Depletion Potential", "OPEC Organization of Petroleum Exporting Countries", "Organization of Petroleum Exporting Countries", "OPEX Operative Expenses", "Operative Expenses", "OSHA Occupational Safety and Health Administration", "Occupational Safety and Health Administration", "PCOP PhotoChemical Oxidation Potential", "PhotoChemical Oxidation Potential", "PDC Predictive Conceptual Design", "Predictive Conceptual Design", "p-DIPB para\u2010diisopropylbenzene", "para\u2010diisopropylbenzene", "PEI Potential Environmental Impact", "Potential Environmental Impact", "PEL Permissible Exposure Limit", "Permissible Exposure Limit", "p-IPB para\u2010isopropylbenzene", "para\u2010isopropylbenzene", "PSE Process Systems Engineering", "Process Systems Engineering", "TTP Terrestrial Toxicity Potential", "Terrestrial Toxicity Potential", "WAR WAste Reduction", "WAste Reduction", "WCED World Commission on Environment and Development", "World Commission on Environment and Development", "WTI West Texas Intermediate", "West Texas Intermediate", "Economic sustainability", "Environmental sustainability", "Predictive conceptual design", "Process design", "Multi objective optimization", "Pareto curve", "Cumene plant"]},
    {"article name": "Towards a sustainable hydrogen economy: Optimisation-based framework for hydrogen infrastructure development",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.08.005",
     "publication date": "07-2017",
     "abstract": "This work studies the development of a sustainable hydrogen infrastructure that supports the transition towards a low-carbon transport system in the United Kingdom (UK). The future hydrogen demand is forecasted over time using a logistic diffusion model, which reaches 50% of the market share by 2070. The problem is solved using an extension of SHIPMod, an optimisation-based framework that consists of a multi-period spatially-explicit mixed-integer linear programming (MILP) formulation. The optimisation model combines the infrastructure elements required throughout the different phases of the transition, namely economies of scale, road and pipeline transportation modes and carbon capture and storage (CCS) technologies, in order to minimise the present value of the total infrastructure cost using a discounted cash-flow analysis. The results show that the combination of all these elements in the mathematical formulation renders optimal solutions with the gradual infrastructure investments over time required for the transition towards a sustainable hydrogen economy.",
     "keywords": ["Hydrogen economy", "Infrastructure development", "Multi-period spatially-explicit MILP model", "Economies of scale", "Hydrogen transmission and distribution", "CCS"]},
    {"article name": "A multi-scale energy systems engineering approach to residential combined heat and power systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.10.015",
     "publication date": "07-2017",
     "abstract": "Distributed power production systems, such as residential cogeneration, are a promising alternative to the energy and environmentally intensive centralized power production. A key factor in realizing the potential of distributed systems is their operational capabilities under varying conditions and the interactions of the operation with the design configuration. In this work we present a systematic approach for the optimal design, control and operation of residential scale CHP units. Based on out recently presented PAROC framework, the approach features a detailed dynamic model describing the cogeneration of heat and power, combined with conventional and advanced control schemes. It is shown that a simultaneous approach delivers distinct energy, environmental and financial benefits.",
     "keywords": ["Distributed combined heat and power systems", "Design and control optimization", "Multi-parametric programming and control"]},
    {"article name": "Syntheses of sustainable supply networks with a new composite criterion \u2013 Sustainability profit",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.12.003",
     "publication date": "07-2017",
     "abstract": "This study proposes a new concept and a new metric for multi-criteria evaluation of sustainable systems. The new metric, termed \u201csustainability profit\u201d, is composed of economic, environmental and social indicators. Since all of these are expressed by monetary terms, the different criteria are now merged, and a multi-objective optimization problem can be reduced to a single-objective one. The new concept for measuring sustainability is based on micro-economic (company\u2019s viewpoint) and on wider macro-economic perspectives (combined government\u2019s and company\u2019s viewpoint). The concept and metric presented are illustrated by three examples of supply networks including a large-scale biorefinery supply network. The obtained results give the insights into sustainable technologies from the overall sustainability viewpoint, and also evaluate the stimulations from governments in the form of subsidies and taxes for deployment of (un-)sustainable systems. The results also indicate that this metric provides good compromise solutions between economic, environmental and social pillars of sustainability.",
     "keywords": ["Sustainability profit", "Economic", "Environmental and social sustainability", "Multi-objective optimization", "Composite criterion", "Macro- and microeconomic perspectives", "Supply network"]},
    {"article name": "A bi-criteria optimization model to analyze the impacts of electric vehicles on costs and emissions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.026",
     "publication date": "07-2017",
     "abstract": "Electric vehicles (EV) are emerging as a mobility solution to reduce emissions in the transportation sector. The studies environmental impact analysis of EVs in the literature are based on the average energy mix or pre-defined generation scenarios and construct policy recommendations with a cost minimization objective. However, the environmental performance of EVs depends on the source of the marginal electricity provided to the grid and single objective models do not provide a thorough analysis on the economic and environmental impacts of EVs. In this paper, these gaps are addressed by a four step methodology that analyzes the effects of EVs under different charging and market penetration scenarios. The methodology includes a bi-criteria optimization model representing the electricity market operations. The results from a real-life case analysis show that EVs decrease costs and emissions significantly compared to conventional vehicles.",
     "keywords": ["Electric vehicles", "Impact analysis", "Greenhouse gases", "Bi-criteria optimization"]},
    {"article name": "Municipal solid waste to liquid transportation fuels, olefins, and aromatics: Process synthesis and deterministic global optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.07.024",
     "publication date": "07-2017",
     "abstract": "This paper proposes a comprehensive process superstructure-based approach toward the sustainable production of liquid transportation fuels, olefins, and aromatics from municipal solid waste, MSW. A deterministic global optimization based branch-and-bound algorithm is utilized to solve the resulting nonconvex mixed-integer nonlinear optimization model. Several novel, commercial, and competing technologies are modeled within the proposed framework. The production of higher-value hydrocarbons proceeds through a synthesis gas intermediate that can be subsequently converted via Fischer\u2013Tropsch refining or methanol synthesis. Simultaneous heat, power, and water integration is included in every process design to minimize utility costs. For every proposed process design, two profitability metrics, the overall profit and the net present value, are calculated. The optimal process topologies that produce liquid fuels and high-value chemicals at the highest profit are illustrated for several case studies. The effects of refinery scale and composition of products produced on the overall profit and the selected process topology are investigated. The effect that the tipping fee of MSW has on the overall profitability of the process is investigated parametrically for several values. Complete material, energy, carbon, and greenhouse gas balances are additionally provided for each case study investigated. The results suggest that production of liquid fuels, olefins, and aromatics is profitable at the highest scales (i.e., 5 thousand barrels per day of liquid fuels and 500 metric tons per day of chemicals) investigated with superior environmental performance compared to petroleum-based processes.",
     "keywords": ["Municipal solid waste", "Process synthesis", "Global optimization", "Mathematical modeling", "Mixed-integer nonlinear optimization", "Aromatics", "Olefins"]},
    {"article name": "New performance indicators for adsorbent evaluation derived from a reduced order model of an idealized PSA process for CO2 capture",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.021",
     "publication date": "07-2017",
     "abstract": "As adsorption is being considered as a candidate technology for CO2 capture, a variety of new adsorbents are being developed, such as those based on novel materials like metal-organic frameworks (MOFs) and zeolitic imidazolate frameworks (ZIFs). New adsorbents are typically evaluated based on measures like working capacity and selectivity, which can be easily calculated using isotherm data. However, these indicators do not directly consider the adsorbents' performance in their end application, which typically is the pressure swing adsorption (PSA) process. Motivated by this, this work presents alternative indicators of adsorbent material\u2019s performance evaluated specifically in the context of the PSA process. In order to keep their use simple and general, the proposed indicators are formulated assuming an idealized PSA process. Analytical expressions are derived wherever possible. In a case study involving several real adsorbents, the indicator values are compared to the results from the dynamic simulation of the PSA process and to the conventional working capacity and selectivity values.",
     "keywords": ["Sustainability", "Pressure swing adsorption", "Material evaluation", "CO2 capture"]},
    {"article name": "Economic evaluation of bio-based supply chains with CO2 capture and utilisation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.09.007",
     "publication date": "07-2017",
     "abstract": "Carbon capture and storage (CCS) and carbon capture and utilisation (CCU) are acknowledged as important R&D priorities to achieve environmental goals set for next decades. This work studies biomass-based energy supply chains with CO2 capture and utilisation. The problem is formulated as a mixed-integer linear program. This study presents a flexible supply chain superstructure to answer issues on economic and environmental benefits achievable by integrating biomass-coal plants, CO2 capture and utilisation plants; i.e. location of intermediate steps, fraction of CO2 emissions captured per plant, CO2 utilisation plants' size, among others. Moreover, eventual incentives and environmental revenues will be discussed to make an economically feasible project. A large-size case study located in Spain will be presented to highlight the proposed approach. Two key scenarios are envisaged: (i) Biomass, capture or utilisation of CO2 are not contemplated; (ii) Biomass, capture and CO2 utilisation are all considered. Finally, concluding remarks are drawn.",
     "keywords": ["Mathematical modelling", "MILP", "NPV", "Biomass co-combustion", "Coal power plants", "Bio-based CCU supply chain"]},
    {"article name": "Sustainable batch process retrofit design under uncertainty\u2014An integrated methodology",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.040",
     "publication date": "07-2017",
     "abstract": "This work explores the problem of optimizing batch processes\u2019 retrofit design pursuing an increase of production efficiency and efficacy while accounting for the associated processes\u2019 sustainability, aspect that within the process industry is nowadays of increasing concern. Batch processes are often complex structures, where retrofitting is not always a straightforward procedure for the decision maker, requiring then supporting tools. Such tools are often characterized by having to deal with large scale optimization problems, where solutions are difficult to obtain. Moreover, the presence of uncertainty adds to the already complex problems an extra level of difficulty. Within this context, the present work proposes an integrated solution approach, uSB-Design, which aims to guide decision-makers in the definition of sustainable batch retrofit designs under uncertainty. The applicability of the proposed methodology is shown through a batch plant case study.",
     "keywords": ["Retrofit", "Batch", "SustainPro", "Optimization", "Uncertainty"]},
    {"article name": "A novel MILP approach for simultaneous optimization of water and energy: Application to a Canadian softwood Kraft pulping mill",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.043",
     "publication date": "07-2017",
     "abstract": "An optimization methodology based on Mixed Integer Linear Programming (MILP) has been developed for simultaneous optimization of water and energy (SOWE) in industrial processes. The superstructure integrates non-water process thermal streams and optimizes the consumption of water, while maximizing internal heat recovery to reduce thermal utility consumption. To address the complexity of water and energy stream distribution in pulp and paper processes, three features have been incorporated in the proposed SOWE method: (a) Non-Isothermal Mixing (NIM) has been considered through different locations to reduce the number of thermal streams and decrease the investment cost by avoiding unnecessary investment on heat exchangers; (b) the concept of restricted matches combined with water tanks has been added to the superstructure; and (c) the Integer-Cut Constraint technique has been combined with the MILP model to systematically generate a set of optimal solutions to support the decision-making for cost-effective configurations. The performance of the proposed improved MILP approach has been evaluated using several examples from the literature and applied to a Canadian softwood Kraft pulping mill as an industrial case study. The results indicate that this approach provides enhanced key performance indicators as compared to conceptual and non-linear complex mathematical optimization approaches.",
     "keywords": ["Mathematical programming", "Combined mass and heat optimization", "Non-isothermal mixing", "Process integration", "Industrial application"]},
    {"article name": "Process flowsheet optimization of chemicals production from biomass derived glucose solutions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.09.012",
     "publication date": "07-2017",
     "abstract": "A general framework of process flowsheet optimization is presented that is based on the implementation of surrogate-based models to represent the detailed units. The approach is applied on the production of hydroxymethylfufural (HMF), levulic Acid (LA) and formic Acic (FA) from biomass derived glucose solutions. Two production alternatives are considered and compared including reactive extraction and reactive adsorption. It is found that the products produced from reactive extraction are slightly cheaper than that from reactive adsorption while the capital investment of reactive adsorption is much lower than reactive extraction. Reactive extraction is more economical in terms of HMF production. In both cases, the dominant cost is from glucose feedstock and utility consumption. Higher concentration of biomass feedstock, higher reaction temperature and lower residence time is favored to decrease the minimum production cost.",
     "keywords": ["Flowsheet optimization", "Surrogate-based optimization", "Biomass", "Glucose", "Hydroxymethylfufural", "Levulic acid"]},
    {"article name": "Optimal match between heat source and absorption refrigeration",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.003",
     "publication date": "07-2017",
     "abstract": "In process industries, there is usually a great amount of waste heat available at different temperatures, and at the same time, there are cooling or refrigeration demands at different temperatures. In this paper, a single effect water\u2013lithium bromide absorption refrigeration system is modeled and simulated using the process modeling software Aspen Plus. The optimal matches between heat source temperatures and refrigeration levels of the absorption refrigeration cycle are determined. The performance of the absorption refrigeration cycle is assessed in terms of two indicators: coefficient of performance (COP) and exergy efficiency. At a certain evaporator temperature of the absorption refrigeration cycle, which indicates a certain refrigeration level, the COP of the cycle rises rapidly at first and then gently with increasing heat source temperature because higher temperature generates more refrigerant vapor. The exergy efficiency of the cycle, by contrast, exhibits a maximum value because both the system performance and the system irreversibility increase with increasing heat source temperature. Ensuring a proper match between heat source and absorption refrigeration can lead to efficient use of waste heat and decrease degradation loss of waste heat.",
     "keywords": ["Absorption refrigeration", "Coefficient of performance", "Exergy efficiency", "Temperature of heat source", "Refrigeration level"]},
    {"article name": "A weighted finite volume scheme for multivariate aggregation population balance equation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.011",
     "publication date": "06-2017",
     "abstract": "For solving multivariate aggregation population balance equation, two new discretizations based on number and mass distributions are presented. These proposed schemes are relatively simple to implement and computationally very efficient. Moreover, the mathematical structure of the schemes remains unchanged with the change in dimension. Hence, these methods are ideally suited for solving multidimensional aggregation problems on non-uniform grids. The accuracy of the new schemes is shown by comparing number density as well as different order moments with exact results as well as with the numerical results of Forestier and Mancini (2012). Besides preservation of the zeroth and first order moments, the new schemes also predict higher order moments and number density very accurately. In conclusion, the proposed schemes are more accurate and efficient than the scheme of Forestier and Mancini (2012).",
     "keywords": ["Population balances", "Aggregation", "Multivariate", "Weighted finite volume scheme", "Non-uniform meshes"]},
    {"article name": "Modeling pore processes for particle-resolved CFD simulations of catalytic fixed-bed reactors",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.029",
     "publication date": "06-2017",
     "abstract": "The most rigorous description of catalytic fixed-bed reactors is particle-resolved CFD simulations. Pore processes can have a large effect toward reactor performance, since they limit internal mass transport. In this study, three pore models of different complexity are incorporated into the CFD software STAR-CCM+. Instantaneous diffusion, effectiveness-factor approach, and three-dimensional reaction\u2013diffusion model are validated firstly with experimental data of CO oxidation in a stagnation-flow reactor from Karadeniz et al. (2013). The 3D approach predicts the experiments with high accuracy over the entire temperature range followed by the computationally cheaper effectiveness-factor. In a second study, the effect of the three pore models is evaluated on a catalytic single sphere with Reynolds numbers varying between 10 and 2000. The local interplay between kinetics and transport phenomena are quantified. For all investigated cases internal mass transfer limitations are larger than external ones. This study can be applied for entire packed-bed simulations.",
     "keywords": ["CFD", "Modeling", "Catalysis", "Fixed-bed reactor", "Pore process", "External and internal mass transfer"]},
    {"article name": "Comparative analysis of data mining and response surface methodology predictive models for enzymatic hydrolysis of pretreated olive tree biomass",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.008",
     "publication date": "06-2017",
     "abstract": "The production of biofuels is a process that requires the adjustment of multiple parameters. Performing experiments in which these parameters are changed and the outputs are analyzed is imperative, but the cost of these tests limits their number. For this reason, it is important to design models that can predict the different outputs with changing inputs, reducing the number of actual experiments to be completed. Response Surface Methodology (RSM) is one of the most common methods for this task, but machine learning algorithms represent an interesting alternative. In the present study the predictive performance of multiple models built from the same problem data are compared: the production of bioethanol from lignocellulosic materials. Four machine learning algorithms, including two neural networks, a support vector machine and a fuzzy system, together with the RSM method, are analyzed. Results show that Reg-CO2RBFN, the method designed by the authors, improves the results of all other alternatives.",
     "keywords": ["Predictive models", "Data mining", "Enzymatic hydrolisis", "Olive tree biomass"]},
    {"article name": "Thermal modeling of a basin type solar still enhanced by a natural circulation loop",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.033",
     "publication date": "06-2017",
     "abstract": "A numerical simulation of a basin type solar desalination still, improved by a Natural Circulation Loop (NCL), is presented in this paper. A transient thermal model was proposed for forecasting the still thermal-hydraulic behaviour under different operating conditions. The proposed mathematical model is derived from the energy balance equations of the different components of the still, as well as, momentum equation for the humid air flowing in the system. The main heat and mass transfer phenomena having a place in the system are considered by the simulation. An extensive validation of the model has been performed by comparing the simulation results against experimental data obtained during four typical summer days. The comparison shows a reasonable agreement between the simulation results and the experimental data. The results showed that the root mean square error is ranged from 2.7% to 26% and the coefficient of determination is close to 1.0.",
     "keywords": ["Modeling and simulation", "Solar still", "Natural circulation loop"]},
    {"article name": "Data reconciliation for chemical reaction systems using vessel extents and shape constraints",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.003",
     "publication date": "06-2017",
     "abstract": "Concentrations measurements are typically corrupted by noise. Data reconciliation techniques improve the accuracy of measurements by using redundancies in the material and energy balances expressed as relationships between measurements. Since in the absence of kinetic models these relationships cannot integrate information regarding past measurements, they are expressed in the form of algebraic constraints. This paper shows that, even in the absence of a kinetic model, one can use shape constraints to relate measurements at different time instants, thereby improving the accuracy of reconciled estimates. The construction of shape constraints depends on the operating mode of the reactor. Moreover, it is shown that the representation of the reaction system in terms of vessel extents helps identify additional shape constraints. A procedure for deriving shape constraints from measurements is also described. Data reconciliation using both numbers of moles and extents is illustrated via a simulated case study.",
     "keywords": ["Chemical reaction systems", "Data reconciliation", "Variants and invariants", "Extents", "Shape constraints"]},
    {"article name": "Model predictive control of solar thermal system with borehole seasonal storage",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.023",
     "publication date": "06-2017",
     "abstract": "This work addresses the model predictive controller design for a complex solar boreal thermal storage system which models a real Drake Landing Solar Commercial Community. The overall discrete system is obtained from a coupled finite and infinite dimensional subsystems of solar collector, heat exchanger, hot tank and gas boilers without any spatial approximation or model reduction. The novel model predictive control addresses a house heat regulation by constrained optimization problem with the manipulation constraints, and accounts for possible unstable system dynamics and disturbances arising from solar and geothermal radiations. The realistic output regulation is considered by the inclusion of an observer which constructs finite and infinite dimensional states. The proposed model development and control regulation can successfully account for the long range variability in environmental and/or economic conditions associated with the overall operational costs of the large scale solar energy community. Finally, the controller performance is assessed by numerical simulations.",
     "keywords": ["Solar thermal system", "Borehole seasonal storage", "Model predictive control", "Output observer", "Coupled PDEs\u2013ODEs system"]},
    {"article name": "Generalized similarity transformation method applied to partial differential equations (PDEs) in falling film mass transfer",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.047",
     "publication date": "06-2017",
     "abstract": "The governing equation describing wetted wall column is partial differential equation (PDE) which can be solved by similarity method. The selection of the combined variable to transform the PDE to an ordinary differential equation is very important subject in this method and mainly is selected based on experiences. But, in this work, a general combined variable was introduced for the PDEs transformation leading to a generalized analytical solution. The obtained analytical solutions were compared to literature and examined using some reported experimental data. According to the results, the deviations of calculated and collected experimental values were small which demonstrates accuracy and reliability of the presented method besides its highly computationally consistency and easy-to-use features.",
     "keywords": ["Similarity transformation", "Modeling", "Falling film", "Absorption", "Solid dissolution"]},
    {"article name": "Neural network based computational model for estimation of heat generation in LiFePO4 pouch cells of different nominal capacities",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.044",
     "publication date": "06-2017",
     "abstract": "Significant variance exists in the nominal capacity of lithium ion (Li-ion) pouch cells used for commercial electric vehicle battery packs. Accurate estimation of heat generation in such cells is critical for designing battery thermal management system. However, multi-physics models describing thermal behaviour of these cells are too complex whereas other numerical models discount the effect of cell capacity on heat generation. This paper proposes a new computational model based on artificial neural network (ANN) for estimating battery heat generation rate with cell nominal capacity as one of its key inputs along with ambient temperature, discharge rate and depth of discharge. A custom-designed calorimeter is utilised for experimentally generating the training dataset for the ANN. Problem of data scarcity is addressed analytically and virtual samples are produced via enthalpy formulation for battery heat generation. Subsequently, the model is trained using Levenberg\u2013Marquardt algorithm. Results disclose that a three-layered feedforward ANN with one hidden layer having six neurons is optimum for this application. The architecture of the trained ANN for accurately simulating thermal behaviour of LiFePO4 pouch cells of the nominal capacities from 8 to 20\u00a0Ah under varied conditions is exemplified.",
     "keywords": ["Feedforward network", "Small dataset", "Calorimeter", "Thermal model", "Battery thermal management system", "Electric vehicles"]},
    {"article name": "Simultaneous synthesis of multi-plant heat exchanger networks using process streams across plants",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.039",
     "publication date": "06-2017",
     "abstract": "Interplant heat integration using process streams is an alternative way to synthesize multi-plant heat exchanger networks (HENs). Compared with conventional way using steam, directly using process stream can recover more heat but with more pipelines. Due to the geographically separated locations of individual plants, distance related factors should be fully considered. In addition, the interaction between interplant heat integration and intra-plant heat integration is not fully addressed in traditional design methods. To counter these limitations, we present a new methodology for multi-plant HENs synthesis directly using process streams. The methodology makes use of a generalized disjunctive programming (GDP) formulation which gives rise to a mixed-integer nonlinear programming (MINLP) problem. For illustration purpose, two literature problems are conducted to verify the applicability of the proposed methodology. Our results can automatically offer the optimal design for multi-plant HENs, which are both more cost-optimal than the ones reported in the cited literatures.",
     "keywords": ["Multi-plant heat integration", "HENs", "Process streams", "Piping cost", "MINLP model"]},
    {"article name": "Uncertainty back-propagation in PLS model inversion for design space determination in pharmaceutical product development",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.038",
     "publication date": "06-2017",
     "abstract": "The inversion of latent-variable models is an effective tool to assist the determination of the design space (DS) of a new pharmaceutical product. A challenging issue in partial least-square (PLS) regression model inversion is to describe how the uncertainty on the model outputs (product quality) relates to the uncertainty on the model inputs (raw material properties and process parameters). In this study, a methodology to relate the uncertainty on the output of a PLS model to the uncertainty on the model inputs is proposed. Two uncertainty back-propagation models are formulated and critically compared. Frequentist confidence regions (CRs) for the solution of the inversion problem are built. These CRs represent a subspace of the historical knowledge space within which the DS of the product to be developed is likely to lie with assigned confidence level. The input combinations that belong to these CRs (and that are consistent with the historical calibration data set) should be primarily investigated when an experimental campaign is to be performed to determine the DS. The proposed methodology is tested on three different case studies, two of which involve experimental data taken from the literature, respectively, on a roller compactor and on a wet granulator. It is shown that both uncertainty back-propagation models are effective in bracketing the DS, with the second model outperforming the first one in terms of shrinkage of the space within which experiments should be carried out to identify the DS.",
     "keywords": ["Quality by design", "Design space", "Partial least-squares", "Latent-variable model inversion", "Pharmaceutical development"]},
    {"article name": "Differential-Algebraic numerical approach to the one-dimensional Drift-Flux Model applied to a multicomponent hydrocarbon two-phase flow",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.045",
     "publication date": "06-2017",
     "abstract": "This paper presents a numerical investigation of the solution of the steady-state one-dimensional Drift-Flux Model. It is proposed that these simulations, though often based on finite-volume discretizations and iterative sequential procedures, are preferably performed using established numerical methods specifically devised for Differential-Algebraic Equations (DAE) systems. Both strategies were implemented in a computer code developed for simulations of multicomponent hydrocarbon two-phase flows. The SIMPLER semi-implicit algorithm was employed in the solution of the finite-volume discretized model in order to provide comparison grounds with the adaptive BDF-implementation of DAE integration package DASSLC. Based on test simulations of a naphtha two-phase flow under varying heat-transfer conditions, the DAE approach was proved highly advantageous in terms of computational requirements and accuracy of results, both in the absence and presence of flow-pattern transitions. Numerical difficulties arising from the latter were successfully worked around by continuously switching regime-specific constitutive correlations using adjustable steep regularization functions.",
     "keywords": ["Two-phase flow", "Drift-Flux Model", "Finite volume method", "Differential-Algebraic Equations systems"]},
    {"article name": "Optimal pumping schedule design to achieve a uniform proppant concentration level in hydraulic fracturing",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.035",
     "publication date": "06-2017",
     "abstract": "We present a novel design framework of an optimal and practical pumping schedule to achieve uniform proppant concentration across fracture at the end of pumping. By using the average viscosity to approximate concentration dependence of fracture propagation, a set of constant-concentration pumping schedules is applied to the developed dynamic model, each of which is carefully chosen by taking into account the practical constraints such as the limit on the change of proppant concentration between pumping stages and the desired fracture geometry that has to be satisfied at the end of pumping for maximum productivity. Then, a practically-feasible target concentration profile is obtained via linear combinations of the generated spatial concentration profiles, and mass balance is applied to the practically-feasible target concentration to calculate the duration of each pumping stage. We apply the generated pumping schedule to the high-fidelity hydraulic fracturing model, and the performance is compared with Nolte's pumping schedule.",
     "keywords": ["Hydraulic fracturing", "Optimal pumping schedule", "Unified fracture design", "Dynamic modeling"]},
    {"article name": "A location-inventory-pricing model in a closed loop supply chain network with correlated demands and shortages under a periodic review system",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.027",
     "publication date": "06-2017",
     "abstract": "We consider a three-level closed loop supply chain for location-inventory-pricing decisions where demands across the customer zones are correlated, the inventory policy is a periodic review (R,T), and face shortages. The problem is to determine the number and the location of the collection and distribution centers (CDCs) and the plants to be opened, the assignment of the customer zones to the CDCs and the CDCs to the plants, the price of the new product, the incentive value of the returned product, the inventory review intervals at the CDCs, and the fraction of the demand backordered during the shortage period at the CDCs such that the total profit is maximized. We propose a nonlinear programming model for this purpose. Since this problem is one of the NP-hard problems three meta-heuristic algorithms are used for solving it namely the genetic algorithm (GA), the imperialist competitive algorithm (ICA), and the firefly algorithm (FA). We present an encoding and decoding procedure to represent solutions. To attain the reliable results in the proposed algorithms, the Taguchi method is utilized for calibrating the parameters of these algorithms. The computational results show that in terms of the objective function value there is no significant difference between the mentioned algorithms but in terms of the CPU time the firefly algorithm compared to the other algorithms needs more time for solving the problem.",
     "keywords": ["Closed loop supply chain", "Location-inventory-pricing", "Periodic review", "Genetic algorithm", "Imperialist competitive algorithm", "Firefly algorithm"]},
    {"article name": "Automatic generation of interlock designs using genetic algorithms",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.042",
     "publication date": "06-2017",
     "abstract": "The hazardous units in a modern chemical plant are often equipped with safety interlocks to mitigate the detrimental effects of potential accidents. To achieve a desired level of reliability (or availability), not only the interlock configuration but also the maintenance policies of its components must be properly stipulated before actual installation. Although a number of deterministic programming models have already been developed for this purpose, their laborious formulation and solution steps are usually carried out on a case-by-case basis. To attain the essential qualities of conciseness, portability and maintainability for easy implementation, there is a definite need to develop a generic and modularized code according to an evolutionary algorithm.The structural and maintenance specifications of an interlock can be represented with: (1) the number of measurement channels and the corresponding alarm logic, (2) the numbers of online and spare sensors in each channel and the corresponding voting gate, (3) the number of shutdown channels and the corresponding tripping configuration, (4) the numbers of online and standby actuators in each channel, and the corresponding activation mechanism, and (5) the inspection intervals of shutdown channels. All of them are encoded in this study with binary numbers for use as inputs to implement the genetic algorithm (GA). An interlock superstructure is also developed to facilitate the search for the best configuration and maintenance plan in any application. By minimizing the overall life-cycle expenditure, a generic MATLAB code has been developed for generating all aforementioned specifications in any application. Four examples are provided to demonstrate the benefits of the proposed optimization approach.",
     "keywords": ["Corrective maintenance", "Preventive maintenance", "Interlock", "Cold-standby", "Spare", "Superstructure", "Genetic algorithm"]},
    {"article name": "Well placement and control optimization for WAG/SAG processes using ensemble-based method",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.020",
     "publication date": "06-2017",
     "abstract": "Compared to the CO2 flooding, the alternative injection of water/surfactant solution and CO2 gas (WAG/SAG) can provide good control over the mobility ratio, stabilize the displacement front and improve the macroscopic sweep efficiency of the flooding. We show that the performance of a WAG/SAG flood can be significantly improved by optimizing the locations and optimizing rates or pressure in order to avoid gas/water channelling and improve sweep efficiency. To maximize the life-cycle net present value (NPV) for a WAG/SAG process, we implemented a steepest ascent algorithm combined with a simplex stochastic gradient to find the optimal well trajectories and controls. Both simultaneous and sequential approaches for the joint optimization of well locations and controls are investigated for a synthetic channelized reservoir model. Well spacing constraints are also included in the optimization process using the penalty method in order to keep the distance between wells greater than a specified minimum value. Four synthetic reservoir problems are studied and optimized to illustrate the viability of the methodology.",
     "keywords": null},
    {"article name": "Simultaneous chemical process synthesis and heat integration with unclassified hot/cold process streams",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.024",
     "publication date": "06-2017",
     "abstract": "We propose a mixed-integer nonlinear programming (MINLP) model for the simultaneous chemical process synthesis and heat integration with unclassified process streams. The model accounts for (1) streams that cannot be classified as hot or cold, and (2) variable stream temperatures and flow rates, thereby facilitating integration with a process synthesis model. The hot/cold stream \u201cidentities\u201d are represented by classification binary variables which are (de)activated based on the relative stream inlet and outlet temperatures. Variables including stream temperatures and heat loads are disaggregated into hot and cold variables, and each variable is (de)activated by the corresponding classification binary variable. Stream inlet/outlet temperatures are positioned onto \u201cdynamic\u201d temperature intervals so that heat loads at each interval can be properly calculated. The proposed model is applied to two illustrative examples with variable stream flow rates and temperatures, and is integrated with a superstructure-based process synthesis model to illustrate its applicability.",
     "keywords": ["Mixed-integer nonlinear programming", "Superstructure optimization"]},
    {"article name": "Scaling-up down flow reactors. CPFD simulations and model validation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.034",
     "publication date": "06-2017",
     "abstract": "The present study reports a scaled-up down flow reactor study using 2.54-cm and 5.08-cm internal diameter units both utilizing Computational Particle Fluid Dynamics (CPFD) modeling and experimental data. The solid particles used are Fluid Catalytic Cracking (FCC) catalyst particles with a 84.42\u00a0\u03bcm mean diameter and a 1722\u00a0kg/m3 particle density. The gas velocity and solid mass flux are varied from 1.1 to 2.0\u00a0m/s and from 7 to 50\u00a0kg/m2\u00a0s, respectively. Experimental data is obtained using a CREC-GS-Optiprobe** sensors to be able to account for individual particle clusters. Data analysis involves the setting of the data baseline in compliance with solid mass balances. On the other hand, a CPFD method is applied to simulate the complex CFB downer fluid dynamics. With the CPFD method, cluster behaviour is extensively studied for a wide set of operating conditions. Cluster sizes are obtained experimentally for each of the operating conditions considered. CPFD simulations are compared with experimental results. Good agreement is observed in all cases, except in the near-wall region. It is also proven in the present study that, near-wall discrepancies between experimental findings and hybrid CPFD modeling predictions are significantly reduced in the 5.08-cm ID downer unit. Thus, it is anticipated that this hybrid CPFD model provides a fully trustable computational method for the scaling-up of industrial down flow reactor units with diameters larger than 5.08-cm ID.",
     "keywords": ["Scale-up analysis", "Computational particle fluid dynamics (CPFD) barracuda software", "Circulating fluidized bed (CFB) downers", "Down flow reactors", "Fluid catalytic cracking (FCC) catalyst", "Fluidization"]},
    {"article name": "Constrained NLP via gradient flow penalty continuation: Towards self-tuning robust penalty schemes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.034",
     "publication date": "06-2017",
     "abstract": "This work presents a new numerical solution approach to nonlinear constrained optimization problems based on a gradient flow reformulation. The proposed solution schemes use self-tuning penalty parameters where the updating of the penalty parameter is directly embedded in the system of ODEs used in the reformulation, and its growth rate is linked to the violation of the constraints and variable bounds. The convergence properties of these schemes are analyzed, and it is shown that they converge to a local minimum asymptotically. Numerical experiments using a set of test problems, ranging from a few to several hundred variables, show that the proposed schemes are robust and converge to feasible points and local minima. Moreover, results suggest that the GF formulations were able to find the optimal solution to problems where conventional NLP solvers fail, and in less integration steps and time compared to a previously reported GF formulation.",
     "keywords": ["Gradient flow", "Nonlinear programming problem", "Convergence analysis"]},
    {"article name": "Solution methods for vehicle-based inventory routing problems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.036",
     "publication date": "06-2017",
     "abstract": "A novel method for solving vehicle-based inventory routing problems (IRPs) under realistic constraints is presented. First, we propose a preprocessing algorithm that reduces the problem size by eliminating customers and network arcs that are irrelevant for the current horizon. Second, we develop a decomposition method that divides the problem into two subproblems. The upper level subproblem considers a simplified vehicle routing problem to minimize the distribution cost while satisfying minimum demands, which are calculated based on consumption rate, initial inventory and safety stock. In the lower level, a detailed schedule with drivers is acquired using a continuous-time MILP model, by adopting the routes selected from the upper level. Finally, an iterative approach based on the upper and lower levels is presented, including the addition of different types of integer cuts and parameter updates. Different options of implementing this iterative approach are discussed, and computational results are presented.",
     "keywords": ["Vendor managed inventory", "Mixed-integer programming", "Network reduction algorithm", "Decomposition method"]},
    {"article name": "New a priori and a posteriori probabilistic bounds for robust counterpart optimization: II. A priori bounds for known symmetric and asymmetric probability distributions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.07.002",
     "publication date": "06-2017",
     "abstract": "When optimization problems contain uncertain parameters, their nominal solutions may prove to be overly optimistic or even rendered infeasible given the actual parameter realizations. The application of probabilistic bounds in constructing the robust counterpart formulation of a model under uncertainty can greatly reduce the conservatism of traditional worst-case robust optimization. In Part I, we derived new a priori and a posteriori bounds on the probability of constraint violation for constraints with uncertain parameters whose distributions were unknown. Here, we first present new a priori bounds applicable to uncertain constraints with linearly participating uncertain parameters whose distributions are known or conservatively approximated. We then extend the robust counterpart optimization methodology by allowing attributed known distributions to be symmetric or asymmetric. The new methods greatly reduce the conservatism and significantly augment the performance and applicability of robust counterpart optimization. A mixed-integer linear optimization example and a multiperiod planning problem demonstrate the improvements of the new a priori bounds relative to existing bounds.",
     "keywords": ["Robust counterpart optimization", "Optimization under uncertainty", "Probabilistic bounds", "Mathematical modeling"]},
    {"article name": "Planning and scheduling of steel plates production. Part II: Scheduling of continuous casting",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.01.020",
     "publication date": "06-2017",
     "abstract": "Production planning and scheduling in the steel industry are challenging problems due to large number of products being produced. This work deals with scheduling of the continuous casting of steelmaking, i.e. determination of the number of fixed capacity pots of each grade of steel and the charge sequence in each casting machine. Since a huge number of binary variables make the full-space model mixed-integer linear programming model computationally intractable, we propose a two-level algorithm. At the top level, we solve the planning problem which determines the number of pots of each grade for every planning period by solving the relaxed mixed integer linear model. At the lower level, the scheduling problem is solved by an algorithm which combines ideas from parallel simulated annealing and shuffled frog-leaping algorithm. Real-world steel-plate production data are utilized to demonstrate the effectiveness of the proposed algorithm.",
     "keywords": ["Steel plates production planning and scheduling", "Long term scheduling", "Two level algorithm", "Parallel simulated annealing with leaping"]},
    {"article name": "On-the-fly pruning for rate-based reaction mechanism generation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.003",
     "publication date": "05-2017",
     "abstract": "The number of possible side reactions and byproduct species grows very rapidly with the size of a chemical mechanism. A memory-efficient algorithm for automated mechanism generation is presented for coping with this combinatorial complexity. The algorithm selects normalized flux as a metric to identify unimportant species during model generation and prunes them with their reactions, without any loss of accuracy. The new algorithm reduces memory requirements for building kinetic models with 200\u2013300 species by about a factor of 4, or for fixed computer hardware makes it possible to create models including about twice as many species as was previously possible. The increased capability opens the possibility of discovering unexplored reaction networks and modeling more complicated reacting systems.",
     "keywords": ["Memory reduction", "Mechanism generation", "Pruning"]},
    {"article name": "Economic comparison of a reactive distillation-based process with the conventional process for the production of ethyl tert-butyl ether (ETBE)",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.008",
     "publication date": "05-2017",
     "abstract": "This work addresses the economic comparison of a process based on reactive distillation (RD) with the conventional process for ETBE production. Nonlinear first-principles models for both processes were developed and implemented in gPROMS ModelBuilder, with the corresponding physical properties being calculated in Aspen Plus, connected through the CAPE-OPEN interface. The design of the RD column was performed using an optimisation-based strategy, which couples an evolutionary algorithm with a nonlinear solver.The RD-based process studied eliminated the need for the top ethanol fresh feed, thus eliminating the need for an ethanol recovery section. The payback period was found to be 3 years, if the RD column, condenser and reboiler are purchased and only 1.5 years if only the cost of purchasing a RD column is considered. The advantages of the RD-based process were found to be the higher yield of ETBE product, as well as lower costs of ethanol and steam.",
     "keywords": ["Reactive distillation", "Economic comparison", "Ethyl tert-butyl ether", "Optimisation-based design"]},
    {"article name": "Comparison of heterogeneous azeotropic distillation and extractive distillation methods for ternary azeotrope ethanol/toluene/water separation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.007",
     "publication date": "05-2017",
     "abstract": "Ethanol and toluene are important organic solvents in chemical industry. Ethanol, toluene and water present a ternary minimum-boiling azeotrope and three binary azeotropes. In this article, two methods are studied to separate the ternary azeotrope: heterogeneous azeotropic distillation using the toluene, which is in the system itself, as entrainer and extractive distillation with glycerol as the only one solvent. In the heterogeneous azeotropic distillation, the pressures of the three columns are changed respectively to achieve heat integration. In the extractive distillation, only one solvent and two columns are used to separate the ternary mixture. Experiment shows that the partially heat-integrated heterogeneous azeotropic distillation reduces the energy cost and total annual cost by 27.7% and 13.4% respectively compared with the conventional heterogeneous azeotropic distillation. Unexpectedly, the extractive distillation can save 18.8% and 39.3% in the energy cost and total annual cost respectively compared with the partially heat-integrated heterogeneous azeotropic distillation.",
     "keywords": ["Ternary azeotrope", "Heterogeneous azeotropic distillation", "Extractive distillation", "Ethanol/toluene/water"]},
    {"article name": "Design of optical emission spectroscopy based plasma parameter controller for real-time advanced equipment control",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.009",
     "publication date": "05-2017",
     "abstract": "With the advent of more than Moore\u2019s law era, control of plasma etch process is expected to become inevitable. Given that highly complex plasma is a medium of etch processes, plasma parameters are key factors to be controlled. In addition, highly interactive plasma characteristics require multivariate control schemes. In this paper, we design a multi-loop controller which controls effective plasma parameters in Ar plasma conditions. The effective plasma parameters obtained by optical emission spectroscopy are paired with plasma reactor instrumental variables through relative gain array and singular value decomposition. Each single input-single output (SISO) system based on the pairing result shows successful disturbance rejection performance but interactions between SISO controllers occur. In order to handle the interactions, 2\u00a0\u00d7\u00a02 multiple input-multiple output (MIMO) controllers with and without decouplers are simulated to track set point change. Based on the simulations, a MIMO controller with decouplers is implemented in a capacitively coupled plasma reactor and show feasible control performance without interaction. Hopefully, the results introduced in this paper contribute to making progress in plasma parameter control.",
     "keywords": ["Plasma parameter control", "Variable selection", "Multiple input multiple output", "Optical emission spectroscopy", "Relative gain array", "Singular value decomposition"]},
    {"article name": "Scope and limitations of the irreversible thermodynamics and the solution diffusion models for the separation of binary and multi-component systems in reverse osmosis process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.001",
     "publication date": "05-2017",
     "abstract": "Reverse osmosis process is used in many industrial applications ranging from solute-solvent to solvent\u2013solvent and gaseous separation. A number of theoretical models have been developed to describe the separation and fluxes of solvent and solute in such processes. This paper looks into the scope and limitations of two main models (the irreversible thermodynamics and the solution diffusion models) used in the past by several researchers for solute-solvent feed separation. Despite the investigation of other complex models, the simple concepts of these models accelerate the feasibility of the implementation of reverse osmosis for different types of systems and variety of industries. Briefly, an extensive review of these mathematical models is conducted by collecting more than 70 examples from literature in this study. In addition, this review has covered the improvement of such models to make them compatible with multi-component systems with consideration of concentration polarization and solvent-solute-membrane interaction.",
     "keywords": ["Reverse osmosis", "Modelling", "The irreversible thermodynamics model", "The solution diffusion model"]},
    {"article name": "The flow and heat transfer characteristics of superheated steam in offshore wells and analysis of superheated steam performance",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.045",
     "publication date": "05-2017",
     "abstract": "In this paper, a novel model is presented for predicting thermophysical properties of superheated steam (SHS) in offshore superheated steam injection wells (OSSIW).Firstly, a mathematical model comprised of mass, energy and momentum balance equations is proposed for OSSIW. Secondly, the distribution of temperature, pressure and superheat degree along the wellbores are obtained by finite difference method on space and solved by using the iteration technique. Then, model validation and sensitivity analysis are conducted. The results show that (1). The superheat degree increases at first but then turns to decrease with the increase of injection rate. (3). The superheat degree increases with the increase of injection temperature. (4). The superheat degree at well bottom increases at first but then turns to decrease with the increase of injection pressure. (5). The superheat degree in the wellbore has a drop when seawater starts to flow.",
     "keywords": ["Wellbore modeling", "Numerical simulation", "Superheated steam", "Thermophysical properties", "Turbulent flow of seawater", "Offshore heavy oil recovery"]},
    {"article name": "A multi-objective optimization model for gas pipeline operations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.017",
     "publication date": "05-2017",
     "abstract": "Natural gas is usually transported by pipeline networks. This paper presents a multi-objective optimization model in optimizing the operation of natural gas pipeline networks. A mathematical model is established for different network configurations, namely linear, branch and looped topologies. Two conflicting objectives, power consumption minimization and gas delivery flow rate maximization, are considered subjected to pipeline and compressor constraints. The decision variables are the pressure at the nodes and the rotational speed of the compressors. A set of constraints is developed based on gas flow equations and compressors operating conditions. Due to the nonlinearity of the constraints and the objective, the developed model is a Nonlinear Programming problem. The optimization of the models is performed with NSGA-II algorithm. The solution obtained is a set of Pareto optimal points from which a decision maker can select a specific preferred solution. Sensitivity analysis was performed to determine the impact of parameter changes.",
     "keywords": ["Natural gas", "Gas pipeline network", "Multi-objective optimization", "NSGA-II"]},
    {"article name": "Modeling and simulation of forward osmosis process using agent-based model system",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.005",
     "publication date": "05-2017",
     "abstract": "In recent years forward osmosis (FO) process has attracted considerable attention in the field of water treatment and desalination. However, the lack of sufficient information on all possible parameters, affecting the process performance, is a major obstacle that avoids improving the function of this method. In this study, an algorithm based upon a reliable model was developed for osmosis through asymmetric membrane oriented in FO mode to be applied in agent-based model. This model was built in the NetLogo platform, which is a programmable modeling environment for simulating natural phenomena. The feasibility of this modeling was proven by comparing the simulation results and empirical data obtained from literature. The influence of net bulk osmotic pressure difference between the draw solution (DS) and the feed solution (FS) on average water flux was studied for various draw solutes. Furthermore, the effects of process parameters, including temperature, length of module, FS cross-flow velocity, pure water permeability coefficient, and structural parameter of membrane on average water flux were investigated. As a result, it could be concluded that Netlogo agent-based model had the quite well potential to simulate the FO process.",
     "keywords": ["Agent-based model", "Forward osmosis", "FO mode", "NetLogo", "Simulation"]},
    {"article name": "Adaptive design of experiments for model order estimation in subspace identification",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.028",
     "publication date": "05-2017",
     "abstract": "The first step in subspace methods for identification of multivariable systems is the estimation of the order of the model to be identified. Model order estimation is especially difficult for ill-conditioned systems. In previous work we showed heuristically that appropriately designed experiments with rotated PRBS inputs greatly facilitate model order estimation, hence overall model accuracy. However, design of such experiments depends on the very system to be identified. To overcome that difficulty, in this paper we propose an adaptive design of experiments. The proposed approach follows rigorous justification of the need for rotated PRBS inputs, and is tested through computer simulations on two case studies involving a high-purity distillation column and a fluidized catalytic cracking unit. Comparisons of the approach to open- and closed-loop alternatives are presented, and suggestions for further development are made.",
     "keywords": null},
    {"article name": "Dynamic metabolic modeling of lipid accumulation and citric acid production by Yarrowia lipolytica",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.013",
     "publication date": "05-2017",
     "abstract": "Yarrowia lipolytica has the capacity to accumulate large amounts of lipids triggered by a depletion of nitrogen in excess of carbon source. However, under similar conditions this yeast also produces citric acid decreasing the lipid conversion yield. Three dynamic metabolic models are presented to describe lipid accumulation and citric acid production by Yarrowia lipolytica. First and second models were respectively based on the Hybrid Cybernetic Modeling (HCM) and the Macroscopic Bioreaction Modeling (MBM) approaches. The third model was a new approach based on the coupling between MBM and fuzzy sets. Simulation results of the three models fitted acceptably the experimental data sets for calibration and validation. However, MBM is time-dependent to consider metabolic shifts, and thus impractical for further applications. HCM and Fuzzy MBM adequately managed and described metabolic shifts presenting highlighting features for control and optimization. HCM and Fuzzy MBM were statistically compared reflecting similar results.",
     "keywords": ["Dynamic metabolic modeling", "Elementary modes (EMs)", "Hybrid Cybernetic Model (HCM)", "Macroscopic Bioreaction Model (MBM)", "Lipid accumulation", "Yarrowia lipolytica"]},
    {"article name": "Holistic framework for land settlement development project sustainability assessment: Comparison of El Hierro Island hydro wind project and Sivens dam project",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.002",
     "publication date": "05-2017",
     "abstract": "Project developer in the domain of land settlement project are involved with many stakeholders and are usually overflown by data relative to technical, economic and social issues. This paper contributes to the necessary multi-scale approach challenge and we propose a holistic framework that enables to describe the development process of land settlement project and assess its sustainability. It would help developers to take decisions compliant with the project complexity. In the model driven engineering perspective, the metamodel framework is described with the ISO 19440 four views to represent complex systems: architectural, structural, functional and behavioural. We confront it to describe two case studies: the successful project of hydro-wind power plant in El Hierro in the Canaries, and the Sivens Dam project in France sadly famous for its deadly outcome. Their comparison enables us to draw hypothesis on what are the ingredients of success and validate the framework.",
     "keywords": ["Framework", "Eco-energy system", "Multi-stakeholders", "Multiperspective design"]},
    {"article name": "Designing a Reliable Multi-Objective Queuing Model of a Petrochemical Supply Chain Network under Uncertainty: A Case Study",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.12.012",
     "publication date": "05-2017",
     "abstract": "This paper shows a multi-objectives mixed-integer non-linear programming (MINLP) model for a petrochemical supply chain under uncertainty environments, namely disruption risks and less knowledge of parameters. In this model, two efficient queuing systems are applied in nylon plastic manufacturing and recycling centers, in which a Jackson network is also used. The aims are to minimize the average tardiness to deliver products, total cost and transportation cost. In addition the developed model specifies the optimal locations for a new distribution center (DC), collection center and disposal center as well as the optimal allocation of customer zones to each DC. This model is solved in three stages: (1) a Jackson network determines the queuing parameters, (2) an Lp-metric approach makes the multi-objectives into a single objective, and (3) an efficient Lagrangian relaxation based on a sub-gradient approach solves the presented model. Additionally, a real case study is shown in details to depict the application of the presented model. At the end, the sensitivity analyses are carried out to check the optimal objectives by changing the important parameters.",
     "keywords": ["Closed-loop petrochemical supply chain", "Robust optimization", "Queuing theory", "Jackson network", "Lagrangian relaxation", "Uncertainty"]},
    {"article name": "Modeling of the maleic anhydride circulating fluidized bed reactor",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.012",
     "publication date": "05-2017",
     "abstract": "Maleic Anhydride (MA) production by selective oxidation of n-butane in a Multi-Tubular Fixed Bed reactor is constrained by the flammability limits. The use of Fluidized Bed circumvents this constraint but suffers from high back mixing. The Circulating Fluidized Bed (CFB) reduces the back mixing of the catalyst and enhances the catalyst activity by reactivating the catalyst. A simple reactor model with suitable hydrodynamic and kinetic models at the operating conditions of both the pilot and commercial MA CFB reactors is proposed. A dispersion model with variable gas density is used for modeling the hydrodynamics of these reactors. The reactors' configurational complexities are also accounted for in the model. The data fitting exercise of the proposed reactor model is performed. The proposed method simultaneously minimizes the total bias based on the individual biases for each component of the multi component reactor system and the total error in a multi-objective framework.",
     "keywords": ["Circulating fluidized bed", "Maleic anhydride", "Reactor modeling", "Variable density reactor", "Data fitting exercise", "Multi-objective optimization"]},
    {"article name": "A semantic framework for enabling model integration for biorefining",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.02.004",
     "publication date": "05-2017",
     "abstract": "This paper introduces a new paradigm for establishing a framework that enables interoperability between process models and datasets using ontology engineering. Semantics are used to model the knowledge in the domain of biorefining including both tacit and explicit knowledge, which supports registration and instantiation of the models and datasets. Semantic algorithms allow the formation of model integration through input/output matching based on semantic relevance between the models and datasets. In addition, partial matching is employed to facilitate flexibility to broaden the horizon to find opportunities in identifying an appropriate model and/or dataset. The proposed algorithm is implemented as a web service and demonstrated using a case study.",
     "keywords": ["Ontology engineering", "Model integration", "Computer aided process engineering (CAPE)", "Biorefining"]},
    {"article name": "Integrating hydrodynamics and biokinetics in wastewater treatment modelling by using smoothed particle hydrodynamics",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.12.020",
     "publication date": "04-2017",
     "abstract": "In this work a novel integrated biokinetic wastewater treatment (WWT) model based on the smoothed particle hydrodynamics (SPH) method as hydraulic model is proposed. Significant outcomes of this work are the computation of a detailed spatial distribution of compounds in WWT basins and the quantification of the effects of the stirrer induced mixing on the evolution of compounds. SPH is a meshfree particle method which herein is applied to compute a treatment plant's reactor hydraulics. The characteristic feature of SPH is to describe a fluid's dynamics by particles that move along with the flow. The present WWT model takes advantage of this principle by assigning the compounds\u2019 concentrations, which are defined by the activated sludge model no. 1, to SPH particles. A full-scale treatment plant simulation with a variable stirrer induced mixing intensity in the denitrification basin is performed. A spatial distribution of the compounds\u2019 concentrations, which up to now was unknown for unsteady flows, is computed and analysed. The developed framework is generic and therefore expected to be applicable for modelling chemical engineering processes which are influenced by hydrodynamic effects.",
     "keywords": ["Activated sludge model", "CFD in wastewater treatment", "Modelling of pollutant compounds", "Process dynamics", "Smoothed particle hydrodynamics method", "Wastewater treatment simulation"]},
    {"article name": "Practical optimization for cost reduction of a liquefier in an industrial air separation plant",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.12.011",
     "publication date": "04-2017",
     "abstract": "Commercial and in-house simulation software used by industrial practitioners are often of a \u201cblack box\u201d type from which derivatives cannot be directly obtained. This paper demonstrates a linkage between available industrial tools and cost reduction opportunity creation through the application of a derivative-free optimization technique. An operational liquefier in an air separation unit is used in our study due to the increasing importance of liquid production in the plant's overall operation strategy, and limited evaluation on the operation of such systems under disturbances. Particle swarm optimization is implemented, and optimization results show that when the plant is forced to operate away from its nominal operating/design conditions, it is possible to reduce the unit power consumption by adjusting different operation set-points. A reference map is generated to guide the operation under selected realizations of cooling water temperature, production load and feed conditions.",
     "keywords": ["Nitrogen liquefier", "Particle swarm optimization", "Operation under disturbances", "Simulation-based optimization", "Industrial application"]},
    {"article name": "Design of a chemical batch plant with parallel production lines: Plant configuration and cost effectiveness",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.12.013",
     "publication date": "04-2017",
     "abstract": "We present a model for the design of multiproduct sequential batch plants extended with parallel production lines. This model is meant to support strategic capacity decisions and is formulated as a MILP model. First, we introduce the concept of parallel production lines as a new design option into existing plant design models. Then, we optimise the number of production lines, their design and the product assignment to the installed lines by minimising capital costs of the equipment. Furthermore, we extend the objective function with startup and contamination costs and study the influence of these costs on the chosen plant design options. We find the presence of parallel production lines beneficial as not all products have to share all equipment anymore. Moreover, we show that the incorporation of operating costs affects volume-wise asset utilisation per batch. An example to illustrate the applicability of our model is presented.",
     "keywords": ["Chemical batch plant", "Plant design", "Parallel production lines"]},
    {"article name": "Modeling the effect of aging on the electrical and thermal behaviors of a lithium-ion battery during constant current charge and discharge cycling",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.006",
     "publication date": "04-2017",
     "abstract": "This paper reports a two-dimensional modeling to predict the aging effect on the variation of the electrical and thermal behaviors of a lithium-ion battery (LIB) cell under the constant current (CC) charge and discharge cycling over a long time. To account for the aging effects of the LIB cell due to cycling, the key modeling parameters are expressed as a function of cycling number. In order to validate the modeling methodology introduced in this paper, the modeling results towards the changes of the discharge curves and two-dimensional temperature distributions of the aged LIB cell during CC discharge at different C rates are compared with the experimental measurements after every thousand cycles. The electrical and thermal behaviors predicted by the modeling for the aged LIB cell show good agreement with the experimental data.",
     "keywords": ["Lithium-ion battery", "Battery modeling", "Battery aging", "Cycling test"]},
    {"article name": "Dynamic modelling and optimization of an LNG storage tank in a regasification terminal with semi-analytical solutions for N2-free LNG",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.012",
     "publication date": "04-2017",
     "abstract": "A comprehensive dynamic model for an LNG storage tank in a typical regasification terminal, operating in holding mode, is presented. The model incorporates LNG recirculation for cooling the transfer lines for loading/unloading. It assumes a hypothetical thin vapour interface in equilibrium with the liquid to compute LNG evaporation, which allows the boil-off gas to be hotter than the tank LNG, as observed in practice. A numerical procedure based on the secant method is implemented in MATLAB for solving the governing ordinary differential equations. For the special case of N2-free LNG, a semi-analytical solution is proposed to solve the dynamic model for this relatively complex system and compute the amount of boil-off gas (BOG). The semi-analytical solution is subsequently used to optimize the LNG storage tank and recirculation loop designs.",
     "keywords": ["LNG", "Modelling", "Dynamic simulation", "Storage", "BOG", "Regasification"]},
    {"article name": "Integrating renewables into multi-period heat exchanger network synthesis considering economics and environmental impact",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.017",
     "publication date": "04-2017",
     "abstract": "This paper presents a further development of synthesis methods that considers economics and environmental impact in the integration of renewable energy into the optimisation of heat exchanger networks involving multiple periods of operations and multiple options of utilities. The multi-period process stream parameters, and those of the utility sources are integrated in a systematic approach using an expanded version of the simplified stage-wise superstructure multi-period model. Two examples were used to demonstrate the benefits of the expanded synthesis method and the quality of solutions obtained were judged by representation on a Pareto curve and by the use of a modified goal solution method. It was found that various combinations of utility sources were selected for use at various periods/seasons of operations, while utilities from solar photovoltaic were not selected for use at any of the periods/season of operation due to its relatively high cost and limited periods of availability.",
     "keywords": ["Multi-period", "Superstructure", "Synthesis", "Heat exchange", "Environmental impact"]},
    {"article name": "Simultaneous design and control under uncertainty: A back-off approach using power series expansions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.12.015",
     "publication date": "04-2017",
     "abstract": "A methodology for simultaneous design and control for dynamic systems under uncertainty has been developed. The algorithm moves away (back-off) from the optimal steady-state design (which is often found to be dynamically infeasible) to a new feasible operating point under process dynamics and parameter uncertainty by solving a set of optimization problems in an iterative manner. Power Series Expansions (PSE) are employed to represent the cost function and constraints in the optimization problems. The challenge in this method is to calculate in a systematic fashion the amount of back-off needed to accommodate the transient operation of the process. The method has been tested on an isothermal storage tank and a waste water treatment plant and the results compared with the formal integration. The results have shown that this method has the potential to address the simultaneous design and control of dynamic systems under uncertainty at lower computational costs.",
     "keywords": ["Simultaneous design and control", "Power series expansions", "Back-off approach"]},
    {"article name": "Prediction of bubble fluidisation during chemical looping combustion using CFD simulation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.009",
     "publication date": "04-2017",
     "abstract": "Bubble fluidisation in the fuel reactor adopted in chemical looping combustion (CLC) has a siganificant impact on the operation efficiency. Although a variety of numerical modellings of fluid dynamic process in the fuel reactors have been conducted, studies on predicting the fluidised bubble behaviours in the cylindrical fuel reactor where the effect of the heterogeneous reaction is also considered are still lacking. In this paper, the use of correlations of fluid dynamic parameters to characterise the fluidised bubble formation in the fuel reactor was proposed. A correlation parameter relating the time-dependent fluidised bubbles to the local eddies was introduced by correlating the local gas velocity fluctuation with the pressure fluctuation. The existence of a strong correlation between the concentration of gaseous reactants or products and local vortices was also demonstrated. Three-dimensional multiphase CFD model coupled with the heterogenoeous reaction kinetics was employed to study the details of CLC process in the fuel reactor. The results clearly indicated that the approach used in the present work can effectively monitor the formation of fluidised bubble in the dense fluidised bed during the heterogeneous reaction and may be used in the CLC as an indicator for monitoring the reduction rate as the locally embedded large eddies are strongly associated with the fluidised bubble occurrance.",
     "keywords": ["Chemical looping combustion", "Fuel reactor", "CFD", "Fluidised bed"]},
    {"article name": "Shape constrained splines as transparent black-box models for bioprocess modeling",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.12.017",
     "publication date": "04-2017",
     "abstract": "Empirical model identification for biological systems is a challenging task due to the combined effects of complex interactions, nonlinear effects, and lack of specific measurements. In this context, several researchers have provided tools for experimental design, model structure selection, and optimal parameter estimation, often packaged together in iterative model identification schemes. Still, one often has to rely on a limited number of candidate rate laws such as Contois, Haldane, Monod, Moser, and Tessier. In this work, we propose to use shape-constrained spline functions as a way to reduce the number of candidate rate laws to be considered in a model identification study, while retaining or even expanding the explanatory power in comparison to conventional sets of candidate rate laws. The shape-constrained rate laws exhibit the flexibility of typical black-box models, while offering a transparent interpretation akin to conventionally applied rate laws such as Monod and Haldane. In addition, the shape-constrained spline models lead to limited extrapolation errors despite the large number of parameters.",
     "keywords": ["Mathematical models", "Microbial growth-rate kinetics", "Monod equation", "Shape-constrained spline function", "Wastewater treatment"]},
    {"article name": "A mixed-integer linear programming-based scheduling model for refined-oil shipping",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.031",
     "publication date": "04-2017",
     "abstract": "The refined oil transportation problem investigated in this paper lies on the intersection of the scheduling and routing of tramp shipping and the petroleum supply chain, with unprecedented large-scale and complex rules. Two mixed-integer linear programming formulations are developed for the assignment between tasks, vessels, and timing issues. The first model uses a time-slot concept under a continuous-time representation, where the constraints that deal with vessel assignment, capacity, timing, demand, and slack stock control are considered. The second model uses a discrete-time representation with time assignment, portal counting, and strict stock control constraints. By virtue of the data collected from an oil company, this modeling approach is validated and used to generate feasible scheduling solutions with lower costs than are currently achieved in the real situation. Finally, the impact of the model parameters is analyzed under different optimization scenarios.",
     "keywords": ["Refined-oil", "Tramp shipping", "Scheduling", "Time-slot", "Continuous-time", "Discrete-time"]},
    {"article name": "Comparison of different extractive distillation processes for 2-methoxyethanol/toluene separation: Design and control",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.025",
     "publication date": "04-2017",
     "abstract": "In this work, the extractive distillation process is proposed for the first time to separate the 2-methoxyethanol/toluene mixture. Three configurations, including the conventional extractive distillation (CED), extractive distillation under reduced pressure (EDRP) and extractive dividing-wall column (EDWC) processes are investigated in order to obtain the optimal separation configurations. Their economic feasibilities are estimated by calculating the total annual cost (TAC). The results show that the EDRP process operating at a relatively low pressure is more economic than that operating at the lowest pressure. The TAC of the EDRP and EDWC processes can be saved by 4.88% and 11.04% respectively compared to the CED process, thus the EDWC sequence is superior to the other two processes in economic benefits. Three control strategies are proposed to investigate the controllability of the EDWC process. The results show that the improved dynamic structure can overcome large feed disturbances in flowrate and composition effectively.",
     "keywords": ["2-methoxyethanol-toluene-DMSO", "Extractive distillation under reduced pressure", "Extractive dividing-wall column", "Economic evaluation", "Dynamic performance"]},
    {"article name": "Dynamic optimization of constrained semi-batch processes using Pontryagin\u2019s minimum principle\u2014An effective quasi-Newton approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.019",
     "publication date": "04-2017",
     "abstract": "This work considers the numerical optimization of constrained batch and semi-batch processes, for which direct as well as indirect methods exist. Direct methods are often the methods of choice, but they exhibit certain limitations related to the compromise between feasibility and computational burden. Indirect methods, such as Pontryagin\u2019s Minimum Principle (PMP), reformulate the optimization problem. The main solution technique is the shooting method, which however often leads to convergence problems and instabilities caused by the integration of the co-state equations forward in time.This study presents an alternative indirect solution technique. Instead of integrating the states and co-states simultaneously forward in time, the proposed algorithm parameterizes the inputs, and integrates the state equations forward in time and the co-state equations backward in time, thereby leading to a gradient-based optimization approach. Constraints are handled by indirect adjoining to the Hamiltonian function, which allows meeting the active constraints explicitly at every iteration step. The performance of the solution strategy is compared to direct methods through three different case studies. The results show that the proposed PMP-based quasi-Newton strategy is effective in dealing with complicated constraints and is quite competitive computationally.",
     "keywords": ["Constrained dynamic optimization", "Pontryagin\u2019s minimum principle", "Indirect optimization methods", "Quasi-Newton algorithm", "Semi-batch processes"]},
    {"article name": "Design of an embedded inverse-feedforward biomolecular tracking controller for enzymatic reaction processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.027",
     "publication date": "04-2017",
     "abstract": "Feedback control is widely used in chemical engineering to improve the performance and robustness of chemical processes. Feedback controllers require a \u2018subtractor\u2019 that is able to compute the error between the process output and the reference signal. In the case of embedded biomolecular control circuits, subtractors designed using standard chemical reaction network theory can only realise one-sided subtraction, rendering standard controller design approaches inadequate. Here, we show how a biomolecular controller that allows tracking of required changes in the outputs of enzymatic reaction processes can be designed and implemented within the framework of chemical reaction network theory. The controller architecture employs an inversion-based feedforward controller that compensates for the limitations of the one-sided subtractor that generates the error signals for a feedback controller. The proposed approach requires significantly fewer chemical reactions to implement than alternative designs, and should have wide applicability throughout the fields of synthetic biology and biological engineering.",
     "keywords": ["CRN chemical reaction network", "chemical reaction network", "DNA deoxyribonucleic acid", "deoxyribonucleic acid", "LHS left-hand-side", "left-hand-side", "RHS right-hand-side", "right-hand-side", "ODE ordinary differential equation", "ordinary differential equation", "PI proportional-integral", "proportional-integral", "FF feedforward", "feedforward", "IMC internal model control", "internal model control", "DSD DNA strand displacement", "DNA strand displacement", "Process control", "Enzymatic reaction process", "Chemical reaction network theory", "Synthetic biology", "Biological engineering"]},
    {"article name": "Copula-based approximation of particle breakage as link between DEM and PBM",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.023",
     "publication date": "04-2017",
     "abstract": "In process engineering, the breakage behavior of particles is needed for the modeling and optimization of comminution processes. A popular tool to describe (dynamic) processes is population balance modeling (PBM), which captures the statistical distribution of particle properties and their evolution over time. It has been suggested previously to split up the description of breakage into a machine function (modeling of loading conditions) and a material function (modeling of particle response to mechanical stress). Based on this idea, we present a mathematical formulation of machine and material functions and a general approach to compute them. Both functions are modeled using multivariate probability distributions, where in particular so-called copulas are helpful. These can be fitted to data obtained by the discrete element method (DEM). In this paper, we describe the proposed copula-based breakage model, and we construct such a model for an artificial dataset that allows to evaluate the prediction quality.",
     "keywords": ["Discrete element method", "Population balance modeling", "Breakage probability", "Breakage function", "Copula"]},
    {"article name": "Development of a multi-compartment population balance model for high-shear wet granulation with discrete element method",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.022",
     "publication date": "04-2017",
     "abstract": "This paper presents a multi-compartment population balance model for wet granulation coupled with DEM (discrete element method) simulations. Methodologies are developed to extract relevant data from the DEM simulations to inform the population balance model. First, compartmental residence times are calculated for the population balance model from DEM. Then, a suitable collision kernel is chosen for the population balance model based on particle\u2013particle collision frequencies extracted from DEM. It is found that the population balance model is able to predict the trends exhibited by the experimental size and porosity distributions by utilising the information provided by the DEM simulations.",
     "keywords": ["Granulation", "Stochastic Weighted Algorithm", "Compartmental model", "Discrete element method", "Majorant kernel"]},
    {"article name": "Scalable modeling and solution of stochastic multiobjective optimization problems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.021",
     "publication date": "04-2017",
     "abstract": "We present a scalable computing framework for the solution stochastic multiobjective optimization problems. The proposed framework uses a nested conditional value-at-risk (nCVaR) metric to find compromise solutions among conflicting random objectives. We prove that the associated nCVaR minimization problem can be cast as a standard stochastic programming problem with expected value (linking) constraints. We also show that these problems can be implemented in a modular and compact manner using PLASMO (a Julia-based structured modeling framework) and can be solved efficiently using PIPS-NLP (a parallel nonlinear solver). We apply the framework to a CHP design study in which we seek to find compromise solutions that trade-off cost, water, and emissions in the face of uncertainty in electricity and water demands.",
     "keywords": ["Large scale", "Optimization", "Stochastic", "Multiobjective", "CVaR"]},
    {"article name": "Model-based design of optimal experiments for nonlinear systems in the context of guaranteed parameter estimation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.029",
     "publication date": "04-2017",
     "abstract": "An approach to the design of experiments is presented in the framework of bounded-error (guaranteed) parameter estimation for nonlinear static and dynamic systems. The guaranteed parameter estimation determines non-asymptotic confidence limits on the unknown parameters of a mathematical model. An essential part of the solution procedure is the approximation of the joint-confidence region. In this contribution, we develop and analyze the procedure and different ways of achieving a tight over-approximation of the solution set of guaranteed parameter estimation based on the expected values of parameters. Finally we propose to solve the problem of the design of experiments as a bilevel program. We demonstrate our approach and analyze the nature of the problem in the static and dynamic case studies. The proposed approach is also compared to the experiment design in the context of least-squares estimation and to the linearization-based techniques for optimal experiment design proposed in the literature earlier.",
     "keywords": ["Optimal experiment design", "Estimation algorithms", "Parameter estimation", "Bounded noise", "Bounded-error estimation"]},
    {"article name": "Economic performance evaluation of process system design flexibility options under uncertainty: The case of hydrogen production plants with integrated membrane technology and CO2 capture",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.020",
     "publication date": "04-2017",
     "abstract": "A hydrogen production plant with integrated catalytic membrane reactor modules (HP-CMR) represents a new technology option with potentially enhanced environmental performance characteristics. Therefore, HP-CMR techno-economic performance in the presence of irreducible sources of uncertainty (market, regulatory) ought to be comprehensively evaluated in order to accelerate the realization of future demonstration plants. The present study introduces a systematic methodological framework allowing the economic value assessment of various flexibility options in the design and operation of an HP-CMR plant under the above uncertainty sources. The primary objective is to demonstrate the potentially value-enhancing prospects of design flexibility options that capture the inherent optionality element in managerial decision-making to actively respond to uncertainties as they are progressively resolved. A detailed Net Present Value (NPV)-based assessment framework is first developed within which the above sources of uncertainty are integrated through Monte Carlo techniques. Various constructional and operational flexibility options are introduced pertaining to the installation decision and operating mode choice of the carbon capture and sequestration (CCS) unit, and HP-CMR economic performance is comparatively assessed. Finally, under certain scenarios of regulatory action on CO2 emissions, it is demonstrated that quite appealing economic performance outcomes could emerge for HP-CMR plants once design flexibility is introduced.",
     "keywords": ["Catalytic membrane reactor", "Hydrogen production", "CO2 capture", "Process system design flexibility options", "Economic performance evaluation under uncertainty"]},
    {"article name": "Dynamic response to process disturbances\u2014A comparison between TMB/SMB models in transient regime",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.026",
     "publication date": "04-2017",
     "abstract": "The modelling and design of Simulated moving bed (SMB) processes is normally done using the True moving bed (TMB) approximation. Several studies show that average values obtained at cyclic steady state for SMB units approach the TMB unit at steady state and that this approach is better as the number of columns in the SMB increases. However, studies that evaluate this equivalence under dynamic conditions are scarce. The objective of this work is to perform an analysis of the transient behaviour of two SMB units, with four and eight columns, and compare the results with the ones obtained for a TMB unit. An analysis of the impact of operating variables on the processes performance parameters is performed. The results show that TMB/SMB equivalence is valid only for conditions that do not violate the regeneration/separation regions and that the transient behaviour of the four columns SMB can resemble more the TMB.",
     "keywords": ["True moving bed", "Simulated moving bed", "Enantiomers separation", "Dynamic behaviour"]},
    {"article name": "Oxygen integration of autothermal reforming of ethanol with oxygen production, through ion transport membranes in countercurrent configuration",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.039",
     "publication date": "04-2017",
     "abstract": "In this work, we propose a new arrangement for integrating an Autothermal Reforming of Ethanol process with oxygen production with the technology of ITM membranes. In the conventional configuration O2 is first separated from the air and then injected in the reforming process, while in the new configuration O2 is depleted from the air in a counter-current arrangement with a reforming process stream, used as sweep gas. We took from the literature a process for Autothermal Reforming of Ethanol in its optimal operating condition, and scaled it up to pilot size. We assessed the performance of both configurations with Aspen Plus V8.7 and found that the configuration in counter-current arrangement with a process sweep stream has a reduction of the total annualized cost of 27.3% with respect to the conventional separation configuration. Furthermore, we optimize the operating conditions and ancillary structure of the counter-current integrated process, achieving a total annualized cost reduction of 72.2% with respect to the conventional design.",
     "keywords": ["Oxygen", "Mass", "Exchange", "Heuristic", "Reforming", "Integration"]},
    {"article name": "Multicomponent and multi-dimensional modeling and simulation of adsorption-based carbon dioxide separation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.040",
     "publication date": "04-2017",
     "abstract": "A two-dimensional transient numerical study of the adsorption of CO2/N2 and CO2/H2 mixtures on activated carbon and MOF-177 in a fixed bed is presented. Because most of the commercial CFD codes are not capable of simulating adsorption processes in a straightforward fashion, we developed an additional code to solve for the different species transport including adsorption and diffusion and used Fluent to simulate the adsorption process, fluid flow, heat and, mass transfer.We simulated the adsorption process at high temperature-low pressure and low temperature-high pressure conditions as well as the pressure swing adsorption. For adsorption of CO2/N2 and CO2/H2 mixtures, Toth and Viral models are used calculate equilibrium isotherms. The breakthrough curves obtained from the simulations compare well with the experimental data. Moreover, the effects of aspect ratio and geometric shapes were studied. The results show that varying the bed aspect ratio from 7.77 to 2 has an insignificant effect on the adsorption capacity and performance.",
     "keywords": ["Adsorption", "Carbon capture", "Carbon-dioxide", "Separation", "Activated carbon", "MOFs"]},
    {"article name": "Comparison of objective functions for batch crystallization using a simple process model and Pontryagin\u2019s minimum principle",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.017",
     "publication date": "04-2017",
     "abstract": "In this contribution different objective functions based on the moments of the product crystal size distribution are compared using optimal control theory to solve for the optimal batch trajectory for each objective. For a simple crystallization process model with only nucleation and ordinary crystal growth, and neglecting the contribution of the nucleated mass to the nucleation rate and material balance, mostly analytic expressions are obtained for the optimal control vector. Different objective functions lead to different final values for the costates, which lead to different sets of coupled differential and algebraic equations which must be solved to determine the values of constants numerically. The results of nine different objective functions for three crystal systems are presented. The objective functions based on the lower moment of the nucleated crystals lead to late-growth trajectories while the objective functions based on the higher moment of the nucleated crystals lead to early-growth trajectories, consistent with previous findings. The effect of seed loading is also investigated.",
     "keywords": ["Batch crystallization", "Pontryagin\u2019s minimum principle", "Optimal control theory"]},
    {"article name": "Dynamic modeling of ultrafiltration membranes for whey separation processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.035",
     "publication date": "04-2017",
     "abstract": "In this paper, we present a control relevant rigorous dynamic model for an ultrafiltration membrane unit in a whey separation process. The model consists of a set of differential algebraic equations and is developed for online model based applications such as model based control and process monitoring. In this model, membrane resistance concept is adjusted to describe the membrane fouling. Based on the observations regarding the permeate flux, we propose a membrane resistance expression consisting of static and dynamic resistances. The empirical expressions for the membrane resistances are identified by solving a parameter estimation problem. The dynamic model is investigated for its predictive capabilities and is further utilised for the study of optimal operation strategies.",
     "keywords": null},
    {"article name": "Supersaturation controlled morphology and aspect ratio changes of benzoic acid crystals",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.010",
     "publication date": "04-2017",
     "abstract": "Supersaturation is a factor of great industrial importance to the crystal growth by affecting the final aspect ratio and morphology of organic compounds. In this work, the qualitative relationship between aspect ratio and supersaturation of benzoic acid (BA) was elucidated for the first time by experimental and simulative study. Experimentally, it was found that the crystal shape of BA changes from needle-like crystal to rectangular sheet and then to hexagonal particles with the increasing supersaturation ranging from 1.029 to 2.941. The increment of supersaturation decreases the average aspect ratio of crystallized particles from \u223c20.5 to \u223c1.3. Furthermore, a higher supersaturation (\u03c3\u00a0=\u00a01.618) leads to more isotropic hexagonal crystals due to less face discrimination at high crystallization rates. Additionally, we predicted the supersaturation-dependent crystal habit by the modified attachment energy (MAE) model, which yield good agreement with the experimental observed crystals at medium and high supersaturations.",
     "keywords": ["Supersaturation", "Crystal growth", "Aspect ratio", "Molecular dynamic simulation", "Benzoic acid"]},
    {"article name": "Fast Offshore Wells Model (FOWM): A practical dynamic model for multiphase oil production systems in deepwater and ultra-deepwater scenarios",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.036",
     "publication date": "04-2017",
     "abstract": "This work describes a simplified dynamic model for control and real time applications in offshore deepwater and ultra-deepwater petroleum production systems. Literature about simplified dynamical models, capable of cover the global architecture of an offshore multiphase production system, is scarce. Hence, the proposed model integrates and adapts partial models available in the literature in order to generate a single model of the whole system. The model, designed to represent slugs generated by the casing heading and terrain/riser concomitantly, was evaluated by comparison with a traditional commercial simulator and was also implemented in two actual production systems. As a result, the model showed the capability of capturing complex dynamical behaviors, such as limit cycles, demonstrated to be numerically more stable than similar models in literature, fast enough to be used in real time applications and proved to be adherent to the commercial simulator and actual operating data from Petrobras production systems.",
     "keywords": ["Simplified multiphase dynamic model", "Severe slug flow", "Limit cycle", "Oil and gas production system", "Offshore"]},
    {"article name": "Cyclic scheduling for an ethylene cracking furnace system using diversity learning teaching-learning-based optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.024",
     "publication date": "04-2017",
     "abstract": "The ethylene cracking furnace system is central to an olefin plant. Multiple cracking furnaces are employed for processing different hydrocarbon feeds to produce various smaller hydrocarbon molecules, such as ethylene, propylene, and butadiene. We develop a new cyclic scheduling model for a cracking furnace system, with consideration of different feeds, multiple cracking furnaces, differing product prices, decoking costs, and other more practical constraints. To obtain an efficient scheduling strategy and the optimal operational conditions for the best economic performance of the cracking furnace system, a diversity learning teaching-learning-based optimization (DLTLBO) algorithm is used to simultaneously determine the optimal assignment of multiple feeds to different furnaces, the batch processing time and sequence, and the optimal operational conditions for each batch. The performance of the proposed scheduling model and the DLTLBO algorithm is illustrated through a case study from a real-world ethylene plant: experiments show that the new algorithm out-performs both previous studies of this set-up, and the basic TLBO algorithm.",
     "keywords": ["Ethylene cracking furnace", "Cyclic scheduling", "Teaching-learning-based optimization"]},
    {"article name": "Optimization of multipurpose process plant operations: A multi-time-scale maintenance and production scheduling approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2017.01.007",
     "publication date": "04-2017",
     "abstract": "Scheduling of production and maintenance plays a fundamental role in the effective operation of process plants. Frequently the two decision processes are independently addressed, overlooking the tight relation existing between the way the plant is operated to produce the required goods and the appearance of maintenance requirements. The presence of degradation phenomena affecting the performance of plant units and limiting the operational choices makes the integration of the two decision processes even more important. In this paper an industrial framework for the integration of maintenance and production scheduling of process plants is presented as well as some considerations on how the presence of plant unit degradation impacts on the scheduling problem. The proposed approach ties industrial key-components such as asset management and production scheduling closer together and represents therefore a contribution to the smart manufacturing revolution. The integrated maintenance and production scheduling problem is formulated as a mixed integer linear program (MILP) and tested on a generic process plant described as a State Task Network (STN).",
     "keywords": ["Plant operations", "Maintenance", "Scheduling", "Optimization", "Performance degradation"]},
    {"article name": "Feeding trajectory optimization in fed-batch reactor with highly exothermic reactions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.12.008",
     "publication date": "03-2017",
     "abstract": "Process engineers and operators should understand reactor runaway phenomenon to avoid hazard situations and accidents. Reactor runaway occurs when an exothermic reaction takes place in a reactor with insufficient heat transfer area. The reaction rate increases due to the temperature rise, causing a further increase in reaction rate and temperature. Without any safety action, only the reactant consumption limits the maximum temperature. One of the safety actions can be the application of runaway criteria to characterize the safe operating regimes where the optimal operating conditions can be found. This paper presents runaway criteria and the application of these in case of the feeding trajectory optimization of a fed-batch reactor model. The most important runaway criteria and the relations between them are presented, and critical curves of criteria are calculated using a simple dimensionless tubular reactor. The optimal feeding trajectory is determined in case a pilot plant fed-batch reactor using different runaway criteria as a non-linear constraint based on particle swarm optimization and sequential quadratic programming. Designers need to know how they can choose the right criterion, and the results can help with it. Selectivity and profit can be decreased if runaway occurs in a fed-batch reactor, therefore it is important to deal with this problem.",
     "keywords": ["Fed-batch reactor", "Reactor runaway", "Runaway criterion", "Optimization", "Feeding trajectory"]},
    {"article name": "PhotoBioLib: A Modelica library for modeling and simulation of large-scale photobioreactors",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.12.002",
     "publication date": "03-2017",
     "abstract": "This paper presents PhotoBioLib, a library developed in Modelica language for modeling and simulation of industrial tubular photobioreactors without necessity of a previous knowledge of programming neither the involved dynamics. Therefore, it provides models very similar to the real ones, with a high configuration potential, interconnection possibilities with other libraries and a high simulation speed. The library has been validated with real data.",
     "keywords": ["Photobioreactors", "Modeling", "Simulation", "Modelica", "PhotoBioLib"]},
    {"article name": "Efficient interpolation of precomputed kinetic data employing reduced multivariate Hermite Splines",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.12.005",
     "publication date": "03-2017",
     "abstract": "Detailed surface kinetics can be efficiently implemented into complex reactor simulations by the use of precomputed solutions of the chemical rate equations. In this work we propose reduced Hermite splines as an interpolation method for the precomputed rate data. The reduced Hermite splines require significantly less storage space and less execution time than the tensor product splines that have been previously used for this task. Using previously published test cases on NH3 oxidation, we demonstrate that despite of the reduced storage requirements and faster interpolation times, reduced Hermite interpolation achieves the same interpolation accuracy as conventional full tensor product splines.In many cases, the derivatives of the interpolated function with respect to the input quantities can be cheaply obtained during the computation of the function values. If these derivatives are provided as additional information, the accuracy of the reduced Hermite method even outperforms conventional tensor product splines, in our demonstration example by a factor of three.The advantage of the reduced Hermite schemes with respect to storage requirements and evaluation time strongly increase with the dimensionality of the interpolation problem. It is therefore expected that the reduced Hermite splines will allow to significantly extend the application range of solution mapping methods to higher dimensional problems (i.e. problems with a larger number of relevant chemical species).",
     "keywords": ["Repro-modeling", "Splines", "Interpolation", "Heterogeneous catalysis", "Reaction engineering", "Simulation", "Ammonia oxidation", "Surface kinetic model"]},
    {"article name": "Actuation of spatially-varying boundary conditions for reduction of concentration polarisation in reverse osmosis channels",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.045",
     "publication date": "03-2017",
     "abstract": "Reduction of concentration polarisation is of great value in reverse osmosis membrane systems. Concentration polarisation leads to a reduction in flux, which corresponds to a reduction in separation efficiency. This paper studies an approach by which to reduce concentration polarisation in reverse osmosis channels using a steady, spatially variant slip velocity profile. A method is developed to identify the most effective wall slip velocity profile for increasing the diffusive driving force away from the wall, which in turn increases mass transfer away from the membrane and reduces concentration polarisation. The nonlinear partial differential equations (PDEs) that govern fluid and mass transport behaviours are difficult to solve. In this work, an approximate solution to the nonlinear system is developed using systems of linearised ordinary differential equations (ODEs) to approximate the behaviour of the PDEs and determine the steady-state actuation profile that most effectively increases mass transfer at the wall. This leads to a systematic method for determination of a spatially-varying actuation profile (the most effective slip velocity profile) that decreases concentration polarisation in reverse osmosis membrane channels.",
     "keywords": ["Concentration polarisation", "Reverse osmosis", "Infinite-dimensional systems", "Distributed actuation"]},
    {"article name": "Using a novel parallel genetic hybrid algorithm to generate and determine new zeolite frameworks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.036",
     "publication date": "03-2017",
     "abstract": "Zeolite structure determination and zeolite framework generation are not new problems but due to the increasing computer power, these problems came back and they are still a challenge despite the recent progress in terms of structural resolution from X-rays and electron diffraction. The infinite number of potential solutions and the computational cost of this problem make the use of metaheuristics significant for this problem. In this paper, we propose a new approach based on parallel genetic hybrid algorithm for zeolites using a modified modelization of the objective function to find hypothetical zeolite structures, close to the thermodynamic feasibility criterion. A population made of random atoms is initialized. At each generation, a crossover operator and a mutation heuristic are applied. Each individual of the population generates a potential zeolitic structure by applying the symmetry operators of a given crystallographic space group. This structure is evaluated with our objective function. From the unit cell parameters and the number of T atoms in the asymmetric unit, 6 possible zeolitic interesting structures have been found.",
     "keywords": ["Genetic algorithm", "Zeolites", "Structure evaluation", "New topologies"]},
    {"article name": "Modifier adaptation with guaranteed feasibility in the presence of gradient uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.027",
     "publication date": "03-2017",
     "abstract": "In the context of real-time optimization, modifier-adaptation schemes use estimates of the plant gradients to achieve plant optimality despite plant-model mismatch. Plant feasibility is guaranteed upon convergence, but not at the successive operating points computed by the algorithm prior to convergence. This paper presents a strategy for guaranteeing rigorous constraint satisfaction of all iterates in the presence of plant-model mismatch and uncertainty in the gradient estimates. The proposed strategy relies on constructing constraint upper-bounding functions that are robust to the gradient uncertainty that results when the gradients are estimated by finite differences from noisy measurements. The performance of the approach is illustrated for the optimization of a continuous stirred-tank reactor.",
     "keywords": ["Real-time optimization", "Modifier adaptation", "Feasible operation", "Gradient uncertainty"]},
    {"article name": "A single events microkinetic model for hydrocracking of vacuum gas oil",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.035",
     "publication date": "03-2017",
     "abstract": "The single events microkinetic modelling approach is extended to include saturated and unsaturated cyclic molecules, in addition to paraffins. The model is successfully applied to hydrocracking (HCK) of an hydrotreated Vacuum Gas Oil (VGO) residue in a pilot plant, under industrial operating conditions, on a commercial bi-functional catalyst. The molecular composition of the VGO feed is obtained by reconstruction based on a combination of analytical data (SIMDIS, GCxGC, mass spectrometryspectrometry). The necessary extensions to the single events methodology, which has previously only been applied to much simpler reacting systems (i.e. HCK of paraffins) are detailed in this work. Feeds typically used in the petrochemical industry typically contain a far more complex mixture of hydrocarbons, including cyclic species (i.e. naphthenes & aromatics). A more complex reaction network is therefore required in order to apply a single events model to such feeds. Hydrogenation, as well as endo- and exo-cyclic reactions have been added to the well-known acyclic \u03b2-scission and PCP-isomerization reactions. A model for aromatic ring hydrogenation was included in order to be able to simulate the reduction in aromatic rings, which is an important feature of HCK units. The model was then applied to 8 mass balances with a wide range of residue conversion (20\u201390%). The single events model is shown to be capable of correctly simulating the macroscopic effluent characteristics, such as residue conversion, yield structure, and weight distribution of paraffinic, naphthenic, and aromatic compounds in the standard cuts. This validates the overall model. The single events model provides far more detail about the fundamental chemistry of the system. This is shown in a detailed analysis of the reaction kinetics. The evolution of molecule size (i.e. carbon number), number of saturated/unsaturated rings, or the ratio of branched and un-branched species can be followed along the reactor. This demonstrates the explanatory power of this type of model. Calculations are performed on the IFPEN high performance computing cluster, with parallelization via MPI (message passing interface). This was very useful in order to reduce time consuming problems especially for the parameter fitting step",
     "keywords": ["Hydrocracking", "Single events", "Microkinetic modelling", "Catalysis"]},
    {"article name": "Identification problem in plug-flow chemical reactors using the adjoint method",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.029",
     "publication date": "03-2017",
     "abstract": "The aim of this work is to solve identification problems in plug-flow chemical reactors. For this purpose an adjoint-based algorithm for parameter identification problems in systems of partial differential equations is presented. The adjoint method allows us to calculate the gradient of the objective function and the constraint functions with respect to the unknown parameters significantly reducing the computer time. This leads to solve a minimization problem, in which an objective function is defined in order to quantify the mismatch between the observed data and the numerical solution of the parameterized chemical model. For solving the initial and boundary-value problem we use finite-difference schemes. More precisely, we propose a second-order BDF method initialized with a first-order one. The algorithm proposed was implemented in a computer program and some numerical results are shown. The efficiency of the adjoint method, compared with the classical formula of incremental quotients, is also presented.",
     "keywords": ["Chemical kinetics", "Plug-flow reactor", "Identification", "Adjoint method", "Integral method"]},
    {"article name": "Bi-objective optimization of dynamic systems by continuation methods",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.025",
     "publication date": "03-2017",
     "abstract": "As a proof of concept the properties of path-following methods are studied for multi-objective optimization problems involving dynamic systems (also called multi-objective dynamic optimization or multi-objective optimal control problems), which have never been presented before. Two case studies with two objectives are considered to cover convex, as well as non-convex trade-off curves or Pareto sets. In order for the method to be applicable, the infinite dimensional dynamic problems have to be discretized and scalarization parameters have to be introduced, which leads to large-scale parametric nonlinear optimization problems. For both the chemical tubular reactor and the fed-batch bioreactor case study it is found that a path-following continuation approach is able to compute the Pareto fronts accurately and efficiently. A branch switching technique is required whenever a constraint switches from active to inactive or vice versa. When dealing with non-convex problems, a technique for detecting inflection points is required. Simple switching techniques are suggested and have been tested successfully.",
     "keywords": ["Multi-objective optimization", "Continuation techniques", "Karush\u2013Kuhn\u2013Tucker conditions", "Fritz\u2013John conditions", "Scalarization", "Pareto front"]},
    {"article name": "Dynamic modelling of carboxylic acid filtration in forward osmosis process: The role of membrane CO2 permeability",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.10.017",
     "publication date": "03-2017",
     "abstract": "This research intends to develop the mathematical model of forward osmosis process during the filtration of a single carboxylic acid and a mixture of two carboxylic acids by investigating and compiling each single logical phenomenon to formulate simultaneous equation models of the associated process variables in the operation of forward osmosis system (Dynamic Modelling). The developed model demonstrates that the permeating CO2 from NaCl draw solution performs the major role in sequentially generating the true carbonic acid ( P k a = 3.45 ), causing the substantial impact on the pH reduction in acid feed solution. Based on inverse problems techniques, the dynamic model, which takes into account the presence of true carbonic acid formation, was fitted to experimental pH profiles. The unobserved membrane CO2 permeability (0.0025\u00a0L/m2/h) could directly be obtained. Referred to Levenberg-Marquardt algorithm, all time-dependent process variables could be defined by the dynamic simulation model at any point in simulating time.",
     "keywords": ["Forward osmosis", "Carboxylic acid", "Filtration", "Dynamic modelling", "True carbonic acid"]},
    {"article name": "On Generative Topographic Mapping and Graph Theory combined approach for unsupervised non-linear data visualization and fault identification",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.12.009",
     "publication date": "03-2017",
     "abstract": "Process monitoring of chemical plants relies on two steps: discriminating anomalies (fault detection) and characterizing them (fault identification). This work proposes a combined Generative Topographic Mapping (GTM) and Graph Theory (GT) approach. GTM highlights system features, reducing variable dimensionality and providing a strategy for calculating similarity between samples. GT then clusters them using networks, discriminating normal and anomalous entries. Because of biased normal and anomalous labeling, however, the methodology proposed is unsupervised, meaning that labels are inexistent. Three case studies were considered: a simulation data set, Tennessee Eastman process and an industrial data set. Principal Component Analysis (PCA), dynamic PCA and kernel PCA indexes (Q and T2) alongside GTM and GT independent monitoring methodologies were used for comparison, considering supervised and unsupervised approaches. For the industrial scenario, soft sensors were used for assessing discrimination performance. The proposed method, while unsupervised, discriminated normal states similarly to supervised strategies, justifying its development.",
     "keywords": ["Generative Topographic Mapping", "Graph clustering", "Fault detection", "Fault identification", "Process monitoring"]},
    {"article name": "Model-based design and analysis of glucose isomerization process operation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.12.016",
     "publication date": "03-2017",
     "abstract": "The application of model-based methods for design and analysis of operational improvements of an industrial glucose isomerization (GI) process is highlighted. First, a multi-scale mathematical model representing important phenomena encountered in the reaction system of a glucose isomerization reactor is developed. Next, model analysis, model identification and model validation based on available reactor operational data are performed. The reactor model is found to describe accurately important phenomena, such as, reaction kinetics, enzyme decay and internal diffusion of the substrate in the enzymatic pellet as a function of the temperature, thereby confirming that the model is ready for use in design-analysis studies. Operation of the GI process is then analyzed in a single reactor and based on this, the reactor model is used as a building block to represent the operation of a GI reactor plant consisting of 10\u201320 reactors in parallel. The design of the GI plant operation is evaluated through the analysis of simulated results of different operational scenarios.",
     "keywords": ["Systems engineering", "Mathematical modelling", "Biochemical processes", "Glucose isomerization"]},
    {"article name": "Systematic approach for modeling reaction networks involving equilibrium and kinetically-limited reaction steps",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.12.014",
     "publication date": "03-2017",
     "abstract": "Chemical systems often exhibit dynamics in different time scales owing to fast and slow reactions. Thus deriving models suitable for computation with standard numerical methods is challenging. In this tutorial we present a systematic approach for modeling chemical reaction systems including (known) slow reactions and fast reactions that can be assumed at equilibrium. The presented approach consists of the following steps: (i) identifying an independent set of reactions; (ii) writing the overall mass balance; (iii) writing a species balance for each species; (iv) writing the species transformation rates as a function of the net reaction rates; (v) introducing a constitutive equation for each reaction (either kinetic rate or equilibrium condition); (vi) performing index reduction of the differential-algebraic-equation (DAE) system. The resulting reduced system can be readily solved with standard DAE integrators. We discuss the number of initial conditions to be specified and illustrate the method through simple examples: methane reforming, Michaelis\u2013Menten reaction and hydrogen-deuterium exchange.",
     "keywords": ["AE algebraic equation", "algebraic equation", "AV algebraic variable", "algebraic variable", "DAE differential algebraic equation", "differential algebraic equation", "DE differential equation", "differential equation", "DV differential variable", "differential variable", "NMR nuclear magnetic resonance", "nuclear magnetic resonance", "ODE ordinary differential equation", "ordinary differential equation", "PDAE partial-differential algebraic equation", "partial-differential algebraic equation", "WGS water gas shift", "water gas shift", "Reaction networks", "Chemical equilibrium", "Chemical kinetics", "Differential-algebraic equations", "Differential index", "Tutorial"]},
    {"article name": "Simulation of solute dispersion in particle packs by the volume averaging method",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.12.021",
     "publication date": "03-2017",
     "abstract": "The ability to predict solute dispersion behavior in homogeneous random particle packs by the volume averaging method is evaluated. Unit cells with periodic boundaries and random pore connectivity are numerically constructed. The asymptotic longitudinal and transverse dispersion coefficients are predicted by the volume averaging method in terms of these unit cells. The Peclet number is in the range of 0\u20131000. The Reynolds number is less than 1. Dispersion coefficients of eighteen unit cells with different sizes and pore geometries are compared. It is found that a significant scatter of dispersion coefficients exists when the size of the unit cell is small. The scatter decreases with increasing size of the unit cell. The predicted dispersion coefficients are compared with correlations obtained according to experimental and simulation results for homogeneous random particle packs and reasonable agreements are observed when the size of the unit cell is suitable.",
     "keywords": ["Periodic", "Random", "Dispersion", "Packed", "Volume averaging"]},
    {"article name": "Multi-model approach based on parametric sensitivities \u2013 A heuristic approximation for dynamic optimization of semi-batch processes with parametric uncertainties",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.12.004",
     "publication date": "03-2017",
     "abstract": "Optimal processes often exhibit active path constraints. Parametric uncertainties in the process model might thus lead to constraint violations. A heuristic approach is presented to overcome this challenge. The nominal model is optimized with additional path constraints due to worst-case models. A heuristic method of choosing these models is proposed based on sensitivities of the constraints with respect to the uncertain parameters. The presented approximation does not guarantee robust feasibility, but path constraint violations are less likely to occur compared to the optimization using the nominal model solely. Two case studies are presented: a complex emulsion copolymerization process (DAE with 139 equations) and the penicillin formation (four differential equations and two algebraic equations). The results of both case studies show that, in contrast to the optimization in the nominal case, the multi-model approach does not violate the path constraints for different scenarios of the parametric uncertainty set.",
     "keywords": ["Parameter uncertainty", "Uncertain dynamic systems", "Optimal control problem", "Robust dynamic optimization"]},
    {"article name": "Model-based analysis of stirred cooling crystallizer of high aspect ratio crystals with linear and nonlinear breakage",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.028",
     "publication date": "03-2017",
     "abstract": "2D population balance model is presented for stirred cooling crystallizer of high aspect ratio crystals including primary and secondary nucleation, size-dependent growth of the two characteristic crystal facets and linear and nonlinear breakage along the crystal length. Three breakage mechanisms are modelled: crystal-impeller and crystal-wall collisions linear and crystal\u2013crystal collisions nonlinear breakage processes. The 2D population balance equation is reduced into a system of moment equations for the mixed moments of length and width variables which is closed applying the quadrature method of moments and solved by using a three point QMOM-ODE method.It was shown that strong interactions exist between the secondary nucleation, crystal growth and breakage processes connected by the CSD. The stirring rate has strong impact on crystallization of high aspect ratio crystals forming the crystalline product directly by breakage. The crystal-impeller breakage proved to be the dominant process but the crystal\u2013crystal breakage also play significant role.",
     "keywords": ["Cooling crystallization", "High aspect ratio crystals", "2D Population balance", "Linear-nonlinear breakage", "Crystal-crystal collisions", "Crystal-vessel collisions"]},
    {"article name": "A widely applicable tool for modeling precipitation processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.12.007",
     "publication date": "03-2017",
     "abstract": "This work presents a generalized tool for modeling precipitation processes for the parallel formation of multiple solid phases. A symmetric mixing model for T-mixers is presented which mimics the mixing process in a numerically highly efficient way. This model can easily be extended to other reactor types such as stirred tank reactors following the implementation given in this work. Modelling ion activities and ion complexation is strongly accelerated by the analytical formulation of the Jacobian of the corresponding system of equations. Solid formation processes are described via the numerically efficient Direct Quadrature Method of Moments (DQMOM) which is parallelized for treating multiple solid phases simultaneously. Expressions for agglomeration of multiple solid phases and for particle transfer between different mixer zones are given. Both the models of the individual processes and the entire precipitation tool are validated and tested in multiple scenarios proving the flexibility of the tool.",
     "keywords": ["Precipitation", "Mixing", "Ion complexation", "Multi-phase", "Population Balance Equation", "T-Mixer"]},
    {"article name": "Integrated game-theory modelling for multi enterprise-wide coordination and collaboration under uncertain competitive environment",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.041",
     "publication date": "03-2017",
     "abstract": "In this work, an integrated Game Theory (GT) approach is developed for the coordination of multi-enterprise Supply Chains (SCs) in a competitive uncertain environment. The conflicting goals of the different participants are solved through coordination contracts using a non-cooperative non-zero-sum Stackelberg game under the leadership of the manufacturer. The Stackelberg payoff matrix is built under the nominal conditions, and then evaluated under different probable uncertain scenarios using a Monte-Carlo simulation. The competition between the Stackelberg game players and the third parties is solved through a Nash Equilibrium game. A novel way to analyze the game outcome is proposed based on a win\u2013win Stackelberg set of \u201cPareto-frontiers\u201d. The benefits of the resulting MINLP tactical models are illustrated by a case study with different vendors around a client SC. The results show that the coordinated decisions lead to higher expected payoffs compared to the standalone case, while also leading to uncertainty reduction.",
     "keywords": ["CPU central processing unit", "central processing unit", "EWO enterprise-wide optimization", "enterprise-wide optimization", "GAMS the general algebraic modeling system", "the general algebraic modeling system", "GB gigabyte", "gigabyte", "GHz gigahertz", "gigahertz", "GloMIQO global mixed-integer quadratic optimizer", "global mixed-integer quadratic optimizer", "GT game theory", "game theory", "KKT Karush\u2013Kuhn\u2013Tucker", "Karush\u2013Kuhn\u2013Tucker", "LP linear programming", "linear programming", "M-EWC multi-enterprise-wide coordination", "multi-enterprise-wide coordination", "MILP mixed integer linear programming", "mixed integer linear programming", "MINLP mixed integer non-linear programming", "mixed integer non-linear programming", "MW megawatt", "megawatt", "NE Nash equilibrium", "Nash equilibrium", "NLP non-linear programming", "non-linear programming", "OR operational research", "operational research", "PSE process system engineering", "process system engineering", "RM raw material", "raw material", "SBDN scenario-based dynamic negotiation", "scenario-based dynamic negotiation", "SCM supply chain management", "supply chain management", "SC supply chain", "supply chain", "SS standalone scenario", "standalone scenario", "WWTP wastewater treatment plant", "wastewater treatment plant", "\u03bc mean", "mean", "\u03c3 standard deviation", "standard deviation", "Decentralized multi-participant SC", "Coordination", "Game theory", "Uncertainty", "Competition", "Pareto-frontiers"]},
    {"article name": "Using Pareto filters to support risk management in optimization under uncertainty: Application to the strategic planning of chemical supply chains",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.10.008",
     "publication date": "03-2017",
     "abstract": "Optimization under uncertainty has attracted recently an increasing interest in the process systems engineering literature. The inclusion of uncertainties in an optimization problem inevitably leads to the need to manage the associated risk in order to control the variability of the objective function in the uncertain parameters space. So far, risk management methods have focused on optimizing a single risk metric along with the expected performance. In this work we propose an alternative approach that can handle several risk metrics simultaneously. First, a multi-objective stochastic model containing a set of risk metrics is formulated. This model is then solved efficiently using a tailored decomposition strategy inspired on the Sample Average Approximation. After a normalization step, the resulting solutions are assessed using Pareto filters, which identify solutions showing better performance in the uncertain parameters space. The capabilities and benefits of our approach are illustrated through a design and planning supply chain case study.",
     "keywords": ["Financial risk metrics", "Uncertainty", "Multi-objective", "Pareto filters"]},
    {"article name": "Fast algorithms for hp-discretized univariate population balance aggregation integrals",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.002",
     "publication date": "02-2017",
     "abstract": "The efficient numerical simulation of population balance equations requires sophisticated techniques in order to combine accuracy with efficiency. We will focus on the numerical treatment of aggregation integrals that often dominate the overall time in population balance simulations. Following a finite element approach, the density distribution is discretized through a piecewise polynomial of order p\u00a0>\u00a00 on a nested grid that is refined locally toward an arbitrary point. The proposed method conserves mass while reducing the quadratic complexity (in the dimension of the solution space) of the direct computation to an almost linear complexity. The complexity improvement is based on recursion formulas exploiting orthogonality properties of basis functions along with FFT on locally equidistant portions of the grid. We present numerical results for various initial conditions and provide heuristic criteria for the choice of polynomial degree and grid refinement.",
     "keywords": ["Population balance equation", "Convolution", "Integro-partial differential equation", "Fast Fourier transformation", "Nested grids", "High order polynomials"]},
    {"article name": "A comparison of the shrinking core model and the grain model for the iron ore pellet indurator simulation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.005",
     "publication date": "02-2017",
     "abstract": "The current study revisits the particle combustion modeling in the simulation of iron ore pellet indurator, which is the process to dry and fire the pellets as a pretreatment for blast furnace. Although the shrinking core model has been frequently used in the previous studies due to its simplicity, its limitation for the porous pellet should have been evaluated. Instead, the grain model could have been used as it conceptually gives the better description. In that context, the shrinking core model is compared against the grain model in the simplified isothermal condition and the complete indurator simulation to demonstrate the applicability. Despite the possible differences in the conversion profiles along the reaction regimes, the models provide apparently reasonable bed temperature results for the normal indurating conditions. However, the shrinking core model needs to be applied with caution and its validity should be questioned when the operating conditions change.",
     "keywords": ["Iron ore pellet", "Straight-grate indurator", "Coke combustion", "Shrinking core model", "Grain model"]},
    {"article name": "Novel active LiFePO4 battery balancing method based on chargeable and dischargeable capacity",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.014",
     "publication date": "02-2017",
     "abstract": "A lithium iron phosphate battery (LiFePO4) pack is one of the main power resources for electric vehicles and the non-uniformity of cells in the battery pack has become the bottleneck to improve the pack capacity. An active balancing method based on chargeable and dischargeable capacities, derived from the dynamically estimated state of charge (SOC) and capacity in the pack, is proposed to tackle this problem in both the charging and discharging processes. To determine the current of each cell in balancing operation, one extra current sensor is added with a chosen flyback balancing circuit. The balancing simulation of a LiFePO4 battery pack has been conducted in the moderate and severe capacity imbalance scenarios. The simulation results show that the proposed battery balancing method has better performance than the other balancing methods based on voltage or SOC in increasing the charged and discharged pack capacity in the charging and discharging process.",
     "keywords": ["Balancing criterion", "Balancing circuit", "State of charge", "LiFePO4", "Pack capacity", "Balancing current"]},
    {"article name": "Superstructure optimization and energetic feasibility analysis of process of repetitive extraction of hydrocarbons from Botryococcus braunii \u2013 a species of microalgae",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.018",
     "publication date": "02-2017",
     "abstract": "Microalgae are potential resources for producing renewable fuel; however, the process of fuel production from microalgae is itself highly energy consuming and not commercially feasible, yet. Repetitive non-destructive extraction, also called repetitive milking, is a novel method for production of hydrocarbons from Botryococcus braunii \u2013 a species of microalgae. In this study, superstructure optimization technique is used to analyse the energetic feasibility of the repetitive milking process and to find the suitable technology options for each stage involved. The repetitive milking process is found to be energetically positive with an average net energy ratio of two for the optimum route. Open pond, cylindrical sieve rotator filter and nanofiltration were found to be the optimum technologies for growth and hydrocarbon production, dewatering and solvent recovery stages, respectively. Belt filter press and vibratory screen filter for dewatering and distillation for solvent recovery are also found to be energetically feasible technologies.",
     "keywords": ["Biofuel", "Milking", "Superstructure optimization", "Microalgae"]},
    {"article name": "Dynamic simulation of LNG loading, BOG generation, and BOG recovery at LNG exporting terminals",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.006",
     "publication date": "02-2017",
     "abstract": "Liquefied natural gas (LNG) is a prominent clean energy source available in abundance. LNG has high calorific value, while lower price and emissions. Vapors generated from LNG due to heat leak and operating-condition-changes are called boil-off gas (BOG). Because of the very dynamic in nature, the rate of BOG generation during LNG loading (jetty BOG, or JBOG) changes significantly with the loading time, which has not been well studied yet. In this work, the LNG vessel loading process is dynamically simulated to obtain JBOG generation profiles. The effect of various parameters including holding-mode heat leak, initial-temperature of LNG ship-tank, JBOG compressor capacity, and maximum cooling-rate for ship-tank, on JBOG profile is studied. Understanding JBOG generation would help in designing and retrofitting BOG recovery facilities in an efficient way. Also, several JBOG utilization strategies are discussed in this work. The study would help proper handling of BOG problems in terms of minimizing flaring at LNG exporting terminals, and thus reducing waste, saving energy, and protecting surrounding environments.",
     "keywords": ["BOG Boil-off gas", "Boil-off gas", "C3 Propane", "Propane", "C3-MR Propane-and-Mixed-Refrigerant (Natural Gas Liquefaction Process)", "Propane-and-Mixed-Refrigerant (Natural Gas Liquefaction Process)", "FBOG Boil-off Gas from depressurization of LNG after MCHE", "Boil-off Gas from depressurization of LNG after MCHE", "FBOG2 Boil-off Gas from depressurization of liquefied BOG", "Boil-off Gas from depressurization of liquefied BOG", "FL BOG generated due to depressurization (flashing) of inlet stream", "BOG generated due to depressurization (flashing) of inlet stream", "GHG Greenhouse Gas", "Greenhouse Gas", "HE BOG generated due to heat added by equipment like pumps", "BOG generated due to heat added by equipment like pumps", "HL BOG generated due to heat leak from surrounding into container/pipeline", "BOG generated due to heat leak from surrounding into container/pipeline", "HT BOG generated due to hot tank/container", "BOG generated due to hot tank/container", "JBOG Boil-off gas from jetty (while loading a Cargo)", "Boil-off gas from jetty (while loading a Cargo)", "LIN Liquid nitrogen", "Liquid nitrogen", "LNG Liquefied natural gas", "Liquefied natural gas", "MCXB Main cryogenic heat exchanger bottom section", "Main cryogenic heat exchanger bottom section", "MCHE Main cryogenic heat exchanger (MCXB and MCXT)", "Main cryogenic heat exchanger (MCXB and MCXT)", "MCXT Main cryogenic heat exchanger top section", "Main cryogenic heat exchanger top section", "MR Mixed refrigerant", "Mixed refrigerant", "MTPA Million Tonnes Per Annum", "Million Tonnes Per Annum", "N2 Nitrogen", "Nitrogen", "NG Natural gas", "Natural gas", "NRU Nitrogen removal unit used for LNG", "Nitrogen removal unit used for LNG", "NRU2 Nitrogen removal unit used for BOG", "Nitrogen removal unit used for BOG", "PI \u2018Proportional, Integral\u2019 type of process controller", "\u2018Proportional, Integral\u2019 type of process controller", "TBOG Boil-off Gas from LNG storage tanks", "Boil-off Gas from LNG storage tanks", "VD BOG generated due to vapor displacement caused by inlet stream", "BOG generated due to vapor displacement caused by inlet stream", "VRA Vapor return arm", "Vapor return arm", "Dynamic simulation", "Boil off gas", "Flare minimization", "Liquefied natural gas", "C3-MR process", "BOG recovery"]},
    {"article name": "Continuous diisobutylene manufacturing: Conceptual process design and plantwide control",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.007",
     "publication date": "02-2017",
     "abstract": "The complete process design cycle encompassing flowsheet synthesis, design and controllability evaluation is studied for continuous DIB manufacturing. The residue curve map tool is applied to synthesize two new flowsheets, FS1 and FS2, exploiting pressure swing distillation. A unique feature of these is the use of a decanter for recovery and recycle of water, which allows maintaining the reactor tert-butyl alcohol content at the desired level for suppressing the side reaction. Unlike the literature flowsheet (FS0), this innovation makes it possible to achieve both high conversion and high yield for an economically superior design. Between FS1 and FS2, FS1 is found to be superior with significantly lower capital and energy costs as well as lower fresh water consumption. A \u201csmart\u201d control strategy for holding the single-pass reactor conversion and the overall process yield towards economic process operation is also developed. Rigorous dynamic simulations demonstrate the controllability of FS1 and FS2.",
     "keywords": ["Simultaneous process design", "Flowsheet synthesis", "Conceptual process design", "Sustainable process design"]},
    {"article name": "Real time model identification using multi-fidelity models in managed pressure drilling",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.008",
     "publication date": "02-2017",
     "abstract": "Highly accurate model predictions contribute to the performance and stability of model predictive control. However, high fidelity models are difficult to implement in real time control due to the large and often nonconvex optimization problem that must be completed within the feedback cycle time. To address this issue, a switched control scheme that uses high fidelity model predictions in real time control is presented. It uses real time simulated data to identify a linear empirical control model. The real time model identification procedure does not interrupt the process, and is suitable for nonlinear processes where offline model identification is difficult. Controller stability is discussed, and the control scheme is demonstrated in a managed pressure drilling simulation. The switched controller provides improved performance over both a high fidelity model based controller and a nonadaptive empirical model.",
     "keywords": ["Drilling automation", "Nonlinear model predictive control", "Switched control", "High fidelity control models", "Managed pressure drilling"]},
    {"article name": "Wastewater minimization in batch plants with sequence dependent changeover",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.016",
     "publication date": "02-2017",
     "abstract": "The minimization of water in batch plants through process integration has been well established in the literature. While many existing methods have relied on a fixed batch schedule, a few have employed variable production schedules. Nevertheless, methods employing variable schedules in fixed mass load problems have assumed that water-using operations (washing) are sequence independent. This paper presents a mathematical formulation for the simultaneous optimization of batch production and minimization of wastewater in fixed mass load problems. Wastewater minimization is achieved by exploring the sequence of tasks in a unit. The effectiveness of the formulation was demonstrated using three case studies. The results show that even without process integration, water usage can be minimized through the appropriate selection of task sequences in a unit. Reductions in water usage of 33% and 48.8% were observed in case study I and case study II respectively.",
     "keywords": ["Wastewater minimization", "Multipurpose", "Scheduling", "Changeover"]},
    {"article name": "Automatic model reduction of differential algebraic systems by proper orthogonal decomposition",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.004",
     "publication date": "02-2017",
     "abstract": "Proper orthogonal decomposition (POD) is an attractive way to obtain nonlinear low-dimensional models. This article reports on the automatization of the mentioned reduction method. An automatic procedure for the reduction of differential algebraic systems is presented, which is implemented in the modeling and simulation environment ProMoT/Diana. The software tool has been applied to a nonlinear heat conduction model and a continuous fluidized bed crystallizer model. The automatically generated reduced models are significantly smaller than the reference models, while the loss of accuracy is negligible.",
     "keywords": ["Nonlinear model reduction", "Proper orthogonal decomposition", "Empirical interpolation", "Computer aided modeling", "Differential algebraic systems"]},
    {"article name": "A quantile-based scenario analysis approach to biomass supply chain optimization under uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.015",
     "publication date": "02-2017",
     "abstract": "Supply chain optimization for biomass-based power plants is an important research area due to greater emphasis on renewable power energy sources. Biomass supply chain design and operational planning models are often formulated and studied using deterministic mathematical models. While these models are beneficial for making decisions, their applicability to real world problems may be limited because they do not capture all the complexities in the supply chain, including uncertainties in the parameters. This paper develops a statistically robust quantile-based approach for stochastic optimization under uncertainty, which builds upon scenario analysis. We apply and evaluate the performance of our approach to address the problem of analyzing competing biomass supply chains subject to stochastic demand and supply. The proposed approach was found to outperform alternative methods in terms of computational efficiency and ability to meet the stochastic problem requirements.",
     "keywords": ["Uncertainty", "Scenario analysis", "Optimization", "Renewable energy systems", "Biomass"]},
    {"article name": "In situ adaptive tabulation (ISAT) for combustion chemistry in a network of perfectly stirred reactors (PSRs)",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.023",
     "publication date": "02-2017",
     "abstract": "This paper presents an efficient computational implementation of the in situ adaptive tabulation (ISAT) approach (Pope, 1997) for combustion chemistry in a network of perfectly stirred reactors (PSRs). A series of PSR calculations is carried out using the direct integration (DI) and ISAT approaches, and validation of DI is performed through comparisons with previous experiments. Assessment of the accuracy of ISAT approach is conducted through direct comparisons with DI calculations. Moreover, accessed region of the composition space, sensitivity of ISAT calculations with respect to the absolute error tolerance values and speedup are analyzed for two different test cases, a hydrogen\u2013air and an ethylene\u2013air combustion case. In summary, the hydrogen\u2013air case resulted in a speedup of 9.8 for 1 million of PSRs in series, whereas for the ethylene\u2013air case it was 42 for 0.3 million PSRs.",
     "keywords": ["Perfectly stirred reactor", "ISAT", "Direct integration", "Reaction mapping gradient"]},
    {"article name": "A multitasking continuous time formulation for short-term scheduling of operations in multipurpose plants",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.012",
     "publication date": "02-2017",
     "abstract": "Short-term scheduling in multipurpose batch plants has received significant attention in the past two decades. Both discrete-time and continuous time formulations have been proposed to model the problem; however, multipurpose plants that have machines with the ability to process multiple tasks at the same time, i.e. multitasking, have been overlooked by the available continuous time formulations in the literature. This paper presents a novel MILP formulation that is capable of accommodating the multitasking feature in the machines of a facility. The performance of the presented formulation is studied in comparison with a single-tasking formulation. The results show that, while the multitasking formulation is not more costly in terms of solution time, it is capable of producing significantly better solutions. The presented formulation takes into account several other operational constraints of multipurpose facilities and can be readily applied to facilities that have machines capable of multitasking, including plants in the analytical services sector.",
     "keywords": ["Scheduling", "Continuous time formulation", "MILP", "Analytical services sector", "Multitasking", "Multipurpose batch plants"]},
    {"article name": "A volume-consistent discrete formulation of particle breakage equation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.013",
     "publication date": "02-2017",
     "abstract": "We introduce a finite volume scheme to approximate the one dimensional breakage equations. An interesting feature is that it is simple in mathematical formulation and predicts particle number density and its moments with improved accuracy. Efficiency of the new scheme is compared with the existing finite volume scheme proposed by Bourgade and Filbet (2008) over some test problems. It is seen that the new scheme preserves the volume conservative property of the previous scheme and additionally gives an improved estimation of the particle number density and its zero-order moment. Furthermore, the new scheme is computationally more efficient than the existing one. A detailed mathematical analysis including convergence and consistency of the new scheme is also performed. This analysis proves that the new scheme follows a second order convergence rate irrespective of the nature of the meshes. Several example problems are solved numerically to validate the results.",
     "keywords": ["65R20", "Fragmentation equation", "Finite volume schemes", "Volume conservation", "Consistency", "Convergence"]},
    {"article name": "A parametric evaluation of powder flowability using a Freeman rheometer through statistical and sensitivity analysis: A discrete element method (DEM) study",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.034",
     "publication date": "02-2017",
     "abstract": "The flowability of powders in a Freeman Rheometer (FT4) is explored in this study using discrete element method (DEM). Five DEM input parameters describing particle properties: static and rolling friction coefficients, coefficient of restitution, Young\u2019s modulus and cohesion energy density (using JKR cohesion model) were explored in a matrix of simulations using Design of Simulation (DoS) principles. The impact of these parameters was assessed against two responses from the FT4 test: the basic flowability energy (BFE) and specific energy (SE). By using a combination of empirical effects and interactions analysis and principal component analysis (PCA), it was found that static and rolling friction parameters play a critical role in determining the BFE and SE of a powder, whilst cohesion energy density also plays a significant role in influencing BFE. The combination of these methods has helped deliver a roadmap to show which parameters would be effectively calibrated on an FT4.",
     "keywords": ["Discrete element method", "Freeman rheometer", "Flowability", "Particle properties", "Parameter estimation", "Principal component analysis"]},
    {"article name": "Cell agglomeration algorithm for coupling microkinetic modeling and steady-state CFD simulations of catalytic reactors",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.033",
     "publication date": "02-2017",
     "abstract": "We propose the application of a Cell Agglomeration (CA) algorithm for coupling detailed microkinetic models with multi-region steady-state Computational Fluid Dynamics (CFD) simulations of catalytic reactors. This numerical methodology \u2013 originally developed for dynamic CFD simulation with detailed gas-phase kinetics \u2013 is herein applied in the context of steady-state microkinetic CFD simulations of catalytic reactors by exploiting the particular structure of the governing equations of the adsorbed species, which are characterized by the absence of the transport term. The potentialities of the method are assessed by the analysis of different reactor geometries and microkinetic mechanisms in a wide range of operating conditions. Our tests show this method to allow for a reduction of the computational time up to an order of magnitude. Thus, the CA algorithm turns out to be a useful tool to enable the detailed and fundamental simulation of catalytic devices in steady-state conditions.",
     "keywords": ["Cell agglomeration", "Microkinetic", "Heterogeneous catalysis", "CFD"]},
    {"article name": "Concentrated high intensity electric field (CHIEF) system for non-thermal pasteurization of liquid foods: Modeling and simulation of fluid mechanics, electric analysis, and heat transfer",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.044",
     "publication date": "02-2017",
     "abstract": "This study develops an integrated, multi-scale model of the pilot concentrated high intensity electric field (CHIEF) system using the finite element method (FEM). The CHIEF system is a novel non-thermal pasteurization technology that uses medium to low voltage (\u226410\u00a0kV) frequency AC (60\u00a0Hz) power supply, and offers satisfactory pasteurization results for liquid food such as juice and milk. The model in this study provides accurate predictions for the fluid behavior, electric field distribution and temperature profile and is validated using experimental results. Results from the model shows that the CHIEF system can provide electric field up to 4000\u00a0kV/m with a power supply of 10\u00a0kV, enabling a 6-log reduction of bacteria kill. Furthermore, recommendations and optimizations are made based on the modeling results, which would benefit for the process scale-up and design.",
     "keywords": ["Non-thermal pasteurization", "Electric field treatment", "Fluid simulation", "Finite element method (FEM)"]},
    {"article name": "Integrated model of refining and petrochemical plant for enterprise-wide optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.020",
     "publication date": "02-2017",
     "abstract": "In this paper, a novel integrated optimization approach which couples the up-stream refinery and the down-stream ethylene plant is proposed. When the material balances of the intermediate products between the two complex processes are considered, the potential of increasing the overall margin can be explored. A multi-period enterprise-wide mixed-integer nonlinear programming (MINLP) model is formulated to optimize the production planning of the processing units in the refinery and the ethylene plant simultaneously. Due to the model complexity, the Lagrangian algorithm is applied to decompose the integrated mathematical model into an MILP problem for the refinery and a small-scale MINLP problem for the ethylene plant. The performance of the proposed model was investigated on industrial examples to illustrate the economic potential and trade-offs involved in the enterprise-wide network. Results show that the integrated approach gains improvement in overall profit compared with the traditional sequential approach.",
     "keywords": ["Integrated optimization", "Production planning", "Refinery", "Ethylene plant", "Lagrangian decomposition"]},
    {"article name": "An energy-efficient operation system for a natural gas liquefaction process: Development and application to a 100 ton-per-day plant",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.046",
     "publication date": "02-2017",
     "abstract": "Production of liquefied natural gas (LNG) is a highly energy-intensive process. The required liquefaction temperature is approximately \u2212160\u00a0\u00b0C at atmospheric pressure. In this study, we propose a two-level energy-efficient operation system for an LNG process, which consists of real-time steady-state optimization (RTSSO) and real-time control subsystems. The RTSSO follows the typical design to minimize set point energy losses. In contrast, the real-time control subsystem was configured to incorporate the concept of self-optimizing control, which minimizes both implementation energy losses and set point energy losses. Special attention was given to the loss of the evaluation step so that the implementation energy loss is accurately estimated; thus, the designed system can be seamlessly applied to the actual process with guaranteed optimal energy efficiency. The performance of the proposed system was validated in a simulated LNG plant that precisely replicated an actual plant that produces 100 tons of LNG per day.",
     "keywords": ["Natural gas", "Liquefaction", "Refrigeration", "Control", "Optimization", "Mixed refrigerant"]},
    {"article name": "Automata-based operating procedure for abnormal situation management in batch processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.042",
     "publication date": "02-2017",
     "abstract": "\u201cAbnormal situation management\u201d (ASM) in general refers to the various tasks required for online fault diagnosis and also hazard mitigation. Although quite a few ASM-related studies have already been carried out in the past, none of them addressed the wide range of issues consistently and rigorously with the same modeling tool. An automata-based strategy is therefore proposed in this work to synthesize all operating procedures needed for diagnostic tests and also other emergency response operations in the batch processes. The proposed model building techniques are suitable not only for characterizing all components in any given process, but also for representing the operation targets of all ASM tasks. Finally, notice that every resulting procedure can be readily expressed with an implementable sequential function chart (SFC).",
     "keywords": ["Automata", "Abnormal situation management", "Batch processes", "Emergency response procedure", "Dynamic simulation"]},
    {"article name": "Modelling and dynamic simulation of a large MSF plant using local phase equilibrium and simultaneous mass, momentum, and energy solver",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.039",
     "publication date": "02-2017",
     "abstract": "Seawater desalination is an important method for producing fresh water, a critical resource in many areas of the globe. Multi-stage flash distillation (MSF) is a leading technology within the thermal desalination field. Mathematical modelling and simulation provides a means to enhance engineering and operation of complicated production plants. This paper presents a new method for one-dimensional modelling and dynamic simulation of thermal desalination processes. The approach combines the simultaneous mass, momentum, and energy solution, local phase equilibrium by Rachford-Rice equation, and rigorous calculation of the seawater properties as function of temperature, pressure and salinity. A brine recycling MSF plant was modelled as a case study, presenting advanced and unpublished simulated features and transients. The successful results suggest that the method presented is a competent approach for dynamic simulation of thermal desalination processes.",
     "keywords": ["Seawater", "Desalination", "Multistage flashing", "Modelling", "Dynamic simulation"]},
    {"article name": "Assessment of a POD method for the dynamical analysis of a catalyst pellet with simultaneous chemical reaction, adsorption and diffusion: Uniform temperature case",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.009",
     "publication date": "02-2017",
     "abstract": "A model reduction method applied to the dynamic model of a single isothermal catalyst pellet with simultaneous chemical reaction, diffusion and adsorption is presented. The model of the pellet accounts for both internal and external resistances to mass transfer, and variable bulk gas concentration.The reduction of the model was performed by means of proper orthogonal decomposition (POD). Accuracy and computational efficiency of the reduced order model (ROM) were discussed for two kinetic models, i.e. a first order chemical kinetics and a Langmuir\u2013Hinshelwood kinetics. To improve the computational efficiency of the model described by the Langmuir\u2013Hinshelwood equation, the non-linear terms were approximated by a discrete empirical interpolation method (DEIM). High accuracy and efficiency of the proposed reduction approach was demonstrated.",
     "keywords": ["Catalyst pellets", "Dynamics", "Model reduction", "Proper orthogonal decomposition", "Discrete empirical interpolation method"]},
    {"article name": "Integrated modeling to capture the interaction of physiology and fluid dynamics in biopharmaceutical bioreactors",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.037",
     "publication date": "02-2017",
     "abstract": "The performance of a bioreactor is sensitive to local gradients of chemical and physical stimuli. Thus, this work presents a model, which captures spatial heterogeneity and interactions of biotic and abiotic phases in animal cell cultures. A computational fluid dynamics simulation that includes gas-liquid mass transfer and kinetics of carbon dioxide dissolution is developed to capture the variations of environmental parameters. Unstructured modeling is implemented to integrate growth, viability and productivity of cells. While predictive accuracy is valuable, it is important to balance it with computational feasibility. In this work, evolutions of hydrodynamics and cell population are obtained sequentially. The outcome is a deterministic model with extended integration between physical and biological phenomena which is computationally tractable. The model calculates the bioreactor performance as a function of time and process parameters such as impeller rotation speed and gas sparging flow rate, which makes it useful for bioprocess design and scheduling.",
     "keywords": ["Bioreactor modeling", "CFD", "Unstructured metabolic model", "Integrated modeling", "Dynamic process simulation"]},
    {"article name": "Multi-objective synthesis of energy systems: Efficient identification of design trade-offs",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.10.010",
     "publication date": "02-2017",
     "abstract": "The synthesis of energy systems usually has to consider several conflicting objectives leading to a large set of Pareto-optimal solutions with multiple trade-offs. From this large set of solutions, good compromise solutions have to be identified which is a complex and computationally demanding task. We therefore propose a method to reduce both the set of objectives and the solution space: First, the set of objectives is reduced by employing a method from the literature to determine the objectives best representing the design trade-offs. However, in practice, aggregated costs are the decisive criterion. Thus, in a second step, the solution space of the synthesis problem is restricted to an acceptable deviation from minimal aggregated costs. Thereby, only relevant solutions are obtained. The two steps significantly reduce the effort for multi-objective optimization focusing on the most relevant part of the solutions. The proposed method is applied to a real-world case study.",
     "keywords": ["Energy systems", "Synthesis", "Multi-objective optimization", "Objective function", "MILP"]},
    {"article name": "A superstructure-based framework for bio-separation network synthesis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.10.007",
     "publication date": "01-2017",
     "abstract": "Modern biotechnologies enable the production of chemicals using engineered microorganisms. However, the cost of downstream recovery and purification steps is high, which means that the feasibility of bio-based chemicals production depends heavily on the synthesis of cost-effective separation networks. To this end, we develop a superstructure-based framework for bio-separation network synthesis. Based on general separation principles and insights obtained from industrial processes for specific products, we first identify four separation stages: cell treatment, product phase isolation, concentration and purification, and refinement. For each stage, we systematically implement a set of connectivity rules to develop stage-superstructures, all of which are then integrated to generate a general superstructure that accounts for all types of chemicals that can be produced using microorganisms. We further develop a superstructure reduction method to solve specific instances, based on product attributes, technology availability, case-specific considerations, and final product stream specifications. A general optimization model, including short-cut models for all technologies, is formulated. The proposed framework enables preliminary synthesis and analysis of bio-separation networks, and thus estimation of separation costs.",
     "keywords": ["Mixed integer nonlinear programing", "Process optimization", "Global optimization", "Renewable chemicals"]},
    {"article name": "Variants to a knapsack decomposition heuristic for solving R&D pipeline management problems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.10.011",
     "publication date": "01-2017",
     "abstract": "The knapsack decomposition algorithm (KDA) (Christian and Cremaschi, 2015) decomposes the R&D pipeline management problem into a series of knapsack problems, which are solved along the planning horizon. It yields tight feasible solutions, and improves the solution times by several orders of magnitude for large instances. This paper investigates the impact of problem parameters and size, and KDA decision rules on KDA solution quality and time. The decision rules are (1) timing of new knapsack problem generations, and (2) formulation of the resource constraints in knapsack problems. The results revealed that the KDA decision trees were insensitive to problem parameters, and the KDA solution times grew super-linearly with linear increases in the length of the planning horizon and the number of products. The results suggest that the KDA where knapsack problems are generated after each realization with the original resource constraint yields the most accurate solutions in the quickest time.",
     "keywords": ["Multistage stochastic programming", "Pharmaceutical R&D pipeline management", "Decision dependent uncertainty", "Endogenous uncertainty", "Knapsack decomposition algorithm"]},
    {"article name": "Numerical solution of the population balance equation using an efficiently modified cell average technique",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.10.012",
     "publication date": "01-2017",
     "abstract": "In order to increase the accuracy of the modified cell average technique (mCAT) alongside the zeroth and first moments and increase the speed of the technique, a new supplemental solver has been developed in the region of high amount time, a region where the ratio of the zeroth moment to the initial zeroth moment (\u03bc0(t)/\u03bc0(0)) is too small or too large depending on the process and the amount of error is high there. The solver is conceptually easy to understand and straightforward for programming. A number of analytically solved problems were simulated by incorporating this solver into the mCAT and the results were compared with the previous solver. Significant improvements for different phenomenon especially the aggregation related systems were observed.",
     "keywords": ["mCAT", "Pivot-based technique", "Population balance equation", "Region of high amount of time"]},
    {"article name": "Multiple adaptive mechanisms for data-driven soft sensors",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.08.017",
     "publication date": "01-2017",
     "abstract": "Recent data-driven soft sensors often use multiple adaptive mechanisms to cope with non-stationary environments. These mechanisms are usually deployed in a prescribed order which does not change. In this work we use real world data from the process industry to compare deploying adaptive mechanisms in a fixed manner to deploying them in a flexible way, which results in varying adaptation sequences. We demonstrate that flexible deployment of available adaptive methods coupled with techniques such as cross-validatory selection and retrospective model correction can benefit the predictive accuracy over time. As a vehicle for this study, we use a soft-sensor for batch processes based on an adaptive ensemble method which employs several adaptive mechanisms to react to the changes in data.",
     "keywords": ["Soft sensors", "Adaptive mechanisms", "Streaming data", "Ensemble methods"]},
    {"article name": "Preemptive multi-skilled resource investment project scheduling problem: Mathematical modelling and solution approaches",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.11.001",
     "publication date": "01-2017",
     "abstract": "Many chemical production systems encounter with a functionally flexible workplace which need multi-skilled workers. This research integrates the multi-skilled project scheduling problem with the resource investment problem which aims to obtain concurrent optimal policy of project scheduling and skills\u2019 recruitment. The objective of the proposed model is minimization of total recruitment cost for different levels of the skills. An integer programming formulation is developed for the problem, and it is validated through solving several small scale instances using the GAMS software. To tackle real-life problem scales, a genetic-based and a particle-swarm-based algorithm with calibrated parameters and chromosome structures that guarantee feasibility are proposed. The performances of the solution procedures are evaluated by comparing the results obtained and the ones resulted by the GAMS software. In addition, the robustness of the algorithms\u2019 results for large scale instances is evaluated in different runs confirms the applicability of the proposed methodology.",
     "keywords": ["Multi-skilled", "Resource investment problem", "RCPSP", "Preemption", "Meta-heuristics"]},
    {"article name": "Application of dodecahedron to describe the switching strategies of asynchronous simulated-moving-bed",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.10.016",
     "publication date": "01-2017",
     "abstract": "Among various separation techniques, the simulated moving bed (SMB) process has received special attention, especially the asynchronous SMB that allocates the columns into four zones in a flexible way. With the relative switching times as variables in the Cartesian coordinates system, all applicable switching strategies in asynchronous SMB can be visualized in a dodecahedron with the initial configuration as the origin. Thus, any point in the dodecahedron represents a switching strategy, which could be easily obtained with initial configuration and coordinates. The dodecahedron includes 14 traditional SMB configurations, represented by the 14 vertexes of the dodecahedron. Based on the dodecahedron, the optimization of the asynchronous SMB was conducted through two case studies. Compared to traditional SMB, the feed flow rates were increased by 87% in the enantioseparation of 1,1\u2032-bi-2-naphthol and 15% in the separation of glucose and fructose without the loss of product purities.",
     "keywords": ["Asynchronous SMB", "Dodecahedron", "Visualization", "Optimization", "Separation techniques"]},
    {"article name": "Parameter estimation with estimability analysis applied to an industrial scale polymerization process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.10.013",
     "publication date": "01-2017",
     "abstract": "This paper aims to estimate the parameters of a complex model representing an industrial scale polymerization process. The estimability analysis of the parameters prior to estimation allows simplifying the optimization problem but it is usually neglected in literature when industrial data is used for estimation. In this case, though, the estimability analysis would be even more important since usually less data is available, they are associated with a higher uncertainty and the experiments might not be designed as in laboratory or pilot plant. The orthogonalization method reduced from 68 to 29 the number of parameters of the model. Polymer properties, which are measured offline with low frequency, as well as process temperatures and flow rates are used for validating the model. Small deviations, up to 5%, between model prediction and experimental data indicate the quality of fit of the model and the importance of carrying out first an estimability analysis.",
     "keywords": ["Orthogonalization", "Estimability analysis", "Polymerization", "Parameter estimation"]},
    {"article name": "Bi-level optimizing operation of natural gas liquefaction process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.10.009",
     "publication date": "01-2017",
     "abstract": "The production of liquefied natural gas (LNG) is a highly energy intensive process, as required liquefaction temperature is approximately \u2212160\u00a0\u00b0C at atmospheric pressure. In this study, we propose a novel bi-level optimizing operation system for an LNG process, which consists of a real-time steady-state optimizer (RTSSO) and a decentralized control system. The RTSSO computes the optimal operating conditions such that the compressor power is minimized, while the decentralized control system performs real-time feedback actions to attain the target operating points against various disturbances. Special attention was given to the decentralized control system so that i) the process operation can be rapidly stabilized, and ii) the developed system can be seamlessly applied to an actual process. The performance of the proposed operation system was validated in a numerical LNG plant that precisely replicates an actual plant that produces 100\u00a0t of LNG per day.",
     "keywords": ["Natural gas", "Liquefaction", "Refrigeration", "LNG", "Optimizing control"]},
    {"article name": "Smart Sampling Algorithm for Surrogate Model Development",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.10.006",
     "publication date": "01-2017",
     "abstract": "Surrogate modelling aims to reduce computational costs by avoiding the solution of rigorous models for complex physicochemical systems. However, it requires extensive sampling to attain acceptable accuracy over the entire domain. The well-known space-filling techniques use sampling based on uniform, quasi-random, or stochastic distributions, and are typically non-adaptive. We present a novel technique to select sample points systematically in an adaptive and optimized manner, assuring that the points are placed in regions of complex behaviour and poor representation. Our proposed smart sampling algorithm (SSA) solves a series of surrogate-based nonlinear programming problems for point placement to enhance the overall accuracy and reduce computational burden. Our extensive numerical evaluations using 1-variable test problems suggest that our SSA performs the best, when its initial sample points are generated using uniform sampling. For now, this conclusion is valid for 1-variable functions only, and we are testing our algorithm for n-variable functions.",
     "keywords": ["Smart sampling", "Adaptive surrogate modelling", "Point placement"]},
    {"article name": "Numerical investigation of erosion of tube sheet and tubes of a shell and tube heat exchanger",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.10.005",
     "publication date": "01-2017",
     "abstract": "The failure of shell and tube heat exchangers caused by solid particle erosion has been a major problem in the oil and gas and other industries. Predicting erosion is still a developing art, an accurate simulation method is then significant to analyze the erosion characteristics in such complex geometry and determine erosion rate of metal surface. In this work a physical model was proposed to simulate the erosion of two-pass shell and tube heat exchangers with computational fluid dynamics. The simulation was performed for different feed fluid rates and a range of sand particle sizes from 0.1 to 1000\u00a0\u03bcm. The erosion rates of tube sheet, tube ends in the inlet plenum and the inner wall of tubes were monitored and the influences of flow pattern, particle size and particle behaviors on erosion were studied. The predictions are compared with the earlier studies and a good agreement was found. The particles can be classified into three groups based on the dependence of erosion rates of tube sheet and tubes on the particle size. The large particles (>200\u00a0\u03bcm) exhibited a near-linear influence on the erosion rates. The small particles (about 50\u2013200\u00a0\u03bcm) produced approximate size-independence facet-average erosion rate of tubes, but the maximum local erosion rates of the tubes and tube sheet sharply increased with the decrease of particle size. The fine particles (<about 50\u00a0\u03bcm) resulted in low facet-average erosion rates but very high local erosion rate. The erosion at the tube sheet, tube end and tube surface also show different aspects of relation with particle size.",
     "keywords": ["Erosion", "Heat exchanger", "Sand particles", "Computational fluid dynamics"]},
    {"article name": "Numerical simulation of nanocrystal synthesis in a microfluidic reactor",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.10.004",
     "publication date": "01-2017",
     "abstract": "Modeling of continuous and controlled nanocrystal synthesis in a microfluidic reactor is presented. The population balance model that describes the nanocrystal synthesis consists of a population balance equation and a set of species concentration equations. In order to incorporate the effects of both reaction and diffusion limited growth conditions, a kinetic model with size-dependent growth and nucleation rate expressions are considered. An efficient finite element scheme based on Strang splitting that handles size-dependent particle diffusion and non-uniform growth expressions in the high dimensional population balance equation is proposed to solve the model equations. After the validation of the numerical scheme, an array of parametric studies is performed to study the effects of the flow condition and the growth environment on the nanocrystal synthesis in the microfluidic reactor. The computational results are consistent with the experimental observations.",
     "keywords": ["Nanocrystal synthesis", "Microfluidic reactor", "Population balance model", "Strang-splitting", "Finite element modeling"]},
    {"article name": "Optimal control of a laboratory binary distillation column via regionless explicit MPC",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.10.003",
     "publication date": "01-2017",
     "abstract": "This paper shows how to construct and implement explicit MPC feedback laws for systems with a large number of states. Specifically, the construction of critical regions of the explicit MPC solution is replaced by a two-step procedure. First, all optimal active sets are enumerated off-line and corresponding KKT matrices are pre-factored. Then, in the on-line step, the optimal control inputs are identified by checking primal and dual feasibility conditions using the pre-factored data. The feasibility and performance of the proposed approach are experimentally demonstrated on the control of the laboratory binary distillation column, described by a dynamical model with 10 state variables. We show that the control algorithm requires only modest computational resources on-line. A comparison of the memory and computational demands of the proposed method with the on-line solution of the corresponding quadratic program via the state-of-the-art solver as well as the approximated solution via the first-order method is made.",
     "keywords": ["Explicit predictive control", "Optimization", "Process control", "Distillation"]},
    {"article name": "A novel disjunctive model for the simultaneous optimization and heat integration",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.10.002",
     "publication date": "01-2017",
     "abstract": "This paper introduces a new disjunctive formulation for the simultaneous optimization and heat integration of systems with variable inlet and outlet temperatures in process streams as well as the possibility of selecting and using different utilities. The starting point is the original compact formulation of the Pinch Location Method, however, instead of approximating the \u201cmaximum\u201d operator with smooth, but non-convex functions, these operators are modeled by means of a disjunction. The new formulation has shown to have equal or lower relaxation gap than the best alternative reformulation, thus reducing computational time and numerical problems related to non-convex approximations.",
     "keywords": ["Heat integration", "Variable temperatures", "Disjunctive model", "Simultaneous optimization"]},
    {"article name": "Novel method for looped pipeline network resolution",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.10.001",
     "publication date": "01-2017",
     "abstract": "It is proposed a novel method to solve looped pipeline network problems that seeks to deal with limitations of the available methods The problem is modeled as a nonlinear system of equations formed by equations that cannot be solved sequentially, characterizing the resolution as a simultaneous-modular procedure. The equations of the system are the differences between the final pressure of the pipes that end at the same network nodes and the difference between the specified and calculated design variables. At the solution both Kirchhoff\u2019s laws are met, being the method main advantages the no need of independent loops selection and the formulation of a reduced system of equation. Case studs with a small and a big looped water pipeline network, and an industrial installation with looped pipeline configuration, are solved. The latter shows the method applicability for design process, highlighting its advantages in comparison with the traditional simulation procedures.",
     "keywords": ["Looped pipeline network", "Simultaneous-modular procedure", "Hardy-cross", "Kirchhoff\u2019s laws", "Nonlinear system of equations", "Process simulation"]},
    {"article name": "Fully compositional multi-scale reservoir simulation of various CO2 sequestration mechanisms",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.09.021",
     "publication date": "01-2017",
     "abstract": "A multi-scale reservoir simulation framework for large-scale, multiphase flow with mineral precipitation in CO2-brine systems is proposed. The novel aspects of this reservoir modeling and simulation framework are centered around the seminal coupling of rigorous reactive transport with full compositional modeling and consist of (1) thermal, multi-phase flow tightly coupled to complex phase behavior, (2) the use of the Gibbs-Helmholtz Constrained (GHC) equation of state, (3) the presence of multiple homogeneous/heterogeneous chemical reactions, (4) the inclusion of mineral precipitation/dissolution, and (5) the presence of homogeneous/heterogeneous formations. The proposed modeling and simulation framework is implemented using the ADGPRS/GFLASH system. A number of examples relevant to CO2 sequestration including salt precipitation and solubility/mineral trapping are presented and geometric illustrations are used to elucidate key attributes of the proposed modeling framework.",
     "keywords": ["Mineral deposition/dissolution", "Carbon sequestration", "Gibbs-Helmholtz constrained equation of state", "Numerical reservoir simulation"]},
    {"article name": "Optimal operation and stabilising control of the concentric heat-integrated distillation column (HIDiC)",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.09.020",
     "publication date": "01-2017",
     "abstract": "This paper presents the application of a systematic control configuration design procedure on the HIDiC with a reboiler. The application is illustrated through two case studies of industrial relevance, namely the separation of benzene/toluene and a multicomponent mixture of aromatic compounds. Results of static optimisations and dynamic simulations are presented based on a realistic column model, which accounts for dynamic pressure drops and liquid holdups, dynamic energy balances and more. Using a decentralised control scheme, good stabilising and economic performance are achieved by controlling both column section pressures and the temperature profile in one of the sections, while the economic variables are controlled by cascade control loops. Guidelines for the design of both the regulatory control layer and the supervisory control layer are provided.",
     "keywords": ["Control system design", "Distillation columns", "HIDiC", "Optimal operation", "Heat integration"]},
    {"article name": "Simultaneous scheduling of front-end crude transfer and refinery processing",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.09.019",
     "publication date": "01-2017",
     "abstract": "Scheduling of front-end crude-oil transfer and refinery processing are two critically important and challenging tasks to petroleum refineries. However, the simultaneous scheduling of front-end crude-oil transfer and refinery operations has never been considered in previous studies due to the large scale and complexity of the resultant optimization problem. In this paper, a systematic methodology for simultaneous scheduling of front-end crude transfer and refinery processing has been developed. It provides a large-scale continuous-time based scheduling model for crude unloading, transferring, and processing (CUTP) to simulate and optimize the front-end and refinery crude-oil operations simultaneously. The CUTP model consists of a newly developed refinery processing sub model, a crude processing status transition sub-model, and a borrowed front-end crude transferring sub model. The objective is to maximize the total operational profit while satisfying various constraints such as operation and production specifications, inventory limits, and production demands. The efficacy of the proposed scheduling model has been demonstrated by an industrial-scale case study.",
     "keywords": ["Crude scheduling", "Refinery processing", "Production scheduling", "MINLP", "Optimization"]},
    {"article name": "Accelerating optimization and uncertainty quantification of nonlinear SMB chromatography using reduced-order models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.09.017",
     "publication date": "01-2017",
     "abstract": "A parametrized reduced-order model is constructed and employed as a surrogate for the full-order model in optimization and uncertainty quantification of nonlinear simulated moving bed chromatography. The reduced-order model is obtained by the reduced basis method using an efficient error estimation. The complexity of the model is reduced by an empirical interpolation method applied to the nonlinear part of the model. Due to the reduced size and complexity of the surrogate model, the processes of optimization and uncertainty quantification are sped up by a factor of 10.",
     "keywords": ["Reduced basis method", "Empirical interpolation", "Error estimation", "Simulated moving bed chromatography", "Optimization", "Uncertainty quantification"]},
    {"article name": "A predictive model for spiral wound reverse osmosis membrane modules: The effect of winding geometry and accurate geometric details",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.07.029",
     "publication date": "01-2017",
     "abstract": "A new one-dimensional predictive model for spiral wound modules (SWMs) applied to reverse osmosis membrane systems is developed by incorporating a detailed description of the geometric features of SWMs and considering flow in two directions. The proposed model is found to capture existing experimental data well, with similar accuracy to the widely-used plate model in which the SWM is assumed to consist of multiple thin rectangular channels. However, physical parameters that should in principle be model-independent, such as membrane permeability, are found to differ significantly depending on which model is used, when the same data sets are used for parameter estimation. Conversely, when using the same physical parameter values in both models, the water recovery predicted by the plate-like model is 12\u201320% higher than that predicted by the spiral model. This discrepancy is due to differences in the description of geometric features, in particular the active membrane area and the variable channel heights through the module, which impact on predicted performance and energy consumption. A number of design variables \u2013 the number of membrane leaves, membrane dimensions, centre pipe radius and the height of feed and permeate channels \u2013 are varied and their effects on performance, energy consumption and calculated module size are analysed. The proposed spiral model provides valuable insights into the effects of complex geometry on the performance of the SWM as well as of the overall system, at a low computational cost.",
     "keywords": ["Reverse osmosis", "Spiral wound module", "Predictive modelling", "Winding effects", "Geometric parameters"]},
    {"article name": "Some properties of the GHC equation of state",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.09.004",
     "publication date": "12-2016",
     "abstract": "The composition functionality of the mixture energy parameter, aM, used in the Gibbs-Helmholtz Constrained (GHC) equation of state is studied. An analysis is presented that shows that aM for liquid mixtures is approximately quadratic in composition. All non-quadratic behavior is due solely to ln(TcM) in the GHC up-scaling equation. If a mean value approximation of this term is used, then aML is quadratic, but non-symmetric, in composition. For vapors, non-quadratic behavior is coupled to the molar volume of the mixture through the term \u03b2M\u00a0=\u00a0(VM\u00a0+\u00a0bM)/VM. It is shown that the difference between \u03b2M and some average \u03b2M is very small and aMV is quadratic in composition. Sensitivity analyses of aM and resulting molar density to these approximations are also presented. Finally, the non-symmetric composition functionality of aM is discussed along with GHC predictions of mixture second virial coefficients. Numerical examples are presented to support all claims.",
     "keywords": ["Composition functionality of GHC energy parameter", "Sensitivity analysis", "Second virial coefficients"]},
    {"article name": "Economic model predictive control of chemical processes with parameter uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.08.010",
     "publication date": "12-2016",
     "abstract": "This work proposes an EMPC (Economic Model Predictive Control) algorithm that integrates RTO (Real Time Optimization) and EMPC objectives within a single optimization calculation. Robust stability conditions are enforced on line through a set of constraints within the optimization problem.A particular feature of this algorithm is that it constantly calculates a set point with respect to which stability is ensured by the aforementioned constraints while searching for economic optimality over the horizon. In contrast to other algorithms reported in the literature, the proposed algorithm does not require terminal constraints or penalty terms on deviations from fixed set points that may lead to conservatism.Changes in model parameters over time are also compensated for through parameter updating. The latter is accomplished by including the parameters\u2019 values as additional decision variables within the optimization problem.Several case studies are presented to demonstrate the algorithm\u2019s performance.",
     "keywords": ["Real time optimization", "Economic nonlinear predictive control"]},
    {"article name": "Optimal integrated energy systems design incorporating variable renewable energy sources",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.08.007",
     "publication date": "12-2016",
     "abstract": "The effect of variability in renewable input sources on the optimal design and reliability of an integrated energy system designed for off-grid mining operation is investigated via a two-stage approach. Firstly, possible energy system designs are generated by solving a deterministic non-linear programming (NLP) optimization problem to minimize the capital cost for a number of input scenarios. Two measures of reliability, the loss of power supply probability (LPSP) and energy index of reliability (EIR), are then evaluated for each design based on the minimization of the external energy required to satisfy load demands under a variety of input conditions. Two case studies of mining operations located in regions with different degrees of variability are presented. The results show that the degree of variability has an impact on the design configuration, cost and performance, and highlights the limitations associated with deterministic decision making for high variability systems.",
     "keywords": ["Energy storage", "Power generation", "Reliability", "Renewables variability", "Systems design"]},
    {"article name": "An adaptive discretization MINLP algorithm for optimal synthesis of decentralized energy supply systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.09.008",
     "publication date": "12-2016",
     "abstract": "Decentralized energy supply systems (DESS) are highly integrated and complex systems designed to meet time-varying energy demands, e.g., heating, cooling, and electricity. The synthesis problem of DESS addresses combining various types of energy conversion units, choosing their sizing and operations to maximize an objective function, e.g., the net present value. In practice, investment costs and part-load performances are nonlinear. Thus, this optimization problem can be modeled as a nonconvex mixed-integer nonlinear programming (MINLP) problem. We present an adaptive discretization algorithm to solve such synthesis problems containing an iterative interaction between mixed-integer linear programs (MIPs) and nonlinear programs (NLPs). The proposed algorithm outperforms state-of-the-art MINLP solvers as well as linearization approaches with regard to solution quality and computation times on a test set obtained from real industrial data, which we made available online.",
     "keywords": ["Mixed-integer nonlinear programming", "Decentralized energy supply system", "Synthesis", "Structural optimization", "Adaptive discretization"]},
    {"article name": "Thermodynamic interpolation for the simulation of two-phase flow of non-ideal mixtures",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.09.005",
     "publication date": "12-2016",
     "abstract": "This paper describes the development and application of a technique for the rapid interpolation of thermodynamic properties of mixtures for the purposes of simulating two-phase flow. The technique is based on adaptive inverse interpolation and can be applied to any Equation of State and multicomponent mixture. Following analysis of its accuracy, the method is coupled with a two-phase flow model, based on the homogeneous equilibrium mixture assumption, and applied to the simulation of flows of carbon dioxide (CO2) rich mixtures. This coupled flow model is used to simulate the experimental decompression of binary and quinternary mixtures. It is found that the predictions are in good agreement with the experimental data and that the interpolation approach provides a flexible, robust means of obtaining thermodynamic properties for use in flow models.",
     "keywords": ["Carbon dioxide transport", "Two-phase flow", "Equations of state", "Pipeline safety"]},
    {"article name": "A diagnostic method based on clustering qualitative event sequences",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.09.001",
     "publication date": "12-2016",
     "abstract": "A diagnostic algorithm is described in this article that is based on clustering qualitative event sequences called traces. A sufficient number of training traces are used instead of an internal model to specify the faulty models of the system. The diagnosis consists of two phases. In the off-line training phase diagnostic clusters representing nominal and faulty behavior are formed from the set of training traces, while the centroids of these clusters are stored. Arbitrary measured traces in the on-line diagnosis phase are compared with the centroids, to recognize the most probable faulty scenario for the trace. The effects of different mapping functions and different qualitative ranges on the clustering are investigated, and the diagnostic resolution of the method is compared and discussed using a simple process system. A diagnostic case study using the benchmark of Tennessee Eastman process (TEP) is utilized to illustrate the efficiency of the proposed method.",
     "keywords": ["Fault diagnostics", "Qualitative diagnosis", "Clustering", "Tennessee Eastman process"]},
    {"article name": "On the various local solutions to a two-input dynamic optimization problem",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.09.003",
     "publication date": "12-2016",
     "abstract": "Solving a multi-input dynamic optimization of a batch processes is a complex problem involving interactions between input variables and constraints over time. The problem gets more difficult due to the presence of local solutions that have almost the same cost but widely varying structures. This paper studies various local optimal solutions for a non-isothermal semi-batch reactor with the feed rate and temperature as inputs and a heat removal constraint. Three solution patterns were studied, all consisting in meeting the heat removal constraint for the first part and seeking the compromise between the main and side reactions in the later part. A sensitivity analysis shows that the best solution pattern among those studied does not change with variations in parameters or initial conditions.",
     "keywords": ["Process optimization", "Batch processes", "Numerical methods", "Analytical methods", "Constrained optimization", "Sensitivity"]},
    {"article name": "A multilevel coordinate search algorithm for well placement, control and joint optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.09.006",
     "publication date": "12-2016",
     "abstract": "Determining optimal well placements and controls are two important tasks in oil field development. These problems are computationally expensive, nonconvex, and contain multiple optima. The practical solution of these problems requires efficient and robust algorithms. In this paper, the multilevel coordinate search (MCS) algorithm is applied for well placement and control optimization problems. MCS is a derivative-free algorithm that combines global and local search. Both synthetic and real oil fields are considered. The performance of MCS is compared to generalized pattern search (GPS), particle swarm optimization (PSO), and covariance matrix adaptive evolution strategy (CMA-ES) algorithms. Results show that the MCS algorithm is strongly competitive, and outperforms for the joint optimization problem and with a limited computational budget. The effect of parameter settings for MCS is compared for the test examples. For the joint optimization problem we compare the performance of the simultaneous and sequential procedures and show the utility of the latter.",
     "keywords": ["Well placement", "Well control", "Joint optimization", "Multilevel coordinate search", "Derivative-free optimization", "Reservoir simulation-based optimization"]},
    {"article name": "Application of a simulated annealing algorithm to design and optimize a pressure-swing distillation process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.09.014",
     "publication date": "12-2016",
     "abstract": "The design and optimization of pressure-swing distillation (PSD) have a critical impact on its economics. An optimization method based on simulated annealing algorithm (SAA) was proposed. The move generator and cooling schedule of the SAA were discussed, and suitable parameter settings were investigated. Two cases of PSD with and without heat integration were optimized by the SAA-based optimization method using procedures of pressure specified and pressure optimized. The results of the process without heat integration were compared with conventional optimization methods. For the acetone-methanol system, the total annual cost (TAC) shows a 5.69% decrease with the pressure specified and a 17.32% decrease with the pressure optimized. For the methanol-chloroform system, the TAC shows a 1.79% decrease with the pressure specified and a 9.04% decrease with the pressure optimized. The SAA-based optimization method has the advantages of a high probability to obtain the global optimum, automatic calculation, and less computing time.",
     "keywords": ["Pressure-swing distillation", "Simulated annealing algorithm", "Optimization method", "Azeotropic mixtures"]},
    {"article name": "Parameterisation of a biodiesel plant process flow sheet model",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.06.019",
     "publication date": "12-2016",
     "abstract": "This paper presents results of parameterisation of typical input\u2013output relations within process flow sheet of a biodiesel plant and assesses parameterisation accuracy. A variety of scenarios were considered: 1, 2, 6 and 11 input variables (such as feed flow rate or a heater's operating temperature) were changed simultaneously, 3 domain sizes of the input variables were considered and 2 different surrogates (polynomial and high dimensional model representation (HDMR) fitting) were used. All considered outputs were heat duties of equipment within the plant. All surrogate models achieved at least a reasonable fit regardless of the domain size and number of dimensions. Global sensitivity analysis with respect to 11 inputs indicated that only 4 or fewer inputs had significant influence on any one output. Interaction terms showed only minor effects in all of the cases.",
     "keywords": ["Process flow sheet model", "Parameterisation", "Biodiesel", "Sensitivity analysis"]},
    {"article name": "Development of a fuzzy analytical network process to evaluate alternatives on vitamin B12 adsorption from wastewater",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.09.009",
     "publication date": "12-2016",
     "abstract": "The use of adsorption methods to recover vitamin B12 (VB12) from wastewater has been increasingly studied. However, there is a lack of knowledge on optimization of the methods. This study established a feedback network to evaluate alternatives regarding VB12 adsorption from wastewater. The network comprises environmental, economic and technological criteria and their feedbacks. Based on the network, the fuzzy matter-element theory was integrated with an analytical network process to rank the alternatives. Among five alternatives, activated carbon with KOH as activation media was suggested to be the optimal alternative for VB12 recycling from wastewater, while mesoporous activated carbon fibre was the least preferred alternative. Particularly, the adsorption technology reusing biomass ranked second to the optimal alternative, and has great application potential due to low costs and biological waste reuse. Sensitivity analysis does show that the ranking of alternatives was robust and was not subject to the change in weight.",
     "keywords": ["Adsorption", "Vitamin B12", "Wastewater", "Analytical network process", "Activated carbon"]},
    {"article name": "Particle filtering without tears: A primer for beginners",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.08.015",
     "publication date": "12-2016",
     "abstract": "The main purpose of this primer is to systematically introduce the theory of particle filters to readers with limited or no prior understanding of the subject. The primer is written for beginners and practitioners interested in learning about the theory and implementation of particle filtering methods. Throughout this primer we highlight the common mistakes that beginners and first-time researchers make in understanding and implementing the theory of particle filtering. We also discuss and demonstrate the use of particle filtering in nonlinear state estimation applications. We conclude the primer by providing an implementable version of MATLAB code for particle filters. The code not only aids in improving the understanding of particle filters, it also serves as a template for building and implementing advanced nonlinear state estimation routines.",
     "keywords": ["Monte Carlo method", "Particle filter", "State estimation", "Bayesian inference"]},
    {"article name": "Systematic optimization methodology for heat exchanger network and simultaneous process design",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.09.013",
     "publication date": "12-2016",
     "abstract": "Distillation units require huge amounts of energy for the separation of the multicomponent mixtures involved in refineries and petrochemical industries. The overall efficiency of the distillation column system is determined from the trade-offs of the Operating Expenditures (OPEX) and Capital investment cost (CAPEX), as there is a strong interaction between the distillation columns and the Heat Exchanger Network (HEN) of the interconnecting streams. In this paper, a systematic Mixed Integer Non-Linear Programming (MINLP) optimization methodology for process integration of distillation column complex is presented. The highly nonlinear rigorous models of the distillation column and phase change are being substituted with simple surrogate models that generate operating responses with adequate accuracy. The methodology is applied on two case studies of the aromatics separation PARAMAX complex. The results illustrate significant reductions on the Total Annualized Cost. With a scope limited to the benzene and toluene columns, the gain reaches about 15%.",
     "keywords": ["MINLP", "Exchanger network", "Operating conditions", "Optimization"]},
    {"article name": "Robust optimization of the heteroextractive distillation column for the purification of water/acetic acid mixtures using p-xylene as entrainer",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.09.015",
     "publication date": "12-2016",
     "abstract": "The separation of water and acetic acid mixtures is a challenging task in the terephthalic acid production process. This study addresses the techno-economical optimization of an heteroextractive distillation column that uses p-xylene as entrainer. The optimization of this column is performed by means of robust optimizers belonging to the BzzMath library, coupled with the detailed process simulation of the downstream section performed with the commercial software SimSci PRO/II. A comparison with respect to the commercial simulator optimizer and a quantification of benefits are given. Results show that the BzzMath robust optimizer reaches a lower Total Annual Cost (\u22123.5%) with respect to the commercial software optimization. Finally, the comparison with other entrainers proposed in literature for the advanced distillation of the water/acetic acid mixtures demonstrates the economical and operational appeal of this process configuration.",
     "keywords": ["Acetic acid/water/p-xylene", "Heterogeneous extractive distillation", "UNIQUAC activity model", "Robust optimization", "Techno-economical assessment"]},
    {"article name": "Time scale decomposition in complex reaction systems: A graph theoretic analysis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.09.011",
     "publication date": "12-2016",
     "abstract": "The formulation of a kinetic model for a complex reaction network typically yields reaction rates which vary over orders of magnitude. This results in time scale separation that makes the model inherently stiff. In this work, a graph-theoretic framework is developed for time scale decomposition of complex reaction networks to separate the slow and fast time scales, and to identify pseudo-species that evolve only in the slow time scale. The reaction network is represented using a directed bi-partite graph and cycles that correspond to closed walks are used to identify interactions between species participating in fast/equilibrated reactions. Subsequently, an algorithm which connects the cycles to form the pseudo-species is utilized to eliminate the fast rate terms. These pseudo-species are used to formulate reduced, non-stiff kinetic models of the reaction system. Two reaction systems are considered to show the efficacy of this framework in the context of thermochemical and biochemical processing.",
     "keywords": ["Graph theory", "Model reduction", "Bi-partite graph", "Lumping"]},
    {"article name": "Optimal planning and feedstock-mix selection for multiproduct polymer production",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.09.002",
     "publication date": "12-2016",
     "abstract": "In this paper, we describe a nonlinear programming model to determine the optimal balance of feedstocks to manufacture multiple polymer grades in a polypropylene production facility. The main units of the process are a distillation column and a polymerization reactor, for which accurate short-cut process models were developed. Both a single and multiple product formulations are presented. The proposed models seek to maximize the plant throughput while minimizing the production costs. The possibility of adding extra production is also considered. The formulations are applied to several case studies, both to analyze the performance of the model and to illustrate its potential economic impact. The trade-off between feedstocks costs and production rates is analyzed by solving the multiple-product model with different time horizons. An annualized-slate long term case study is presented. The proposed formulation with a user-friendly interface has been deployed to assist with commercial and operation decisions at the plant.",
     "keywords": ["Polymer production", "Polymer scheduling", "Continuous processes", "Plant and process optimization"]},
    {"article name": "A joint model-based experimental design approach for the identification of kinetic models in continuous flow laboratory reactors",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.05.009",
     "publication date": "12-2016",
     "abstract": "Continuous flow laboratory reactors are typically used for the development of kinetic models for catalytic reactions. Sequential model-based design of experiments (MBDoE) procedures have been proposed in literature where experiments are optimally designed for discriminating amongst candidate models or for improving the estimation of kinetic parameters. However, the effectiveness of these procedures is strongly affected by the initial model uncertainty, leading to suboptimal design solutions and higher number of experiments to be executed. A joint model-based design of experiments (j-MBDoE) technique, based on multi-objective optimization, is proposed in this paper for the simultaneous solution of the dual problem of discriminating among competitive kinetic models and improving the estimation of the model parameters. The effectiveness of the proposed design methodology is tested and discussed through a simulated case study for the identification of kinetic models of methanol oxidation over silver catalyst.",
     "keywords": ["Model-based design of experiments", "Joint design of experiments", "Model discrimination", "Parameter estimation"]},
    {"article name": "Particle filter based hybrid prognostics of proton exchange membrane fuel cell in bond graph framework",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.08.018",
     "publication date": "12-2016",
     "abstract": "This paper presents a holistic solution towards prognostics of industrial Proton Exchange Membrane Fuel Cell. It involves an efficient multi-energetic model suited for diagnostics and prognostics, developed using some specific properties of Bond Graph (BG) theory. The benefits of Particle Filters (PF) are integrated with the BG model derived fault indicators named Analytical Redundancy Relations, for prognostics of the Electrical-Electrochemical part. The hybrid prognostics involves statistical degradation model obtained using real degradation tests. Prognostics problem is formulated as the joint state-parameter estimation problem in PF framework where estimations of state of health (SOH) is obtained in probabilistic domain. This in turn is used for prediction of Remaining Useful Life (RUL) under constant current as well as dynamic current solicitations. The SOH estimation and RUL prediction is obtained with very high accuracy and precise confidence bounds. Moreover, a comparative analysis with Extended Kalman Filter demonstrates the usefulness of PF.",
     "keywords": ["ARR analytical redundancy relations", "analytical redundancy relations", "BG bond graph", "bond graph", "BG-LFT bond graph in linear fractional transformation", "bond graph in linear fractional transformation", "DM degradation model", "degradation model", "DPP degradation progression parameter", "degradation progression parameter", "EE electrical-electrochemical", "electrical-electrochemical", "EKF extended kalman filter", "extended kalman filter", "EOL end of life", "end of life", "ESCA electro-chemical active surface area", "electro-chemical active surface area", "FC1 fuel cell under constant current load", "fuel cell under constant current load", "FC2 fuel cell under variable current load", "fuel cell under variable current load", "GDL gas diffusion layer", "gas diffusion layer", "I-ARR interval valued analytical redundancy relations", "interval valued analytical redundancy relations", "OCV open circuit voltage", "open circuit voltage", "PDF probability density function", "probability density function", "PEMFC proton exchange membrane fuel cell", "proton exchange membrane fuel cell", "PF particle filters", "particle filters", "PHM prognostic and health management", "prognostic and health management", "RA relative accuracy", "relative accuracy", "RMAD relative median absolute deviation", "relative median absolute deviation", "RMSE root mean square error", "root mean square error", "RUL remaining useful life", "remaining useful life", "SIR sampling importance resampling", "sampling importance resampling", "SOH state of health", "state of health", "Prognostics", "Bond graph", "Particle filters", "PEM fuel cell", "Remaining useful life"]},
    {"article name": "A hybrid heuristic for the inventory routing problem under dynamic regional pricing",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.09.018",
     "publication date": "12-2016",
     "abstract": "The inventory routing problem (IRP) seeks to meet the demands of customers during consecutive time periods. Because of the geographical distribution of customers and variations in willingness to pay of the consumers in distinct locations and time, regional and time-based pricing are powerful ways to improve profitability. In this study, a quadratic mixed-integer programming model for single product, multi-period Inventory Routing under the dynamic regional pricing problem (IRDRP) has been proposed. A hybrid heuristic approach is developed to solve it. This algorithm comprises five phases: initialization, demand generation, demand adjustment, inventory routing, and neighborhood search, which are embedded in a simulated annealing framework. Experimental results indicate as the problem size increases, the difference between CPLEX and the proposed heuristic algorithm optimality gap exhibits an upward trend and that the heuristic outperforms CPLEX. A sensitivity analysis demonstrates that by intensifying the scarce capacity, approaching an optimal solution will be more difficult.",
     "keywords": ["Inventory routing problem (IRP)", "Dynamic pricing", "Regional and time-based pricing", "Inventory routing under dynamic regional pricing problem (IRDRP)", "Hybrid heuristic"]},
    {"article name": "Mathematical modeling and optimal design of multi-stage slug-flow crystallization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.09.010",
     "publication date": "12-2016",
     "abstract": "Inspired from experimental progress in continuous crystallizer designs based on air/liquid slug flow that generate crystals of target sizes at high production rates and low capital costs (e.g., Eder et al., 2010; 2011; Jiang et al., 2014; 2015; and citations therein), a mathematical model and procedure are derived for the design of slug-flow crystallizers with spatially varying temperature profiles. The method of moments is applied to a population balance model for the crystals, to track the spatial variation of characteristics of the crystal size distribution along the crystallizer length. Design variables for the cooling slug-flow crystallizer such as tubing lengths and types and numbers of heat exchangers are analyzed and optimized for product crystal quality (e.g., minimized secondary nucleation and impurity incorporation) and experimental equipment costs, while ensuring high yield. This study provides guidance to engineers in the design of slug-flow crystallizers including their associated heat exchanger systems.",
     "keywords": ["Crystallization", "Pharmaceutical engineering", "Continuous manufacturing", "Particulate processes", "Pharmaceutical manufacturing"]},
    {"article name": "Multiscale lattice Boltzmann modeling of two-phase flow and retention times in micro-patterned fluidic devices",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.08.016",
     "publication date": "12-2016",
     "abstract": "Recent advances for fabricating micro-featured architectures such as posts or pillars in fluidic devices provide exciting opportunities for multiphase flow management. Here we describe a novel, multiscale modeling approach for two-phase flows in microfeatured architectures developed within the Shan and Chen Lattice Boltzmann method. In our approach a fine scale is used to resolve the true microfeatured architecture, with a coarser scale used to model the gross geometry of the device. We develop the basic features of the approach and demonstrate its applicability to modeling retention times of droplets of a dispersed phase in an array of microposts \u2013 an architecture used in microfluidic reactors, bioreactors, and biomedical devises. Additionally we show that it is feasible to model the microfeatured geometry in a piecewise manner which includes extrapolating dispersed phase flow characteristics in the entire system based on simulations in smaller subdomains.",
     "keywords": ["Multiscale modeling", "Lattice Boltzmann method", "Two-phase flow", "Microtechnology"]},
    {"article name": "Optimization of grade transitions in polyethylene solution polymerization process under uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.08.002",
     "publication date": "12-2016",
     "abstract": "Model-based dynamic optimization is an effective tool for control and optimization of chemical processes, especially during transitions in operation. This study considers the dynamic optimization of grade transitions for a solution polymerization process. Here, a detailed dynamic model comprises the entire flowsheet and includes a method-of-moments reactor model to determine product properties, a simple yet accurate vapor\u2013liquid equilibrium (VLE) model derived from rigorous calculations, and a variable time delay model for recycle streams. To solve the grade transition problem, both single stage and multistage optimization formulations have been developed to deal with specification bands of product properties.This dynamic optimization framework demonstrates significant performance improvements for grade transition problems. However, performance can deteriorate in the presence of uncertainties, disturbances and model mismatch. To deal with these uncertainties, this study applies robust optimization formulations through the incorporation of back-off constraints within the optimization problem. With back-off terms calculated from Monte Carlo simulations, the resulting robust optimization formulation can be solved with the same effort as the nominal dynamic optimization problem, and the resulting solution is shown to be robust under various uncertainty levels with minimal performance loss. Additional case studies show that our optimization approach extends naturally to different regularizations and multiple sources of uncertainty.",
     "keywords": ["Dynamic optimization", "Uncertainty", "Polymer processes", "Grade transitions", "Back-off constraints"]},
    {"article name": "Optimising heat exchanger network synthesis using convexity properties of the logarithmic mean temperature difference",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.07.001",
     "publication date": "11-2016",
     "abstract": "Industrial processes typically involve heating and cooling fluids via networks of heat exchangers which reuse excess process heat onsite. Optimally synthesising these networks of heat exchangers is a mixed-integer nonlinear optimisation problem with nonlinear terms including bilinear stream mixing, concave cost functions, and the logarithmic mean temperature difference (LMTD), which characterises the nonlinear nature of heat exchange. LMTD is typically associated with numerical difficulties, but, after adding the limits, this manuscript proves the strict convexity of LMTD\u03b2, \u03b2\u00a0<\u00a00, and also characterises the shape of the function for all \u03b2\u00a0\u2264\u00a01. These proofs motivate why previous, heuristic-based approaches work best when the problem is reformulated to move the LMTD terms into the objective. The convexity results also lead to an effective algorithm bounding the simultaneous synthesis model SYNHEAT from the online test set MINLPLib2; this algorithm solves a series of mixed-integer linear optimisation problems converging to the global objective value of the original problem.",
     "keywords": ["Heat exchanger network", "Mixed-integer nonlinear optimisation", "Log mean temperature difference", "Deterministic global optimisation", "Adaptive refinement"]},
    {"article name": "Development of an equilibrium theory solver applied to pressure swing adsorption cycles used in carbon capture processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.07.020",
     "publication date": "11-2016",
     "abstract": "An equilibrium theory simulator (Esim) for the simulation of cyclic adsorption processes is presented. The equations are solved with a Godunov upwind flux scheme that does not require either the evaluation of characteristics or shock equations or the imposition of a numerical entropy condition to track shocks. Esim is able to simulate non-trace and non-isothermal adsorption systems with any adsorption isotherm. Esim has been validated against gPROMS based simulations that use the full set of governing equations (including mass and heat transfer resistances and axial dispersion) carried out under conditions close to the limits where equilibrium theory is valid. Esim enables the establishment of bounds for the optimal performance of an equilibrium driven separation and requires only the measurement of adsorption isotherms.",
     "keywords": ["Adsorption dynamics", "Equilibrium theory", "Hyperbolic systems", "Godunov method", "Carbon capture", "Pressure swing adsorption"]},
    {"article name": "Maximum likelihood estimation of noise covariance matrices for state estimation of autonomous hybrid systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.07.010",
     "publication date": "11-2016",
     "abstract": "A critical aspect of developing Bayesian state estimators for hybrid systems, that involve a combination of continuous and discrete state variables, is to have a reasonably accurate characterization of the stochastic disturbances affecting their dynamics. Recently, Bavdekar et al. (2011) have proposed a maximum likelihood (ML) based framework for estimation of the noise covariance matrices from operating input\u2013output data when an EKF is used for state estimation. In this work, the ML framework is extended to estimation of the noise covariance matrices associated with autonomous hybrid systems, and, to a wider class of recursive Bayesian filters. Under the assumption that the innovations generated by an estimator form a white noise sequence, the proposed ML framework computes the noise covariance matrices such that they maximize the log-likelihood function of the estimator innovations. The efficacy of the proposed scheme is demonstrated through the simulation and experimental studies on the benchmark three-tank system.",
     "keywords": ["Nonlinear state estimation", "Unscented Kalman filter", "Ensemble Kalman filter", "Autonomous hybrid systems", "Probability density function estimation"]},
    {"article name": "Probabilistic reactor design in the framework of elementary process functions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.06.008",
     "publication date": "11-2016",
     "abstract": "Computational process models in combination with innovative design methodologies provide a powerful reactor design platform. Yet, model-based design is mostly done in a pure deterministic way. Possible uncertainties of the underlying model parameters, prediction errors due to simplifying assumptions regarding the reactor behavior and suboptimal realizations of the design along the reaction coordinate are in general not considered. Here we propose a systematic design approach to directly account for the impact of such variabilities during the design procedure. The three level design approach of Peschel et al. (2010) based on the concept of elementary process functions (EPF) serves as basis. The dynamic optimizations on each level are extended within a probabilistic framework to account for different sources of randomness. The impact of these sources on the performance prediction of a design is quantified and used to robustify the reactor design aiming at a more reliable performance and thus design prediction. The uncertainties of model parameters, non-idealities of the reactor behavior and inaccuracies in the design are included via statistical moments. By means of the sigma point method (Julier and Uhlmann, 1996) random variables are mapped to the design objective space via the nonlinear process model. Importantly, this work introduces a full probabilistic orthogonal collocation approach, i.e. random and stochastic variables can be described. Whereas the former one relates to randomness independent on the reaction time (e.g. kinetic model parameters or initial conditions), the latter one describes stochasticity along the reaction time (e.g. fluctuating pressure or temperature control). As an example process the hydroformylation of 1-dodecene in a thermomorphic solvent system consisting of n-decane and N,N-dimethylformamide is considered.Our probabilistic EPF approach allows designing robust optimal reactors, which operate within an estimated confidence at their expected optimum considering almost any kind of randomness arising in the design procedure. An additional value is that with increased predictive power of the reactor performance its embedding in an overall process is strongly simplified.",
     "keywords": ["Hydroformylation", "Multiphase systems", "Probabilistic reactor design", "Robust design optimization", "Optimization under uncertainty"]},
    {"article name": "D-RM Builder: A software tool for generating fast and accurate nonlinear dynamic reduced models from high-fidelity models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.07.021",
     "publication date": "11-2016",
     "abstract": "Dynamic reduced models (D-RMs) derived from rigorous models are highly desired for speeding up dynamic simulations. A useful software tool named D-RM Builder was developed to automatically generate data-driven D-RMs from high-fidelity dynamic models. It allows a user to configure input/output variables, sample input space and generate sequences of step changes, launch high-fidelity model simulations, fit simulation results to a D-RM, and finally visualize and validate the D-RM. The Decoupled A-B Net (DABNet) nonlinear system identification model was used as the main D-RM type and was enhanced to model nonlinear multiple input and multiple output dynamic systems with options for double-pole formulation to handle fast/slow time scales and pole value optimization. The D-RM Builder tool has been successfully used to generate D-RMs for a highly nonlinear pH neutralization reactor system and a two-time-scale bubbling fluidized bed adsorber-reactor for CO2 capture.",
     "keywords": ["Data-driven dynamic reduced models", "Nonlinear system identification models", "Dynamic simulation", "Model predictive control", "Engineering software development", "Carbon capture"]},
    {"article name": "Comprehensive Pareto Efficiency in robust counterpart optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.07.022",
     "publication date": "11-2016",
     "abstract": "In this paper, an innovative concept named Comprehensive Pareto Efficiency is introduced in the context of robust counterpart optimization, which consists of three sub-concepts: Pareto Robust Optimality (PRO), Global Pareto Robust Optimality (GPRO) and Elite Pareto Robust Optimality (EPRO). Different algorithms are developed for computing robust solutions with respect to these three sub-concepts. As all sub-concepts are based on the Probability of Constraint Violation (PCV), formulations of PCV under different probability distributions are derived and an alternative way to calculate PCV is also presented. Numerical studies are drawn from two applications (production planning problem and orienteering problem), to demonstrate the Comprehensive Pareto Efficiency. The numerical results show that the Comprehensive Pareto Efficiency has important significance for practical applications in terms of the evaluation of the quality of robust solutions and the analysis of the difference between different robust counterparts, which provides a new perspective for robust counterpart optimization.",
     "keywords": ["Robust optimization", "Pareto optimality", "Linear programming", "Integer programming"]},
    {"article name": "Scheduling of cracking production process with feedstocks and energy constraints",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.07.023",
     "publication date": "11-2016",
     "abstract": "This paper addresses the short-term scheduling problem for the ethylene cracking process with feedstocks and energy constraints. The cracking production of ethylene is a process with units that have decaying performance, requiring periodic cleanup to restore their performance. Under the condition of limited feedstocks, the production operating mode of the cracking furnaces is to keep yields constant by continuously increasing the coil temperature. We present a hybrid MINLP/GDP formulation based on continuous-time representation for the scheduling problem over a finite time horizon. In order to solve the proposed model, which is reformulated as an MINLP model, an improved outer approximation algorithm with multi-generation cuts and problem-dependent integer cuts are developed to solve real large-scale problems. Numerical examples are presented to illustrate the application of the model. Based on analyzing the optimal solution and sensitivity of the model, some conclusions are obtained to provide useful suggestions for real cracking process production.",
     "keywords": ["Scheduling", "Continuous process", "MINLP", "GDP", "Outer approximation"]},
    {"article name": "Real-time adaptive input design for the determination of competitive adsorption isotherms in liquid chromatography",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.07.009",
     "publication date": "11-2016",
     "abstract": "The adaptive input design (also called online redesign of experiments) for parameter estimation is very effective for the compensation of uncertainties in nonlinear processes. Moreover, it enables substantial savings in experimental effort and greater reliability in modeling.We present theoretical details and experimental results from the real-time adaptive optimal input design for parameter estimation. The case study considers separation of three benzoate by reverse phase liquid chromatography. Following a receding horizon scheme, adaptive D-optimal input designs are generated for a precise determination of competitive adsorption isotherm parameters. Moreover, numerical techniques for the regularization of arising ill-posed problems, e.g. due to scarce measurements, lack of prior information about parameters, low sensitivities and parameter correlations are discussed. The estimated parameter values are successfully validated by Frontal Analysis and the benefits of optimal input designs are highlighted when compared to various standard/heuristic input designs in terms of parameter accuracy and precision.",
     "keywords": ["Adaptive input design", "Regularization techniques", "Model based optimal experimental re-design", "Parameter estimation"]},
    {"article name": "Computational modeling of megakaryocytic differentiation of umbilical cord blood-derived stem/progenitor cells",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.07.027",
     "publication date": "11-2016",
     "abstract": "Quantifying the effect of exogenous parameters regulating megakaryopoiesis would enhance the design of robust and efficient protocols to produce platelets. We developed a computational model based on time-dependent ordinary differential equations (ODEs) which decoupled expansion and differentiation kinetics of cells using a subpopulation dynamic model. The model described umbilical cord blood (UCB)-derived cell's behavior in response to the external stimuli during expansion and megakaryocytic differentiation ex vivo. We observed that the rate of expansion of Mk progenitors and production of mature Mks were higher when TPO was included in the expansion stage and cytokines were added during differentiation stage. Our computational approach suggests that the Mk progenitors were an important intermediate population that their dynamic should be optimized in order to establish an efficient protocol. This model provides important insights into dynamics of cell subpopulations during megakaryopoiesis process and could potentially contribute toward the rational design of cell-based therapy bioprocesses.",
     "keywords": ["Hematopoietic stem/progenitor cells", "Umbilical cord blood", "Computational modeling", "Megakaryocyte", "Platelet"]},
    {"article name": "Heat and mass transfer enhancement in a double diffusive mixed convection lid cavity under pulsating flow",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.06.018",
     "publication date": "11-2016",
     "abstract": "This work assess the effectiveness of the pulsating flow regime for the enhancement of heat and mass transfer in double diffusive problems by studying a two-dimensional, heated, lid cavity. The research characterizes the influence of pulsating parameters such as temporal frequency, wave number and amplitude in the process. Results show that the pulsating regime enhances heat/mass transfer within a square cavity up to a 14%/38% respectively with respect to the non-pulsating case, due to the promotion of additional shear stress fields. As Richardson number, Brownian diffusion or the solute/thermal buoyancy ratio increase or the cavity becomes narrower, heat/mass transfer increases.",
     "keywords": ["80A20", "80M25", "Double diffusive process", "Pulsating flow", "Oscillating flow regime", "Network Simulation Method", "Heat transfer", "Mass transfer"]},
    {"article name": "Multi-week MILP scheduling for an ice cream processing facility",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.07.025",
     "publication date": "11-2016",
     "abstract": "This paper presents a multi-week mixed integer linear programming (MILP) scheduling model for an ice cream processing facility. The ice cream processing is a typical complex food manufacturing process and a simplified version of this processing has been adapted to investigate scheduling problems in the literature. Most of these models only considered the production scheduling for a week. In this paper, multi-week production scheduling is considered. The problem has been implemented as an MILP model. The model has been tested on a set of cases from the literature, and its results were compared to the results of problems solved using hybrid MILP-heuristics methods in the literature. The inclusion of clean-up session, weekend break and semi-processed product from previous week were also assessed with two additional sets of experiments. The experiments result show that the proposed MILP is able to handle multi-week scheduling efficiently and effectively within a reasonable time limit.",
     "keywords": ["MILP", "Production scheduling", "Food processing"]},
    {"article name": "Simultaneous optimization of scheduling, equipment dimensions and operating conditions of sequential multi-purpose batch plants",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.05.012",
     "publication date": "11-2016",
     "abstract": "The design of multi-purpose batch plants is a challenging task, because the number of degrees of freedom for optimization is high. Important optimization variables are the scheduling, operating conditions and the sizes of the equipment items. Since all factors interact, a simultaneous consideration would be beneficial in order to reduce capital and operating costs. In this article, this complex task is tackled for a new case study of a sequential plant for protein production. The case study contains comprehensive models of the unit operations to evaluate equipment dimensions, mass balances and operating times. Variable changeover times and semicontinuous unit operations are considered. For optimization, a MINLP model is used that consists of smaller NLP and MILP submodels in order to simplify modeling. Simulation runs for different product demands are performed and it is shown that good solutions can be found in an adequate time.",
     "keywords": ["AEX anion exchange chromatography", "anion exchange chromatography", "CIP cleaning in place", "cleaning in place", "CVI chemical virus inactivation", "chemical virus inactivation", "DF diafiltration", "diafiltration", "HIC hydrophobic interaction chromatography", "hydrophobic interaction chromatography", "mab monoclonal antibody", "monoclonal antibody", "PAC protein A chromatography", "protein A chromatography", "RP recombinant protein", "recombinant protein", "SIP sterilization in place", "sterilization in place", "STN state-task network", "state-task network", "UF ultrafiltration", "ultrafiltration", "VF virusfiltration", "virusfiltration", "WFI water for injection", "water for injection", "Optimization", "Scheduling", "Plant design", "Process design", "Multi-purpose batch plant", "Biochemical proces"]},
    {"article name": "Assessing plant design with regard to MPC performance",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.07.007",
     "publication date": "11-2016",
     "abstract": "Model Predictive Control is ubiquitous in the chemical industry and offers great advantages over traditional controllers. Notwithstanding, new plants are being projected without taking into account how design choices affect the MPC\u2019s ability to deliver better control and optimization. Thus a methodology to determine if a certain design option favours or hinders MPC performance would be desirable. This paper presents the economic MPC optimization index whose intended use is to provide a procedure to compare different designs for a given process, assessing how well they can be controlled and optimised by a zone constrained MPC. The index quantifies the economic benefits available and how well the plant performs under MPC control given the plant\u2019s controllability properties, requirements and restrictions. The index provides a monetization measure of expected control performance.This approach assumes the availability of a linear state-space model valid within the control zone defined by the upper and lower bounds of each controlled and manipulated variable. We have used a model derived from simulation step tests as a practical way to use the method. The impact of model uncertainty on the methodology is discussed. An analysis of the effects of disturbances on the index illustrates how they may reduce profitability by restricting the ability of a MPC to reach dynamic equilibrium near process restrictions, which in turn increases product quality giveaway and costs. A case of study consisting of four alternative designs for a realistically sized crude oil atmospheric distillation plant is provided in order to demonstrate the applicability of the index.",
     "keywords": ["Integrated process design and control", "Model predictive control (MPC)", "Zone constrained model predictive control", "Zone control", "Controllability analysis", "Crude oil distillation"]},
    {"article name": "Multi-purpose economic optimal experiment design applied to model based optimal control",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.07.004",
     "publication date": "11-2016",
     "abstract": "In contrast to classical experiment design methods, often based on alphabetic criteria, economic optimal experiment design assumes that our ultimate goal is to solve an optimization or optimal control problem. As the system parameters of physical models are in practice always estimated from measurements, they cannot be assumed to be exact. Thus, if we solve the model based optimization problem using the estimated, non-exact parameters, an inevitable loss of optimality is faced. The aim of economic optimal experiment design is precisely to plan an experiment in such a way that the expected loss of optimality in the optimization is minimized. This paper analyzes the question how to design economic experiments under the assumption that we have more than one candidate objective function. Here, we want to take measurements and estimate the parameters before we actually decide which objective we want to minimize.",
     "keywords": ["Optimal experiment design", "Optimality loss", "Multi-purpose design", "Optimal control", "Variance\u2013covariance matrix"]},
    {"article name": "Rigorous approach to scheduling of sterile drug product manufacturing",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.07.028",
     "publication date": "11-2016",
     "abstract": "Optimizing the scheduling of liquid drug product manufacturing is paramount for pharmaceutical companies in their increasingly competitive environment and requires the modelling of industry-specific constraints. Such constraints include: (i) changing sequence-dependent setup times; (ii) maintaining a sterile production environment (e.g., through sterile holding times); (iii) periods with limited or no plant activity (e.g., no workforce during weekends); and (iv) demand timing (i.e., delivery deadline and release date constraints). In this work, an immediate precedence model is formulated to optimize the scheduling of liquid drug product manufacturing, considering the industry-specific constraints. The primary objective is to minimize the production makespan.Four case studies comprising up to 38 batches from a real multi-product facility illustrate the performance of the rigorous optimization approach. The makespan could be reduced by up to 7.9% compared to expert schedules.",
     "keywords": ["Mathematical optimization", "Mixed-integer linear programming", "Scheduling", "Batch production", "Industrial application", "Campaigning"]},
    {"article name": "Application of a two-level rolling horizon optimization scheme to a solid-oxide fuel cell and compressed air energy storage plant for the optimal supply of zero-emissions peaking power",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.08.004",
     "publication date": "11-2016",
     "abstract": "We present a new two-level rolling horizon optimization framework applied to a zero-emissions coal-fueled solid-oxide fuel cell power plant with compressed air energy storage for peaking applications. Simulations are performed where the scaled hourly demand for the year 2014 from the Ontario, Canada market is met as closely as possible. It was found that the proposed two-level strategy, by slowly adjusting the SOFC stack power upstream of the storage section, can improve load-following performance by 86% compared to the single-level optimization method proposed previously. A performance analysis indicates that the proposed approach uses the available storage volume to almost its maximum potential, with little improvement possible without changing the system itself. Further improvement to load-following is possible by increasing storage volumes, but with diminishing returns. Using an economically-focused objective function can improve annual revenue generation by as much as 6.5%, but not without a significant drop-off in load-following performance.",
     "keywords": ["Solid oxide fuel cells", "Compressed air energy storage", "Peaking power", "Optimization", "Coal", "Carbon capture"]},
    {"article name": "On-line estimation of VFA concentration in anaerobic digestion via methane outflow rate measurements",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.07.005",
     "publication date": "11-2016",
     "abstract": "This paper deals with the design of a robust nonlinear observer as a software sensor to achieve the on-line estimation of the concentration of Volatile Fatty Acids (VFA) in a class of continuous anaerobic digesters (AD). Taking into account the limited availability of on-line sensors for AD process, in this contribution is assumed that only the methane outflow rate is available for on-line measurement. The estimation method is based on a modified version for a two-dimensional mathematical model of AD process. From the differential algebraic observability approach it is shown that the VFA concentration is detectable from the methane outflow rate measurements. The observer convergence is analyzed by using Lyapunov stability techniques. Numerical simulations illustrate the effectiveness of the proposed estimation method for a four-dimensional AD model with uncertainties associated with unmodeled dynamics and disturbances in the inflow composition.",
     "keywords": ["Anaerobic digestion", "State observer", "VFA estimation", "Uncertain reaction systems", "Software sensors"]},
    {"article name": "Optimal design and operation of an industrial three phase reactor for the oxidation of phenol",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.07.018",
     "publication date": "11-2016",
     "abstract": "Among several treatment methods catalytic wet air oxidation (CWAO) treatment is considered as a useful and powerful method for removing phenol from waste waters. In this work, mathematical model of a trickle bed reactor (TBR) undergoing CWAO of phenol is developed and the best kinetic parameters of the relevant reaction are estimated based on experimental data (from the literature) using parameter estimation technique. The validated model is then utilized for further simulation and optimization of the process. Finally, the TBR is scaled up to predict the behavior of CWAO of phenol in industrial reactors. The optimal operating conditions based on maximum conversion and minimum cost in addition to the optimal distribution of the catalyst bed is considered in scaling up and the optimal ratio of the reactor length to reactor diameter is calculated with taking into account the hydrodynamic factors (radial and axial concentration and temperature distribution).",
     "keywords": ["Catalytic wet air oxidation", "Phenol", "Trickle bed reactor", "Parameter estimation technique", "Scaled-up"]},
    {"article name": "Improved numerical inversion methods for the recovery of bivariate distributions of polymer properties from 2D probability generating function domains",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.07.017",
     "publication date": "11-2016",
     "abstract": "The 2D probability-generating function technique is a powerful method for modeling bivariate distributions of polymer properties. It is based on the transformation of bivariate population balance equations using 2D probability generating functions (pgf) followed by a recovery of the distributions from the transform domain by numerical inversion. A key step of this method is the inversion of the pgf transforms. Available numerical inversion methods yield excellent results for pgf transforms of distributions with independent dimensions with similar orders of magnitude, for example bivariate molecular weight distributions in copolymerization systems. However, numerical problems are found for 2D distributions in which the independent dimensions have very different ranges of values, such as the molecular weight distribution-branching distribution in branched polymers. In this work, two new 2D pgf inversion methods are developed, which regard the pgf as a complex variable. The superior accuracy of these innovative methods makes them suitable for recovering any type of bivariate distribution. This enhances the capabilities of the 2D pgf modeling technique for simulation and optimization of polymer processes. An application example of the technique in a polymeric system of industrial interest is presented.",
     "keywords": ["Modeling", "Polymerization", "Bivariate distribution", "2D probability generating function"]},
    {"article name": "A modeling strategy for integrated batch process development based on mixed-logic dynamic optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.07.030",
     "publication date": "11-2016",
     "abstract": "This paper introduces an optimization-based approach for the simultaneous solution of batch process synthesis and plant allocation, with decisions like the selection of chemicals, process stages, task-unit assignments, operating modes, and optimal control profiles, among others. The modeling strategy is based on the representation of structural alternatives in a state-equipment network (SEN) and its formulation as a mixed-logic dynamic optimization (MLDO) problem. Particularly, the disjunctive multistage modeling strategy by Oldenburg and Marquardt (2008) is extended to combine and organize single-stage and multistage models for representing the sequence of continuous and batch units in each structural alternative and for synchronizing dynamic profiles in input and output operations with material transference. Two numerical examples illustrate the application of the proposed methodology, showing the enhancement of the adaptability potential of batch plants and the improvement of global process performance thanks to the quantification of interactions between process synthesis and plant allocation decisions.",
     "keywords": ["Batch process synthesis", "Plant allocation", "Dynamic optimization", "Generalized disjunctive programming", "Synchronization", "Multistage modeling"]},
    {"article name": "On deterministic online scheduling: Major considerations, paradoxes and remedies",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.08.006",
     "publication date": "11-2016",
     "abstract": "Despite research in the area, the relationship between the (open-loop) optimization problem and the quality of the (closed-loop) implemented schedule is poorly understood. Accordingly, we first show that open-loop and closed-loop scheduling are two different problems, even in the deterministic case. Thereafter, we investigate attributes of the open-loop problem and the rescheduling algorithm that affect closed-loop schedule quality. We find that it is important to reschedule periodically even when there are no \u201ctrigger\u201d events. We show that solving the open-loop problem suboptimally does not lead to poor closed-loop solutions; instead, suboptimal solutions are corrected through feedback. We also observe that there exist thresholds for rescheduling frequency and moving horizon length, operating outside of which leads to substantial performance deterioration. Fourth, we show that the design attributes work in conjunction, hence, studying them simultaneously is important. Finally, we explore objective function modifications and constraint addition as methods to improve performance.",
     "keywords": ["Chemical production scheduling", "Rescheduling", "Mixed-integer programming", "Closed-loop solution"]},
    {"article name": "Dynamic modelling of K\u00fchni liquid extraction columns using the sectional quadrature method of moments (SQMOM)",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.08.003",
     "publication date": "11-2016",
     "abstract": "In this work, the Sectional Quadrature Method Of Moments (SQMOM) is extended to a one-dimensional physical spatial domain and resolved using the finite volume method. To close the mathematical model, the required quadrature nodes and weights are calculated using the analytical solution based on the Two Unequal Weights Quadrature (TUEWQ) formula derived by Attarakih et al. (Attarakih, M., Drumm, C., & Bart, H.-J., (2009), Solution of the population balance equation using the Sectional Quadrature Method of Moments (SQMOM). Chemical Engineering Science, 64, 742\u2013752). By applying the finite volume method to the spatial domain, we end up with a semi-discreet ordinary differential equation system which is solved using the MATLAB standard ODE solvers (ode45). As a case study, the SQMOM is used to investigate the dynamic behavior of a K\u00fchni DN150 liquid\u2013liquid extraction column. As an independent validation step, the SQMOM prediction is compared with the PPBLab software which utilizes the extended fixed pivot technique as a built-in population balance model solver. Furthermore, the SQMOM is validated using the available dynamic experimental data from a K\u00fchni liquid extraction column using water-acetone-toluene chemical test system. The dynamic analyses of the K\u00fchni column show very interesting features concerning the coupled column hydrodynamics and mass transfer and the droplet breakage and coalescence as well.",
     "keywords": ["Population balances", "SQMOM", "PPBLab", "Dynamics", "Liquid extraction", "K\u00fchni"]},
    {"article name": "Multivariate statistical process control charts for batch monitoring of transesterification reactions for biodiesel production based on near-infrared spectroscopy",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.08.013",
     "publication date": "11-2016",
     "abstract": "This work describes an application of Multivariate Statistical Process Control to monitor soybean oil transesterification. For the development of multivariate control charts, near infrared spectra were acquired in-line during the evolution of ten batches under Normal Operating Conditions. They were then organized in a three-way array (batch\u00a0\u00ed\u00a0spectral variable\u00a0\u00ed\u00a0time). This structure was analysed by the two most commonly used approaches to develop batch monitoring schemes for handling such kind of data, referred to as Nomikos-MacGregor (NM) and Wold-Kettaneh-Friden-Holmberg (WKFH), respectively. To assess the performance of the approaches, eight test batches, during which specific interferences were induced, were manufactured. When applied for off-line monitoring, both NM and WKFH correctly pointed out such intentionally produced failures. On the other hand, concerning on-line monitoring, NM exhibited a better fault detection capability than WKFH. Contribution plots were found to highlight the spectral region mostly affected by the disturbances regardless of the modelling strategy resorted to.",
     "keywords": ["Biodiesel", "Batch process monitoring", "Near infrared spectroscopy (NIRS)", "Multivariate statistical process control (MSPC)", "Soybean oil methanolysis"]},
    {"article name": "GAMS supported optimization and predictability study of a multi-objective adsorption process with conflicting regions of optimal operating conditions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.08.014",
     "publication date": "11-2016",
     "abstract": "In process systems engineering, it is critical to design an effective and optimized process in a short period with minimum experimental trials. However, improvement of some process variables may deteriorate some other criteria due to conflicting regions of factor interests for optimal solution in multi-objective optimization (MOO) processes. Here, the global optimization of an adsorption case study with conflicting optimal solutions based on multi-objective Response Surface Methodology (RSM) design is facilitated with the implementation of BARON solver based on General Algebraic Modeling System (GAMS) with identical factor variables, levels, and model equations. RSM suggested fifteen different optimum settings of which the validation is quite expensive and onerous, whereas GAMS suggested a single optimum setting which makes it more economically viable especially for large scale systems. In addition, the GAMS-based optimization provided more accurate and reliable results when experimentally validated as compared to the RSM-based solution.",
     "keywords": ["GAMS", "RSM", "Multi-objective", "Optimization", "Adsorption", "Wastewater"]},
    {"article name": "Improvement of principal component analysis modeling for plasma etch processes through discrete wavelet transform and automatic variable selection",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.08.012",
     "publication date": "11-2016",
     "abstract": "To cope with a cost-effective manufacturing approach driven by more than Moore\u2019s law era, plasma etching which is one of the major processes in semiconductor manufacturing has developed plasma sensors and their applications. Among the plasma sensors, optical emission spectroscopy (OES) has been widely utilized and its high dimensionality has required multivariate analysis (MVA) techniques such as principal component analysis (PCA). PCA, however, might devaluate physical meaning of target process during its statistical calculation. In addition, inherent noise from charge coupled devices (CCD) array in OES might deteriorate PCA model performance. Therefore, it is desirable to pre-select physically important variables and to filter out noisy signals before modeling OES based plasma data. For these purposes, this paper introduces a peak wavelength selection algorithm for selecting physically meaningful wavelength in plasma and discrete wavelet transform (DWT) for filtering out noisy signals from a CCD array. The effectiveness of the PCA model introduced in this paper is verified by comparing fault detection capabilities of conventional PCA model under the various source power or pressure faulty situations in a capacitively coupled plasma etcher. Even though the conventional PCA model fails to detect all of the faulty situations under the tests, the PCA model introduced in this paper successively detect even extremely small variation such as 0.67% of source power fault. The results introduced in this paper is expected to contribute to OES based plasma monitoring capability in plasma etching for more than Moore\u2019s law era.",
     "keywords": ["Principal component analysis", "Variable selection", "Discrete wavelet transform", "Optical emission spectroscopy", "Plasma monitoring"]},
    {"article name": "Automated heat exchanger network synthesis by using hybrid natural algorithms and parallel processing",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.08.009",
     "publication date": "11-2016",
     "abstract": "Heat exchanger network (HEN) synthesis can be formulated as an optimization problem, which can be solved by meta-heuristics. These approaches account for a large computational time until convergence. In the present paper the potentialities of applying parallel processing techniques to a non-deterministic approach based on a hybridization between Genetic Algorithms (GA) and Particle Swarm Optimization (PSO) were investigated. Six literature examples were used as benchmarks for the solutions obtained. Comparative experiments were carried out to investigate the time efficiency of the method while implemented using series or parallel processing. The solutions obtained led to lower Total Annual Costs (TAC) than those presented by the literature. As expected, parallel processing usage multiplied the algorithm speed by the number of cores used. Hence, it can be concluded that the proposed method is capable of finding excellent local optimal solutions, and the application of multiprocessing techniques represented a substantial reduction in execution time.",
     "keywords": ["Heat exchanger network synthesis", "Optimization", "Meta-heuristics", "Parallel processing"]},
    {"article name": "Discrete-time mixed-integer programming models and solution methods for production scheduling in multistage facilities",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.034",
     "publication date": "11-2016",
     "abstract": "We address the problem of production scheduling in multi-product multi-stage batch plants. Unlike most of the previous works, which propose continuous-time models, we study discrete-time mixed-integer programming models and solution methods. Specifically, we discuss two models based on network representations of the facility and develop two new models inspired by the Resource-Constrained Project Scheduling Problem. Furthermore, we propose different solution methods, including tightening methods based on processing unit availability, a reformulation based on processing unit occupancy, and an algorithm to refine approximate solutions for large-scale instances. Finally, we present a comprehensive computational study which shows that speedups of up to four orders of magnitude in are observed when our models and methods are compared to existing approaches.",
     "keywords": ["Sequential production environments", "Resource-constrained project scheduling", "Tightening constraints", "Reformulations"]},
    {"article name": "Numerical approximation of nonlinear and non-equilibrium two-dimensional model of chromatography",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.08.008",
     "publication date": "11-2016",
     "abstract": "This article is concerned with the numerical approximation of a nonlinear model describing the two-dimensional non-equilibrium transport of multi-component mixtures in a chromatographic column of cylindrical geometry. In contrast to previous studies, the work includes joint analysis of deviations from equilibrium and the possibility that radial concentration profiles can develop. The considered radial gradients are typically ignored, which can be problematic in the case of non perfect injections. The model consists of nonlinear convection-diffusion partial differential equations coupled with some differential and algebraic equations. A high resolution finite volume scheme is applied to solve the model equations numerically. The considered case studies include single-component, two-component and three-component elution on fixed (non-movable) beds of liquid chromatography. The developed numerical algorithm is an efficient tool to study the effects of mass transfer kinetics on the elution profiles.",
     "keywords": ["Two-dimensional model of chromatography", "Non-equilibrium transport", "Nonlinear adsorption isotherm", "Mass transfer", "Finite volume scheme"]},
    {"article name": "Dynamic operation of flat sheet desalination-membrane elements: A comprehensive model accounting for organic fouling",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.06.001",
     "publication date": "10-2016",
     "abstract": "Reliable simulation of membrane-module dynamic operation is essential in optimizing its detailed geometric characteristics and operation, as well as that of membrane plants, for various types of fluid treatment applications. This paper is part of systematic efforts toward development of such a comprehensive model, considering temporal variability caused by organic membrane fouling. To render the mathematical problem tractable, justified simplifications (retaining the physical parameter interdependencies) lead to a system of basic equations in two spatial planar coordinates, enabling to obtain a realistic temporal evolution of all process parameters. The flexible model structure allows integration of sub-models, for phenomena occurring (and researched) at small spatial scales, which account for retentate spacer effects on friction losses and mass transfer, and constitutive expressions for fouling rates during desalination. The robust numerical algorithm developed to solve the system of differential and algebraic equations exhibits satisfactory convergence, appropriate for applications. The results presented herein demonstrate the versatility of the numerical code and its potential to analyze the interaction of mechanisms involved in fouling evolution, which is impossible by the much simpler one dimensional models. Directions for future developments are indicated.",
     "keywords": ["Desalination", "Reverse osmosis", "Mathematical modeling", "Organic fouling", "Spiral wound membranes"]},
    {"article name": "Nonlinear control of the dissolved oxygen concentration integrated with a biomass estimator for production of Bacillus thuringiensis \u03b4-endotoxins",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.05.017",
     "publication date": "10-2016",
     "abstract": "Bacillus thuringiensis is a microorganism that allows the biosynthesis of \u03b4-endotoxins with toxic properties against some insect larvae, being often used for the production of biological insecticides. A key issue for the bioprocess design consists in adequately tracking a pre-specified optimal profile of the dissolved oxygen concentration. To this effect, this paper aims at developing a novel control law based on a nonlinear dynamic inversion method. The closed-loop strategy includes an observer based on a Bayesian Regression with Gaussian Process, which is used for on-line estimating the biomass present in the bioreactor. Unlike other approaches, the proposed controller leads to an improved response time with effective disturbance rejection properties, while simultaneously prevents undesired oscillations of the dissolved oxygen concentration. Simulation results based on available experimental data were used to show the effectiveness of the proposal.",
     "keywords": ["\u03b4-Endotoxins production dissolved oxygen control", "Nonlinear dynamic inversion", "Batch bioprocess", "Bacillus thuringiensis"]},
    {"article name": "Estimation of exhaust steam enthalpy and steam wetness fraction for steam turbines based on data reconciliation with characteristic constraints",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.05.019",
     "publication date": "10-2016",
     "abstract": "Wetness fraction of exhaust steam is important to the economy and safety of steam turbines. Due to lack of commercially available measurement technologies, it is usually obtained from model based calculation via other measurements. However, accuracy of relevant measurement data is usually unsatisfactory due to limits of measuring instruments, and data reconciliation can be applied to improve the accuracy of these measurements. Traditionally, balance constraints of steam turbines are mostly considered in data reconciliation, and results of previous studies illustrate that there is still potential for further improvement. In this work, we present a generalized data reconciliation approach with both balance and characteristic constraints for estimation of wet steam parameters in steam turbines, with case studies on a real-life 1000\u00a0MW coal-fired power plant. Results show that uncertainty reduction is enhanced for all measurements. Better estimates of exhaust steam enthalpy and steam wetness fraction can be therefore obtained after data reconciliation.",
     "keywords": ["Steam turbine", "Data reconciliation", "Characteristic constraints", "Uncertainty reduction", "Wetness fraction"]},
    {"article name": "Mass transfer simulation of biodiesel synthesis in microreactors",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.05.010",
     "publication date": "10-2016",
     "abstract": "A coupled nonlinear mathematical model for the mass transfer of the species involved in the transesterification reaction between soybean oil and methanol in a parallel plates geometry microreactor is presented. The set of partial differential equations that governs the concentration profile of these species were obtained from the general mass balance equation for the case of isothermal flow and steady state with constant physical properties. The velocity profile was obtained from the Navier-Stokes equations assuming fully developed stratified laminar flow for two immiscible Newtonian fluids, with a plane interface between them, based on experimental observation of this flow pattern. The second order kinetic equations for the species were developed assuming homogeneous and reversible chemical reactions and these equations were written as source terms in the main equations. The mathematical model was solved using the hybrid method known as Generalized Integral Transform Technique (GITT). The simulation results were critically compared with those obtained by using the COMSOL multiphysics platform, showing a good agreement between the hybrid and fully numerical simulations. The effects of governing parameters such as residence time, temperature and microreactor dimensions were investigated. It was observed that higher triglycerides conversion rates occurred at higher temperatures and residence times and lower microreactor depths.",
     "keywords": ["Microreactor", "Transesterification", "Biodiesel", "GITT approach", "Reaction-convection-diffusion equation", "Hybrid methods"]},
    {"article name": "A global optimization method for model selection in chemical reactions networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.05.016",
     "publication date": "10-2016",
     "abstract": "Model inference is a challenging problem in the analysis of chemical reactions networks. In order to empirically test which, out of a catalogue of proposed kinetic models, is governing a network of chemical reactions, the user can compare the empirical data obtained in one experiment against the theoretical values suggested by the models under consideration. It is thus fundamental to make an adequate choice of the decision variables (e.g. initial concentrations of the different species in the tank) in order to have maximal separation between sets of concentrations provided by the theoretical models, making then easier to identify which of the theoretical models yields data closest to those obtained empirically under identical conditions.In this paper we illustrate how global optimization techniques can be successfully used to address the problem of model separation, as a basis for model selection. Some examples illustrate the usefulness of our approach.",
     "keywords": ["Model selection", "Chemical reactions networks", "Kinetic models", "Global optimization", "Variable neighborhood search"]},
    {"article name": "Semantically enabled process synthesis and optimisation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.05.018",
     "publication date": "10-2016",
     "abstract": "This paper introduces a new framework to support synthesis of complex engineering problems using a paradigm that combines optimisation with ontological knowledge modelling. The framework registers and analyzes new solutions by introducing a mechanism of digital certificates to translate structural information and solution features through semantics of an ontology. The solutions are respectively clustered by design features. Tested against complex synthesis of reactor networks, the framework offers a potential to visualize optimization in the course of its development and demonstrates noticeable advantages over conventional methods of a similar basis in convergence and performance.",
     "keywords": ["Process synthesis", "Ontologies", "Tabu search"]},
    {"article name": "Configuration of inter-city high-speed passenger transport infrastructure with minimal construction and operational energy consumption: A superstructure based modelling and optimization framework",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.06.003",
     "publication date": "10-2016",
     "abstract": "Inter-city high-speed passenger transport, mainly aviation and high-speed railway, has been increasing around the world, in accordance with economic development and penetration of high-speed transport technologies. The energy consumption over the lifetime of transport infrastructure and operation is a significant factor at the planning stage. In this paper, we present a superstructure modelling and optimization framework of inter-city high-speed transport systems, accounting energy consumption during infrastructure construction and during subsequent operation, to optimize connections between large population centers and between modes of transport. Energy consumption during infrastructure construction is obtained from investment cost using lifecycle assessment. The first two cases considered differences between infrastructure construction and lifetime operation while the second case narrowed the study scope. Sensitivity analysis in the third case compared impacts of both transport means on system design. Model results have implications for actual high-speed transport technology development and infrastructure layout.",
     "keywords": ["High-speed passenger transport", "Construction energy consumption", "Operational energy consumption", "Superstructure", "Mathematical programming optimization"]},
    {"article name": "A novel constraint programming model for large-scale scheduling problems in multiproduct multistage batch plants: Limited resources and campaign-based operation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.030",
     "publication date": "10-2016",
     "abstract": "This contribution introduces an efficient constraint programming (CP) model that copes with large-scale scheduling problems in multiproduct multistage batch plants. It addresses several features found in industrial environments, such as topology constraints, forbidden product-equipment assignments, sequence-dependent changeover tasks, dissimilar parallel units at each stage, limiting renewable resources and multiple-batch orders, among other relevant plant characteristics. Moreover, the contribution deals with various inter-stage storage and operational policies. In addition, multiple-batch orders can be handled by defining a campaign operating mode, and lower and upper bounds on the number of batches per campaign can be fixed. The proposed model has been extensively tested by means of several case studies having various problem sizes and characteristics. The results have shown that the model can efficiently solve medium and large-scale problems with multiple constraining features. The approach has also rendered good quality solutions for problems that consider multiple-batch orders under a campaign-based operational policy.",
     "keywords": ["Scheduling", "Constraint programming", "Multiproduct multistage batch plants", "Campaign operating mode"]},
    {"article name": "Economic trade-offs in acrylic acid reactor design",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.06.005",
     "publication date": "10-2016",
     "abstract": "The acrylic acid process using air oxidation of propylene presents many interesting design trade-offs, particularly in the design of the reactor. The desired and undesired reactions are highly exothermic and very temperature dependent (large activation energies), so a large flowrate of inert water is also fed to the reactor to act as a thermal sink. Propylene conversion increases with temperature and reactor size, but acrylic acid yield decreases with increasing temperature. The heat of reaction is removed by generaing steam, and the steam pressure is an important design optimization variable since it sets low limits on reactor temperature. Using low-pressure steam gives high acrylic acid yield and lower carbon dioxide generation but requires large reactors. Larger air flowrates increase reactor oxygen concentrations, which reduce reactor volume but increase air compression costs.This paper explores the effects of the many design trade-offs on capital investment, energy cost and product selectivity.",
     "keywords": ["Acrylic acid", "Reactor design", "Reactor design trade-offs"]},
    {"article name": "Multivariate probabilistic safety analysis of process facilities using the Copula Bayesian Network model",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.06.011",
     "publication date": "10-2016",
     "abstract": "Integrated safety analysis of hazardous process facilities calls for an understanding of both stochastic and topological dependencies, going beyond traditional Bayesian Network (BN) analysis to study cause-effect relationships among major risk factors. This paper presents a novel model based on the Copula Bayesian Network (CBN) for multivariate safety analysis of process systems. The innovation of the proposed CBN model is in integrating the advantage of copula functions in modelling complex dependence structures with the cause-effect relationship reasoning of process variables using BNs. This offers a great flexibility in probabilistic analysis of individual risk factors while considering their uncertainty and stochastic dependence. Methods based on maximum likelihood evaluation and information theory are presented to learn the structure of CBN models. The superior performance of the CBN model and its advantages compared to traditional BN models are demonstrated by application to an offshore managed pressure drilling case study.",
     "keywords": ["Correlation", "Dependence structure", "Multivariate probabilistic model", "Akaike\u2019s information criterion"]},
    {"article name": "Estimation of percentiles using the Kriging method for uncertainty propagation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.05.015",
     "publication date": "10-2016",
     "abstract": "The use of simulation-based uncertainty propagation approaches (e.g., Monte Carlo simulation method) can be computationally expensive if evaluating the function requires a relatively large computation time. To reduce the computation time, uncertainty propagation methods that use surrogate models (e.g., the Kriging method) may be used. In this paper, we extend the Kriging method to propagate the uncertainties from multiple sources, and for cases where the distribution of the prediction is produced at each trial (replication) of the simulation-based uncertainty propagation approach (i.e., at each sample point). The outputs of the methodology are the approximate percentiles of the output distribution. The capability of the methodology is tested using a Case Study involving the transport of solid particles in pipelines to prevent solid particle deposition and improve pipeline efficiency. Statistical comparisons suggest that our methodology successfully replicates the outputs from the Monte Carlo simulation method with a 94% reduction in computational cost.",
     "keywords": ["Uncertainty propagation", "Model output uncertainty", "Kriging surrogate model", "Solid particle transport", "Threshold velocity"]},
    {"article name": "On the modelling of multidisciplinary electrochemical systems with application on the electrochemical conversion of CO2 to formate/formic acid",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.06.012",
     "publication date": "10-2016",
     "abstract": "This paper presents a model-based approach on the analysis of complex multidisciplinary electrochemical processes, with implementation on a reactor for the electrochemical conversion of CO2 to formate/formic acid. The process is regarded as a system of interacting physical and electrochemical mechanisms. A process model is developed by combining individual mathematical sub-models of the mechanisms, organised at groups of compartments following the physical process structure. This approach results in a generic reconfigurable model that can be used as a part of integrated systems, and to test design modifications. The approach is demonstrated on an electrochemical cell, where CO2 is converted to formate/formic acid. The model captures the molar transportation under electric field, the two-phase flow effects, and the key electrochemical reactions. The model is calibrated and validated against experimental data obtained from a continuous flow cell. The key parameters affecting the process performance are discussed through scale-up analysis.",
     "keywords": ["ECFORM Electrochemical Reduction of CO2 to Formate/Formic acid", "Electrochemical Reduction of CO2 to Formate/Formic acid", "PDAEs Partial Differential and Algebraic Equations", "Partial Differential and Algebraic Equations", "MSE mean sum of squared errors", "mean sum of squared errors", "Electrochemical conversion", "CO2 utilization", "Mathematical modelling", "Process scale-up"]},
    {"article name": "A new approach to stochastic reduced order modeling",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.06.010",
     "publication date": "10-2016",
     "abstract": "This short note presents a new method for stochastic reduced order (SROM) model based on BONUS reweighting scheme. An illustrative case study of IGCC power plant compares the new method with the neural network based reduced order model. The new method shows promising results.",
     "keywords": ["Reweighting scheme", "BONUS algorithm", "IGCC system", "Reduced order model"]},
    {"article name": "MILP-based optimization of oxygen distribution system in integrated steel mills",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.06.015",
     "publication date": "10-2016",
     "abstract": "Simultaneous multiperiod optimization is conducted for minimizing the oxygen emission of an oxygen-distribution system, based on the generalized MILP-based model, which covers various configurations of the captive oxygen factory in integrated steel mills. By simultaneously optimizing all of the variables, such as the load of air separation units (ASU), the on-off states of compressors, the load of liquefiers, etc., the model can promptly provide mill managers with responsive solutions for adjusting the variables involved on the supply-side to minimize oxygen emission. The case study in this paper shows that the proposed model performs well in minimizing oxygen emission, and provides a global optimization result covering the entire planning horizon. Moreover, based on the proposed model, the emission amounts can be rapidly and readily calculated for various scheduling scenarios of ASU maintenance, which is helpful to the manager seeking to optimally schedule ASU maintenance in time.",
     "keywords": ["Iron and steel making", "MILP", "Oxygen distribution system", "Scheduling", "Air separation unit"]},
    {"article name": "Economic evaluation of energy saving alternatives in extractive distillation process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.06.013",
     "publication date": "10-2016",
     "abstract": "Until now, there has not been consensus about the superiority of thermally coupled sequence over the conventional sequence in the extractive distillation process. In this sense, the main goal of this paper is to analyze three approaches for saving energy in the extractive distillation process: optimization, thermal integration and thermal coupling. Three azeotropic mixtures were investigated: ethanol and water (M1); tetrahydrofuran and water (M2); and acetone and methanol (M3). The solvents were ethylene glycol for M1 and M2, and water for M3. The results are shown in terms of the total annual cost (TAC) and specific energy consumption (SEC), and revealed that a thermally coupled extractive distillation sequence with a side rectifier did not always present the best results. Taking the case studies from literature as a starting point (without thermal integration), the optimization procedure used in this work found that TACs are always lower. The inclusion of thermal integration in configurations led to reducing TAC for all mixtures under investigation when compared to the sequences without this integration. When comparing two modifications in the layout of extractive distillation, it can be seen that it is more advantageous to use the preheating of the azeotropic feed with the recycle stream from the recovery column of the conventional sequence than using a thermally coupled sequence.",
     "keywords": ["Extractive distillation", "Optimization", "Thermal integration and thermal coupling", "Energy saving"]},
    {"article name": "An inverse method to estimate adsorption kinetics of light hydrocarbons on activated carbon",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.06.014",
     "publication date": "10-2016",
     "abstract": "An inverse algorithm to estimate the adsorption kinetics inside the spherical particles in a constant molar flow (CMF) gas adsorber reservoir by measuring the bulk pressure is developed. The formulation includes Knudsen diffusion, surface diffusion, slip and viscous flows. To obtain an efficient algorithm, the conjugate gradient method (CGM) for optimization procedure and the incremental differential quadrature method (IDQM) for solving the governing equations are adopted. The results show that the Knudsen diffusion, surface diffusion, slip and viscous flows effects depend on the type of adsorbate and adsorbent gases. It is shown that the effective diffusivity is not constant and goes through a minimum at an intermediate pressure. Also, it is found that the Knudsen diffusion and the viscous flow are the dominant parts of the mass transfer process at low and high pressure, respectively, and despite the viscous flow, the Knudsen diffusion is highly sensitive to temperature change.",
     "keywords": ["Effective diffusivity", "Inverse problem", "Incremental differential quadrature", "Conjugate gradient method"]},
    {"article name": "Multi-objective design optimization of natural gas transmission networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.06.006",
     "publication date": "10-2016",
     "abstract": "This paper proposes the multi-objective optimization of the design of natural gas transmission networks to support the decision of regulatory authorities. The problem formulation involves two objective functions: the minimization of the transportation fare and the maximization of the transported gas volume. These design parameters of the pipeline project must be previously established by the regulatory agency, considering an attractive return on the investment for the entrepreneurs and the demands of current and future consumers. The solution of this problem without an optimization tool may imply in unfair gas prices or the lack of investors interest. The proposed analysis is focused on growing markets, associated to a continuous increase of the natural gas consumption. Constraints associated to gas flow and compressor stations guarantee the feasibility of the set of design options found. Aiming to illustrate the performance of the proposed approach, the tool was applied to a typical trunkline example.",
     "keywords": ["Natural gas", "Pipeline network", "Optimization", "Multi-objective design"]},
    {"article name": "Shared and practical approach to conserve utilities in eco-industrial parks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.05.003",
     "publication date": "10-2016",
     "abstract": "Conserving utilities in an eco-industrial park (EIP) by exploiting the synergistic heating/cooling needs of its inhabitants can have significant economic and environmental benefits. However, a successful implementation of an EIP-wide heat integration involves much more than the simple minimization of utility usage. Like any collaborative endeavour involving independent and diverse profit-making enterprises, an EIP-wide heat integration faces several real and practical challenges such as exchanger locations, stream transports over long distances, etc. In this work, we propose a mixed-integer nonlinear programming model (MINLP) for configuring an EIP-wide multi-enterprise heat exchanger network (HEN). We propose a practical and rational strategy that (1) considers all the major capital and operating costs, and utility savings, (2) selects an optimum HEN location with the highest net present value, (3) uses a third-party logistics provider for managing and operating the HEN, and (4) ensures an identical rate of return on investment for all participating enterprises.",
     "keywords": ["EIP", "Heat exchanger network synthesis", "Inter-plant integration", "Mixed integer non-linear programming", "Optimization"]},
    {"article name": "Development of a discrete element model with moving realistic geometry to simulate particle motion in a Mi-Pro granulator",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.06.021",
     "publication date": "10-2016",
     "abstract": "This paper presents the implementation of a methodology incorporating a 3D CAD geometry into a 3D discrete element method (DEM) code; discussing some of the issues which were experienced. The 3D CAD model was discretised into a finite element mesh and the finite wall method was employed for contact detection between the elements and the spherical particles. The geometry was based on a lab scale Mi-Pro granulator. Simulations were performed to represent dry particle motion in this piece of equipment. The model was validated by high speed photography of the particle motion at the surface of the Mi-Pro's clear bowl walls. The results indicated that the particle motion was dominated by the high speed impeller and that a roping regime exists. The results from this work give a greater insight into the particle motion and can be used to understand the complex interactions which occur within this equipment.",
     "keywords": ["Discrete element method", "High shear mixers", "Particle velocity field", "Contact detection", "Moving boundary"]},
    {"article name": "Domain reduction for Benders decomposition based global optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.06.009",
     "publication date": "10-2016",
     "abstract": "While domain reduction has been successfully applied in branch-and-bound based global optimization over the last two decades, it has not been systematically studied for decomposition based global optimization, which is usually more efficient for problems with decomposable structures. This paper discusses integration of domain reduction in Benders decomposition based global optimization, specifically, generalized Benders decomposition (GBD) and nonconvex generalized Benders decomposition (NGBD). Revised GBD and NGBD frameworks are proposed to incorporate bound contraction operations or/and range reduction calculations, which can reduce the variable bounds and therefore improve the convergence rate and expedite the solution of nonconvex subproblems. Novel customized bound contraction problems are proposed for GBD and NGBD, and they are easier to solve than the classical bound contraction problems because they are defined on reduced variable spaces. The benefits of the proposed methods are demonstrated through a gas production operation problem and a power distribution system design problem.",
     "keywords": ["Domain reduction", "Bound contraction", "Generalized Benders decomposition", "Nonconvex generalized Benders decomposition", "Global optimization"]},
    {"article name": "Computational optimization and sensitivity analysis of fuel reformer",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.07.013",
     "publication date": "10-2016",
     "abstract": "In this study, the catalytic partial oxidation of methane is numerically investigated using an unstructured, implicit, fully coupled finite volume approach. The nonlinear system of equations is solved by Newton\u2019s method. The catalytic partial oxidation of methane over rhodium catalyst in a coated honeycomb reactor is studied three-dimensionally, and eight gas-phase species (CH4, CO2, H2O, N2, O2, CO, OH and H2) are considered for the simulation. Surface chemistry is modeled by detailed reaction mechanism including 38 heterogeneous reactions with 20 surface-adsorbed species for the Rh catalyst. The numerical results are compared with experimental data and good agreement is observed. Effects of the design variables, which include the inlet velocity, methane/oxygen ratio, catalytic wall temperature, and catalyst loading on the cost functions representing methane conversion and hydrogen production, are numerically investigated. The sensitivity analysis for the reactor is performed using three different approaches: finite difference, direct differentiation and an adjoint method. Two gradient-based design optimization algorithms are utilized to improve the reactor performance.",
     "keywords": ["Computational fluid dynamics", "Fuel reformer", "Numerical optimization", "Sensitivity analysis", "Catalytic reactor"]},
    {"article name": "Iterative learning model predictive control for constrained multivariable control of batch processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.07.011",
     "publication date": "10-2016",
     "abstract": "In this paper, we propose a model predictive control (MPC) technique combined with iterative learning control (ILC), called the iterative learning model predictive control (ILMPC), for constrained multivariable control of batch processes. Although the general ILC makes the outputs converge to reference trajectories under model uncertainty, it uses open-loop control within a batch; thus, it cannot reject real-time disturbances. The MPC algorithm shows identical performance for all batches, and it highly depends on model quality because it does not use previous batch information. We integrate the advantages of the two algorithms. The proposed ILMPC formulation is based on general MPC and incorporates an iterative learning function into MPC. Thus, it is easy to handle various issues for which the general MPC is suitable, such as constraints, time-varying systems, disturbances, and stochastic characteristics. Simulation examples are provided to show the effectiveness of the proposed ILMPC.",
     "keywords": ["Iterative learning control", "Model predictive control", "Disturbance rejection", "Offset-free control", "Constrained multivariable control", "Iterative learning model predictive control"]},
    {"article name": "Process control using finite Markov chains with iterative clustering",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.05.007",
     "publication date": "10-2016",
     "abstract": "Finite state Markov Decision Processes (MDP) for process control are considered. MDP provide robust tools to perform optimization in closed-loop, and their finite state description enables an easy implementation of Bayesian state estimation. An approach to tackle the curse of dimensionality problem, yet retaining the benefits of the finite state MDP in control and estimator design, is proposed. The suggested approach uses iterative re-discretization based on clustering of closed-loop data. An efficient modification of the k-means clustering technique is proposed. The performance of the approach is demonstrated using a challenging benchmark from chemical engineering, the van der Vusse continuous stirred tank reactor control problem. It is shown that the requirements of the benchmark are met, and that the suggested iterated clustering significantly improves the performance. It is concluded that the finite state MDP approach is a viable alternative for small-to-medium scale problems of practical process control and state estimation.",
     "keywords": ["Markov decision processes", "Bayesian state estimation", "Model-based process control", "Model predictive control", "Clustering", "Process simulation"]},
    {"article name": "A parallel function evaluation approach for solution to large-scale equation-oriented models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.07.015",
     "publication date": "10-2016",
     "abstract": "The equation-oriented (EO) approach is widely used for process simulation and optimization. Nevertheless, large-scale EO models consist of a huge number of nonlinear equations and make the solution procedure a challenging and time-consuming task. For most gradient-based numerical algorithms, function evaluations are the dominant step during the solution procedure. Here, a parallel computation method is developed for function evaluations within EO optimization strategies. After dividing the equations into several groups, function evaluations are calculated by using multiple threads on a parallel hardware platform simultaneously. Theoretical analysis for the speedup ratio is conducted. The implementation of the proposed method on a multi-core processor platform as well as a graphics processing unit (GPU) platform is then presented with several case studies. Numerical results are compared and discussed to show that the multi-core processor implementation has good computational performance, whereas the GPU implementation only achieves computational acceleration under relatively specific conditions.",
     "keywords": ["Equation-oriented model", "Function evaluation", "Parallel computing", "GPU", "Multi-core processor"]},
    {"article name": "Quick identification of a simple enzyme deactivation model for an extended-Michaelis-Menten reaction type. Exemplification for the D-glucose oxidation with a complex enzyme deactivation kinetics",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.07.014",
     "publication date": "10-2016",
     "abstract": "One essential engineering problem when developing an industrial enzymatic process concerns the model-based design and optimal operation of the enzymatic reactor based on the process and enzyme inactivation kinetics. For a complex enzymatic system, the \u201cdefault\u201d used first-order enzyme deactivation model has been proved to lead to inadequate process design or sub-optimal operating policies. The present study investigates if a complex enzyme deactivation can be approximated with simple 1st, 2nd, or a novel proposed model with variable deactivation constant. The approached complex enzymatic system is those of the oxidation of D-glucose to 2-keto-D-glucose in the presence of pyranose 2-oxidase. The necessary \u201csimulated experimental data\u201d have been generated by means of an extended kinetic model from literature used to simulate a batch reactor under well-defined nominal conditions. The proposed enzyme deactivation model has been found to be the best lumping alternative, presenting several advantages: simplicity, flexibility, and a very good adequacy.",
     "keywords": ["Enzyme deactivation kinetics", "Kinetic model discrimination", "D-glucose oxidation", "Pyranose oxidase", "Pseudo first order deactivation"]},
    {"article name": "Telescopic projective Adams multiscale modeling of electrochemical reactions in tubular solid oxide fuel cells",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.07.016",
     "publication date": "10-2016",
     "abstract": "In order to predict the electrochemical performance of Solid Oxide Fuel Cells (SOFCs), a telescopic projective Adams (TPA) multiscale simulation method is proposed in this work. This method is constructed on the basis of the equation-free method (EFM). A lattice Boltzmann model is used as the fine-scale simulator of the proposed method. The electrochemical reaction-diffusion process was simulated by the TPA and the lattice Boltzmann method (LBM). The results of the two methods were found to be in good agreement, and the TPA method can give accurate results with lower computational costs. The electrochemical reactions were also simulated based on the TPA method. The results were consistent with the experimental data, indicating that the proposed TPA method is an effective tool to simulate the electrochemical reactions of SOFCs. Also, the proposed method is suggested to be helpful in multiscale modeling of other energy systems.",
     "keywords": ["Solid oxide fuel cell", "Electrochemical performance", "Multiscale modeling", "Coarse projective integration"]},
    {"article name": "Fermentable sugars from Eucalyptus globulus: Process optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.07.012",
     "publication date": "10-2016",
     "abstract": "In the last years, bioethanol production from lignocellulosic materials is receiving more attention from researchers since it is an abundant raw material in certain regions, it is cheaper, does not compete with foods like sugarcane or corn, and can reduce up to 86% of carbon dioxide emissions when compared with gasoline. However, the process economy is linked to the pretreatment needed to make the cellulose accessible for further steps of enzymatic hydrolysis and fermentation. Then, the motivation of this work is to propose a MILP model to perform an optimal synthesis of the pretreatment for obtaining fermentable sugars from the Eucalyptus globulus specie. For this purpose, a General Disjunctive Program (GDP) is formulated. The obtained results suggest that there is a small difference between a two-step pretreatment process (pretreatment with posthydrolysis) and one-step pretreatment of diluted acid process.",
     "keywords": ["Bioethanol", "Lignocellulosic raw material", "Pretreatment", "Process synthesis"]},
    {"article name": "Constraint back-offs for safe, sufficient excitation: A general theory with application to experimental optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.07.006",
     "publication date": "10-2016",
     "abstract": "In many experimental settings, one is tasked with obtaining information about certain relationships by applying perturbations to a set of independent variables and noting the changes in the set of dependent ones. While traditional design-of-experiments methods are often well-suited for this, the task becomes significantly more difficult in the presence of constraints, which may make it impossible to sufficiently excite the experimental system without incurring constraint violations. The key contribution of this paper consists in deriving constraint back-off sizes sufficient to guarantee that one can always perturb in a ball of radius \u03b4e without leaving the constrained space, with \u03b4e set by the user. Additionally, this result is exploited in the context of experimental optimization to propose a constrained version of G. E. P. Box's evolutionary operation technique. The proposed algorithm is applied to three case studies and is shown to consistently converge to the neighborhood of the optimum without violating constraints.",
     "keywords": ["Design measures for robustness", "Evolutionary operation", "Optimization under uncertainties", "Experiment design"]},
    {"article name": "Extension of modifier adaptation for controlled plants using static open-loop models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.07.008",
     "publication date": "10-2016",
     "abstract": "Model-based optimization methods suffer from the limited accuracy of the available process models. Because of plant-model mismatch, model-based optimal inputs may be suboptimal or, worse, unfeasible for the plant. Modifier adaptation (MA) overcomes this obstacle by incorporating measurements in the optimization framework. However, the standard MA formulation requires that (1) the model satisfies adequacy conditions and (2) the model and the plant share the same degrees of freedom. In this article, three extensions of MA to problems where (2) does not hold are proposed. In particular, we consider the case of controlled plants for which the only a model of the open-loop plant is available. These extensions are shown to preserve the ability of MA to converge to the plant optimum despite disturbances and plant-model mismatch. The proposed methods are illustrated in simulation for the optimization of a CSTR.",
     "keywords": ["Real-time optimization", "Plant-model mismatch", "Measurements", "Modifier adaptation"]},
    {"article name": "Control structure selection for four-product Kaibel column",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.07.019",
     "publication date": "10-2016",
     "abstract": "Dividing wall column configurations have a large savings potential in terms of capital and energy. This paper uses dynamic simulation to investigate three alternative control structures for one of these configurations, namely the Kaibel column. Four components, here selected as methanol, ethanol, n-propanol and n-butanol, are separated into pure products within a single column shell. Control structure 1 (CS1) uses only temperature controllers and is therefore particularly interesting from an industrial point of view. Since the control objective is to control the four product compositions, the two other control structures use also composition controllers. Surprisingly, for composition control, the simple temperature control scheme (CS1) is almost as good at steady-state and much better from a dynamic point of view than the two other more complex control structures.",
     "keywords": ["Dividing wall column", "Kaibel column", "PID control"]},
    {"article name": "Source-based discrete and continuous-time formulations for the crude oil pooling problem",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.06.016",
     "publication date": "10-2016",
     "abstract": "The optimization of crude oil operations in refineries is a challenging scheduling problem due to the need to model tanks of varying composition with nonconvex bilinear terms, and complicating logistic constraints. Following recent work for multiperiod pooling problems of refined petroleum products, a source-based mixed-integer nonlinear programming formulation is proposed for discrete and continuous representations of time. Logistic constraints are modeled through Generalized Disjunctive Programming while a specialized algorithm featuring relaxations from multiparametric disaggregation handles the bilinear terms. Results over a set of test problems from the literature show that the discrete-time approach finds better solutions when minimizing cost (avoids source of bilinear terms). In contrast, solution quality is slightly better for the continuous-time formulation when maximizing gross margin. The results also show that the specialized global optimization algorithm can lead to lower optimality gaps for fixed CPU, but overall, the performance of commercial solvers BARON and GloMIQO are better.",
     "keywords": ["Mathematical modeling", "Mixed-integer quadratically constrained problem", "Mixed-integer linear relaxations", "Global optimization"]},
    {"article name": "Optimal design of ionic liquids for thermal energy storage",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.008",
     "publication date": "10-2016",
     "abstract": "Ionic liquids (ILs) are an emerging group of chemicals which, with their tunable physicochemical properties, exhibit promise for use as novel materials in many applications. Thermal (e.g. solar) energy storage (TES) is one such area where they show potential to be thermally stable at high temperatures and store high amount of heat energy. A large number of ILs, through the combination of different cations and anions, can be potentially synthetized thereby presenting a good platform for design. However, since it is not possible to study this large number of compounds experimentally it is necessary to use computational methods to evaluate them. In this article, we present a computer-aided framework to design task-specific ionic liquids (ILs), using structure- property models and optimization methods. Thermal energy storage density (capacity) was used as a measure of the ability of an IL to store thermal (solar) energy. An hydroxyl functionalized imidazolium-based IL, [3-hydroxy-imidazolium]+[BF4] \u2212, was found to be the optimal candidate with highest thermal energy storage capacity along with appropriate melting point and decomposition temperature.",
     "keywords": ["Computer- aided molecular design", "Ionic liquids", "Optimization", "Thermal energy storage", "Group contribution"]},
    {"article name": "Model based approach to synthesize spare-supported cleaning schedules for existing heat exchanger networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.06.020",
     "publication date": "10-2016",
     "abstract": "Almost every modern chemical process is equipped with a heat-exchanger network (HEN) for optimal energy recovery. However, as time goes on after startup, fouling on the heat-transfer surface in an industrial environment is unavoidable. If the heat exchangers in an operating plant are not cleaned regularly, the targeted thermal efficiency of HEN can only be sustained for a short period of time. To address this practical issue, several mathematical programming models have already been developed to synthesize online cleaning schedules. Although the total utility cost of a HEN could be effectively reduced accordingly, any defouling operation still results in unnecessary energy loss due to the obvious need to temporarily take the unit to be cleaned out of service. The objective of the present study is thus to modify the available model so as to appropriately assign spares to replace them. Specifically, two binary variables are adopted to respectively represent distinct decisions concerning each online exchanger in a particular time interval, i.e., whether it should be cleaned and, if so, whether it should be substituted with a spare. The optimal solution thus includes not only the cleaning schedule but also the total number of spares, their capacities and the substitution schedule. Finally, the optimization results of a series of case studies are also presented to verify the feasibility of the proposed approach.",
     "keywords": ["Heat-exchanger network", "Spare", "Cleaning schedule"]},
    {"article name": "Robust leak detection and its localization using interval estimation for water distribution network",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.027",
     "publication date": "09-2016",
     "abstract": "Water distribution network in cities is complicated and damage to pipes can occur in the form of leakage causing economic losses. Early detection of abnormalities and accurate determination of damage location are required. Cumulative sum (CUSUM) test, wavelet-based method, and point localization have been used. However, low accuracy in noisy situation, loss of time information, and absence of mathematical reliability remain unsolved issues. In this paper, we propose a new robust algorithm that detects leakage in water network addressing these issues using pressure measurements. This method consists of cumulative integral of shifted pressure data, floor function with three parameters followed by curvature function, and localization based on statistical estimation. Verification was performed using two different field leakage data sets and normal data sets. The detection scheme exhibits much fewer false alarms and localization is more practical to implement in real networks than the previous methods.",
     "keywords": ["Fault detection", "Fault localization", "Statistical analysis", "Pipe networks", "Pressure measurements"]},
    {"article name": "Economic model predictive control designs for input rate-of-change constraint handling and guaranteed economic performance",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.026",
     "publication date": "09-2016",
     "abstract": "Economic model predictive control (EMPC) has been a popular topic in the recent chemical process control literature due to its potential to improve process profit by operating a system in a time-varying manner. However, time-varying operation may cause excessive wear of the process components such as valves and pumps. To address this issue, input magnitude constraints and input rate-of-change constraints can be added to the EMPC optimization problem to prevent possible frequent and extreme changes in the requested inputs. Specifically, we develop input rate-of-change constraints that can be incorporated in Lyapunov-based EMPC (LEMPC) that ensure controller feasibility and closed-loop stability. Furthermore, we develop a terminal equality constraint for LEMPC that can ensure that the performance of LEMPC is at least as good as that of a Lyapunov-based controller in finite-time and in infinite-time. Chemical process examples demonstrate the incorporation of input rate-of-change constraints and terminal state constraints in EMPC.",
     "keywords": ["Economic model predictive control", "Chemical processes", "Process control", "Rate of change constraints", "Economic performance"]},
    {"article name": "Prediction of viscosity of imidazolium-based ionic liquids using MLR and SVM algorithms",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.035",
     "publication date": "09-2016",
     "abstract": "In this work, two models, one integrating the fragment contribution-corresponding states (FC-CS) method with multiple linear regression (MLR) algorithm and another. With support vector machine (SVM) algorithm, are proposed to predict the viscosity of imidazolium-based ionic liquids (ILs). The FC-CS method is applied to calculate the pseudo-critical volume and compressibility factor (Vc and Zc) as well as the boiling point temperature (Tb) which are employed to predict the viscosity with the MLR and SVM algorithms. A large data set of 1079 experimental data points of 45 imidazolium-based ILs covering a wide range of pressure and temperature is applied to validate the two models. The average absolute relative deviation (AARD) of the entire data set of the MLR and SVM is 24.2% and 3.95%, respectively. The nonlinear model developed by the SVM algorithm is much better than the linear model built by the MLR, which indicates the SVM algorithm is more reliable in the prediction of the viscosity of imidazolium-based ILs.",
     "keywords": ["Ionic liquids", "Viscosity", "Support vector machine (SVM)", "Multiple linear regression (MLR)", "Fragment contribution-corresponding states (FC-CS) method"]},
    {"article name": "Methodology for solar and wind energy chemical storage facilities design under uncertainty: Methanol production from CO2 and hydrogen",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.05.001",
     "publication date": "09-2016",
     "abstract": "Production facilities that store solar or wind energy in the form of chemicals present underused capacity. The problem needs to address uncertain and variable operating conditions and prices for complex process models. An MILP formulation is developed including surrogate models based on the detailed NLP steady state models. The model solves the tradeoffs between investment and production capacity. This approach is applied to the case of the production of methanol from CO2 and solar or wind based hydrogen. Two cases are evaluated, Spain and UK. For the Spanish case, if electricity can be sold and there is no area restrictions, the process produces an excess of electricity with the solar panels available during summer time. Otherwise, electricity is only produced when excess capacity is available. In the UK, only wind turbines are used and the excess of electricity is produced during winter time.",
     "keywords": ["Wind power", "Solar energy", "Synthetic methanol", "Hydrogen", "CO2", "Design under uncertainty"]},
    {"article name": "CFD modeling of catalytic reactions in open-cell foam substrates",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.031",
     "publication date": "09-2016",
     "abstract": "Open cell foams are regarded with interest for applications as catalytic substrates for combustion, reformers and after-treatment converters for the pollutant emissions control. In this context, CFD represents a reliable and convenient tool for investigating and understanding the physical phenomena occurring at the micro-scale, in order to design and optimize these substrates. A CFD model for the simulation of the catalytic reactions occurring over the surface of open-cell foams is implemented and validated. The approach is based on a coupled finite-volume/finite-area strategy capable to describe the fluid-dynamic and the chemical phenomena occurring in both the fluid phase and solid phases. The adsorption/desorption of the reactants on the active sites and the surface reaction is modeled on the basis of a Langmuir\u2013Hinshelwood mechanism. The model is able to describe the reactants conversion under both kinetics and diffusion control, allowing to predict the light-off curve characterizing the catalyst-coated foam substrate.",
     "keywords": ["Open-cell foams", "CFD", "Mass-transfer", "Surface reactions", "Kelvin-cell"]},
    {"article name": "Design of multi-parametric NCO tracking controllers for linear dynamic systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.038",
     "publication date": "09-2016",
     "abstract": "A methodology for combining multi-parametric programming and NCO tracking is presented in the case of linear dynamic systems. The resulting parametric controllers consist of (potentially nonlinear) feedback laws for tracking optimality conditions by exploiting the underlying optimal control switching structure. Compared to the classical multi-parametric MPC controller, this approach leads to a reduction in the number of critical regions. It calls for the solution of more difficult parametric optimization problems with linear differential equations embedded, whose critical regions are potentially nonconvex. Examples of constrained linear quadratic optimal control problems with parametric uncertainty are presented to illustrate the approach.",
     "keywords": ["Multi-parametric programming", "NCO-tracking", "Feedback control", "Explicit MPC"]},
    {"article name": "Downstream oil supply chain management: A critical review and future directions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.05.002",
     "publication date": "09-2016",
     "abstract": "The oil industry has been playing a particular role in the modern economy, acting globally in different countries within competitive business environments. Due to the complexity of the oil supply chain, the associated decision making process is a difficult task, which involves numerous elements from oil supply, going through oil refining, up to oil product distribution. Thus, decision-support tools are often required to assist the decision making in the context of the oil supply chain. The improvement of such decision-support tools is a continuous goal for corporations. From this background, this work aims to review the scientific production about the application of mathematical programming techniques to the distribution problems, faced by diverse entities in the downstream oil supply chain. The main objectives are to point out main contributions, besides identifying the major voids and new trends in order to establish an agenda for future research directions.",
     "keywords": ["Downstream oil supply chain", "Distribution planning", "Refined product", "Optimization method", "Mathematical programming technique", "Decision-support tool"]},
    {"article name": "Framework in PYOMO for the assessment and implementation of (as)NMPC controllers",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.05.005",
     "publication date": "09-2016",
     "abstract": "Model predictive control (MPC) is an advanced control strategy that has a growing interest for research and applications because of its good performance in many kind of processes and its ability to handle constraints, perform optimization, and consider economic aspects and nonlinearities of the process. However, its design, evaluation and implementation require a high level of expertise which might restrict the developments in this area. This paper presents a software framework developed in Pyomo, a mathematical modelling language embedded in Python, for the assessment and implementation of ideal nonlinear MPC and advanced step nonlinear MPC. The framework automates many of the aspects of MPC defining new classes in Pyomo for ideal NMPC (iNMPC) and advanced step NMPC (asNMPC). The user only has to define the prediction model of the process using new classes for manipulated variables, disturbances and initial conditions, and the real plant function to access to the states of the process in a similar way of other algebraic modelling languages. The model discretizaion, controller set-up, receding horizon and solution are done automatically. Three examples are presented in detail for explaining the use and advantages of the framework to evaluate iNMPC and asNMPC controllers. The software is freely available upon request and in the future it is expected to be an official extension of Pyomo.",
     "keywords": ["asNMPC", "Model predictive control", "Pyomo", "NLP sensitivity", "Dynamic optimization", "Python"]},
    {"article name": "Automatic derivation of qualitative plant simulation models from legacy piping and instrumentation diagrams",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.040",
     "publication date": "09-2016",
     "abstract": "Confronted with the need of plant modernization, facility owners and contractors in the process industry invest significant efforts to create digital plant models allowing for simulation and thereby validation of new engineering solutions. Although an important part of the information required for this task already exists in form of legacy engineering documentation, current computer-aided methods for generating digital plant models cannot exploit this source of knowledge owing to the non-computer-interpretable nature of the available information sources. In an effort to bridge the existing gap, this contribution presents a method based on optical recognition and semantic analysis, which is capable of automatically converting legacy engineering documents, specifically piping and instrumentation diagrams, into object-oriented plant descriptions and ultimately into qualitative plant simulation models. Resulting simulation models can serve as a basis to support engineering tasks requiring low-fidelity simulation, such as the validation of base control functions during the factory acceptance test (FAT).",
     "keywords": ["Digital plant", "Legacy engineering documentation", "Piping and instrumentation diagram", "Optical recognition", "Automatic model generation", "Qualitative simulation model"]},
    {"article name": "Model analysis and optimization under uncertainty using thinned cubature formulae",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.05.006",
     "publication date": "09-2016",
     "abstract": "Model analysis and optimization under uncertainty needs efficient n -dimensional integration techniques, particularly when n (number of uncertain parameters) is large and the numerical model heavy. New thinned cubature formulae, recently tested by us and still practically unknown in engineering areas, have significantly changed the status of cubatures vs. quasi-Monte Carlo integration, for moderately high values of n . This paper presents these new cubatures (based on orthogonal arrays) from a practitioner\u2019s point of view and illustrates their remarkable efficiency in solving process systems engineering problems, namely those under the classes of simulation under uncertainty, variance-based global sensitivity analysis and optimization under uncertainty. Thinned cubatures allow efficient solution of these problems up to dimension n around 20, producing very reasonable estimates with only a few hundred or thousand of integration points. Three practical applications are provided: (i) analysis of a large-scale mass transfer model, (ii) optimal planning of a production network, (iii) preliminary design of a batch process under high levels of uncertainty and from different sources.",
     "keywords": ["Uncertainty analysis", "Sensitivity analysis", "Optimization under uncertainty", "Multidimensional integration", "Cubature formulae"]},
    {"article name": "Large scale optimization of a sour water stripping plant using surrogate models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.039",
     "publication date": "09-2016",
     "abstract": "In this work, we propose a new methodology for the large scale optimization and process integration of complex chemical processes that have been simulated using modular chemical process simulators. Units with significant numerical noise or large CPU times are substituted by surrogate models based on Kriging interpolation. Using a degree of freedom analysis, some of those units can be aggregated into a single unit to reduce the complexity of the resulting model. As a result, we solve a hybrid simulation-optimization model formed by units in the original flowsheet, Kriging models, and explicit equations.We present a case study of the optimization of a sour water stripping plant in which we simultaneously consider economics, heat integration and environmental impact using the ReCiPe indicator, which incorporates the recent advances made in Life Cycle Assessment (LCA).The optimization strategy guarantees the convergence to a local optimum inside the tolerance of the numerical noise.",
     "keywords": ["Process simulation", "Process optimization", "Kriging interpolation", "Heat exchanger network", "Life cycle assessment"]},
    {"article name": "An inexact interior point method for optimization of differential algebraic systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.013",
     "publication date": "09-2016",
     "abstract": "This paper presents an inexact factorization technique in the context of an interior point method for optimal control of systems described by Differential Algebraic Equations (DAEs). The proposed method is based on a numerically banded structure arising in the normal equations. The structure implies that banded factorization techniques can be used to calculate inexact Newton directions at a low computational cost. Numerical experiments show that a narrow band in the normal equations can be selected without impeding the convergence of the method.",
     "keywords": ["Interior-point methods", "Differential algebraic systems", "Inexact search directions"]},
    {"article name": "Ensuring integral controllability for robust multivariable control",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.05.004",
     "publication date": "09-2016",
     "abstract": "Integral controllability (IC) is a desired property of multivariable models used in robust controller design. IC requires satisfaction of eigenvalue-based inequalities involving the real process and identified model. Design of experiments for identification of models that satisfy these inequalities is cumbersome. To address this issue, Darby and Nikolaou (2009) developed a general mathematical framework, that relies on a much simpler inequality as its starting point. However that inequality is only sufficient, which could potentially be conservative. This paper examines this concern through a numerical simulation study and analysis on a number of systems. The results suggest that conservatism is fairly low but increasing as the identified system size increases. In addition, a rigorous argument is used to establish that it is generally impossible to build an IC-compliant model once a least-squares model is not, thus emphasizing the importance of design of experiments for IC-compliant model identification.",
     "keywords": ["Integral controllability", "Design of experiments", "Multivariable systems", "Identification"]},
    {"article name": "Circulating fluidized bed combustion reactor: Computational Particle Fluid Dynamic model validation and gas feed position optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.05.008",
     "publication date": "09-2016",
     "abstract": "A 3D Computational Particle Fluid Dynamic (CPFD) model is validated against experimental measurements in a lab-scale cold flow model of a Circulating Fluidized Bed (CFB). The model prediction of pressure along the riser, downcomer and siphon as well as bed material circulation rates agree well with experimental measurements. Primary and secondary air feed positions were simulated by varying the positions along the height of the reactor to get optimum bed material circulation rate. The optimal ratio of the height of primary and secondary air feed positions to the total height of the riser are 0.125 and 0.375 respectively. The model is simulated for high-temperature conditions and for reacting flow including combustion reactions. At the high temperature and reaction conditions, the bed material circulation rate is decreased with the corresponding decrease in pressure drop throughout the CFB for the given air feed rate.",
     "keywords": ["CFB", "Biomass gasification", "Dual fluidized bed", "CPFD"]},
    {"article name": "Optimality-based grid adaptation for input-affine optimal control problems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.041",
     "publication date": "09-2016",
     "abstract": "This paper studies the solution accuracy of direct single-shooting in comparison to the solution of the continuous optimal control problem (OCP). First, a convergence relation between the solution of the nonlinear program and that of continuous OCP is analyzed by means of an exemplary problem. This example reveals that Pontryagin's minimum principle cannot be used as a stopping criterion for optimality-based control grid adaptation. Consequently, a novel grid refinement strategy is introduced, which is rather based on the switching function and thus limited to the class of input-affine OCPs. Grid points are eliminated and inserted such that the approximation of the optimality condition of the OCP, elucidated by the switching function, is improved. The suggested methodology is illustrated and compared to a previously published wavelet-based adaptation approach by means of two reactor optimization problems with different solution characteristics.",
     "keywords": ["Optimal control problem", "Optimality-based adaptation", "Grid refinement", "Convergence relation", "Switching function"]},
    {"article name": "Modeling and energy reduction of multiple effect evaporator system with thermal vapor compression",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.05.011",
     "publication date": "09-2016",
     "abstract": "A general and rigorous mathematical model is developed for a multiple effect evaporator system which includes various energy reduction schemes (ERSs). These ERSs are thermal vapor compression, vapor bleeding, condensate flashing and solution flashing. The model can achieve the function of pumping steam at any effect and can work as an effective screening tool for the selection of optimal feed flow sequence (OFFS). In order to solve the model, the iteration method combining with matrix methods is proposed. To study effect of different ERSs on steam consumption (SC), an example of the co-current quadruple effect evaporator system is considered. These schemes can reduce the SC up to 46.56% if the feed is heated up to 88\u00a0\u00b0C and ejection coefficient at 3rd effect is set to 0.3. The OFFS is forward sequence as long as preheating temperature is high enough when constraints of heat transfer driving force can be satisfied.",
     "keywords": ["Rigorous mathematical model", "Multiple effect evaporator", "Thermal vapor compression", "Optimal feed flow sequence", "Steam consumption"]},
    {"article name": "Simulations of reactive settling of activated sludge with a reduced biokinetic model",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.037",
     "publication date": "09-2016",
     "abstract": "Denitrification during the sedimentation process in secondary settlers in waste-water treatment plants has been reported to be still a significant problem. The modelling of such a process with the conservation of mass leads to a nonlinear convection\u2013diffusion\u2013reaction partial differential equation, which needs non-standard numerical methods to obtain reliable simulations. The purpose of this study is to provide a first extension of the B\u00fcrger\u2013Diehl settler model, which models sedimentation without reactions, to include biological reactions. This is done with a reduced biokinetic model that contains only the relevant particulate and soluble components. Furthermore, the development is limited to batch settling. The final model describes the last settling stage of a sequencing batch reactor with denitrification. The main result of the paper is a numerical scheme that is tested for simulations with several initial conditions.",
     "keywords": ["Batch sedimentation", "Numerical scheme", "Degenerate parabolic PDE", "Sequencing batch reactor"]},
    {"article name": "An optimization framework for the integration of water management and shale gas supply chain design",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.03.025",
     "publication date": "09-2016",
     "abstract": "This study presents the mathematical formulation and implementation of a comprehensive optimization framework for the assessment of shale gas resources. The framework simultaneously integrates water management and the design and planning of the shale gas supply chain, from the shale formation to final product demand centers and from fresh water supply for hydraulic fracturing to water injection and/or disposal. The framework also addresses some issues regarding wastewater quality, i.e., total dissolved solids (TDS) concentration, as well as spatial and temporal variations in gas composition, features that typically arise in exploiting shale formations. In addition, the proposed framework also considers the integration of different modeling, simulation and optimization tools that are commonly used in the energy sector to evaluate the technical and economic viability of new energy sources. Finally, the capabilities of the proposed framework are illustrated through two case studies (A and B) involving 5 well-pads operating with constant and variable gas composition, respectively. The effects of the modeling of variable TDS concentration in the produced wastewater is also addressed in case study B.",
     "keywords": ["Optimization framework", "Water management", "Shale gas", "Supply chain", "Geographic information systems", "Reservoir simulation"]},
    {"article name": "Optimizing inventory policies in process networks under uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.05.014",
     "publication date": "09-2016",
     "abstract": "We address the inventory planning problem in process networks under uncertainty through stochastic programming models. Inventory planning requires the formulation of multiperiod models to represent the time-varying conditions of industrial process, but multistage stochastic programming formulations are often too large to solve. We propose a policy-based approximation of the multistage stochastic model that avoids anticipativity by enforcing the same decision rule for all scenarios. The proposed formulation includes the logic that models inventory policies, and it is used to find the parameters that offer the best expected performance. We propose policies for inventory planning in process networks with arrangements of inventories in parallel and in series. We compare the inventory planning strategies obtained from the policy-based formulation and the analogous two-stage approximation of the multistage stochastic program. Sequential implementation of the planning strategies in receding horizon simulations shows the advantages of the policy-based model, despite the increase in computational complexity.",
     "keywords": ["Inventory planning", "Process network", "Stochastic programming", "Inventory policies", "Receding horizon"]},
    {"article name": "Recent advances in mathematical programming techniques for the optimization of process systems under uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.03.002",
     "publication date": "08-2016",
     "abstract": "Optimization under uncertainty has been an active area of research for many years. However, its application in Process Systems Engineering has faced a number of important barriers that have prevented its effective application. Barriers include availability of information on the uncertainty of the data (ad-hoc or historical), determination of the nature of the uncertainties (exogenous vs. endogenous), selection of an appropriate strategy for hedging against uncertainty (robust/chance constrained optimization vs. stochastic programming), large computational expense (often orders of magnitude larger than deterministic models), and difficulty of interpretation of the results by non-expert users. In this paper, we describe recent advances that have addressed some of these barriers for mostly linear models.",
     "keywords": ["Decision rule", "Robust optimization", "Stochastic programming", "Exogenous uncertainty", "Endogenous uncertainty", "Scenario generation"]},
    {"article name": "A grand model for chemical product design",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.03.009",
     "publication date": "08-2016",
     "abstract": "Chemical engineering has been expanding its focus from primarily business-to-business products (B2B) to business-to-consumer (B2C) products. The production of B2B products generally emphasizes on process design and optimization, whereas the production of B2C products focuses on product quality, ingredients and structure. Market and competitive analysis, government policies and regulations have to be explicitly considered in product design. All these considerations are accounted for in the Grand Product Design Model, which consists of a process model, a property model, a quality model, a cost model, a pricing model, an economic model as well as factors such as company strategy, government policies and regulations. This article introduces the model and highlights selected aspects of the model with two case studies. One is a die attach adhesive that illustrates how pricing affects profitability, and how product composition changes with market conditions. Another is a hand lotion that illustrates how product quality affects the profit.",
     "keywords": ["Product design", "Product ingredients", "Product structure", "Product price", "Government policies"]},
    {"article name": "A modeling tool for the personalization of pharmacokinetic predictions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.03.008",
     "publication date": "08-2016",
     "abstract": "A method to apply pharmacokinetic models to assist physicians in therapeutic drug monitoring is proposed. The practice of therapeutic drug monitoring is required for drugs characterized by a narrow therapeutic index, which consequently present toxicity concerns. The proposed method employs a physiologically based pharmacokinetic (PBPK) model to determine an initial assessment of the pharmacokinetics (PK) of a specific patient. To further increase the precision of this prediction, the method uses two experimental datasets: (i) the PK data from a group of reference subjects, and (ii) limited drug blood concentration measures of the specific patient under study.By combining the available information, it is possible to assess the precision of the initial model prediction and determine a correction factor to improve it. The resulting patient-specific PBPK model produces encouraging results as there is a concrete reduction in prediction errors of the individualized PK with respect to experimental data.",
     "keywords": ["PBPK modeling", "Personalized medicine", "Individualized treatment", "Therapeutic drug monitoring", "Pharmacokinetics"]},
    {"article name": "Multi-scale modeling of fixed-bed Fischer Tropsch reactor",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.03.035",
     "publication date": "08-2016",
     "abstract": "A multi-scale pseudo-homogeneous one-dimensional model for the Fischer Tropsch fixed-bed reactor has been developed in this study using a detailed mechanistic kinetic scheme proposed in an earlier study (Todic et al., 2013). The developed model is capable of predicting the concentration and temperature profiles at the micro-(catalyst pores) level as well as the macro-(reactor bed) level for a cobalt-based catalyst (15\u00a0wt% Co/Al2O3). The uniqueness of this model is that it tries to combine different levels of complexity (product distribution modeling, particle diffusion and reactor bed modeling) into one single model. The predictability of this model has been validated experimentally using an advanced high-pressure FTS reactor unit over a wide range of testing conditions. It quite accurately predicts experimentally measured CH4 selectivity at different gas hourly space velocities but less accurately predicted CO conversion. On the other hand, a hydrocarbon product distribution has been predicted using a MATLAB\u00ae code that was written to estimate the FTS kinetic model\u2019s parameters (Todic et al., 2013) based on the experimental data collected using bench scale FTS fixed-bed reactor. The optimization of this model was done using a Genetic Algorithm (GA). The findings showed excellent predictability of the experimentally measured hydrocarbon product distribution profile of the catalyst, specifically paraffin formation rates which are the main products of the cobalt-based catalyst. This comprehensive model also involved modified thermodynamic equation of state and currently is upgraded to enable a direct comparison of the gas-phase and supercritical solvent-assisted FTS reactions under a variety of conditions using the experimental data reported in a recent study by our team (Kasht et al., 2015).",
     "keywords": null},
    {"article name": "The water-energy-food nexus and process systems engineering: A new focus",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.03.003",
     "publication date": "08-2016",
     "abstract": "As the global population grows, consumption of water, energy, and food will also increase, placing stresses on these three sectors, raising the importance of the Water-Energy-Food Nexus (WEFN). This article highlights research challenges and identifies process systems engineering research opportunities to appropriately model and optimize the WEFN. A brief overview of relevant, foundational WEFN research is first presented. We then identify challenges in the multiple scales of the WEFN, ranging from the household scale to the global scale. There are further challenges with appropriate system boundary definitions, and challenges in modeling the decision-making and conflicting objectives of multiple stakeholders in the WEFN. Uncertainties of all kinds appear at all scales of the WEFN and must also be considered. We use two motivating WEFN examples to frame these challenges and propose future avenues and opportunities for research. Possible approaches to the abovementioned challenges are proposed.",
     "keywords": ["Water-energy-food nexus", "Modeling and optimization of multiple spatial and temporal scales", "Life cycle optimization", "Multi-scale modeling", "Modeling of multiple stakeholders"]},
    {"article name": "A superstructure-based framework for simultaneous process synthesis, heat integration, and utility plant design",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.02.013",
     "publication date": "08-2016",
     "abstract": "We propose a superstructure optimization framework for process synthesis with simultaneous heat integration and utility plant design. Processing units in the chemical plant can be modeled using rigorous unit models or surrogate models generated from experimental results or off-line calculations. The utility plant subsystem includes multiple steam types with variable temperature and pressure. For the heat integration subsystem, we consider variable heat loads of process streams as well as variable intervals for the utilities. To enhance the solution of the resulting mixed-integer nonlinear programming models, we develop (1) new methods for the calculation of steam properties, (2) algorithms for variable bound calculation, and (3) systematic methods for the generation of redundant constraints. The applicability of our framework is illustrated through a biofuel case study which includes a novel non-enzymatic hydrolysis technology and new separation technologies, both of which are modeled based on experimental results.",
     "keywords": ["Chemical process design", "Mixed-integer nonlinear programming", "Global optimization", "Biofuels"]},
    {"article name": "Towards the integration of process design, control and scheduling: Are we getting closer?",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.11.002",
     "publication date": "08-2016",
     "abstract": "The integration of design and control, control and scheduling and design, control and scheduling, all have been core PSE challenges. While significant progress has been achieved over the years, it is fair to say that at the moment there is not a generally accepted methodology and/or \u201cprotocol\u201d for such an integration \u2013 it is also interesting to note that currently, there is not a commercially available software [or even in a prototype form] system to fully support such an activity.Here, we present the foundations for such an integrated framework and especially a software platform that enables such integration based on research developments over the last 25 years. In particular, we describe PAROC, a prototype software system which allows for the representation, modeling and solution of integrated design, scheduling and control problems. Its main features include: (i) a high-fidelity dynamic model representation, also involving global sensitivity analysis, parameter estimation and mixed integer dynamic optimization capabilities; (ii) a suite/toolbox of model approximation methods; (iii) a host of multi-parametric programming solvers for mixed continuous/integer problems; (iv) a state-space modeling representation capability for scheduling and control problems; and (v) an advanced toolkit for multi-parametric/explicit Model Predictive Control and moving horizon reactive scheduling problems. Algorithms that enable the integration capabilities of the systems for design, scheduling and control are presented on a case of a series of cogeneration units.",
     "keywords": ["Multi-parametric receding horizon policies", "Control", "Design optimization", "Optimal scheduling", "Integration"]},
    {"article name": "Towards sustainable production and consumption: A novel DEcision-Support Framework IntegRating Economic, Environmental and Social Sustainability (DESIRES)",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.03.017",
     "publication date": "08-2016",
     "abstract": "The idea of sustainable production and consumption is becoming a widely-accepted societal goal worldwide. However, its implementation is slow and the world continues to speed down an unsustainable path. One of the difficulties is the sheer complexity of production and consumption systems that would need to be re-engineered in a more sustainable way as well as the number of sustainability constraints that have to be considered and satisfied simultaneously. This paper argues that bringing about sustainable production and consumption requires a systems approach underpinned by life cycle thinking as well as an integration of economic, environmental and social aspects. In an attempt to aid this process, a novel decision-support framework DESIRES has been developed comprising a suite of tools, including scenario analysis, life cycle costing, life cycle assessment, social sustainability assessment, system optimisation and multi-attribute decision analysis. An application of the framework is illustrated by a case study related to energy.",
     "keywords": ["Decision-support framework", "Energy", "Life cycle sustainability assessment", "Sustainable production and consumption", "Systems approach", "System optimisation"]},
    {"article name": "Abnormal situation management: Challenges and opportunities in the big data era",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.011",
     "publication date": "08-2016",
     "abstract": "Although modern chemical processes are highly automatic, abnormal situation management (ASM) still heavily relies on human operators. Process fault detection and diagnosis (FDD) are one of the most important issues of ASM but few FDD systems have been satisfactorily applied in real chemical processes since the concept of FDD was proposed about 40 years ago. In this paper, developments of chemical process FDD are briefly reviewed. The reason why FDD has not been widely implemented in the chemical process industry is discussed. One of the insights gained is that some basic problems in FDD such as how to define faults and how many faults to diagnose have not even been addressed well while researchers tirelessly try to invent new methods to diagnose fault. A new framework is proposed based on the big data in a cloud computing environment of a big chemical corporation for addressing the challenging issues in ASM.",
     "keywords": ["Abnormal situation management", "Process safety", "Fault diagnosis", "Big data", "Artificial immune system", "Cloud computing"]},
    {"article name": "Multi-time scale procurement planning considering multiple suppliers and uncertainty in supply and demand",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.03.024",
     "publication date": "08-2016",
     "abstract": "Inventory management of procurement system is decomposed into sub-problems according to the timescale of decisions: the long-term planning for ordering raw materials and the short-term scheduling for unloading the orders. To ensure more sustainable and robust operation, different decision layers should be integrated (which is nature of multi-scale), and supply and demand uncertainty should be considered. In this study, the planning problem is formulated as a Markov decision process (MDP) to incorporate possible realizations of uncertainty into the decision-making process. The MDP planning model is integrated with a scheduling model expressed by a MILP (or closely approximated by a heuristic approach). Decision policies are obtained from solving the MDP problem through an exact value iteration, as well as an approximate approach intended to alleviate the computational challenges. We compare the results from applying them with those of a reference policy obtained without any rigorous integration with scheduling through benchmark problems.",
     "keywords": ["Procurement planning and scheduling", "Multi-scale decision making", "Supply and demand uncertainty", "Markov decision process", "Approximate dynamic programming"]},
    {"article name": "Deploying scheduling solutions in an industrial environment",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.03.029",
     "publication date": "08-2016",
     "abstract": "For many industrial sites, a production scheduling solution \u2013 be it manual or optimized \u2013 is in many ways still a complex and \u201cexotic\u201d application that seldomly finds its way into the daily practice of the plant floor. As it lies within the interface between business and control systems, scheduling is too often seen as something decoupled and theoretical that requires high-level experts for configuring, using and maintaining it. In this paper, we discuss some of the challenges and hurdles for deploying scheduling solutions. A relevant ongoing technological transition towards stronger integration is also addressed. Some potential for improvements are identified both in academia and industry. By addressing the main pain points it can be seen that with a common mind-set the complexity of deploying a scheduling solution can be lowered significantly.",
     "keywords": ["Scheduling", "Deployment", "Communication", "Integration", "ISA-95"]},
    {"article name": "Perspective for smart factory in petrochemical industry",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.03.006",
     "publication date": "08-2016",
     "abstract": "Opportunities and challenges in the petrochemical industry and the emergence of massive disruptive technologies have triggered a new revolution that has the power to fundamentally change industrial processes including manufacturing, engineering, materials, supply chains, lifecycle management. Recently, the newly arisen smart factory adopted a disruptive manufacturing methodology and has become a key part of the petrochemical industry. The smart factory, which is different from the original production systems used in the petrochemical industry, needs to assess and position its future research agenda including its definition, intension, framework, and technology. Systems thinking and systems problem solving for the smart factory must be prioritized. Based on an analysis of the driving force for smart factory development, this paper proposes a lifecycle blueprint and consensus-based operating and technology roadmap. The definition and features of a smart factory in the petrochemical industry are presented. Furthermore, a summary of the technical systems and future-proof research field of the smart petrochemical factory from an academic and industrial viewpoint is presented.",
     "keywords": ["Process system engineering", "Petrochemical industry", "Smart factory"]},
    {"article name": "Reprint of \u201cPSE for problem solving excellence in industrial R&D\u201d",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.05.013",
     "publication date": "08-2016",
     "abstract": "PSE, process systems engineering, is about the development and application of systematic methods for process studies by the chemical engineer. By means of software tools, the application of these methods is facilitated. Over the last about half a century, CAPE (computer aided process engineering) tools have found their way into process engineering. For example it is unthinkable nowadays to design a plant without a simulation through a process simulator. But there are many more applications of PSE in industry.The aim of this paper is to provide a taste of the meaning of PSE within the industrial R&D environment. The intention is not to provide a complete overview but to give a flavour of what is perceived as the benefits of PSE during process development, and, in which areas PSE should be extended to render further benefits. The combined approach of experiments and modelling offers a very (cost-)effective strategy in industrial R&D. Further improvements are desired in the areas related to process intensification (PI) and (conceptual) product design. It is believed that the current methods would be more beneficial and have a stronger applicability in industry by inclusion of semi-predictive models and uncertainty considerations.",
     "keywords": ["PSE", "CAPE", "PI", "Industrial process and product innovation"]},
    {"article name": "A comprehensive mathematical analysis of a novel multistage population balance model for cell proliferation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.02.012",
     "publication date": "08-2016",
     "abstract": "Multistage population balances provide a more detailed mathematical description of cellular growth than lumped growth models, and can therefore describe better the physics of cell evolution through cycles. These balances can be formulated in terms of cell age, mass, size or cell protein content and they can be univariate or multivariate. A specific three stage population balance model based on cell protein content has been derived and used recently to simulate evolution of cell cultures for several applications. The behavior of the particular mathematical model is studied in detail here. A one equation analog of the multistage model is formulated and it is solved analytically in the self-similarity domain. The effect of the initial condition on the approach to self-similarity is studied numerically. The three equations model is examined then by using asymptotic and numerical techniques. It is shown that in the case of sharp interstage transition the discontinuities of the initial condition are preserved during cell growth leading to oscillating solutions whereas for distributed transition, the cell distribution converges to a self-similar (long time asymptote) shape. The closer is the initial condition to the self similar distribution the faster is the convergence to the self-similarity and the smaller the amplitude of oscillations of the total cell number. The findings of the present work lead to a better understanding of the multistage population balance model and to its more efficient use for description of experimental data by employing the expected solution behavior.",
     "keywords": ["Multistage", "Cell cycle", "Population balance", "Cell growth", "Analytical solutions", "Mathematical analysis"]},
    {"article name": "Graphical processing unit (GPU) acceleration for numerical solution of population balance models using high resolution finite volume algorithm",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.03.023",
     "publication date": "08-2016",
     "abstract": "Population balance modeling is a widely used approach to describe crystallization processes. It can be extended to multivariate cases where more internal coordinates i.e., particle properties such as multiple characteristic sizes, composition, purity, etc. can be used. The current study presents highly efficient fully discretized parallel implementation of the high resolution finite volume technique implemented on graphical processing units (GPUs) for the solution of single- and multi-dimensional population balance models (PBMs). The proposed GPU-PBM is implemented using CUDA C++ code for GPU calculations and provides a generic Matlab interface for easy application for scientific computing. The case studies demonstrate that the code running on the GPU is between 2\u201340 times faster than the compiled C++ code and 50\u2013250 times faster than the standard MatLab implementation. This significant improvement in computational time enables the application of model-based control approaches in real time even in case of multidimensional population balance models.",
     "keywords": ["Population balance modelling", "Finite volume algorithm", "GPU", "Crystallization modelling"]},
    {"article name": "Improvement of ethylene cracking reaction network with network flow analysis algorithm",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.020",
     "publication date": "08-2016",
     "abstract": "Reaction network model is central to ethylene cracking process simulation. Studying an ethylene cracking reaction (ECR) network, which involves hundreds of components and thousands of reactions, becomes a difficult task. To facilitate a rapid and comprehensive reaction network analysis and improve ECR network, this paper introduced a ranking algorithm called network flow analysis algorithm (NFAA) to a reaction network analysis procedure. NFAA analyses the reaction network with comprehensive information such as network topological structure, reaction mechanism, and process model data. Following NFAA, reactions and species are ranked based on their significances. According to the ranking of reactions, unimportant reactions (lower ranking) in ECR network are removed to reduce ECR network complexity and decrease computational scale without loss of prediction accuracy. On the other hand, rankings provide guidance on adjusting parameters of ECR network. The application of NFAA makes a progress in improvement of ECR reaction network in an industrial case.",
     "keywords": ["Ethylene cracking reaction", "Network flow analysis algorithm (NFAA)", "Reaction network reduction", "Reaction network adjustment"]},
    {"article name": "A methodology for direct exploitation of available information in the online model-based redesign of experiments",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.03.016",
     "publication date": "08-2016",
     "abstract": "Online model-based design of experiments techniques were proposed to exploit the progressive increase of the information resulting from the running experiment, but they currently exhibit some limitations: the redesign time points are chosen \u201ca-priori\u201d and the first design may be heavily affected by the initial parametric mismatch.In order to face such issues an information driven redesign optimisation (IDRO) strategy is here proposed: a robust approach is adopted and a new design criterion based on the maximisation of a target profile of dynamic information is introduced. The methodology allows determining when to redesign the experiment in an automatic way, thus guaranteeing that an acceptable increase in the information content has been achieved before proceeding with the intermediate estimation of the parameters and the subsequent redesign of the experiment. The effectiveness of the new experiment design technique is demonstrated through two simulated case studies.",
     "keywords": ["Online model-based design of experiments", "Robust design", "Model identification", "Parameter estimation"]},
    {"article name": "A multi-scale approach for the discovery of zeolites for hydrogen sulfide removal",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.03.015",
     "publication date": "08-2016",
     "abstract": "Removing H2S from industrial gases is important to avoid operational hazards and to meet environmental regulation. Microporous zeolites are potential adsorbents for separating H2S from other gases. While large number of candidate zeolites exists, it is not trivial to select cost-effective zeolites capable of satisfying process constraints and specifications. In this work, a novel method for the zeolite-based H2S separation is put forward which pertains to a multi-scale modeling, simulation, and optimization framework for combined material screening and process optimization to reduce the overall process cost. The framework spans the atomistic and mesoscopic scales for the screening and selection of zeolites and the macroscopic scale for the simulation and selection of optimal conditions for pressure swing adsorption (PSA)-based H2S separation technology. Applying this framework, several novel zeolites have been identified for the first time for separation of H2S from representative H2S/CO2, H2S/N2, and H2S/CH4 mixtures. The zeolites which are screened are capable of removing H2S from natural gas, acid gas, tail gas, flue gas, refinery gas, biogas, landfill gas, and other gases of industrial importance. Results show that it is possible to perform cost-effective H2S removal by exploiting reverse selectivity of the gas molecules using novel micro-porous materials. We have also identified zeolite ABW as an adsorbent with high potential for commercialization for multi-purpose gas separation including acid gas removal from natural gas and carbon capture from power plants.",
     "keywords": ["H2S removal", "Multi-scale optimization", "Zeolites", "Separation", "Process optimization"]},
    {"article name": "A systematic visual methodology to design ionic liquids and ionic liquid mixtures: Green solvent alternative for carbon capture",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.006",
     "publication date": "08-2016",
     "abstract": "Ionic liquids (ILs) have gained great interest recently to substitute volatile organic compounds (VOCs), since their properties can be tuned to match certain targets and applications. Further to this, another possibility to optimise ILs for their specific application is through IL mixtures. In this work, an insightful and yet simple systematic approach to design pure ILs and their mixtures is presented. This newly presented approach allows the visualisation of IL mixture design problem, and hence provides insights and allows users to solve the problem visually. The visualisation of problem and solutions is achieved by applying property integration framework in this proposed methodology. In property integration framework, IL products design problem is mapped from property domain into cluster domain through property clustering technique. Therefore, the proposed methodology provides a property based platform to visualise the overall performance of the designed IL products with graphical tools. A feasible IL product is always designed to fit a purpose based on consideration of multiple target properties, but these properties can be contradicting one another. The presented approach allows multiple target properties consideration during the design process, by portraying these properties and target of each clearly on a single graphical tool. To date, the study of properties of pure ILs and IL mixtures is still in the infant phase, and these data are still scarce. Hence, some of the prediction models do not cover all available ILs. To overcome this problem, the proposed approach is developed to adapt property data of pure ILs directly, together with existing property prediction models to predict the properties of the designed IL mixtures. The presented approach is able to generate a list of potential solutions to users, and the final decision can be made by users accordingly, through further screening and experimental validations. An illustrative case study, which focuses on the design of carbon capture solvents, is solved to demonstrate the proposed approach.",
     "keywords": ["Group contribution", "Property clustering", "Property integration", "Ternary diagram"]},
    {"article name": "An integrated reactive distillation process for biodiesel production",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.01.008",
     "publication date": "08-2016",
     "abstract": "An integrated reactive distillation process for biodiesel production is proposed. The reactive separation process consists of two coupled reactive distillation columns (RDCs) considering the kinetically controlled reactions of esterification of the fatty acids (FFA) and the transesterification of glycerides with methanol, respectively. The conceptual design of the reactive distillation columns was performed through the construction of reactive residue curve maps in terms of elements. The design of the esterification reactive distillation column consisted of one reactive zone loaded with Amberlyst 15 catalyst and for the transesterification reactive column two reactive zones loaded with MgO were used. Intensive simulation of the integrated reactive process considering the complex kinetic expressions and the PC-SAFT EOS was performed using the computational environment of Aspen Plus. The final integrated RD process was able to handle more than 1% wt of fatty acid contents in the vegetable oil. However, results showed that the amount of fatty acids in the vegetable oil feed plays a key role on the performance (energy cost, catalyst load, methanol flow rate) of the integrated esterification\u2013transesterification reactive distillation process.",
     "keywords": ["Reactive distillation", "Biodiesel production", "Conceptual design", "Reactive residue curve maps", "Element concept"]},
    {"article name": "Steady state optimization of design and operation of desalination systems using Aspen Custom Modeler",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.024",
     "publication date": "08-2016",
     "abstract": "In this paper Multistage flash (MSF), Reverse Osmosis (RO) and hybrid MSF/RO desalination systems are optimized. A superstructure is set up for analyzing various process configurations in a single flowsheet. Detailed steady state models for MSF and RO sections of the superstructure are developed incorporating comprehensive physio-chemical properties and design characteristics. The model for the hybrid system combines individual models of MSF and RO systems and additional separators and mixers. The optimization variables consist of the operating and design variables, and the objective function is developed on the basis of economic and technical performance indicators. Primary results show that the hybridization of MSF and RO systems sharing common intake-outfall facilities presents lower cost with the additional benefits of higher overall recovery than MSF system and higher product quality than RO system. The sensitivity analysis of the cost parameters is performed to realize their effects on the selection of process configuration.",
     "keywords": ["Superstructure", "Modeling", "Optimization", "Desalination", "Hybrid MSF/RO"]},
    {"article name": "Using product driven process synthesis in the biorefinery",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.03.030",
     "publication date": "08-2016",
     "abstract": "The design of a biorefining process is challenging due to the high number of products that can be obtained from one feedstock, and the fact that some products can be negatively affected by processing conditions that are essential for other products. To facilitate this design, we propose the use of the product driven process synthesis methodology, with some adaptations. Four novel steps were introduced: (1) decomposition of the feedstock into its main compound classes, (2) identification of the potential uses of the compound classes found in the feedstock, based on the functionalities that they can deliver, (3) selection of the product-targets by evaluating their economic potential, and (4) identification of \u201ccritical tasks\u201d, i.e., tasks that negatively affect the quantity and/or quality of each product during their separation. To illustrate how this new approach can be used in practice, a case study of a sugar beet leaves biorefinery is presented.",
     "keywords": ["Biorefinery", "Sugar beet leaves", "Functionalities", "Product-driven-process-synthesis"]},
    {"article name": "Plantwide design and economic evaluation of two Continuous Pharmaceutical Manufacturing (CPM) cases: Ibuprofen and artemisinin",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.005",
     "publication date": "08-2016",
     "abstract": "Increasing Research and Development (R&D) costs, growing competition from generic manufacturers and dwindling market introduction rates for novel drug products bolster the efforts of pharmaceutical firms to secure competitiveness by investigating Continuous Pharmaceutical Manufacturing (CPM). The present paper explores the CPM of two key Active Pharmaceutical Ingredients (APIs), ibuprofen and artemisinin: cost savings and material efficiency benefits are evaluated for CPM vs. batch processing, with two continuous options for each API. Capital Expenditure (CapEx) savings of up to 57.0% and 19.6% and corresponding Operating Expenditure (OpEx) savings of up to 51.6% and 29.3% have been determined for ibuprofen and artemisinin, respectively. Total projected cost savings for a 20-year plant lifetime can reach 54.5% and 20.1%, respectively. Environmental (E)-factors (mass of waste generated per unit mass of product) of 43.4 (for ibuprofen) and 12.2 (for artemisinin) have been computed, indicating environmental and material efficiency advantages for these conceptual continuous pharmaceutical processes.",
     "keywords": ["Continuous Pharmaceutical Manufacturing (CPM)", "Ibuprofen", "Artemisinin", "Process design", "Reactor design", "Separation design"]},
    {"article name": "Synthesis of industrial park water reuse networks considering treatment systems and merged connectivity options",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.02.003",
     "publication date": "08-2016",
     "abstract": "A number of optimization approaches for the synthesis and design of effective wastewater regeneration and reuse networks in industrial parks have been proposed to support decision making for careful management of water resources by the industrial sector. The approaches allow the identification of optimal wastewater treatment and reuse strategies. All such available methods for interplant water network synthesis assign a single pipeline for every viable water allocation identified, which results in inefficient and costly pipe networks. Instead, this work presents a water network design approach that accounts for a number of pipeline merging scenarios for wastewater reuse and regeneration networks considering central and decentral treatment options. Merging common pipe segments that carry similar water qualities allows for cost-improvements in network design, in addition to reducing the overall pipeline network complexity due to fewer required interconnecting pipes. The benefits of the proposed method are illustrated with a case study.",
     "keywords": ["Process integration", "Water management", "Network synthesis", "Eco-industrial parks", "Optimization"]},
    {"article name": "Synthesis of water networks for industrial parks considering inter-plant allocation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.03.013",
     "publication date": "08-2016",
     "abstract": "Since an industrial park is a cluster of multiple company individuals, there is requirement to develop specific reuse strategies so as to improve the utilization of resources across plants. This article presents a \u2018plant-based\u2019 mode respecting to the water allocation problem within industrial parks. In the mode, mixers and splitters are involved to present the mixing, conveying and splitting operations for reusing streams across plants. Such that, the mixing possibilities can also be investigated and many redundant solutions can be avoided by considering the number limit of inter-plant stream connections at the building stage of network superstructures. On base of this mode, both direct and mixed (direct\u2013indirect) integration scenarios are studied in this study. Superstructures are established and mathematically formulated aiming to minimum fresh water consumption as well as the total annualized cost. At last, three integration cases are explored based on an example from literature for illustration.",
     "keywords": ["Inter-plant", "Allocation", "Water network", "Industrial park", "Superstructure"]},
    {"article name": "A reliable modifier-adaptation strategy for real-time optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.03.019",
     "publication date": "08-2016",
     "abstract": "In model-based real-time optimization, plant-model mismatch can be handled by applying bias- and gradient-corrections to the cost and constraint functions in an iterative optimization procedure. One of the major challenges in practice is the estimation of the plant gradients from noisy measurement data, in particular for several optimization variables. In this paper we propose a new real-time optimization scheme that explores the inherent smoothness of the plant mapping to enable a reliable optimization. The idea here is to combine the quadratic approximation approach used in derivative-free optimization techniques with the iterative gradient-modification optimization scheme. The convergence of the scheme is analyzed. Simulation studies for the optimization of a ten-variable synthetic example and a reactor benchmark problem with considerable plant-model mismatch show its promising performance.",
     "keywords": ["Real-time optimization", "Model mismatch", "Quadratic approximation", "Modifier adaptation"]},
    {"article name": "Supercritical fluid recycle for surge control of CO2 centrifugal compressors",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.03.012",
     "publication date": "08-2016",
     "abstract": "This paper presents computer-based design and analysis of control systems for centrifugal compressors when the operating fluid is supercritical CO2.It reports a non-linear dynamic model including a main forward compression line and two different configurations for the recycle antisurge line. Disturbance scenarios are proposed for testing the configurations and performance indicators are suggested to evaluate control performance and power consumption of the compression system.The paper demonstrates that compared to the hot recycle, the process configuration including a cold gas recycle has better overall stability, but higher power consumption and lower values for the control performance indicators. Based on the previous considerations, the paper gives suggestions regarding the choice of the recycle configuration. Moreover it compares subcritical and supercritical compression during surge prevention and highlights the importance of the selection of the gas recycle configuration when full recycle is needed.",
     "keywords": ["Modelling", "Dynamic simulation", "Supercritical", "CO2", "Compressor", "Surge"]},
    {"article name": "Time-optimal control of diafiltration processes in the presence of membrane fouling",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.018",
     "publication date": "08-2016",
     "abstract": "This paper deals with time-optimal operation of batch diafiltration processes in the presence of membrane fouling. Fouling causes a decrease in the effective membrane area and, hence, an increase in processing time. In this work we study a time-optimal operation with several fouling models available in literature. Pontryagin's minimum principle is applied to characterize the structure of the optimal operation which voids any further on-line optimization. Due to the specific structure of the problem, it is possible, in several problem setups, to derive and verify an explicit analytic solution. Obtained results are applied in case studies where we provide a comparison between traditional operation and the proposed time-optimal operation. In cases, where the optimal operation cannot be identified analytically, we analyze the performance of sub-optimal control derived from neighboring analytical solution and compare it to optimal operation found via numerical optimization techniques.",
     "keywords": ["Optimal control", "Membrane fouling", "Pontryagin's minimum principle", "Diafiltration"]},
    {"article name": "Constrained Unscented Gaussian Sum Filter for state estimation of nonlinear dynamical systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.021",
     "publication date": "08-2016",
     "abstract": "This work presents a novel constrained Bayesian state estimation approach for nonlinear dynamical systems. The proposed approach uses the recently proposed Unscented Gaussian Sum Filter to represent the underlying non-Gaussian densities as sum of Gaussians, and explicitly incorporates constraints on states during the measurement update step. This approach, labeled Constrained-Unscented Gaussian Sum Filter (C-UGSF), can thus model non-Gaussianity in constrained, nonlinear state estimation problems. Its applicability is demonstrated using three nonlinear, constrained state estimation case studies taken from literature, namely, (i) a gas phase batch reactor, (ii) an isothermal batch process, and (iii) a continuous polymerization process. Results demonstrate superior estimation performance along with a significant improvement in computational time when compared to Unscented Recursive Nonlinear Dynamic Data Reconciliation (URNDDR), which is a popular nonlinear, constrained state estimation approach available in literature.",
     "keywords": ["Nonlinear/non-Gaussian systems", "Unscented transformation", "Constrained Bayesian update", "Linear constraints", "Optimization", "Sum of Gaussians"]},
    {"article name": "Shape anomaly detection for process monitoring of a sequencing batch reactor",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.012",
     "publication date": "08-2016",
     "abstract": "Anomaly detection is critical to process modeling, monitoring, and control since successful execution of these engineering tasks depends on access to validated data. Classical methods for data validation are quantitative in nature and require either accurate process knowledge, large representative data sets, or both. In contrast, a small section of the fault diagnosis literature has focused on qualitative data and model representations. The major benefit of such methods is that imprecise but reliable results can be obtained under previously unseen process conditions. This work continues with a line of work focused on qualitative trend analysis which is the qualitative approach to data series analysis. An existing method based on shape-constrained spline function fitting is expanded to deal explicitly with discontinuities and is applied here for the first time for anomaly detection. An experimental test case and a comparison with the principal component analysis method bear out the benefits of the qualitative approach to process monitoring.",
     "keywords": ["Anomaly detection", "Batch process monitoring", "Fault identification", "Principal component analysis", "Qualitative trend analysis", "Statistical process control"]},
    {"article name": "Synthesis of tri-generation systems: Technology selection, sizing and redundancy allocation based on operational strategy",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.003",
     "publication date": "08-2016",
     "abstract": "Tri-generation system is a facility which produces heat, power and cooling simultaneously from a single fuel source. In the industry, such system is commonly operated via two strategies; Following Electrical Load (FEL) or Following Thermal Load (FTL). However, these operating strategies may lead to huge amount of energy that is wasted. In this respect, several works have proposed a switching strategy, whereby tri-generation systems would interchange between FEL and FTL modes depending on energy demand. Unfortunately, the design of tri-generation based on this strategy has received limited attention. Besides, tri-generation operations often face challenges in equipment reliability. As tri-generation systems contain a network of interconnected equipment, equipment failures would disrupt the overall performance of a tri-generation system. As such, this work proposes a novel systematic optimisation approach to design a robust tri-generation system which can operate optimally in its operating strategies. In addition, the proposed approach can simultaneously determine type, size and required equipment redundancy (e.g. operating and standby units) of technologies while considering operating strategies in a tri-generation system. A palm biomass-based tri-generation system (BTS) case study is solved to illustrate the proposed approach.",
     "keywords": ["Tri-generation", "Operation strategy", "Redundancy allocation", "Palm-based biomass"]},
    {"article name": "Implementing land-use and ecosystem service effects into an integrated bioenergy value chain optimisation framework",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.02.011",
     "publication date": "08-2016",
     "abstract": "This study presents a multi-objective optimisation model that is configured to account for a range of interrelated or conflicting questions with regard to the introduction of bioenergy systems. A spatial-temporal mixed integer linear programming model ETI-BVCM (Energy Technologies Institute \u2013 Bioenergy Value Chain Model) (ETI, 2015b, Newton-Cross, 2015, Samsatli et al., 2015) was adopted and extended to incorporate resource-competing systems and effects on ecosystem services brought about by the land-use transitions in response to increasing bioenergy penetration over five decades. The extended model functionality allows exploration of the effects of constraining ecosystem services impacts on other system-wide performance measures such as cost or greenhouse gas emissions. The users can therefore constrain the overall model by metric indicators which quantify the changes of ecosystem services due to land use transitions. The model provides a decision-making tool for optimal design of bioenergy value chains supporting an economically and land-use efficient and environmentally sustainable UK energy system while still delivering multiple ecosystem services.",
     "keywords": ["Optimisation", "MILP", "Bioenergy supply chain", "Ecosystem services", "Food production", "Non-energy system"]},
    {"article name": "A methodology for the sustainable design and implementation strategy of CO2 utilization processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.01.019",
     "publication date": "08-2016",
     "abstract": "This work presents a systematic methodology that has been developed for the design of sustainable CO2 utilization processes that can mitigate CO2 and also guarantee profitability. First, the three-stage methodology, evaluation criteria and applicable tools are described. Especially, the process design and analysis is discussed as only limited amounts of process data is available for determining the optimal processing path and in the third stage the issue of implementation strategy is considered. As examples, two CO2 utilization methods for methanol production, combined reforming and direct synthesis are considered. Methanol plants employing such methods are developed using synthesis-design and simulation tools and their evaluation indicators are calculated under various implementation strategies. It is demonstrated that integrating or replacing an existing conventional methanol plant by a combined reforming method represents a sustainable solution. Additionally, producing methanol through direct hydrogenation is a promising way to convert CO2 when cheap H2 feeds are available.",
     "keywords": ["Sustainability", "CO2 utilization", "Process synthesis", "Process design", "Economic evaluation"]},
    {"article name": "Optimal planning and campaign scheduling of biopharmaceutical processes using a continuous-time formulation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.009",
     "publication date": "08-2016",
     "abstract": "This work addresses the optimal planning and campaign scheduling of biopharmaceutical manufacturing processes, considering multiple operational characteristics, such as the campaign schedule of batch and/or continuous process steps, multiple intermediate deliveries, sequence dependent changeovers operations, product storage restricted to shelf-life limitations, and the track-control of the production/campaign lots due to regulatory policies. A new mixed integer linear programing (MILP) model, based on a Resource Task Network (RTN) continuous time single-grid formulation, is developed to comprise the integration of all these features. The performance of the model features is discussed with the resolution of a set of industrial problems with different data sets and process layouts, demonstrating the wide application of the proposed formulation. It is also performed a comparison with a related literature model, showing the advantages of the continuous-time approach and the generality of our model for the optimal production management of biopharmaceutical processes.",
     "keywords": ["Biopharmaceutical plants", "Planning and campaign scheduling", "Optimisation", "Mixed integer linear programming"]},
    {"article name": "Scenario-based dynamic negotiation for the coordination of multi-enterprise supply chains under uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.004",
     "publication date": "08-2016",
     "abstract": "A novel scenario-based dynamic negotiation approach is proposed for the coordination of decentralized supply chains under uncertainty. The relations between the involved organizations (client, provider and third parties) and their respective conflicting objectives are captured through a non-zero-sum and non-symmetric roles SBDN negotiation. The client (leader) designs coordination agreements considering the uncertain reaction of the provider (follower) resulting from the uncertain nature of the third parties, which is modeled as a probability of acceptance function. Different negotiation scenarios are studied: (i) cooperative, and (ii) non-cooperative and (iii) standalone cases. The use of the resulting models is illustrated through a case study with different vendors around a \u201cleader\u201d (client) in a decentralized scenario. Although the usual cooperation hypothesis will allow higher overall profit expectations, using the proposed approach it is possible to identify non-cooperative scenarios with high individual profit expectations which are more likely to be accepted by all individual partners.",
     "keywords": ["CNS Cooperative-negotiation scenario", "Cooperative-negotiation scenario", "CPU Central Processing Unit", "Central Processing Unit", "GAMS The General Algebraic Modeling System", "The General Algebraic Modeling System", "GloMIQO Global Mixed-Integer Quadratic Optimizer", "Global Mixed-Integer Quadratic Optimizer", "LP Linear programming", "Linear programming", "MINLP Mixed integer non-linear programming", "Mixed integer non-linear programming", "MILP Mixed integer linear programming", "Mixed integer linear programming", "MW Megawatt", "Megawatt", "nCNS Non-cooperative-negotiation scenario", "Non-cooperative-negotiation scenario", "NLP Non-linear programming", "Non-linear programming", "PMAGA Pinch Multi-Agent Genetic Algorithm", "Pinch Multi-Agent Genetic Algorithm", "PSE Process System Engineering", "Process System Engineering", "RM Raw material", "Raw material", "SBDN Scenario-based dynamic negotiation", "Scenario-based dynamic negotiation", "SC Supply chain", "Supply chain", "SCM Supply chain management", "Supply chain management", "SS Standalone scenario", "Standalone scenario", "\u03bc Mean", "Mean", "\u03c3 Standard deviation", "Standard deviation", "Decentralized supply chain", "Tactical planning", "Uncertainty management", "Competition management"]},
    {"article name": "An optimal control-based safety system for cost efficient risk management of chemical processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.029",
     "publication date": "08-2016",
     "abstract": "The design of conventional safety systems is based on failure likelihood and accident severity, which is normally obtained empirically, leaving the system vulnerable to process nonlinearities. To ensure process safety, control actions are conservative and small deviations from setpoints may lead to shutdown, generating economic losses. In this work, periodic simulations of system behavior against failures is proposed in order to determine the potential risk to which the system is subjected. Depending on this potential, preventive actions can be taken in order to guarantee the system safety and integrity and avoid potential shutdown. These actions are calculated to provoke least possible disturbance in order to reduce impact on product quality, while keeping the process operating. The goal is to increase annual operating time of the plant without compromising safety and product quality. Results show that the proposal is feasible to real time applications and unnecessary shutdowns can be avoided.",
     "keywords": ["Process safety", "Nonlinear model predictive control", "Optimization"]},
    {"article name": "Optimized integration of renewable energy technologies into Alberta\u2019s oil sands industry",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.03.028",
     "publication date": "07-2016",
     "abstract": "An energy optimization model for the integration of renewable technologies into the energy infrastructure of the oil sands industry is presented. The proposed model determines the optimal configuration of oil producers and the energy infrastructure required to meet their energy demands. The model is geared toward the minimization of cost subject to carbon dioxide emission constraints. A mixed integer non-linear optimization model is developed that simultaneously optimizes capacity expansion and new investment decisions of conventional and renewable energy technologies. To illustrate its applicability, the proposed model was applied to a case study using data reported in the literature for various years of oil sands operations. A rolling horizon approach was implemented to determine the effect of investment decisions of previous operational years on the selection of new investment options. Results were compared with and without the incorporation of renewable energy technologies. The results obtained indicate that the proposed model is a practical tool that can be employed to evaluate and plan oil sands and energy producers for future scenarios. Moreover, the results show that renewable energy technologies have significant potential in reducing reliance on fossil-fuel based technologies and their associated CO2 emissions. The emission constraints set for the operational year 2025 can only be achieved by the incorporation of renewables in the energy production mix.",
     "keywords": ["Process system engineering", "Oil sands", "Renewable energy", "Mathematical modeling", "Process optimization", "Process modeling", "Carbon management"]},
    {"article name": "Uncertainty quantification and global sensitivity analysis of complex chemical process using a generalized polynomial chaos approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.03.020",
     "publication date": "07-2016",
     "abstract": "Uncertainties are ubiquitous and unavoidable in process design and modeling. Because they can significantly affect the safety, reliability and economic decisions, it is important to quantify these uncertainties and reflect their propagation effect to process design. This paper proposes the application of generalized polynomial chaos (gPC)-based approach for uncertainty quantification and sensitivity analysis of complex chemical processes. The gPC approach approximates the dependence of a process state or output on the process inputs and parameters through expansion on an orthogonal polynomial basis. All statistical information of the interested quantity (output) can be obtained from the surrogate gPC model. The proposed methodology was compared with the traditional Monte-Carlo and Quasi Monte-Carlo sampling-based approaches to illustrate its advantages in terms of the computational efficiency. The result showed that the gPC method reduces computational effort for uncertainty quantification of complex chemical processes with an acceptable accuracy. Furthermore, Sobol\u2019s sensitivity indices to identify influential random inputs can be obtained directly from the surrogated gPC model, which in turn further reduces the required simulations remarkably. The framework developed in this study can be usefully applied to the robust design of complex processes under uncertainties.",
     "keywords": ["Generalized polynomial chaos", "Uncertainty quantification", "Process uncertainty", "Sensitivity analysis", "Monte-Carlo approach"]},
    {"article name": "Inference of chemical reaction networks using mixed integer linear programming",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.019",
     "publication date": "07-2016",
     "abstract": "The manual determination of chemical reaction networks (CRN) and reaction rate equations is cumbersome and becomes workload prohibitive for large systems. In this paper, a framework is developed that allows an almost entirely automated recovery of sets of reactions comprising a CRN using experimental data. A global CRN structure is used describing all feasible chemical reactions between chemical species, i.e. a superstructure. Network search within this superstructure using mixed integer linear programming (MILP) is designed to promote sparse connectivity and can integrate known structural properties using linear constraints. The identification procedure is successfully demonstrated using simulated noisy data for linear CRNs comprising two to seven species (modelling networks that can comprise up to forty two reactions) and for batch operation of the nonlinear Van de Vusse reaction. A further case study using real experimental data from a biodiesel reaction is also provided.",
     "keywords": ["Chemical reaction network", "Structural optimization", "Kinetic fitting", "Mixed-integer-linear programming"]},
    {"article name": "Sequential synthesis of heat integrated water networks: A new approach and its application to small and medium sized examples",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.016",
     "publication date": "07-2016",
     "abstract": "This paper presents a new step-wise sequential solution strategy for the synthesis of heat integrated water networks (HIWNs) comprising of two mathematical models. The first model minimizes water and energy costs. The optimal freshwater and water flow rates within the HIWN are determined from this model. Using these optimal flow rates, different configurations of HIWN are evaluated for the purpose of heat integration. The second model is the stage-wise heat exchanger network (Yee and Grossmann, 1990), which is solved for each of the evaluated network configuration in a sequential manner. Using this proposed strategy, a set of locally optimal HIWNs are produced and the best one is chosen based on the minimum total annual cost (TAC). The proposed sequential strategy is applied to small and medium sized examples in the literature. The results show that the proposed new sequential solution strategy can be successfully applied to small and medium sized HIWN problems.",
     "keywords": ["Heat-integrated water network", "Mathematical programming", "Superstructure", "Water network", "Heat exchanger network"]},
    {"article name": "Robust probabilistic principal component analysis for process modeling subject to scaled mixture Gaussian noise",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.03.031",
     "publication date": "07-2016",
     "abstract": "Conventionally, for probabilistic principal component analysis (PPCA) based regression models, noise with a Gaussian distribution is assumed for both input and output observations. This assumption makes the model to be vulnerable to large random errors, known as outliers. In this article, unlike the conventional noise assumption, a mixture noise model with a contaminated Gaussian distribution is adopted for probabilistic modeling to diminish the adverse effect of outliers, which usually occur due to irregular process disturbances, instrumentation failures or transmission problems. This is done by downweighing the effect of the noise component which accounts for contamination on output prediction. Outliers are common in process industries; therefore, handling this issue is of practical importance. In comparison with conventional PPCA based regression model, prediction performance of the developed robust probabilistic regression model is improved in presence of data contamination. To evaluate the model performance two case studies were carried out. A simulated set of data with specific characteristics to highlight the presence of outliers was used to demonstrate the robustness of the developed model. The advantages of this robust model are further illustrated via a set of real industrial process data.",
     "keywords": ["High fidelity modeling", "Outlier", "Robustness", "Probabilistic principal component analysis (PPCA)", "Mixture Gaussian distribution"]},
    {"article name": "CPFD modeling and experimental validation of gas\u2013solid flow in a down flow reactor",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.007",
     "publication date": "07-2016",
     "abstract": "This manuscript reports the fluid dynamics in the developed flow region of a cocurrent gas\u2013solid down flow fluidized bed unit. Gas\u2013solid flows are simulated using a Computational Particle Fluid Dynamics (CPFD) numerical scheme and experimental data from the CREC-GS-Optiprobes. This model represents clusters in a downer unit. It is hypothesized that in downers, clusters are formed via a Random Particle-Selection Method (RPSM) ensuring cluster dynamic stability. To accomplish this, a statistical particle-selection of clusters (SPSC) method is developed. This hybrid model is validated with experimental data obtained in a 2\u00a0m height and 2.57\u00a0cm diameter column. Observed time-averaged axial and radial velocities and solid concentration profiles are successfully simulated by the Hybrid CPFD/CREC-GS-Optiprobes Data Model. These findings support: (a) a narrow distribution of particle cluster residences, (b) the relatively flat radial solid concentrations and solid cluster velocities, and (c) a valuable approach for establishing slip velocities in downer units.",
     "keywords": ["Fluidized downer unit", "Computational Particle Fluid Dynamics", "Particle clusters"]},
    {"article name": "Automated quantitative model-based fault diagnosistic protocol via Assumption State Differences",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.03.021",
     "publication date": "07-2016",
     "abstract": "This treatment describes the details of a systematic protocol useful for performing optimal automated process fault analysis. This implementation generalizes the underlying Boolean logic version of the Method of Minimal Evidence (MOME) developed previously to a highly comprehensive algorithm for performing model-based fault diagnostics. This generalization allows for a more compact treatment of potential single and multiple fault situations, at all levels of possible diagnostic resolution, with both elegant and efficient uniform sensor validation and proactive fault analysis (SV&PFA) diagnostic rules for diagnosing those situations. This Assumption State Differences (ASD) Protocol version of the MOME algorithm thus automates the diagnostic reasoning necessary to continuously perform optimal process fault analysis so that only the underlying well-formulated models are required to achieve such performance. Using this algorithm consequently directly simplifies the solution of the more complicated problem of automated process fault analysis into the much more tractable, and incrementally solvable, problem of adequately modeling normal process operations.",
     "keywords": ["Optimal automated process fault analysis", "Continuous sensor validation", "Intelligent process supervision", "Proactive process safety software", "Quantitative model-based diagnostic strategy", "Method of Minimal Evidence", "On-line real-time fault diagnosis"]},
    {"article name": "Regularized maximum likelihood estimation of sparse stochastic monomolecular biochemical reaction networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.03.018",
     "publication date": "07-2016",
     "abstract": "A sparse parameter matrix estimation method is proposed for identifying a stochastic monomolecular biochemical reaction network system. Identification of a reaction network can be achieved by estimating a sparse parameter matrix containing the reaction network structure and kinetics information. Stochastic dynamics of a biochemical reaction network system is usually modeled by a chemical master equation (CME) describing the time evolution of probability distributions for all possible states. This paper considers closed monomolecular reaction systems for which an exact analytical solution of the corresponding chemical master equation can be derived. The estimation method presented in this paper incorporates the closed-form solution into a regularized maximum likelihood estimation (MLE) for which model complexity is penalized. A simulation result is provided to verify performance improvement of regularized MLE over least-square estimation (LSE), which is based on a deterministic mass-average model, in the case of a small population size.",
     "keywords": ["Sparse parameter estimation", "Regularized maximum likelihood estimation", "Mono-molecular biochemical reaction network", "Chemical master equation", "Stochastic simulation algorithm"]},
    {"article name": "Efficient implementation of step response models for embedded Model Predictive Control",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.002",
     "publication date": "07-2016",
     "abstract": "This paper proposes efficient step response model implementation strategies that lead to accurate control and high computational performance in an embedded Model Predictive Control (MPC) scheme. Different implementations of the step response prediction model are examined, and inherent properties that directly affect control performance in the presence of disturbances are discussed. Model errors that are inconsistent with bias updates (i.e. the model of unknown disturbances commonly used in step response MPC) are identified, and it is shown that the bias updates may worsen the effect of the errors in some cases. Particular attention is paid to the robustness of the prediction models to small truncation errors and errors in the input or measured disturbance history. Several implementation aspects that are crucial for embedded targets with limited resources are discussed. The findings are illustrated by simple simulation examples and an industrial case-study involving hardware-in-the-loop simulation of a subsea compact separation process.",
     "keywords": ["Model Predictive Control", "Step response models", "Robustness to errors", "Unknown disturbances", "Embedded systems"]},
    {"article name": "A framework for multi-stakeholder decision-making and conflict resolution",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.03.034",
     "publication date": "07-2016",
     "abstract": "We propose a decision-making framework to compute compromise solutions that balance conflicting priorities of multiple stakeholders on multiple objectives. In our setting, we shape the stakeholder dissatisfaction distribution by solving a conditional-value-at-risk (CVaR) minimization problem. The CVaR problem is parameterized by a probability level that shapes the tail of the dissatisfaction distribution. The proposed approach allows us to compute a family of compromise solutions and generalizes multi-stakeholder settings previously proposed in the literature that minimize average and worst-case dissatisfactions. We use the concept of the CVaR norm to give a geometric interpretation to this problem and use the properties of this norm to prove that the CVaR minimization problem yields Pareto optimal solutions for any choice of the probability level. We discuss a broad range of potential applications of the framework that involve complex decision-making processes. We demonstrate the developments using a biowaste facility location case study in which we seek to balance stakeholder priorities on transportation, safety, water quality, and capital costs.",
     "keywords": ["Multi-stakeholder", "Multiobjective", "Optimization", "Pareto optimality", "Compromise", "Conditional value-at-risk"]},
    {"article name": "Application of N-phase algebraic slip model and direct quadrature method of moments to the simulation of air-water flow in vertical risers and bubble column reactor",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.023",
     "publication date": "07-2016",
     "abstract": "In the present work the Direct Quadrature Method of Moments (DQMOM) has been implemented into the commercial CFD code TransAT. DQMOM has recently become a very attractive approach for solving population balance equation (PBE) due to its capability of representing the most interesting properties of the population, eg. Sauter mean diameter, void fraction, number of particles. The DQMOM technique was coupled with the turbulent N-phase Algebraic Slip Model (ASM) model in order to extend the model to handle dispersed phase populations such that each class has its own velocity field. The results compared to experimental data show that the developed numerical model accurately predicts void fraction profile in a long riser within a bubbly flow regime. Moreover the model is used for the simulation of bubble column, proving that it accurately predicts the gas hold up and the Sauter mean diameter.",
     "keywords": ["DQMOM", "Multiphase flow", "Algebraic slip model", "Population balance", "Bubble column", "Pipe flow"]},
    {"article name": "General optimization strategy of simulated moving bed units through design of experiments and response surface methodologies",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.028",
     "publication date": "07-2016",
     "abstract": "The optimization of simulated moving bed (SMB) units is often performed through detailed phenomenological models and require extensive computation time. Hence several optimization methods like the Triangle Theory, and the concept of separation volume have been proposed. However, they do not provide accurate results, when mass transfer limitations are significant, or require a large number of simulations.In this work, a combined Design of Experiments and Response Surface Methodology (DoE-RSM) approach is proposed for SMB optimization, aimed at providing good results with simplicity and reduced number of simulations. The separation of trans-stilbene oxide enantiomers is selected as case study in order to compare DoE-RSM with previous approaches. In the whole, accurate results are obtained with a few number of simulations, allowing for purities above 99.60% for both enantiomers, and productivity of 65.41\u00a0kg/(m3adsorbent\u00a0day). The versatility of DoE-RSM tool is also discussed, emphasising their advantages and general applicability.",
     "keywords": ["CSS cyclic steady state", "cyclic steady state", "DoE design of experiments", "design of experiments", "LDF linear driving force", "linear driving force", "RSM response surface methodology", "response surface methodology", "SMB simulated moving bed", "simulated moving bed", "TMB true moving bed", "true moving bed", "TSO trans-stilbene oxide", "trans-stilbene oxide", "Chromatography", "Design of experiments", "Enantiomers", "Simulated moving bed", "Trans-stilbene oxide"]},
    {"article name": "A new method of cyclic hoist scheduling for multi-recipe and multi-stage material handling processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.014",
     "publication date": "07-2016",
     "abstract": "Multi-recipe and multi-stage material handling (M3H) processes are widely employed by various industries where complex multi-product manufacturing and assembly tasks are required. Cyclic hoist scheduling (CHS) could significantly improve the productivity of an M3H process. In this paper, a novel CHS methodology has been developed, which considers employing multiple sub-cycles to efficiently deal with job duality issues associated with multi-capacity processing units. The methodology contains three modeling and solving stages. In Stage I, a mixed-integer linear programing (MILP) model is developed to obtain the optimal solution of a sub-cycle CHS problem with the tolerance of job duality. In Stage II, the obtained solution will be examined to see if the job duality exists. Once a job duality issue is identified, another MILP model in Stage III will be performed to schedule an additional sub-cycle CHS problem, which targets the minimum slot usage discrepancy caused by the identified job duality. After that, the combined CHS solutions from the previous and additional sub-cycles will be fed back to Stage II for the job duality examination again. Iteratively checking and rescheduling between Stages II and III, job duality issues will be eventually eliminated and a full scheduling cycle coupling multiple sub-cycles will be accomplished. The methodology can significantly increase the CHS optimality for M3H processes, which are demonstrated by two case studies.",
     "keywords": ["Optimization", "Hoist scheduling", "Cyclic scheduling", "Process design", "MILP"]},
    {"article name": "Enhanced data envelopment analysis for sustainability assessment: A novel methodology and application to electricity technologies",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.022",
     "publication date": "07-2016",
     "abstract": "Quantifying the level of sustainability attained by a system is a challenging task due to the need to consider a wide range of economic, environmental and social aspects simultaneously. This work explores the application of data envelopment analysis (DEA) to evaluate the sustainability \u2018efficiency\u2019 of a system. We propose an enhanced DEA methodology that uses the concept of \u2018order of efficiency\u2019 to compare and rank alternatives according to the extent to which they adhere to sustainability principles. The capabilities of the proposed approach are illustrated through a sustainability assessment of different technologies for electricity generation in United Kingdom. In addition to screening the alternatives based on sustainability principles, enhanced DEA provides improvement targets for the least sustainable alternatives that, if achieved, would make them more sustainable. The enhanced DEA shows clearly the ultimate distance to sustainability, helping industry and policy makers to improve the efficiency of technologies, products and policies.",
     "keywords": ["Enhanced data envelopment analysis", "Order of efficiency", "Sustainability efficiency", "Life cycle sustainability assessment", "Sustainability targets", "Electricity generation"]},
    {"article name": "Cutting planes for improved global logic-based outer-approximation for the synthesis of process networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.017",
     "publication date": "07-2016",
     "abstract": "In this work, we present an improved global logic-based outer-approximation method (GLBOA) for the solution of nonconvex generalized disjunctive programs (GDP). The GLBOA allows the solution of nonconvex GDP models, and is particularly useful for optimizing the synthesis of process networks, which yields MINLP models that can be highly nonconvex. However, in many cases the NLP that results from fixing the discrete decisions is much simpler to solve than the original problem. The proposed method exploits this property. Two enhancements to the basic GLBOA are presented. The first enhancement seeks to obtain feasible solutions faster by dividing the basic algorithm into two stages. The first stage seeks to find feasible solutions faster by restricting the solution time of the problems and diversifying the search. The second stage guarantees the convergence by solving the original algorithm. The second enhancement seeks to tighten the lower bound of the algorithm by the use of cutting planes. The proposed method for obtaining cutting planes, the main contribution of this work, is a separation problem based on the convex hull of the feasible region of a subset of the constraints. Results and comparison with other global solvers show that the enhancements improve the performance of the algorithm, and that it is more effective in the tested problems at finding near optimal solutions compared to general-purpose global solvers.",
     "keywords": ["MINLP", "Global optimization", "Disjunctive programming", "Process synthesis"]},
    {"article name": "Online identification for batch processes in closed loop incorporating priori controller knowledge",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.025",
     "publication date": "07-2016",
     "abstract": "It is of great importance to develop an online modeling method for chemical processes operated in closed loop for better understanding, monitoring the process or other purposes without endangering the system. This paper intends to devise an online system identification method, particularly for the batch process, by fully exploiting its intrinsic repetitiveness. It properly uses the information from the time direction and the batch direction, thus leading to a gradual performance enhancement. In addition, the identification method formulates the priori controller knowledge such as closed-loop stability as optimization constraints to refine the parameter estimates. A trust region method is employed to overcome the significant computation burden of directly handling these constraints such as solving Lyapunov inequalities. An adaptive filter is introduced to further smooth the parameter estimates. Finally, the effectiveness of the approach is verified by three numerical examples including a two-tank system.",
     "keywords": ["Batch processes", "Closed-loop system identification", "Priori knowledge", "Process modeling", "Trust region method", "Two-time dimensional"]},
    {"article name": "Thermodynamic modeling of a nuclear energy based integrated system for hydrogen production and liquefaction",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.015",
     "publication date": "07-2016",
     "abstract": "A nuclear based integrated system for hydrogen production and liquefaction with a newly developed four-step magnesium\u2013chlorine cycle is proposed. The system uses nuclear energy to supply heat for the Rankine cycle and Mg\u2013Cl cycle, where the power produced by the Rankine cycle is used to run the electrolysis steps of the Mg\u2013Cl cycle and liquefaction cycle compressors. The four-step Mg\u2013Cl cycle is specifically designed to decrease the electrical work consumption of the cycle by capturing HCl in dry form with an additional step to conventional three-step cycle. A performance assessment study is undertaken based on energy and exergy analysis of the subsystems, and total energy and exergy efficiencies of the plant are found to be 18.6%, and 31.35%. The comparisons of the subsystem efficiencies and total exergy destructions show that highest irreversibility ratio belongs to the Mg\u2013Cl cycle by 41%, respectively.",
     "keywords": ["Nuclear power plant", "Mg\u2013Cl cycle", "Hydrogen production", "Hydrogen liquefaction", "Energy", "Exergy"]},
    {"article name": "Numerical simulation of particle/monolithic two-stage catalyst bed reactor with beds-interspace distributed dioxygen feeding for oxidative coupling of methane",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.036",
     "publication date": "07-2016",
     "abstract": "A three-dimensional geometry model of the particle/monolithic two-stage reactor with beds-interspace distributed dioxygen feeding for oxidative coupling of methane (OCM) was set up. The improved Stansch kinetic model adapting different operating temperatures was established to calculate the OCM reactor performance using computational fluid dynamics (CFD) and FLUENT software. The results showed that the calculated values matched well with the experimental values of the conversion of CH4 and the selectivity of products (C2H6, C2H4, CO2, CO) in the OCM reactor. The distributed dioxygen feeding with the percentage of 5\u201320% based oxygen flow rate of top inlet promoted the OCM reaction in monolithic catalyst bed and led to the conversion of CH4 and the selectivity and yield of C2 (C2H6 and C2H4) increase obviously. The distributed dioxygen feeding was 15%, the conversion of CH4, the selectivity and the yield of C2 reached 34.1%, 68.2% and 23.3%, respectively.",
     "keywords": ["Numerical simulation", "Computational fluid dynamics", "Oxidative coupling of methane", "Two-stage reactor", "Beds-interspace distributed dioxygen feeding"]},
    {"article name": "JITL based MWGPR soft sensor for multi-mode process with dual-updating strategy",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.033",
     "publication date": "07-2016",
     "abstract": "Process nonlinearity, multiple operating modes and time-varying characteristics often deteriorate the prediction performance of process models. In this article, a multi-mode moving-window Gaussian process regression (MWGPR) based approach for ARX modeling is proposed to effectively capture process nonlinearity or switching dynamics. The Gaussian mixture model (GMM) is first introduced to separate the data into different operating modes. Then the MWGPR strategy is applied to identify the local ARX model. Just-in-time learning (JITL) and dual updating are applied for more effective tracking of process dynamics. A simulation of a continuous fermentation process and a pilot scale experiment are presented to demonstrate the effectiveness of the proposed method.",
     "keywords": ["Gaussian mixture model", "Just-in-time learning", "Gaussian process regression", "Moving-window", "Dual-updating"]},
    {"article name": "Efficient microscale simulation of intermediate-temperature solid oxide fuel cells based on the electrochemical effectiveness concept",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.032",
     "publication date": "07-2016",
     "abstract": "In this study, a new microscale simulation method based on the electrochemical effectiveness concept is proposed for efficient calculation of intermediate-temperature solid oxide fuel cells (IT-SOFCs). The electrochemical effectiveness model can accurately determine the current generation efficiencies of thin active functional layers in IT-SOFC electrodes, without the need to place many grid points and solve complex electrochemical reaction/charge transport equations. Thus, the effectiveness-based microscale simulation method is developed by modifying a previous microscale model to include the effectiveness model formulation, and the simulation results for one-dimensional, single-cell performances of IT-SOFCs are compared with the results from the microscale models. The proposed microscale simulation method is shown to accurately reproduce the results of more detailed calculations at much lower computational costs, which suggests that the present method can be useful for developing large-scale simulation models for IT-SOFCs.",
     "keywords": ["Intermediate-temperature solid oxide fuel cell (IT-SOFC)", "Microscale model", "Electrochemical effectiveness model", "Reaction/charge transport problem"]},
    {"article name": "Steady state and dynamic modeling of spiral wound wastewater reverse osmosis process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.001",
     "publication date": "07-2016",
     "abstract": "Reverse osmosis (RO) is one of the most important technologies used in wastewater treatment plants due to high contaminant rejection and low utilization of energy in comparison to other treatment procedures. For single-component spiral-wound reverse osmosis membrane process, one dimensional steady state and dynamic mathematical models have been developed based on the solution-diffusion model coupled with the concentration polarization mechanism. The model has been validated against reported data for wastewater treatment from literature at steady state conditions. Detailed simulation using the dynamic model has been carried out in order to gain deeper insight of the process. The effect of feed flow rate, pressure, temperature and concentration of pollutants on the performance of the process measured in terms of salt rejection, recovery ratio and permeate flux has been investigated.",
     "keywords": ["Spiral-wound reverse osmosis", "One dimensional steady state and dynamic modeling", "Wastewater treatment"]},
    {"article name": "Dynamic method for computation of chemical and phase equilibria",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.02.014",
     "publication date": "06-2016",
     "abstract": "A generalized approach for the calculation of complex chemical and phase equilibria is presented that is based on the simulation of the dynamic evolution of a mixture from non-equilibrium initial composition towards the final equilibrium composition. The proposed method is able to solve pure chemical or phase equilibria as well as simultaneous chemical/phase equilibria. The advantage of our approach compared to conventional equilibrium calculations is the fact that the approach is physically motivated and can handle chemical and phase equilibria as well as simultaneous chemical and phase equilibria.",
     "keywords": ["Process simulation", "Chemical equilibria", "Phase equilibria"]},
    {"article name": "Computational uncertainty quantification for a clarifier-thickener model with several random perturbations: A hybrid stochastic Galerkin approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.02.016",
     "publication date": "06-2016",
     "abstract": "Continuous sedimentation processes in a clarifier-thickener unit can be described by a scalar nonlinear conservation law whose flux density function is discontinuous with respect to the spatial position. In the applications of this model, which include mineral processing and wastewater treatment, the rate and composition of the feed flow cannot be given deterministically. Efficient numerical simulation is required to quantify the effect of uncertainty in these control parameters in terms of the response of the clarifier-thickener system. Thus, the problem at hand is one of uncertainty quantification for nonlinear hyperbolic problems with several random perturbations. The presented hybrid stochastic Galerkin method is devised so as to extend the polynomial chaos approximation by multiresolution discretization in the stochastic space. This approach leads to a deterministic hyperbolic system, which is partially decoupled and therefore suitable for efficient parallelisation. Stochastic adaptivity reduces the computational effort. Several numerical experiments are presented.",
     "keywords": ["Clarifier-thickener model", "Polynomial chaos", "Uncertainty quantification", "Galerkin projection", "Hybrid stochastic Galerkin", "Finite volume method"]},
    {"article name": "Dynamic model-based sensor network design algorithm for system efficiency maximization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.01.018",
     "publication date": "06-2016",
     "abstract": "A dynamic model-based sensor network design (DMSND) algorithm has been developed for maximizing system efficiency for an estimator-based control system. The algorithm synthesizes the optimal sensor network in the face of disturbances or set point changes. Computational expense of the large-scale combinatorial optimization problem is significantly reduced by parallel computing and by using combination of three novel strategies: multi-rate sampling frequency, model order reduction, and use of an incumbent solution that enables early termination of evaluation of infeasible sensor sets. The developed algorithm is applied to an acid gas removal unit as part of an integrated gasification combined cycle power plant with carbon capture. Even though there are more than thousand process states and more than hundred candidate sensor locations, the optimal sensor network design problem for maximizing process efficiency could be solved within couple of hours for a given budget.",
     "keywords": ["Dynamic model-based sensor network design", "Efficiency maximization", "Multi-rate sampling", "Incumbent solution", "Parallel computing", "Acid gas removal unit"]},
    {"article name": "Index analysis and reduction of systems of quasi-linear partial-differential and algebraic equations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.02.007",
     "publication date": "06-2016",
     "abstract": "To reliably solve PDAE models in established equation-oriented modeling environments (i) certain mathematical properties are to be fulfilled and (ii) the specified initial- and boundary conditions are to be consistent. For an assessment of both of these aspects an important theoretical framework is the concept of index. In this contribution we propose a new method for a systematic index reduction of quasi-linear PDAE systems. The general idea is to reveal quasi-linear combinations of the differential quantities in the high-index model which are invariant with respect to a specific independent variable. By using these quasi-linear combinations as templates for symbolic manipulations, additional algebraic constraints become explicit. These explicit constraints are then used for index reduction yielding low-index PDAE models. The procedure is demonstrated in the context of a typical modeling work-flow for modeling problems of a tubular reactor, diffusive charge transport in electrolyte mixtures and incompressible fluid flow.",
     "keywords": ["Index analysis and reduction", "PDAE systems", "Distributed model"]},
    {"article name": "An optimization based algorithm for solving design problems of counter-current multistage batch solid\u2013liquid extractors for complex systems: Application to vanilla extract",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.03.004",
     "publication date": "06-2016",
     "abstract": "Solid\u2013liquid extraction is one of the aged unit operations in chemical engineering. However the process design models, universally included in textbooks, continue considering infinite dilution of extractable material and single component solvent. Extraction processes of bioactive compounds with composite solvents (ethanol\u2013water) from plants or foods products require a better understanding of equilibrium between phases of bioactive compounds and their role in the process design. Then, a general mathematical model of counter-current ideal multi-stages batch solid\u2013liquid extractors was developed. The model considers composite solvents and the two equilibrium relations required for the total description of a solid\u2013liquid extraction system. An optimization based algorithm was proposed for solving the design problems. Experimental equilibrium relations were obtained for vanilla\u2013ethanol\u2013water system and used as example of model application. Additionally, the prediction of a particular component (vanillin) concentration in extract was included and experimentally validated.",
     "keywords": ["Multistage solid\u2013liquid batch extractors", "Equilibrium relation", "Specific retained solution", "Optimization based algorithm", "Vanilla extract"]},
    {"article name": "Model-based analysis of a twin-screw wet granulation system for continuous solid dosage manufacturing",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.03.007",
     "publication date": "06-2016",
     "abstract": "Implementation of twin-screw granulation in a continuous from-powder-to-tablet manufacturing line requires process knowledge development. This is often pursued by application of mechanistic models incorporating the underlying mechanisms. In this study, granulation mechanisms considered to be dominant in the kneading element regions of the granulator i.e., aggregation and breakage, were included in a one-dimensional population balance model. The model was calibrated using the experimentally determined inflow granule size distribution, and the mean residence time was used as additional input to predict the outflow granule size distribution. After wetting, the first kneading block caused an increase in the aggregation rate which was reduced afterwards. The opposite was observed in case of the breakage rate. The successive kneading blocks lead to a granulation regime separation inside the granulator under certain process conditions. Such a physical separation between the granulation regimes is promising for future design and advanced control of the continuous granulation process.",
     "keywords": ["Population balance modelling", "Continuous pharmaceutical production", "Granule size analysis"]},
    {"article name": "A system dynamics simulation model of chemical supply chain transportation risk management systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.02.019",
     "publication date": "06-2016",
     "abstract": "Unforeseen events can interrupt the operational process and have a negative impact on the chemical supply chain transportation (CSCT) system. A number of research studies have addressed the risk management issues in chemical supply chain (CSC) or CSCT. However, most of the existing methodologies lack inbuilt and practical techniques that take into consideration the complex interactions and dynamic feedback effects, which can significantly affect the reliability of risk management outcomes. This paper suggests a novel modelling and simulation method to address the dynamic risks effects in the CSCT, especially the consideration of time-dependent system behaviour in different operational conditions. Furthermore, the flexibility of the model modification is adapted to enhance the practice in risk mitigation. A transparent decision support tool is provided to compare the outcomes of different risk mitigation processes, which offers decision makers an alternative CSCRM mitigation package.",
     "keywords": ["Chemical supply chain transportation", "Risk management", "System dynamics", "Modelling and simulation"]},
    {"article name": "A systems based approach for financial risk modelling and optimisation of the mineral processing and metal production industry",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.03.010",
     "publication date": "06-2016",
     "abstract": "Large scale engineering process systems are subject to a variety of risks which affect the productivity and profitability of the industry in the long run. This paper outlines the short comings of the current methods of risk quantification and proposes a systems engineering framework to overcome these issues. The functionality of the developed model is illustrated for the case of mineral processing and metal production industries using a copper ore processing and refined metal production case study. The methodology provides a quantitative assessment of the risk factors and allows the opportunity to minimise financial losses, which would help investors, insurers and plant operators in these sectors to make appropriate risk hedging policies. The models developed can also be coupled with evolutionary or swarm based algorithms for optimising the systems. A numerical example is illustrated to demonstrate the validity of the proposition.",
     "keywords": ["EAs evolutionary algorithms", "evolutionary algorithms", "EVT extreme value theory", "extreme value theory", "FMECA failure mode effect and criticality analysis", "failure mode effect and criticality analysis", "FTM failure time modelling", "failure time modelling", "GARCH generalised autoregressive conditional heteroskedasticity", "generalised autoregressive conditional heteroskedasticity", "HAZOP hazard and operability", "hazard and operability", "HAZSCAN hazardous scenario analysis", "hazardous scenario analysis", "HRA human reliability analysis", "human reliability analysis", "MA moving average", "moving average", "MB maintenance and breakdown", "maintenance and breakdown", "MB cost maintenance and breakdown cost", "maintenance and breakdown cost", "MC Monte Carlo", "Monte Carlo", "MIMO multiple inputs and multiple outputs", "multiple inputs and multiple outputs", "MINLP mixed integer non-linear programming", "mixed integer non-linear programming", "MOEA/D multi-objective evolutionary algorithm with decomposition", "multi-objective evolutionary algorithm with decomposition", "MTBF mean time between failures", "mean time between failures", "MV mean value", "mean value", "NPGA niched-Pareto genetic algorithm", "niched-Pareto genetic algorithm", "NSGA II non-dominated sorting genetic algorithm II", "non-dominated sorting genetic algorithm II", "OD outage duration", "outage duration", "PAES Pareto archived evolution strategy", "Pareto archived evolution strategy", "QRA quantitative risk assessment", "quantitative risk assessment", "SAG semi-autogenous grinding", "semi-autogenous grinding", "SPEA strength Pareto evolutionary algorithm", "strength Pareto evolutionary algorithm", "SWIFT structured what-if technique", "structured what-if technique", "TBF time between failures", "time between failures", "Financial risk modelling", "Reliability based risk modelling", "Quantitative risk assessment", "Process systems optimisation", "Systems thinking"]},
    {"article name": "Simultaneous optimal design of multi-stage organic Rankine cycles and working fluid mixtures for low-temperature heat sources",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.03.005",
     "publication date": "06-2016",
     "abstract": "Energy recovery is a process strategy seeking to improve process efficiency through the capture, recycle and deployment of normally neglected low energy content sources or streams. By proper optimal process design, such low-temperature energy sources can be a feasible and economical manner of approaching the energy recovery issue. In particular, when Rankine cycles with mixtures as working fluids are used, the amount of energy recovery can be improved. The formulation and systematic solution of this problem has shown better results when all the variables of the Rankine cycle and the compositions of the working fluid are considered simultaneously. Another interesting approach is the implementation of multiple cycles coupled together. In this work we propose a nonlinear optimization formulation of two general multistage approaches for the Rankine cycle with mixtures: the cascade and series configurations. As main decision variables, we have considered the heat source conditions and the mixture components. Then, the resulting optimization problem is solved in a deterministic approach as a nonlinear program. The results shown that for some cases the multistage configurations are useful but limited in terms of cost in comparison to the single stage cycle.",
     "keywords": ["Optimization", "Design", "Energy"]},
    {"article name": "PSE for problem solving excellence in industrial R&D",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.03.011",
     "publication date": "06-2016",
     "abstract": "PSE, process systems engineering, is about the development and application of systematic methods for process studies by the chemical engineer. By means of software tools, the application of these methods is facilitated. Over the last about half a century, CAPE (computer aided process engineering) tools have found their way into process engineering. For example it is unthinkable nowadays to design a plant without a simulation through a process simulator. But there are many more applications of PSE in industry.The aim of this paper is to provide a taste of the meaning of PSE within the industrial R&D environment. The intention is not to provide a complete overview but to give a flavour of what is perceived as the benefits of PSE during process development, and, in which areas PSE should be extended to render further benefits. The combined approach of experiments and modelling offers a very (cost-)effective strategy in industrial R&D. Further improvements are desired in the areas related to process intensification (PI) and (conceptual) product design. It is believed that the current methods would be more beneficial and have a stronger applicability in industry by inclusion of semi-predictive models and uncertainty considerations.",
     "keywords": ["PSE", "CAPE", "PI", "Industrial process and product innovation"]},
    {"article name": "Automated structure detection for distributed process optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.03.014",
     "publication date": "06-2016",
     "abstract": "The design and control of large-scale engineering systems, consisting of a number of interacting subsystems, is a heavily researched topic with relevance both for industry and academia. This paper presents two methodologies for optimal model-based decomposition, where an optimization problem is decomposed into several smaller sub-problems and subsequently solved by augmented Lagrangian decomposition methods. Large-scale and highly nonlinear problems commonly arise in process optimization, and could greatly benefit from these approaches, as they reduce the storage requirements and computational costs for global optimization. The strategy presented translates the problem into a constraint graph. The first approach uses a heuristic community detection algorithm to identify highly connected clusters in the optimization problem graph representation. The second approach uses a multilevel graph bisection algorithm to find the optimal partition, given a desired number of sub-problems. The partitioned graphs are translated back into decomposed sets of sub-problems with a minimal number of coupling constraints. Results show both of these methods can be used as efficient frameworks to decompose optimization problems in linear time, in comparison to traditional methods which require polynomial time.",
     "keywords": ["Distributed optimization", "Process optimization", "Design reformulation", "Community detection", "Multilevel graph bisection", "Augmented Lagrangian decomposition"]},
    {"article name": "Autothermal reforming of methane on rhodium catalysts: Microkinetic analysis for model reduction",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.03.032",
     "publication date": "06-2016",
     "abstract": "Methane autothermal reforming has been studied using comprehensive, detailed microkinetic mechanisms, and a hierarchically reduced rate expression has been derived without apriori assumptions. The microkinetic mechanism is adapted from literature and has been validated with reported experimental results. Rate determining steps are elicited by reaction path analysis, partial equilibrium analysis and sensitivity analysis. Results show that methane activation occurs via dissociative adsorption to pyrolysis, while oxidation of the carbon occurs by O(s). Further, the mechanism is reduced through information obtained from the reaction path analysis, which is further substantiated by principal component analysis. A 33% reduction from the full microkinetic mechanism is obtained. One-step rate equation is further derived from the reduced microkinetic mechanism. The results show that this rate equation accurately predicts conversions as well as outlet mole fraction for a wide range of inlet compositions.",
     "keywords": ["Methane", "Autothermal reforming", "Microkinetic", "Simulation", "Model reduction"]},
    {"article name": "Optimization of crude oil hydrotreating process as a function of operating conditions: Application of response surface methodology",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.03.026",
     "publication date": "06-2016",
     "abstract": "In recent years, research has been directed towards upgrading of heavy crude oil as unconventional oil recovery rises. Catalytic hydrotreating of crude oil is an important upgrading option that is rarely discussed in literature. The main aim of crude oil hydrotreating is to reduce adverse environmental effects caused by the concentration of contaminants, increase productivity and improve the quality of middle distillate cuts. In this work, Response surface methodology (RSM) has been adopted to study the influence of various process parameters, such as hydrogen partial pressure, temperature and liquid hourly space velocity on the hydrotreating performance. The significance of these parameters is identified by using the analysis of variance (ANOVA) method. The resulting correlations are capable of predicting sulfur, vanadium, nitrogen and nickel conversions that are in excellent agreement with experimental data. The operating parameters are optimized with LINGO optimization software to achieve maximum conversions of contaminants during hydrotreating processes.",
     "keywords": ["Crude oil hydrotreating", "Hydrodenitrogenation", "Hydrodesulfurization", "Hydrodemetalization", "Response surface methodology"]},
    {"article name": "Dynamic simulation, numerical control and analysis of a novel bottom flashing scheme in batch distillation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.04.010",
     "publication date": "06-2016",
     "abstract": "In this communication, the concept of bottom flashing under the mechanical heat pump system is introduced in the batch distillation columns. Proposing an operating strategy at transient state, a numerical control mechanism is formulated to simultaneously adjust the following variables aiming to ensure the optimal use of internal heat source: flow rate of reboiler liquid subjected to bottom flashing, operating pressure of throttling valve, compression ratio for pressure adjustment in the isentropic compressor and external heat input to the reboiler. The potential of this novel energy efficient batch distillation is numerically quantified in terms of the two performance indexes, namely energy savings and cost. Finally, a binary system is dynamically simulated to demonstrate the proposed configuration.",
     "keywords": ["Transient batch distillation", "Bottom flashing", "Heat integration", "Energy consumption", "Economics"]},
    {"article name": "Integrated design and control of semicontinuous distillation systems utilizing mixed integer dynamic optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.03.022",
     "publication date": "06-2016",
     "abstract": "Semicontinuous distillation systems are notoriously difficult to design and optimize because the structural parameters, operational parameters, and control system must all be determined simultaneously. In the past 15 years of research into semicontinuous systems, studies of the optimal design of these systems have all been limited in scope to small subsets of the parameters, which yields suboptimal and often unsatisfactory results. In this work, for the first time, the problem of integrated design and control of semicontinuous distillation processes is studied by using a mixed integer dynamic optimization (MIDO) problem formulation to optimize both the structural and control tuning parameters of the system. The public model library (PML) of gPROMS is used to simulate the process and the built-in optimization package of gPROMS is used to solve the MIDO via the deterministic outer approximation method. The optimization results are then compared to the heuristic particle swarm optimization (PSO) method.",
     "keywords": ["Semicontinuous system", "Integrated design and control", "Mixed integer dynamic optimization", "Outer approximation method", "Particle swarm optimization", "gPROMS"]},
    {"article name": "Automated image analysis for trajectory determination of single drop collisions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.03.033",
     "publication date": "06-2016",
     "abstract": "The fundamental analysis of drop coalescence probability in liquid/liquid systems is necessary to reliably predict drop size distributions in technical applications. For this crucial investigation two colliding oil drops in continuous water phase were recorded with different high speed camera set-ups under varying conditions. In order to analyze the huge amount of recorded image sequences with varying resolutions and qualities, a robust automated image analysis was developed. This analysis is able to determine the trajectories of two colliding drops as well as the important events of drop detachment from cannulas and their collision. With this information the drop velocity in each sequence is calculated and mean values of multiple drop collisions are determined for serial examinations of single drop collisions. Using the developed automated image analysis for drop trajectory and velocity calculation, approximately 1\u20132 recorded high speed image sequences can be evaluated per minute.",
     "keywords": ["Image analysis", "Drop collision", "High speed imaging", "Collision velocity", "Trajectory"]},
    {"article name": "Assessment and comparison of distributed model predictive control schemes: Application to a natural gas refrigeration plant",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.03.001",
     "publication date": "06-2016",
     "abstract": "A number of decentralized and distributed control schemes based on model predictive control (MPC) have been introduced in the last years. They have been proposed as viable solutions to the computational, transmission and robustness issues arising in the centralized context in case of large-scale and/or distributed plants. Such MPC-based control schemes are very heterogeneous, based on different model structures and realizations, with different features and infrastructural/memory/computational requirements.In this paper, we test and compare, with a realistic case study, a robust non-cooperative scheme and a cooperative iterative one. The main scope is to analyze and unravel, in a fair comparison scenario, these methods from different viewpoints, spanning from the model realization issues to the communication and computational requirements, to the control performances. The benchmark case study consists of an existing natural gas refrigeration plant. Realistic simulations and validation tests are obtained through in the DynSim industrial process simulation environment.",
     "keywords": ["Distributed model predictive control", "Gas refrigeration plant", "Control of chemical processes", "Dynamic simulation"]},
    {"article name": "MINLP optimization of a heterogeneous azeotropic distillation process: Separation of ethanol and water with cyclohexane as an entrainer",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.03.027",
     "publication date": "06-2016",
     "abstract": "Heterogeneous distillation processes are widely used in industry for the separation of azeotropic and close-boiling mixtures. This paper addresses the optimization of a heterogeneous distillation process for the separation of an azeotropic ethanol/water mixture using cyclohexane as an entrainer. Starting from a given process superstructure a MINLP problem is set up to consider continuous as well as discrete decision variables such as the feed locations and the number of stages of the distillation columns. A modified Generalized Benders Decomposition algorithm to account for non-convexities of the model equations solves the MINLP problem. The algorithm can be attached via Visual Basic for Applications (VBA) to any commercial process simulator with NLP and VBA capabilities. Various optimization runs show that the algorithm is easily applicable and returns solutions independent of the initial values.",
     "keywords": ["MINLP", "Optimization", "Generalized Benders Decomposition", "Heterogeneous azeotropic distillation", "Hybrid process", "Ethanol\u2013water separation"]},
    {"article name": "Model-based optimisation of biodiesel production from microalgae",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.01.014",
     "publication date": "06-2016",
     "abstract": "This work presents a superstructure-based optimisation model to optimise the microalgae to biodiesel production flowsheet for the minimum net annualised life cycle cost (ALCC) of biodiesel. The model includes the important processing steps of converting microalgae into biodiesel, viz. microalgae growth, harvesting, lipid extraction, and transesterification of lipid. Different options to perform these steps are considered. The mass and volumetric balance for each process and equipment, and the equipment capacity limitations constitute the important model constraints. The decision variables include growth duration, medium, as well as the techniques and specifications to be followed in each of the downstream steps. The mixed integer linear programming model was applied to a case study of producing 30,000\u00a0kg/d biodiesel from Chlorella. The minimum ALCC was US $ 13.286/l for the flowsheet and equipment details recommended by the model. Sensitivity analysis showed that lipid extraction was the most crucial step in the flowsheet.",
     "keywords": ["Microalgae", "Biodiesel", "Superstructure", "Optimisation"]},
    {"article name": "Control structure design of an industrial crude terephthalic acid hydropurification process with catalyst deactivation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.01.017",
     "publication date": "05-2016",
     "abstract": "Purified terephthalic acid (PTA) is a fundamental raw material for polyester and textile industry. The p-xylene oxidation process and crude terephthalic acid (CTA) hydropurification process are the two main sections of industrial PTA production. 4-Carboxybenzaldehyde is a byproduct of the first section that can lower the polymerization rate and the average molecular weight of the polymer. In this work, an improved complete plant dynamic model of the second section, CTA hydropurification with catalyst deactivation, was developed based on Aspen Dynamics. The present contribution considered the performance of the proposed catalyst deactivation model (Azarpour and Zahedi, 2012). Moreover, we designed a control structure for this process with catalyst deactivation, and the performance of the resulting control structure was analyzed using several criteria. Results showed that the proposed system provides a better control system and higher profit for the process.",
     "keywords": ["Crude terephthalic acid", "Catalyst deactivation", "Integrated framework of simulation and heuristics", "Control structure design", "Economics"]},
    {"article name": "Multi-mode resource constrained project scheduling under resource disruptions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.01.004",
     "publication date": "05-2016",
     "abstract": "Over the last few decades, research on resource constrained project scheduling has focused on the development of mathematical programming based approaches for the generation of a nominal schedule under a deterministic environment. During the implementation phase, however, the nominal schedule may need to be revised when one or more resources are disrupted for a length of time. In this paper, we formulate two discrete time based models to deal with two different disruption scenarios for multi-mode resource constrained problems. We propose a reactive re-scheduling procedure for a single, as well as a series of disruptions, without having any disruption information in advance. To test the proposed approaches, sets of ten, twenty and thirty-activity multi-mode test instances from Project Scheduling Library (PSLIB) were used after introducing randomly generated disruption events. The experimental studies were also carried out to determine the effect of different factors related to the disruption recovery process.",
     "keywords": ["Multi-mode resource constrained project scheduling", "Rescheduling", "Disruption", "Mixed integer linear programming"]},
    {"article name": "Robust-distributed MPC tolerant to communication loss",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.12.010",
     "publication date": "05-2016",
     "abstract": "Distributed Model Predictive Control (DMPC) algorithms reported in the literature had often assumed that state measurements are available to all subsystems at all time instants. However, this assumption is not valid in the presence of loss of communication or data. Then designing for such losses within DMPC becomes critical to overall performance and stability of the system. In this work communication loss was considered for robust DMPC by using within each subsystem an observer, which continuously and recursively estimates states\u2019 bounds for all the plant states. These state bounds are then posed as extra constraints within a Linear Matrix Inequalities\u2019 formulation that is used to calculate the control actions based on the minimization of an upper bound on robust performance.",
     "keywords": ["Distributed model predictive control", "Robustness", "Linear matrix inequalities"]},
    {"article name": "Gas\u2212liquid multiphase computational fluid dynamics (CFD) of amine absorption column with structured-packing for CO2 capture",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.02.006",
     "publication date": "05-2016",
     "abstract": "The gas\u2212liquid multiphase Eulerian computational fluid dynamics (CFD) model was used to investigate hydrodynamics and CO2 removal efficiency of a pilot-scale amine absorber with structured-packing. The structured-packing was represented by a porous media zone having porous resistance, gas\u2212liquid interfacial drag force, and liquid dispersion force. This study aimed to find a reasonable way to identify four modification factors of the Ergun coefficient that determine the hydrodynamic characteristics of structured-packing. The two modification factors (a and b) for porous resistance were mainly related to the liquid holdup (hL) with respect to the liquid load. The other two factors (c and d) for gas\u2212liquid interfacial drag force depended on the specific wet pressure drop (\u0394Pwet/L) versus the gas load factor. The hL and \u0394Pwet/L increased in parallel with the increase of a and c, respectively, while the slopes of hL and \u0394Pwet/L increased with b and d, respectively.",
     "keywords": ["CO2 capture", "Amine absorber", "Structured-packing", "Computational fluid dynamics (CFD)", "Porous media model", "Gas\u2212liquid flows"]},
    {"article name": "Linear programming-based scenario reduction using transportation distance",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.02.005",
     "publication date": "05-2016",
     "abstract": "One of the major difficulties for scenario-based decision-making problems (e.g. stochastic programming using scenarios) is that the problem complexity quickly increases as the number of scenarios increases. Scenario reduction aims at selecting a small number of scenarios to represent a large set of scenarios for decision making, so as to significantly reduce the computational complexity while preserving the solution quality of using a large number of scenarios. In this work, a new computationally efficient scenario reduction algorithm is proposed based on transportation distance minimization. The proposed algorithm relies on solving linear programming problems. The scenario subset updating step and the probability value assignment step are performed in an iterative manner until the transportation distance converges. Comparison with existing scenario reduction methods reveals that the proposed method is very efficient for the reduction of large scenario set. Application studies on stochastic optimization problems also demonstrate the effectiveness of the proposed method.",
     "keywords": ["Scenario reduction", "Transportation distance", "Linear programming"]},
    {"article name": "Potential catalyst savings in heterogeneous gaseous spiral coiled reactor utilizing selective wall coating \u2013 A computational study",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.02.004",
     "publication date": "05-2016",
     "abstract": "This study numerically evaluates the effect of secondary flow on the reaction performance in heterogeneous gaseous spiral coiled reactor utilizing selective wall coatings. Laminar multispecies gas flow in spiral coiled reactor with circular and square cross-section is investigated using a validated three-dimensional computational fluid dynamics (CFD) model. Various selective wall coating strategies are evaluated within a range of Reynolds number. The reactor performance is measured not only based on the conversion rate but also in terms of figure of merit (FoM) defined as reaction throughput per unit pumping power and catalyst coating active area. The results indicate that secondary flow enhance reaction performance and improve catalyst utilization, especially at the outer wall. By maximizing this effect, the requirement of expensive catalyst materials can be minimized. This study highlight the potential of selective catalyst coating in coiled reactor for process intensification and cost reduction in various applications.",
     "keywords": ["Catalyst saving", "Coiled", "Reaction performance", "Secondary flow", "Selective coating"]},
    {"article name": "Process simulator-based optimization of biorefinery downstream processes under the Generalized Disjunctive Programming framework",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.02.009",
     "publication date": "05-2016",
     "abstract": "Downstream processing of biofuels and bio-based chemicals represents a challenging problem for process synthesis and optimization, due to the intrinsic nonideal thermodynamics of the liquid mixtures derived from the (bio) chemical conversion of biomass. In this work, we propose a new interface between the process simulator PRO/II (SimSci, Schneider-Electric) and the optimization environment of GAMS for the structural and parameter optimization of this type of flowsheets with rigorous and detailed models. The optimization problem is formulated within the Generalized Disjunctive Programming (GDP) framework and the solution of the reformulated MINLP problem is approached with a decomposition strategy based on the Outer-Approximation algorithm, where NLP subproblems are solved with the derivative free optimizer belonging to the BzzMath library, and MILP master problems are solved with CPLEX/GAMS. Several validation examples are proposed spanning from the economic optimization of two different distillation columns, the dewatering task of diluted bio-mixtures, up to the distillation sequencing with simultaneous mixed-integer design of each distillation column for a quaternary mixture in the presence of azeotropes.",
     "keywords": ["Generalized disjunctive programming", "MINLP", "Downstream", "Process simulator", "Azeotropes", "Derivative free optimization"]},
    {"article name": "Optimal design of residential cogeneration systems under uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.02.008",
     "publication date": "05-2016",
     "abstract": "This paper presents a multi-objective optimization method for designing cogeneration systems in residential complexes and accounting for the involved uncertainty. The model accounts for satisfying the hot water and electric energy demands in a residential complex, while minimizing the total annual cost and the associated greenhouse gas emissions. The proposed model incorporates uncertain data for the ambient temperature, energy demands and prices of the local energy market, which are predicted through forecasting methods for determining the financial and environmental risks. Furthermore, the model accounts for determining the type and size of the central cogeneration unit, thermal storage unit, the needed auxiliary units, as well as the operating conditions. A housing complex in central Mexico is presented as case study. The results show significant economic and environmental benefits for the implementation of the proposed scheme as well as the importance of accounting for the involved uncertainty.",
     "keywords": ["CHP", "Cogeneration", "Housing complex", "Uncertainty, Optimal design"]},
    {"article name": "Software platform for the statistical validation of structural partitioning methods",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.01.007",
     "publication date": "05-2016",
     "abstract": "A novel interactive software platform is proposed to support design and development for PSE. It has been implemented to address the statistical validation of structural partitioning methods. According to several features given as model parameters, it produces incidence matrices whose structural partitioning can lead to a more efficient resolution of such models. The global objective is to generate automatically an arbitrary number of incidence matrices, shaped on the basis of statistical parameters associated with real-world PSE models. Then, partitioning methods can be executed on the generated matrices. Computational results for several problem instances are reported. Realistic cases were chosen by increasing the model-complexity level: a standard distillation column and an ammonia synthesis plant. In particular, the Direct Method, the Extended Direct Method and the Improved Extended Direct Method (IEDM) were evaluated. In comparison, the IEDM exhibited statistically significant enhancements of efficiency values for the resolution of the corresponding models.",
     "keywords": ["Structural partitioning", "Incidence matrix", "Statistical validation", "Software platform", "Systems of equations"]},
    {"article name": "Heavy oil recovery efficiency using SAGD, SAGD with propane co-injection and STRIP-SAGD",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.02.010",
     "publication date": "05-2016",
     "abstract": "Primary oil recovery methods in heavy oil basins generally extract 5\u201310% of the available resource, with the vast majority left in the ground and recoverable only through Enhanced Oil Recovery (EOR) methods. Traditional EOR methods, such as SAGD and solvent-assisted SAGD, generate steam in surface facilities and inject it underground to mobilize the oil for production. However, these methods can have considerable energy losses that significantly impact process performance. In contrast, the Solvent Thermal Resource Innovation Process (STRIP) technology, which uses down hole combustion of methane to produce CO2 and steam, reduces the operating and capital costs of surface facilities, saving more than 50% of the energy typically required for thermal production. In this work, simulations of conventional SAGD, SAGD with a non-condensing solvent (propane), and STRIP-SAGD for a typical bitumen reservoir in the Fort McMurray region in Alberta, Canada were performed using the combined software system ADGPRS/GFLASH. SAGD simulations used steam injection with a quality of 0.8 while STRIP simulations injected a vapor\u2013liquid mixture with a quality of 0.8. Furthermore, both solvent-based EOR methods required longer operation periods than conventional SAGD to recover a similar amount of oil. However, when compared on the basis of cumulative oil produced for the same overall energy input, it is shown that STRIP-SAGD recovered more oil per kJ of energy input to the reservoir than either SAGD or SAGD with propane co-injection.",
     "keywords": null},
    {"article name": "Computationally efficient incorporation of microkinetics into resolved-particle CFD simulations of fixed-bed reactors",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.02.015",
     "publication date": "05-2016",
     "abstract": "A method is developed to couple microkinetics with fluid flow in a fixed bed and transport inside the catalyst particles using computational fluid dynamics. Initially, the microkinetics model is solved for a wide range of different temperatures, and partial pressures. Next, reaction rates are mapped into quadratic multivariate splines. Splines coefficients are then imported into our user-defined function, and consequently the reaction rates are evaluated at each iteration simultaneously with the CFD simulations. This method has been applied to our solid particle model to investigate the effects of fluid flow, transport and elementary reaction steps on each other for ethylene and methanol partial oxidations. Reaction rates of all elementary steps as well as species surfaces sites and compositions are evaluated inside the particle. The suggested method can couple complex reaction mechanisms with detailed CFD simulations without increasing the computational time compared with global kinetics methods.",
     "keywords": ["Computational fluid dynamics", "Microkinetics", "Ethylene epoxidation", "Methanol oxidation", "Reaction rate mapping", "Fixed bed"]},
    {"article name": "Lexicographic optimization based MPC: Simulation and experimental study",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.02.002",
     "publication date": "05-2016",
     "abstract": "Multi-variable prioritized control study is carried out using model predictive control (MPC) algorithms. The conventional MPC algorithm implements multi-variable control through one augmented objective function and requires weights adjustment for required performance. In order to implement explicit prioritization in multiple control objectives, we have used lexicographic MPC. To achieve better tracking performance, we have used a new MPC algorithm, by modifying the lexicographic constraint, referred to as MLMPC, where tuning of weights is not required. The effectiveness of MLMPC algorithm is demonstrated on a PMMA reactor for controlling the number average molecular weight and the reactor temperature. We have also verified the benefits of proposed algorithm on an experimental single board heater system (SBHS) for controlling temperature of a thin metal plate. These simulation and experimental studies demonstrate the superiority of the proposed method over conventional MPC and lexicographic MPC. Finally, we have presented generalized mathematical solutions to the optimization problem in MLMPC.",
     "keywords": ["Lexicographic optimization", "Model predictive control", "Multi-objective optimization", "Single board heater system", "PMMA reactor"]},
    {"article name": "Online performance tracking and load sharing optimization for parallel operation of gas compressors",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.01.012",
     "publication date": "05-2016",
     "abstract": "Compressor stations on natural gas pipelines are typically composed of multiple parallel compressors for operational flexibility. The aim of load sharing optimization is to operate the compressor units of a gas compression station in an energy efficient way while satisfying the varying gas demand. This paper presents a problem formulation for compressor load sharing optimization, as well as a novel method to track the performance characteristics of gas compressors using thermodynamic models and historical operating data. An implementation of the proposed algorithm together with a dedicated case study concerning a station with 10 gas turbine driven compressor units are presented. The optimization results, which are based on simulations with actual operating data spanning over a 1 year duration, indicate an annual fuel saving potential of over 5% potentially leading to improved profitability and to a significant reduction of CO2 emissions from the gas turbine drivers.",
     "keywords": ["Centrifugal compressor", "Compressor load sharing", "Performance monitoring", "Process optimization", "Natural gas transport"]},
    {"article name": "A multi-objective invasive weeds optimization algorithm for solving multi-skill multi-mode resource constrained project scheduling problem",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.02.018",
     "publication date": "05-2016",
     "abstract": "A new multi-skill multi-mode resource constrained project scheduling problem with three objectives is studied in this paper. The objectives are: (1) minimizing project's makespan, (2) minimizing total cost of allocating workers to skills, and (3) maximizing total quality of processing activities. A meta-heuristic algorithm called multi-objective invasive weeds optimization algorithm (MOIWO) with a new chromosome structure guaranteeing feasibility of solutions is developed to solve the proposed problem. Two other meta-heuristic algorithms called non-dominated sorting genetic algorithm (NSGA-II) and multi-objective particle swarm optimization algorithm (MOPSO) are used to validate the solutions obtained by the developed MOIWO. The parameters of the developed algorithms are calibrated using Taguchi method. The results of the experiments show that the MOIWO algorithm has better performance in terms of diversification metric, the MOPSO algorithm has better performance regarding mean ideal distance, while NSGA-II algorithm has better performance in terms of spread of non-dominance solution and spacing metrics.",
     "keywords": ["Multi-skill", "Multi-mode", "RCPSP", "Time\u2013cost\u2013quality trade off", "MOIWO", "TOPSIS"]},
    {"article name": "Operational planning of forward and reverse logistic activities on multi-echelon supply-chain networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.02.017",
     "publication date": "05-2016",
     "abstract": "Distribution activities arising from supply-chains of chemical and food industries involve the shipping of products directly and/or via distribution-centers. Also, due to growing ecology concerns, the recycling of recoverable-materials is becoming a common practice. In this paper, a distribution and recovering problem has been studied and modeled. The solution to the problem-model computes the forward and backward flows on a supply-chain network of a company that take into account \u2018green logistics\u2019 considerations. In this problem, vehicles departing from plants/distribution-centers perform delivery of products and pick-up of recyclables at the lowest network-level. At a higher level, larger vehicles re-supply distribution-centers with products and bring back to plants recyclable goods. The operation must coordinate the vehicles-tours to assure efficient forward and backwards flows. The paper presents a column-generation based decomposition-approach for finding near-optimal solutions to the problem. We also present computational results on test problems derived from a real case-study.",
     "keywords": ["Forward and reverse logistics", "Integrated supply chains", "Multi-echelon networks", "Columns generation", "Decomposition"]},
    {"article name": "Production planning optimization of an ethylene plant considering process operation and energy utilization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.01.002",
     "publication date": "04-2016",
     "abstract": "A novel short-term planning model of the ethylene plant that incorporates the operating variables and energy utilization in both the thermal cracking and the down-stream process is proposed to explore the potential for increasing the production margin and reducing the energy losses. A multi-period mixed-integer nonlinear programming (MINLP) model is formulated to attain the scheduling of parallel furnaces and the energy distribution of the overall plant, along with the determination of the key process operation involving the coil outlet temperature (COT) and coke deposition. The behavior of the product yields and coke formation in terms of varying COT profiles is investigated to enhance the profitability of the whole plant. A real industrial example is investigated to exploit the performance of the proposed model. The results show that the integrated approach attains an improvement in overall profit and achieves significant enhancement in energy savings, compared with the original optimization approach.",
     "keywords": ["Production planning", "Cracking furnace", "Process operation", "Energy utilization", "Ethylene production"]},
    {"article name": "An MILP-MINLP decomposition method for the global optimization of a source based model of the multiperiod blending problem",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.12.017",
     "publication date": "04-2016",
     "abstract": "The multiperiod blending problem involves binary variables and bilinear terms, yielding a nonconvex MINLP. In this work we present two major contributions for the global solution of the problem. The first one is an alternative formulation of the problem. This formulation makes use of redundant constraints that improve the MILP relaxation of the MINLP. The second contribution is an algorithm that decomposes the MINLP model into two levels. The first level, or master problem, is an MILP relaxation of the original MINLP. The second level, or subproblem, is a smaller MINLP in which some of the binary variables of the original problem are fixed. The results show that the new formulation can be solved faster than alternative models, and that the decomposition method can solve the problems faster than state of the art general purpose solvers.",
     "keywords": ["Multi-period blending", "Pooling", "MINLP", "Global optimization"]},
    {"article name": "Enhancing the \u03f5-constraint method through the use of objective reduction and random sequences: Application to environmental problems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.12.016",
     "publication date": "04-2016",
     "abstract": "The \u03f5-constraint method is an algorithm widely used to solve multi-objective optimization (MOO) problems. In this work, we improve this algorithm through its integration with rigorous dimensionality reduction methods and pseudo/quasi-random sequences. Numerical examples show that the enhanced algorithm outperforms the standard \u03f5-constraint method in terms of quantity and quality of the Pareto points produced by the algorithm. Our approach, which is particularly suited for environmental problems that tend to contain several redundant objectives, allows dealing with complex MOO models with many objectives.",
     "keywords": ["Multi-objective optimization", "Objective reduction", "Hypervolume", "Epsilon-constraint", "MOO"]},
    {"article name": "A hybrid CPU-Graphics Processing Unit (GPU) approach for computationally efficient simulation-optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.01.001",
     "publication date": "04-2016",
     "abstract": "Simulation-optimization (Sim-Opt) is a widely used optimization technique that enables the use of simulation model so as naturally describe system complexity and stochastics. A key barrier to its broader adoption is the high computational cost associated with simulation that often limits its practicability. In this paper, we propose the use of GPU parallel computing, to enhance the computational efficiency of Sim-Opt. The main objective of this work is to develop a systematic framework that can be used to construct an efficient hybrid CPU-GPU program. The optimization of a process monitoring model using a Genetic Algorithm is used as a case study to illustrate the proposed approach. Our results show an over 100\u00d7 acceleration of computation time by the developed hybrid program in comparison to a traditional CPU-based approach.",
     "keywords": ["Genetic Algorithm", "Parallel computing", "Sim-Opt", "PCA", "Tennessee Eastman challenge process"]},
    {"article name": "Simultaneous subtour elimination model for single-stage multiproduct parallel batch scheduling with sequence dependent changeovers",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.12.024",
     "publication date": "04-2016",
     "abstract": "In this paper a mixed-integer linear programming (MILP) model is presented to minimize makespan of single-stage multiproduct parallel batch production with sequence dependent changeovers. The computational inefficiency and suboptimal problems are addressed by the tight and rigorous formulation of the proposed model. Subtours (subcycles) are eliminated simultaneously so that the optimal solution is obtained in one step. The proposed model is tested with two examples. The results show that the model obtains the global optimal solutions with significant improvement in solution time.",
     "keywords": ["Batch scheduling", "MILP", "Parallel non-uniform production", "Subtour elimination"]},
    {"article name": "Strategic optimisation of biomass-based energy supply chains for sustainable mobility",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.01.003",
     "publication date": "04-2016",
     "abstract": "The identification of alternative and sustainable energy sources has been one of the fundamental research goals of the last two decades, and the transport sector plays a key role in this challenge. Electric cars and biofuel fed vehicles may contribute to tackle this formidable issue. According to this perspective, a multi-echelon supply chain is here investigated considering biomass cultivation, transport, conversion into bioethanol or bioelectricity, distribution, and final usage in alternative bifuel (ethanol and petrol) and electric vehicles. Multiperiod and spatially explicit features are introduced in a Mixed Integer Linear Programming (MILP) modelling framework where economic (in terms of Net Present Value) and environmental (in terms of Greenhouse Gases emissions) objectives are simultaneously taken into account. The first and second generation bioethanol production supply chain is matched with a biopower production supply chain assessing multiple technologies. Both corn grain and stover are considered as biomass sources. In the environmental analysis, the impact on emissions caused by indirect Land Use Change (iLUC) effects is also assessed. Results will show the efficacy of the methodology at providing stakeholders with a quantitative tool to optimise the economic and environmental performance of different supply chain configurations.",
     "keywords": ["Alternative fuel vehicle", "Bioethanol and bioelectricity supply chain", "First and second generation", "Indirect land use change", "Multi-objective optimisation"]},
    {"article name": "Data-driven tiered procedure for enhancing yield in drug product manufacturing",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.12.012",
     "publication date": "04-2016",
     "abstract": "Enhancing efficiency of pharmaceutical batch production processes is an important challenge in times of increasing public pressure on healthcare costs and decreasing research productivity. This study presents a data-based procedure for systematic yield enhancements in drug product manufacturing, based on four steps. On the first step, production is reviewed to select relevant loss causes, which are assessed on the second step deductively with the goal of assigning measurable parameters. Descriptive Statistical Modelling of loss causes is then performed on the third step, enabling model-based enhancements of processes on the fourth step or, if necessary, a loop-back review of a given loss cause.An industrial case study was performed on production data of 88 batches and demonstrated the applicability of the procedure by prioritizing relevant loss causes, reducing required sample quantities by up to 8% and a cosmetic defect by about 70% by a process change.",
     "keywords": ["Sterile drug product manufacturing", "Biologics", "Decision-making", "Multivariate data analysis", "Industrial case study"]},
    {"article name": "An archive-based multi-objective evolutionary algorithm with adaptive search space partitioning to deal with expensive optimization problems: Application to process eco-design",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.12.008",
     "publication date": "04-2016",
     "abstract": "In eco-design, the integration of environmental aspects into the earliest stage of design is considered with the aim of reducing adverse environmental impacts throughout a product's life cycle. An eco-design problem is therefore multi-objective, where several objectives (environmental, economic, and technological) are to be simultaneously optimized.The optimization of industrial processes usually requires solving expensive multi-objective optimization problems (MOPs). Aiming to solve efficiently MOPs, with a limited computational budget, this paper proposes a new framework called AMOEA-MAP. The framework relies on the structure of the NSGAII algorithm and possesses two novel operators: a memory-based adaptive partitioning strategy, which provides an adaptive reticulation of the search space for a quick identification of optimal zones with less computational effort; and a bi-population evolutionary algorithm, tailored for expensive optimization problems.To ascertain its generality, the framework is first tested on several tough benchmarks. Its performance is subsequently validated on a real-world eco-design problem.",
     "keywords": ["Expensive simulation-based multi-objective optimization", "Multi-objective evolutionary algorithms", "Convergence improvement heuristics", "Industrial eco-design"]},
    {"article name": "Solution of bivariate population balance equations with high-order moment-conserving method of classes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.12.013",
     "publication date": "04-2016",
     "abstract": "In this work the high-order moment-conserving method of classes (HMMC) (Alopaeus et al., 2006) is extended to solve the bivariate Population Balance Equation (PBE). The method is capable of guaranteeing the internal consistency of the discretized equations for a generic moment set, including mixed-order moments of the distribution. The construction of the product tables in the case of aggregation, breakage and convection in internal coordinate space are discussed. Eventually, several test cases are considered to assess the accuracy of the method. The application to a realistic mass transfer problems in a liquid\u2013liquid system is preliminarily discussed. The comparison with analytical solutions of pure aggregation problems shows that the proposed method is accurate with only a limited number of categories.",
     "keywords": ["Population balance", "High-order moment-conserving method of classes (HMMC)", "Bivariate", "Two-component aggregation", "Particulate processes", "Numerical methods"]},
    {"article name": "Systematic methods and tools for design of sustainable chemical processes for CO2 utilization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.01.006",
     "publication date": "04-2016",
     "abstract": "A systematic computer-aided framework for sustainable process design is presented together with its application to the synthesis and generation of processing networks for dimethyl carbonate (DMC) production with CO2 utilization. The framework integrated with various methods, tools, algorithms and databases is based on a combined process synthesis\u2013design\u2013intensification method. The method consists of three stages. The synthesis-stage involves superstructure based optimization to identify promising networks that convert a given set of raw materials to a desired set of products. The design-stage involves selection and analysis of the identified networks as a base case design in terms of operational feasibility, economics, life cycle assessment factors and sustainability measures, which are employed to establish targets for improvement in the next-stage. The innovation-stage involves generation and screening of the more sustainable alternatives through a phenomena-based process intensification method. Applications of the framework are highlighted for the DMC production process.",
     "keywords": ["CO2 utilization", "Dimethyl carbonate", "Process intensification", "Superstructure generation", "Mixed-integer non-linear program (MINLP)"]},
    {"article name": "Equation oriented mixed micellization modeling based on asymmetric Margules-type formulations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.12.026",
     "publication date": "04-2016",
     "abstract": "The determination of the composition of mixed micellar systems is a major problem since only the composition of the total micellar solution is accessible to the experimenter. A new approach is introduced in the present work based on Equation Oriented Optimization and Margules asymmetric formulations which is not restricted to the number of components and guarantees the applicability of the Gibbs\u2013Duhem relation. The method is validated through its application to systems from literature and the quality of the solutions is tested with already published data and by comparison with other approaches. We do also show how excess properties can be predicted and that the excess entropy is not zero as assumed by the original regular and sub-regular solution theories.",
     "keywords": ["Mixed micelles", "Margules formulations", "Regular solution theory", "Mathematical modeling"]},
    {"article name": "Simulation-free estimation of reaction propensities in cellular reactions and gene signaling networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.01.010",
     "publication date": "04-2016",
     "abstract": "Classical reaction kinetics based on deterministic rates laws are not valid for the description of cellular events in which the small number of molecules introduces stochasticity with discrete instead of continuous state transitions. Stochastic models are suitable for simulating transcriptional and translational events inside biological cells, but are impractical for solving inverse problems, which aim to estimate unknown reaction propensities from experimental observations. We introduce a new mathematical framework of Ito stochastic differential equations for the modeling of discrete cellular events and the robust and consistent parameter estimation of cellular dynamics where classical reaction kinetics is invalid. The results supported by case studies on gene expression in B. subtilis cells and viral gene transcription and translation inside non-lytic viral cells demonstrate that the proposed methodology performs as reliable as the gold standard Gillespie algorithm for simulating cellular events. More importantly, the new Ito process framework is ideal for estimating unknown reaction propensities from data as readily as in deterministic parameter estimation by using the novel \u2018SPE \u2013 simulation free parameter estimation\u2019 approach. Also, the computation time for the stochastic differential equation models is significantly low when compared to discrete event simulations.",
     "keywords": ["Ito process", "Gillespie algorithm", "Stochastic modeling", "Inversion"]},
    {"article name": "A computationally efficient simulation-based optimization method with region-wise surrogate modeling for stochastic inventory management of supply chains with general network structures",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.01.015",
     "publication date": "04-2016",
     "abstract": "Simulation-based optimization is widely used to improve the performance of an inventory system under uncertainty. However, the black-box function between the input and output, along with the expensive simulation to reproduce a real inventory system, introduces a huge challenge in optimizing these performances. We propose an efficient framework for reducing the total operation cost while satisfying the service level constraints. The performances of each inventory in the system are estimated by kriging models in a region-wise manner which greatly reduces the computational time during both sampling and optimization. The aggregated surrogate models are optimized by a trust-region framework where a model recalibration process is used to ensure the solution's validity. The proposed framework is able to solve general supply chain problems with the multi-sourcing capability, asynchronous ordering, uncertain demand and stochastic lead time. This framework is demonstrated by two case studies with up to 18 nodes with inventory holding capability in the network.",
     "keywords": ["Simulation-based optimization", "Inventory management", "Trust-region algorithm", "Surrogate modeling", "Kriging"]},
    {"article name": "Mathematical model of precipitation of magnesium carbonate with carbon dioxide from the magnesium hydroxide slurry",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.01.013",
     "publication date": "04-2016",
     "abstract": "A mathematical model is proposed for a precipitation process of magnesium carbonate in a heterogeneous stirred tank reactor. The model includes a description of dissolution of Mg(OH)2, absorption of CO2 and precipitation of MgCO3. The Nernst-Planck equation is used in the dissolution model to maintain the mass balance and electroneutrality. The van Krevelen\u2013Hoftijzer expression is introduced to describe the enhancement effect of reaction between dissolved CO2 and OH\u2212 on the mass transfer rate of dissolution and absorption. In the precipitation model, a simplified population balance equation is solved by a moment method for both dissolving and precipitating particles. Unknown precipitation kinetics parameters for Mg(OH)2MgCO3 system are fitted against experimental data and compared with Ca(OH)2CaCO3 system. According to the present analysis, the liquid\u2212solid and gas\u2212liquid mass transfer dominate the global rate of precipitation. The precipitation kinetics and pH have strong influences on the concentration of reactants and the yield of precipitation.",
     "keywords": ["Mass transfer modeling", "Dissolution", "Absorption", "Precipitation", "Nernst-Planck", "Heterogeneous reaction"]},
    {"article name": "Water integration in eco-industrial parks using a multi-leader-follower approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.01.005",
     "publication date": "04-2016",
     "abstract": "The design and optimization of industrial water networks in eco-industrial parks are studied by formulating and solving multi-leader-follower game problems. The methodology is explained by demonstrating its advantages against multi-objective optimization approaches. Several formulations and solution methods for MLFG are discussed in detail. The approach is validated on a case study of water integration in EIP without and with regeneration units. In the latter, multi-leader-single-follower and single-leader-multi-follower games are studied. Each enterprise's objective is to minimize the total annualized cost, while the EIP authority objective is to minimize the consumption of freshwater within the ecopark. The MLFG is transformed into a MOPEC and solved using GAMS\u00ae as an NLP. Obtained results are compared against the MOO approach and between different MLFG formulations. The methodology proposed is proved to be very reliable in multi-criteria scenarios compared to MOO approaches, providing numerical Nash equilibrium solutions and specifically in EIP design and optimization.",
     "keywords": ["Eco-industrial parks", "Multi-leader-follower game", "Nash equilibrium", "Multi-objective optimization", "MPCC", "Game theory"]},
    {"article name": "Polydispersed flow modelling using population balances in an adaptive mesh finite element framework",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.01.011",
     "publication date": "04-2016",
     "abstract": "An open-source finite element framework to model multiphase polydispersed flows is presented in this work. The Eulerian\u2013Eulerian method was coupled to a population balance equation and solved using a highly-parallelised finite element code\u2014Fluidity. The population balance equation was solved using DQMOM. A hybrid finite element\u2013control volume method for solving the coupled system of equations was established. To enhance the efficiency of this solver, fully-unstructured non-homogeneous anisotropic mesh adaptivity was applied to systematically adapt the mesh based on the underlying physics of the problem. This is the first time mesh adaptivity has been applied to the external coordinates of the population balance equation for modelling polydispersed flows. Rigorous model verification and benchmarking were also performed to demonstrate the accuracy of this implementation. This finite element framework provides an efficient alternative to model polydispersed flow problems over the other available finite volume CFD packages.",
     "keywords": ["Polydispersed flow", "Population balance modeling", "DQMOM", "Multiphase flow", "Finite element method", "Mesh adaptivity"]},
    {"article name": "A multi-objective multi-drug model for cancer chemotherapy treatment planning: A cost-effective approach to designing clinical trials",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.12.004",
     "publication date": "04-2016",
     "abstract": "One of the important areas of concentration in medical sciences is the development of the treatment regimens in chronic diseases like cancer. This paper provides insights on how to design new clinical trials for gastric and gastroesophageal cancers which can discover the optimal and cost-effective chemotherapy treatment regimens. First, we extract data from the previously published clinical trials for the mentioned cancers to develop statistical models being capable of predicting trial outcomes. Then, using the statistical models, we present a multi-objective multi-drug model for cancer chemotherapy treatment planning. The proposed model yields a wide range of solutions establishing a reasonable tradeoff between patient's survival and treatment costs while satisfying some prevailing limitations on toxicities and the feasibility of treatment regimens. Results show that the proposed approach needs much less time and cost than the trial-and-error manipulation of cancer treatment. It also takes the advantage of saving and improving the quality of patients\u2019 lives by suggesting the new drug regimens which improve the survival time of patients and have reasonable treatment costs compared to the current practice trials.",
     "keywords": ["Chemotherapy treatment planning", "Cancer", "Optimization", "Clinical trial", "Treatment costs", "Survival"]},
    {"article name": "Plant-wide optimization and control of an industrial diesel hydro-processing plant",
     "doi": "https://doi.org/10.1016/j.compchemeng.2016.01.016",
     "publication date": "04-2016",
     "abstract": "Diesel hydro-processing (DHP) is an important refinery process which removes the undesired sulfur from the oil feedstock followed by hydro-cracking and fractionation to obtain diesel with desired properties. The DHP plant operates with varying feed-stocks. Also, changing market conditions have significant effects on the diesel product specifications. In the presence of such a dynamic environment, the DHP plant has to run in the most profitable and safe way and satisfy the product requirements. In this study, we propose a hierarchical, cascaded model predictive control structure to be used for real-time optimization of an industrial DHP plant.",
     "keywords": ["Hydro-desulfurization reactors", "Blending", "Diesel properties", "Sulfur removal", "Empirical modeling", "Model predictive control"]},
    {"article name": "Sensor network design for contaminant detection and identification in water distribution networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.12.022",
     "publication date": "04-2016",
     "abstract": "Water distribution networks (WDN) are vulnerable to either intentional or accidental contamination. In order to protect against such intrusions, effective and efficient online monitoring systems are needed. Due to cost and maintenance reasons, it is not possible to locate sensors at each and every potential intrusion point. In this work, we design minimal sensor networks which satisfy the two important properties of observability (ability to detect an intrusion) and identifiability (ability to identify the point of intrusion). Based on the hydraulic analysis of the network, a bipartite graph is constructed between intrusion points and the corresponding nodes that can potentially be affected by the contaminant. The problem of sensor network design is converted to a minimum set cover problem on the bipartite graph, and is solved using a greedy heuristic algorithm. The proposed method is illustrated using a medium scale urban WDN.",
     "keywords": ["Water distribution network", "Contaminant detection", "Observability", "Identifiability", "Bipartite graph"]},
    {"article name": "Power scheduling and real-time optimization of industrial cogeneration plants",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.12.023",
     "publication date": "04-2016",
     "abstract": "Scheduling of power and real-time optimization for three industrial cogeneration plants at one of Dow's Louisiana site is presented in this paper. A first principle mathematical model that includes mass and energy balances for gas turbines, heat recovery units, steam turbines, pressure relief valves and steam headers is used to formulate an optimization problem to recommend the best strategy to trade power. The model has detailed operational information that includes equipment status and control curves for different operating scenarios. The model can also accurately predict the effect of ambient temperature, thereby resulting in an optimal day-ahead schedule. Adjustment of power schedule is done in the real-time market 30\u00a0min prior to the hour and implementation of the dispatched power schedule is done using a model predictive controller.",
     "keywords": ["Industrial cogeneration process", "Day-ahead scheduling", "Real-time optimization", "Combined heat and power system modeling", "Model predictive control"]},
    {"article name": "A simple approach to improve the robustness of equation-oriented simulators: Multilinear look-up table interpolators",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.12.014",
     "publication date": "03-2016",
     "abstract": "Equation-oriented simulators have some advantages over the modular sequential ones, but improvements are still necessary to deal with nonlinearities, while preserving the robustness of the solver. Linear approximations and/or surrogate models can be used in place of nonlinear models, but the loss of predictive accuracy may be a drawback. An alternative to circumvent this problem is the use of grid-based look-up tables for interpolating responses from rigorous models. This methodology was integrated in an equation-oriented simulator (EMSO). A case study involving the production of bioethanol from sugarcane is used to demonstrate the robustness of this approach. Look-up tables replaced the models of two distillation column trains and of the cellulose hydrolysis reactor. These models were included into the global process and an optimization problem aiming at the maximum production of ethanol was successfully solved by a PSO algorithm varying the solid mass fraction in the hydrolysis reactor.",
     "keywords": ["Robustness of equation-oriented simulators", "Multilinear look-up table interpolators", "1G\u20132G bioethanol production", "Particle swarm optimization"]},
    {"article name": "Dynamics of a True Moving Bed separation process: Effect of operating variables on performance indicators using orthogonalization method",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.12.009",
     "publication date": "03-2016",
     "abstract": "The assessment of the performance of cyclic adsorption systems is usually addressed in literature in terms of steady state. To reach further developments in this field, the characterization of the dynamic behavior of the processes becomes necessary. This work focus on the application of a method based on Gram-Schmidt Orthogonalization to analyze the impact of the operating variables in the dynamic response of a TMB unit. Another objective of this work is to characterize the dynamic system behavior and compare it with the orthogonalization method results. The results showed that the recycling flow rate is the operating variable with the greatest impact for the system considered. The step perturbation analysis showed the consistence of the proposed method and that some process variables result in a system inverse response for the recovery performance indicator. The importance of taking in consideration the process dynamics in the unit design, control and optimization is demonstrated.",
     "keywords": ["True Moving Bed", "Gram-Schmidt Orthogonalization", "Enantiomers separation"]},
    {"article name": "Dynamic parameter estimation and optimization for batch distillation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.12.001",
     "publication date": "03-2016",
     "abstract": "This work reviews a well-known methodology for batch distillation modeling, estimation, and optimization but adds a new case study with experimental validation. Use of nonlinear statistics and a sensitivity analysis provides valuable insight for model validation and optimization verification for batch columns. The application is a simple, batch column with a binary methanol\u2013ethanol mixture. Dynamic parameter estimation with an \u21131-norm error, nonlinear confidence intervals, ranking of observable parameters, and efficient sensitivity analysis are used to refine the model and find the best parameter estimates for dynamic optimization implementation. The statistical and sensitivity analyses indicated there are only a subset of parameters that are observable. For the batch column, the optimized production rate increases by 14% while maintaining product purity requirements.",
     "keywords": ["Dynamic parameter estimation", "Nonlinear statistics", "Experimental validation", "Batch distillation", "Dynamic optimization"]},
    {"article name": "Mixed-integer bilevel optimization for capacity planning with rational markets",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.12.007",
     "publication date": "03-2016",
     "abstract": "We formulate the capacity expansion planning as a bilevel optimization to model the hierarchical decision structure involving industrial producers and consumers. The formulation is a mixed-integer bilevel linear program in which the upper level maximizes the profit of a producer and the lower level minimizes the cost paid by markets. The upper-level problem includes mixed-integer variables that establish the expansion plan; the lower level problem is an LP that decides demands assignments. We reformulate the bilevel optimization as a single-level problem using two different approaches: KKT reformulation and duality-based reformulation. We analyze the performance of these reformulations and compare their results with the expansion plans obtained from the traditional single-level formulation. For the solution of large-scale problems, we propose improvements on the duality-based reformulation that allows reducing the number of variables and constraints. The formulations and the solution methods are illustrated with examples from the air separation industry.",
     "keywords": ["Capacity planning", "Bilevel optimization", "Rational markets"]},
    {"article name": "Optimal integration of third-parties in a coordinated supply chain management environment",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.12.002",
     "publication date": "03-2016",
     "abstract": "A generic tactical model is developed considering third party price policies for the optimization of coordinated and centralized multi-product Supply Chains (SCs). To allow a more realistic assessment of these policies in each marketing situation, different price approximation models to estimate these policies are proposed, which are based on the demand elasticity theory, and result in different model implementations (LP, NLP, and MINLP). The consequences of using the proposed models on the SCs coordination, regarding not only their practical impact on the tactical decisions, but also the additional mathematical difficulties to be solved, are verified through a case study in which the coordination of a production\u2013distribution SC and its energy generation SC is analyzed. The results show how the selection of the price approximation model affects the tactical decisions. The average price approximation leads to the worst decisions with a significant difference in the real total cost in comparison with the best piecewise approximation.",
     "keywords": ["CEFIC European Chemicals Industry Council", "European Chemicals Industry Council", "CPU Central Processing Unit", "Central Processing Unit", "DC Distribution Center", "Distribution Center", "EPCA European Petrochemical Association", "European Petrochemical Association", "GAMS The General Algebraic Modeling System", "The General Algebraic Modeling System", "GB Gigabyte", "Gigabyte", "GHz Gigahertz", "Gigahertz", "LP Linear Programming", "Linear Programming", "MIP Mixed Integer Programming", "Mixed Integer Programming", "MILP Mixed Integer Linear Programming", "Mixed Integer Linear Programming", "MINLP Mixed Integer Non-Linear Programming", "Mixed Integer Non-Linear Programming", "NLP Non-Linear Programming", "Non-Linear Programming", "PSE Process System Engineering", "Process System Engineering", "RM Raw Material", "Raw Material", "SC Supply Chain", "Supply Chain", "SCM Supply Chain Management", "Supply Chain Management", "SCs Supply Chains", "Supply Chains", "SCsCo Supply Chains Coordination", "Supply Chains Coordination", "Coordinated management", "3rd party", "Supply chain planning", "Pricing models", "Demand elasticity"]},
    {"article name": "Real-time optimization for a laboratory-scale flotation column",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.12.006",
     "publication date": "03-2016",
     "abstract": "In this paper, a supervisory layer with real-time optimization (RTO) has been implemented in an experimental laboratory-scale flotation column for copper concentration. A two-stage and modifier adaptation (MA) methodology for RTO has been compared under structural, experimental and dynamic uncertainty. In addition, a gradient-free alternative for MA, called nested modifier optimization, has been proposed and tested. The results show that the KKT updates of the MA approach allow the process optimum to be determined under uncertain scenarios, unlike the two-stage approach. From the perspective of gradient modifiers, the performance of the nested methodology is comparable to the dual approach because previous past values are used to update the modifiers without requiring the gradient estimation step. In addition, the interaction of RTO with the regulatory layer must be considered to propose an optimal implementation.",
     "keywords": ["Real-time optimization", "Flotation column", "Process optimization", "Modifier adaptation", "Derivative free optimization"]},
    {"article name": "Reactor network synthesis with guaranteed robust stability",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.12.005",
     "publication date": "03-2016",
     "abstract": "This paper proposes a systematic approach to design reactor networks with guaranteed robust stability. The approach is based on the superstructure approach for reactor network synthesis. A structured dynamic model for reactor network modeling is formulated and embedded in a MINLP with robust eigenvalue constraints. Design parameters, structural alternatives and parametric uncertainty are considered simultaneously as design degrees of freedom. Structural alternatives result from decisions on the existence of reactors and flow connections in the superstructure. Parametric uncertainty may either result from model uncertainties such as reaction kinetic constants or heat transfer coefficients, or from process uncertainties including slow disturbances in load or quality of raw materials. A tailored two-step solution strategy is proposed to tackle the robust mixed-integer optimization problem. A case study with five continuous stirred-tank reactors (CSTR) and five plug flow reactors (PFR) is presented for illustration.",
     "keywords": ["Reactor network synthesis", "Robust stability", "Mixed-integer nonlinear programming", "Normal vector approach", "Superstructure approach", "Robust optimization"]},
    {"article name": "Risk-based integrated production scheduling and electricity procurement for continuous power-intensive processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.12.015",
     "publication date": "03-2016",
     "abstract": "For optimal operation of power-intensive plants, production scheduling and electricity procurement have to be considered simultaneously. In addition, uncertainty needs to be taken into account. For this purpose, an integrated stochastic mixed-integer linear programming model is developed that considers the two most critical sources of uncertainty: spot electricity price, and product demand. Conditional value-at-risk is incorporated into the model as a measure of risk. Furthermore, scenario reduction and multicut Benders decomposition are implemented to solve large-scale real-world problems. The proposed model is applied to an illustrative example as well as an industrial air separation case. The results show the benefit from stochastic optimization and the effect of taking a risk-averse rather than a risk-neutral approach. An interesting insight from the analysis is that in risk-neutral optimization, accounting for electricity price uncertainty does not yield significant added value; however, in risk-averse optimization, modeling price uncertainty is crucial for obtaining good solutions.",
     "keywords": ["Production scheduling", "Electricity procurement", "Demand response", "Stochastic programming", "Conditional value-at-risk"]},
    {"article name": "An adjustable robust optimization approach to scheduling of continuous industrial processes providing interruptible load",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.12.018",
     "publication date": "03-2016",
     "abstract": "To ensure the stability of the power grid, backup capacities are called upon when electricity supply does not meet demand due to unexpected changes in the grid. As part of the demand response efforts in recent years, large electricity consumers are encouraged by financial incentives to provide such operating reserve in the form of load reduction capacities (interruptible load). However, a major challenge lies in the uncertainty that one does not know in advance when load reduction will be requested. In this work, we develop a scheduling model for continuous industrial processes providing interruptible load. An adjustable robust optimization approach, which incorporates recourse decisions using linear decision rules, is applied to model the uncertainty. The proposed model is applied to an illustrative example as well as a real-world air separation case. The results show the benefits from selling interruptible load and the value of considering recourse in the decision-making.",
     "keywords": ["Production scheduling", "Demand response", "Interruptible load", "Adjustable robust optimization", "Mixed-integer linear programming"]},
    {"article name": "Optimal response under partial plant shutdown with discontinuous dynamic models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.12.011",
     "publication date": "03-2016",
     "abstract": "This work describes mathematical formulations for modeling aspects of partial shutdowns in multiunit plants. The specific type of partial shutdown considered is one that permits the decoupling of affected units from the rest of the plant, thus enabling continued plant operation, albeit in a more limited fashion. Parsimonious and computationally efficient mixed-integer formulations are presented for specific discontinuous phenomena that arise in partial shutdown modeling, such as shutdown thresholds, induced shutdowns, discontinuous costs, and minimum shutdown durations. It is demonstrated that induced shutdowns (secondary shutdowns triggered by the original shutdown) can be correctly penalized in the objective by capturing the shutdown's true discontinuous economic cost. The computed optimal solution is implemented in closed-loop by employing a multitiered model predictive shutdown controller, in which a discrete-time mixed-integer dynamic optimization (MIDO) problem is embedded. Both objectives of maximizing economics and minimizing restoration (shutdown recovery) time are considered.",
     "keywords": ["Dynamic optimization", "Partial shutdowns", "Mixed integer programming", "Multitiered optimization", "Model predictive control", "Process control"]},
    {"article name": "Techno-economic evaluation of the direct conversion of CO2 to dimethyl carbonate using catalytic membrane reactors",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.12.025",
     "publication date": "03-2016",
     "abstract": "The production of dimethyl carbonate (DMC) caught more interest in the past decades due to its versatile use (e.g. as fuel additive), low toxicity and fast biodegradability. Different \u2018green\u2019 production routes are being developed to replace the conventional and rather toxic production of DMC via phosgene. The direct conversion of CO2 and methanol toward DMC is an environmental and economically interesting production route for the chemical industry.This work describes the process design of the direct conversion of CO2 to dimethyl carbonate, providing a valuable insight and a better understanding of the process limitations. In this design, membrane reactors are used for continuous removal of water by-product, in order to overcome the equilibrium limitations. The rigorous Aspen Plus simulations show that even when using an excess of methanol, the attainable conversion is low and the DMC concentration in the reactor effluent is less than 1.5\u00a0mol%. Purifying this diluted stream to the desired concentrations demands large size equipment and a substantial amount of energy (13.61\u00a0kWh/kg DMC) resulting in high investment and utility costs, thus making the process not profitable. The focus for new membrane reactors could be on the selective removal of DMC (instead of water) from the reaction area to allow for a more concentrated DMC stream.",
     "keywords": ["CO2 valorisation", "DMC", "Membrane reactor", "Process design", "Economic evaluation"]},
    {"article name": "A particle scale model for municipal solid waste and refuse-derived fuels pyrolysis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.12.019",
     "publication date": "03-2016",
     "abstract": "The solid phase decomposition during pyrolysis of municipal solid waste (MSW) and refuse-derived fuels (RDF) is modelled on particle scale accounting for heat and mass transfer. Waste pyrolysis is expressed as a linear combination of pyrolysis of its components. The novel characterization method used expresses waste composition in terms of three reference species. The selected species are a mixture of cellulose and hemicellulose, a mixture of polystyrene and polyethylene terephthalate, and a mixture of polyethylene and polypropylene. The pyrolysis kinetics models for these components are taken from the literature. The fractions of the components in the mixtures are optimized to fit the model to non-isothermal mass loss curves from selected experimental reports. The particle scale model has been evaluated against experimental transient temperature profiles at the centre of a large waste pellet during pyrolysis. The model is able to predict the main trend, but shows a more fluctuating temperature curve.",
     "keywords": ["Municipal solid waste", "MSW", "Refuse-derived fuels", "RDF", "Pyrolysis", "Model"]},
    {"article name": "Modeling and simulation of a multistage absorption hydration hybrid process using equation oriented modeling environment",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.12.021",
     "publication date": "03-2016",
     "abstract": "Separation of light hydrocarbon mixtures is one of the most important topics in chemical engineering research. With development of theories on hydrate equilibriums and kinetics, researchers are trying to apply hydration based separation technology to industrial applications. It is increasingly important to develop the corresponding simulation strategies for process design purposes. In this work we use an equation oriented modeling environment, named Aspen Custom Modeler\u00ae (ACM\u00ae), which enables rapid model development and provides powerful simulation solvers. With the help of ACM\u00ae, a multistage absorption hydration hybrid process (AHHP) for refinery dry gas separation is modeled and simulated. Sensitivities of key parameters such as water content and absorbent flow rate, are analyzed. Features of the multistage AHHP are discussed. For comparison, based on an industrial data, a butane absorption process is established and simulated. Economic evaluation shows that the multistage AHHP is competitive compared to current absorption process.",
     "keywords": ["Hydrate", "Absorption", "Water in oil emulsion", "Separation", "Process simulation", "Refinery dry gas"]},
    {"article name": "Dominating dynamics of the post-combustion CO2 absorption process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.11.003",
     "publication date": "03-2016",
     "abstract": "A dynamic model of the post-combustion CO2 capture process based on chemical absorption is used to investigate the transient behavior and dynamic responses of the process and to detect stabilization time when various disturbances are introduced. Plant dimensions and parameter settings are based on the SINTEF CO2 capture pilot plant at Tiller in Norway, and the overall process model is validated using two sets of steady state pilot plant data. A deviation between model and pilot plant results of \u22120.8% and \u22124.5% in absorbed CO2 and 2.6% and 1.2% in desorbed CO2 is seen for the two cases used in validation, respectively, which is within the observed pilot plant CO2 mass balance error of \u00b16%. The simulated absorber and desorber temperature profiles show also adequate agreement to the pilot plant measurements. The process model is further used to simulate set-point changes in flue gas flow rate, reboiler duty and solvent flow rate in order to investigate typical stabilization times at various locations in the process. As expected, mixing models such as the absorber sump and reboiler will introduce time constants that affect the dynamic response profiles, while plug flow models such as the cross heat exchanger and lean cooler causes pure transport delays and no additional settling time. Mass transfer and chemical reaction rates causes some process inertia, but it is relatively small compared to the inertia of larger mixing vessels such as the absorber sump, reboiler and buffer tank and transport delay caused by plug flow. Changes to the solvent flow rate are also seen as a larger disturbance to the process compared to changes in flue gas flow rate and reboiler duty, reflected by longer process stabilization time to reach new steady state conditions. The estimated 90% settling times for the response in CO2 capture rate in the Tiller pilot plant are less than 1\u00a0h, 3.5\u20136\u00a0h and 3.5\u20134\u00a0h for step changes in flue gas flow rate, solvent flow rate and reboiler duty, respectively.",
     "keywords": ["Post combustion CO2 capture", "Absorption", "Process dynamics", "Time constants", "Stabilization time"]},
    {"article name": "Coupled CFD\u2013DEM of particle-laden flows in a turning flow with a moving wall",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.12.020",
     "publication date": "03-2016",
     "abstract": "The nature of the particle\u2013solid interactions and particle\u2013fluid interactions in rectangular duct bend geometry with/without a moving wall is studied, taking into account particle collision, colloidal, and hydrodynamic forces, and four way coupling between the fluid flow and particles. The focus is on systems where particles and fluid phase have similar length scales, fluid Reynolds number (Ref)\u00a0\u223c\u00a01, and particle's Stokes number (St)\u00a0\u223c\u00a01. Particles move toward the walls of the channel near the bend, and have long residence times in these regions. Buoyancy force has negligible effect on particle motion, where adhesion and drag forces lead to particle motion and agglomeration patterns. The effect of a free surface on agglomeration sites in the turning flow is elucidated.",
     "keywords": ["Particle\u2013fluid flows", "Discrete element method", "Computational fluid dynamics"]},
    {"article name": "Improving uncertainty evaluation of process models by using pedigree analysis. A case study on CO2 capture with monoethanolamine",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.10.006",
     "publication date": "02-2016",
     "abstract": "This article aims to improve uncertainty evaluation of process models by combining a quantitative uncertainty evaluation method (data validation) with a qualitative uncertainty evaluation method (pedigree analysis). The approach is tested on a case study of monoethanolamine based postcombustion CO2 capture from a coal power plant. Data validation was used to quantitatively assess the uncertainty of the inputs and outputs of the MEA model. Pedigree analysis was used to qualitatively assess the uncertainty in the current knowledge base on MEA carbon capture systems, the uncertainty in the MEA process model, and the uncertainty of the MEA model results. The pedigree review was done by 13 international experts in the field of postcombustion carbon capture with chemical solvents.The data validation showed that our MEA model is accurate in predicting specific reboiler duty, and CO2 stream purity (4% and 1% difference respectively between model and pilot plant results), but in first instance it was less accurate in predicting liquid over gas ratio, and cooling water requirement (54% and 23% difference respectively between model and pilot plant results). The pedigree analysis complemented these results by showing that there was fairly high uncertainty in the thermodynamic, and chemistry submodels, as reflected in the low pedigree scores on most indicators. Therefore, the model was improved to better resemble pilot plant results.The results indicate that using a pedigree approach improved uncertainty evaluation in three ways. First, by highlighting sources of uncertainty that quantitative uncertainty analysis does not take into account, such as uncertainty in the knowledge base regarding a specific phenomenon. Second, by providing a systematic approach to uncertainty evaluation, thereby increasing the awareness of modeller and model user. And finally, by presenting the outcomes in easy to understand numerical scores and colours, improving the communication of model uncertainty. In combination with quantitative validation efforts, the pedigree approach can provide a strong method to gain deep insight into the strengths and weaknesses of a process model, and to communicate this to policy and decision-makers.",
     "keywords": ["Process modelling", "Uncertainty evaluation", "Pedigree analysis", "Postcombustion carbon capture", "Monoethanolamine"]},
    {"article name": "Design of bioethanol green supply chain: Comparison between first and second generation biomass concerning economic, environmental and social criteria",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.10.008",
     "publication date": "02-2016",
     "abstract": "This contribution addresses the optimal design of the biomass supply chain as it is crucial to ensure long term viability of such a project. This work is focused on the multi objective optimization by considering all the dimension of the sustainable development, namely economic, environmental, and social. The environmental dimension is quantified through life cycle assessment, and more particularly the Ecocosts method. The social aspect is measured through two indicators: the competition between energy and food, and the total number of local accrued jobs. For the latter a new method based on financial accounting analysis is proposed to estimate the direct, indirect and induced jobs created.Once the superstructure described, the optimization problem is formulated as a mixed integer linear program (MILP) that accounts for biomass seasonality, geographical availability, biomass degradation, process conversion technologies and final product demand. The output results of the model propose the optimal network design, facilities location, process selection and inventory policy. Since multiple conflicting objectives are involved when optimizing the sustainability of the biomass supply chain and the binary variables have an important influence on the resolution, the MILP problem is solved with the goal programming method to reach the trade-off. The approach is illustrated through a bioethanol supply chain case study in France, for the comparison between agricultural and forest residues biomass.",
     "keywords": ["Biomass", "Supply chain", "Multi objective Optimization", "Jobs estimation", "Location problem"]},
    {"article name": "Multi-objective optimization with convex quadratic cost functions: A multi-parametric programming approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.10.011",
     "publication date": "02-2016",
     "abstract": "In this note we present an approximate algorithm for the explicit calculation of the Pareto front for multi-objective optimization problems featuring convex quadratic cost functions and linear constraints based on multi-parametric programming and employing a set of suitable overestimators with tunable suboptimality. A numerical example as well as a small computational study highlight the features of the novel algorithm.",
     "keywords": ["Multi-objective optimization", "Multi-parametric programming", "Explicit Pareto front calculation"]},
    {"article name": "An extended P-graph approach to process network synthesis for multi-period operations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.10.007",
     "publication date": "02-2016",
     "abstract": "An extension of the P-graph approach for multi-period process network synthesis (PNS) is proposed in this work. A modification of a previously published approach enables partial load operational lower limit for process units to be considered via the addition of fictitious streams. A simple case study is presented to illustrate the advantages of this modified approach.",
     "keywords": ["P-graph", "Process network synthesis", "Multi-period operation"]},
    {"article name": "Eye gaze movement studies of control room operators: A novel approach to improve process safety",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.09.012",
     "publication date": "02-2016",
     "abstract": "Process industries continue to suffer from accidents despite significant regulatory intervention since the mid-1980s. Human error is widely considered to be the major cause for most accidents today. Detailed analysis of various incidents indicates that reduced staffing levels in control rooms and inadequate operator training with complex automation strategies as common reasons for human errors. Therefore, there is a need to develop deeper understanding of human errors as well as strategies to prevent them. However, similar to hardware failures, traditionally human error has been quantified using likelihood approaches; this viewpoint abnegates the role of the cognitive abilities of the operators. Recent studies in other safety critical domains (aviation, health-care) show that operator's level of situation awareness as inferred by eye tracking is a good online indicator of human error. In this work, a novel attempt is made to understand the behavior of the operator in a typical chemical plant control room using the information obtained from eye tracker. Experimental studies conducted on 72 participants reveal that fixation patterns contain signatures about the operators learning and awareness at various situations. Implications of these findings on human error in process plant operations them are discussed.",
     "keywords": ["Cognitive engineering", "Human error", "Eye tracking", "Process safety"]},
    {"article name": "Robust optimization under correlated uncertainty: Formulations and computational study",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.10.017",
     "publication date": "02-2016",
     "abstract": "The uncertainty set-induced robust optimization framework has received considerable attention in the past decades. It has been extensively studied in literature and applied to address various decision-making problems. However, existing robust optimization methods generally assume that the uncertain parameters are independent. As a result, the traditional robust optimization methods may lead to a conservative solution in practice when correlations between uncertain parameters exist. In this work, we present novel results on robust optimization under correlated uncertainties that appear in a single constraint. Robust counterpart optimization formulations are derived based on various types of uncertain sets. Numerical and application examples are studied to compare the performance of robust optimization by incorporating various levels of correlation. The results demonstrate that incorporating more accurate correlation into the robust optimization formulation can lead to less conservative robust solution.",
     "keywords": ["Robust optimization", "Uncertainty set", "Correlation", "Computational study"]},
    {"article name": "On the linear approximation of mixture internal energies of departure",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.10.013",
     "publication date": "02-2016",
     "abstract": "Direct Monte Carlo simulation of internal energies of departure for binary mixtures of geological interest are gathered and compared to those calculated using a linear mixing rule. Simulation results for gas\u2013oil, oil\u2013oil, and oil\u2013water mixtures show that the linear mixing rule used in the Gibbs\u2013Helmholtz Constrained (GHC) equation of state framework gives accurate approximations of binary mixture internal energies of departure. A flowchart for computing internal energies of departure using Monte Carlo simulation is included along with a sensitivity analysis for the GHC mixture energy parameter with respect to uncertainty in internal energies of departure.",
     "keywords": ["Internal energy of departure", "Direct Monte Carlo simulation", "Linear mixing rule", "Geological binary mixtures", "GHC equation of state"]},
    {"article name": "An augmented Lagrangian interior-point approach for large-scale NLP problems on graphics processing units",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.10.010",
     "publication date": "02-2016",
     "abstract": "The demand for fast solution of nonlinear optimization problems, coupled with the emergence of new concurrent computing architectures, drives the need for parallel algorithms to solve challenging nonlinear programming (NLP) problems. In this paper, we propose an augmented Lagrangian interior-point approach for general NLP problems that solves in parallel on a Graphics processing unit (GPU). The algorithm is iterative at three levels. The first level replaces the original problem by a sequence of bound-constrained optimization problems using an augmented Lagrangian method. Each of these bound-constrained problems is solved using a nonlinear interior-point method. Inside the interior-point method, the barrier sub-problems are solved using a variation of Newton's method, where the linear system is solved using a preconditioned conjugate gradient (PCG) method, which is implemented efficiently on a GPU in parallel. This algorithm shows an order of magnitude speedup on several test problems from the COPS test set.",
     "keywords": ["Nonlinear programming", "Parallel programming", "GPU", "Augmented Lagrangian method", "Interior-point method"]},
    {"article name": "On the effect of sampling rate and experimental noise in the discrimination between microbial growth models in the suboptimal temperature range",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.10.005",
     "publication date": "02-2016",
     "abstract": "Biochemical and microbial processes benefit from mathematical models. Often microbial kinetics are described as a function of environmental conditions in models exploited in predictive microbiology. Based on the organism different model structures are available. However, the aim is to determine the model that describes the system best.This work deals with secondary models describing microbial kinetics in the suboptimal temperature range and their possibility to be discriminated. The used models are the cardinal temperature model with inflection and its adapted version. The method of Optimal Experiment Design for Model Discrimination is used to investigate the practical (in)feasibility of model discrimination given different noise and sampling frequency values.Results point out the required steps and the possibilities of the method for model discrimination. It has been observed that discrimination is possible at various noise and sampling frequency levels. Moreover, also the corresponding increase in required experimental effort has been obtained.",
     "keywords": ["Predictive microbiology", "Model discrimination", "Dynamic modeling", "Optimization", "Optimal experiment design"]},
    {"article name": "Very large scale droplet microfluidic integration (VLDMI) using genetic algorithm",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.10.018",
     "publication date": "02-2016",
     "abstract": "Droplet microfluidics is likely to play a central role in the development of lab-on-a-chip technologies and as a result, significant research is directed toward this field. Understanding the spatiotemporal dynamics of discrete droplets inside microfluidic devices and the design of microfluidic devices for specific tasks are some of the dominant research topics. These works have since resulted in the development of microfluidic devices with functionalities, such as sorting, merging, synchronization, storing etc. However, the anticipated application of microfluidic devices to more complex problems will require more integrated devices that can incorporate the above functionalities on a single chip. In the current work, we present a genetic algorithm optimization-based design tool for discovering very large-scale integration of discrete microfluidic networks for a given objective function. The application of the algorithm is demonstrated through a combinatorial sequencing problem, where the objective is to achieve three different droplet combinatorial sequences for three different droplet types. Multiple fascinating, but nonobvious designs were discovered for this application. It is difficult to imagine such devices being designed using trial and error experimental procedure, which has been the main route for obtaining microfluidic device designs. With advances in technologies for fabrication of microfluidic devices, the current tool can be a significant step toward drastically cutting down on the laborious trial-and-error design process and help in developing droplet microfluidics-based lab-on-a-chip platforms cheaper and faster.",
     "keywords": null},
    {"article name": "Integrating financial risk measures into the design and planning of closed-loop supply chains",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.10.012",
     "publication date": "02-2016",
     "abstract": "In this paper, a mixed integer linear programming (MILP) formulation is proposed that integrates financial risk measures into the design and planning of closed-loop supply chains, considering demand uncertainty of final products. The goal is to maximize the supply chain expected net present value (ENPV), while simultaneously minimizing the associated risk. The augmented \u025b-constraint method is used to generate an approximation to the Pareto-optimal curve for each risk measure. Four different risk measures, most popular measures within the literature, are implemented, compared and directions for their usage by decision makers are discussed. Managerial insights are outlined based in decision makers\u2019 risk profile and goal of the risk minimization. A European supply chain case study is explored.",
     "keywords": ["Supply chain management", "Design", "Planning", "Uncertainty", "Risk management", "MILP"]},
    {"article name": "Optimal model-based aeration control policies in a sequencing batch reactor",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.11.001",
     "publication date": "02-2016",
     "abstract": "We present a non-linear programming formulation for the computation of optimal aeration policies in a sequencing batch reactor for wastewater streams treatment. We assume that organic matter and nitrogen are the main pollutants to be removed to meet water quality targets. The novelty of the work lies in the fact that no binary variables are required to compute the switching time between the aerobic and anoxic stages of the water treatment process leading to a simpler, robust and easier to compute optimization formulation. Moreover, because the control valve, through which air is fed to the reactor, can take either its minimum or maximum bounds as well as any fractional values between such bounds, improved optimal aeration profiles are reported. Such improved profiles mean that shorter processing times are required, compared to previous solutions, leading also to a reduction in the operation cost of the wastewater treatment process. Although the optimal operation policies were computed for a typical home wastewater stream, the optimization formulation can also be extended for the treatment of other polluted streams.",
     "keywords": ["Process optimization", "Dynamic optimization", "Water treatment", "Process control"]},
    {"article name": "A semi-implicit immersed boundary method and its application to viscous mixing",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.10.019",
     "publication date": "02-2016",
     "abstract": "Computational fluid dynamics (CFD) simulations in the context of single-phase mixing remain challenging notably due the presence of a complex rotating geometry within the domain. In this work, we develop a parallel semi-implicit immersed boundary method based on Open\u2207FOAM, which is applicable to unstructured meshes. This method is first verified on academic test cases before it is applied to single phase mixing. It is then applied to baffled and unbaffled stirred tanks equipped with a pitched blade impeller. The results obtained are compared to experimental data and those predicted with the single rotating frame and sliding mesh techniques. The proposed method is found to be of comparable accuracy in predicting the flow patterns and the torque values while being straightforwardly applicable to complex systems with multiples impellers for which the swept volumes overlap.",
     "keywords": ["Computational fluid dynamics", "Immersed boundary method", "Single rotating frame technique", "Sliding mesh technique", "Mixing", "Open\u2207FOAM"]},
    {"article name": "The advantage of using external financing (leverage) to design/build/operate a new chemical plant",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.10.015",
     "publication date": "02-2016",
     "abstract": "This research note includes a significant theoretical extension and minor errata for an earlier publication [Mellichamp, DA. New discounted cash flow method: estimating plant profitability at the conceptual design level while compensating for business risk/uncertainty. CACE 2013; 48:251\u201363.]. A closed-form theoretical expression is developed that provides a direct estimate of the financial advantage to be obtained by using outside financing rather than internal (enterprise) funds to build a chemical plant. Emphasis is at the conceptual design level, where the reduction in financial profitability (ROIBT) required to justify further work on a project is developed in terms of the financial parameters (enterprise rate, construction rate, bond rate, etc.). An unexpected outcome is that the reduction in required profitability is independent of any specified risk cushion (NPV%) or long-term profitability (NPV); it is solely a function of background financial market rates and project internal timing assumptions vis-a-vis the enterprise\u2019 historic rate of return.",
     "keywords": ["Plant conceptual design", "Profitability/risk", "Discounted cash flow", "Net present value", "Financing/leveraging"]},
    {"article name": "Synthesis and optimisation of an integrated water and membrane network framework with multiple electrodialysis regenerators",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.11.005",
     "publication date": "02-2016",
     "abstract": "The shrinking supplies of freshwater globally, coupled with strict environmental regulations, have driven the manufacturing industry towards sustainable water management for the minimisation of freshwater intake and wastewater generation. By using process integration and its enabling tools, this work considers the synthesis of an optimal water network with multiple regeneration capabilities. Development of the proposed framework is achieved by embedding a subnetwork of detailed electrodialysis models within a water network. Based on a superstructure and fixed flowrate, the optimisation problem is formulated as an MINLP model and solved in GAMS/DICOPT. To demonstrate the applicability of the proposed mathematical model a literature case study on a pulp and paper plant is presented and the results indicate a potential of 12% savings in freshwater intake, 16% reduction in wastewater generated and a 14% saving in the total annualised cost for the entire network.",
     "keywords": ["Sustainable", "Synthesis", "Optimisation", "Electrodialysis"]},
    {"article name": "Symmetry breaking in MILP formulations for Unit Commitment problems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.11.004",
     "publication date": "02-2016",
     "abstract": "This paper addresses the study of symmetry in Unit Commitment (UC) problems solved by Mixed Integer Linear Programming (MILP) formulations, and using Linear Programming based Branch & Bound MILP solvers. We propose three sets of symmetry breaking constraints for UC MILP formulations exhibiting symmetry, and its impact on three UC MILP models are studied. The case studies involve the solution of 24 instances by three widely used models in the literature, with and without symmetry breaking constraints. The results show that problems that could not be solved to optimality within hours can be solved with a relatively small computational burden if the symmetry breaking constraints are assumed. The proposed symmetry breaking constraints are also compared with the symmetry breaking methods included in two MILP solvers, and the symmetry breaking constraints derived in this work have a distinct advantage over the methods in the MILP solvers.",
     "keywords": ["Scheduling", "Symmetry breaking", "MILP", "Unit Commitment"]},
    {"article name": "Ontology evaluation for reuse in the domain of Process Systems Engineering",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.12.003",
     "publication date": "02-2016",
     "abstract": "Ontologies are a useful tool for knowledge representation, sharing and reuse. Although the number of available ontologies is increasing, the concomitant reuse activities are not following respectively. This is particularly true in the domain of Process Systems Engineering where the ontology development has been proven to be a challenging task and respective reusability is at its infancy. This paper presents a framework for evaluation of ontology for reuse. The proposed framework benefits from information about ontologies, such as terminology and ontology structure, to calculate a compatibility metric of ontology suitability for reuse and hence integration. The framework was demonstrated using a Chemical and Process Engineering case.",
     "keywords": ["Ontology reuse", "Compatibility", "Ontology engineering"]},
    {"article name": "Operational optimisation of centrifugal compressors in multilevel refrigeration cycles",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.11.006",
     "publication date": "02-2016",
     "abstract": "Low-temperature energy systems are processes that require cooling at temperatures below ambient, which are accomplished using refrigeration cycles. Little research has addressed the operational optimisation of refrigeration cycles considering the performance of existing equipment. This work develops a methodology for operational optimisation of refrigerated processes, taking into account existing centrifugal compressors. For the optimisation of multilevel cycles, the evaporation temperatures of each level are varied to find a set of operating conditions that minimise shaft work demand. The optimisation takes into account equipment constraints, including compressors on a common shaft, minimum and maximum allowable inlet flow rates, etc. Two examples are presented; the first represents a three-level refrigeration cycle and the second a cascade cycle. For the two examples, the conditions of the base case are optimised, identifying improvements of around 3% in shaft work demand. In addition, both cycles were also optimised for a range of process cooling demands.",
     "keywords": ["Refrigeration cycles", "Propane precooled C3-MR cycle", "LNG"]},
    {"article name": "Steam system network synthesis with hot liquid reuse: II. Incorporating shaft work and optimum steam levels",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.10.016",
     "publication date": "02-2016",
     "abstract": "In this first of a series of two papers, the effects of varying steam levels on the total steam flowrate are analyzed mathematically for the traditional parallel configuration as well as for the case of hot liquid reuse. It is demonstrated that in the case of parallel heat exchangers utilizing only latent heat, a minimum total steam flowrate is obtained by optimally selecting steam levels, but that in the case of hot liquid reuse, introducing multiple steam levels increases the minimum total steam flowrate attainable under those conditions. The flowrate attained utilizing hot liquid reuse, however, remains lower than when only utilizing latent heat. It is concluded that the lowest steam flowrate is attained using hot liquid reuse and only a single level of steam, but that the presence of additional steam levels resulting from turbines requires a more holistic approach to the synthesis of steam networks.",
     "keywords": ["Heat integration", "Steam network synthesis", "Steam system optimization", "Steam flowrate minimization"]},
    {"article name": "Steam system network synthesis with hot liquid reuse: I. The mathematical model for steam level selection",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.10.014",
     "publication date": "02-2016",
     "abstract": "In this first of a series of two papers, the effect of varying steam levels on the total steam flowrate within steam systems is analysed mathematically for the traditional parallel configuration as well as for the case of hot liquid reuse. It is demonstrated that in the case of parallel heat exchangers utilising only latent heat, a minimum total steam flowrate is obtained by optimally selecting steam levels, but that in the case of hot liquid reuse, introducing multiple steam levels increases the minimum total steam flowrate attainable under those conditions. The flowrate attained utilising hot liquid reuse, however, remains lower than when only utilising latent heat. It is concluded that the lowest steam flowrate is attained using hot liquid reuse and only a single level of steam, but that the presence of additional steam levels resulting from turbines requires a more holistic approach to the synthesis of steam networks.",
     "keywords": ["Heat integration", "Steam flowrate minimization", "Steam utilization"]},
    {"article name": "MILP reformulations for the design of biotechnological multi-product batch plants using continuous equipment sizes and discrete host selection",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.08.001",
     "publication date": "01-2016",
     "abstract": "In this article we present a new approach, relying on mixed-integer linear programming (MILP) formulations, for the design of multi-product batch plants with continuous sizes for processing units and host selection. The main advantage of the proposed approach is its scalability, that allows us to solve, within reasonable precision requirements, realistic instances. Furthermore, we show that many other alternatives are either numerically unstable (for the problem sizes that we are interested in), unable to solve large instances, or much slower than the proposed method. We present extensive computational experiments, which show that we are able to solve almost all tested instances, and, in average, we are ten times faster than alternative approaches. As we use a high level implementation language (AMPL) we should get further time improvements if lower level implementations are used (C, C++).Reproducibility of our results can be tested using our models and data available on-line at BPLIB.1",
     "keywords": ["Multi-product batch plant", "MINLP", "MILP", "Production path"]},
    {"article name": "Timed-automata based method for synthesizing diagnostic tests in batch processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.08.007",
     "publication date": "01-2016",
     "abstract": "Hardware failures are inevitable but random events in the useful life of any batch chemical plant. If these incidents are not efficiently diagnosed, the consequences can be very serious. In general, two design measures may be implemented offline to enhance the overall diagnostic performance, i.e., installing sensors and/or stipulating test plans for online implementations. Since the former has already been studied extensively, the present study focuses only upon the latter. In a recent work, Kang and Chang (2014) proposed an effective method to conjecture diagnostic tests using the untimed automata. However, due to a lack of time-tracking mechanisms, the failure-induced behaviours cannot always be characterized adequately with such models. A systematic procedure-synthesis strategy is therefore developed in the present study by making use of the timed automata and the model-checking capabilities of existing software, e.g., UPPAAL (Behrmann et al., 2006). All component models are first constructed, and all possible fault propagation scenarios and their observable event traces (OETs) are next enumerated exhaustively. The optimal test plan for every OET can then be established by generating the supervisory controller to improve diagnostic resolution. Extensive case studies have also been carried out in this work to confirm the validity and effectiveness of the proposed approach.",
     "keywords": ["Timed automata", "Diagnostic test plans", "Model-checking tools", "Batch processes"]},
    {"article name": "Comparison of Monte Carlo and quasi-Monte Carlo technique in structure and relaxing dynamics of polymer in dilute solution",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.08.014",
     "publication date": "01-2016",
     "abstract": "Structure and dynamics of polymer in solvent solution is an important area of research since the functional properties of polymer are largely dependent on the morphology of the polymers in solution. This structure related properties are especially important in case of surface science where the phase-separated morphology in the micro/nano scale dictates the properties of the product. Modeling polymers in solution is an efficient way to determine the morphology and thus the properties of the products. It saves time as well as helps to design novel materials with desired properties. Polymers in solution systems are generally modeled with bead spring model and Monte Carlo or importance sampling Monte Carlo simulations is used to find the optimal configuration where the energy of the system is minimized. Often in these simulations, random numbers are used in the Monte Carlo steps. Normally random numbers try to form clusters and do not cover the entire dimension of the system. Thus the minimum energy structures obtained from simulations with random numbers are not optimal configuration of the system. In the present work a lattice-based model is used for polymer solution system and importance sampling Monte Carlo is used for simulation. Quasi-random numbers generated from Hammersley sequence sampling (HSS) are used in the simulation steps for stochastic selection polymers and its movements. Quasi-random numbers obtained from HSS are random in nature and they have n-dimensional uniformity. They do not form clusters and the structural configuration obtained using quasi-random numbers are optimal in nature. The optimal configurations of the polymers as obtained from random number and quasi-random number are compared. The result shows that simulation with HSS attains a lower energy state after initial quench. At the late stage of spinodal decomposition, the structure factor decrease-showing Ostwald ripening which is not observed from simulation with random numbers.",
     "keywords": ["Monte Carlo", "Quasi-Monte Carlo", "Hammersley sequence sampling", "Polymer morphology"]},
    {"article name": "In situ adaptive tabulation (ISAT) to accelerate transient computational fluid dynamics with complex heterogeneous chemical kinetics",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.08.020",
     "publication date": "01-2016",
     "abstract": "This paper extends the in situ adaptive tabulation (ISAT) algorithm for accelerating the simulation of complex heterogeneous chemical kinetics within transient, three-dimensional, computational fluid dynamics (CFD). The ISAT algorithm, initially developed for homogeneous combustion kinetics, takes advantage of the fact that initial conditions for the chemistry in a particular cell (i.e., temperature and composition) may have been present in this cell or another cell earlier in the simulation. In such cases, the solution can be extracted from a tabulation of prior solutions more efficiently than solving the local kinetics problem. The ISAT algorithm uses efficient tabulation and retrieval algorithms, greatly accelerating the solution process. Illustrative results are based on the simulation of methane reforming in a catalytic microchannel reactor, considering coupled fluid mechanics, catalytic chemistry, and conjugate heat transfer.",
     "keywords": ["In situ adaptive tabulation (ISAT)", "Heterogeneous kinetics", "Catalysis", "Computational fluid dynamics", "Microchannel reformer"]},
    {"article name": "CFD modelling of a mixing chamber for the realisation of functionally graded scaffolds",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.08.021",
     "publication date": "01-2016",
     "abstract": "Biological tissues are characterised by spatially distributed gradients, intricately linked with functions. It is widely accepted that ideal tissue engineered scaffolds should exhibit similar functional gradients to promote successful tissue regeneration. Focusing on bone, in previous work we proposed simple methods to obtain osteochondral functionally graded scaffolds (FGSs), starting from homogeneous suspensions of hydroxyapatite (HA) particles in gelatin solutions. With the main aim of developing an automated device to fabricate FGSs, this work is focused on designing a stirred tank to obtain homogeneous HA\u2013gelatin suspensions. The HA particles transport within the gelatin solution was investigated through computational fluid dynamics (CFD) modelling. First, the steady-state flow field was solved for the continuous phase only. Then, it was used as a starting point for solving the multi-phase transient simulation. CFD results showed that the proposed tank geometry and setup allow for obtaining a homogeneous suspension of HA micro-particles within the gelatin solution.",
     "keywords": ["Computational fluid dynamics", "Stirred tank", "Particle suspension", "Functionally graded scaffolds", "Tissue engineering"]},
    {"article name": "A new model and a reformulation for the crude distillation unit charging problem with oil blends and sequence-dependent changeover costs",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.08.009",
     "publication date": "01-2016",
     "abstract": "In this paper, we address the problem of planning the crude distillation unit charging process with oil blend. It is well known that blending and splitting operations can lead together to both non-linearities and concavities in mathematical programming models. As result, many proposed models for this problem use simplifying assumptions to keep the formulation computationally tractable. However, we show the existence of splitting operations that can lead to inconsistencies in the solutions obtained by the previous MILP models from the literature. Then, we propose a way to address this issue through an aggregated inventory capacity combined with a disaggregation algorithm. Furthermore, we develop a mathematical reformulation that improves the solving efficiency of the method. Then, we report experiments that show that the reformulated MILP model presents significant gains concerning linear relaxation gaps and run times, and the disaggregation algorithm leads to feasible solutions for all the tested instances.",
     "keywords": ["Crude oil supply", "Sequencing", "Blend", "Mathematical formulation"]},
    {"article name": "Optimal design of integrated agricultural water networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.08.006",
     "publication date": "01-2016",
     "abstract": "This paper presents a mathematical programming model for the optimal design of water networks in the agriculture. The proposed model is based on a new superstructure that includes all configurations in terms of use, reuse and regeneration of water in a field constituted by a number of croplands. The model also includes the allocation of pipelines, pumps and storage tanks in different irrigation periods. The objective function consists in maximizing the annual profit that is formed by the economic incomes owing to the crop sell minus the costs for fresh water, fertilizer, storage tanks, treatment units, piping and pumping. The proposed multi-period optimization problem is formulated as a mixed integer non-linear programming formulation, which was applied to a case study to demonstrate the economic, environmental and social benefits that can be obtained.",
     "keywords": ["Water integration", "Fertilizer reuse", "Agricultural water", "Multi-period optimization"]},
    {"article name": "Optimal strategies for transitions in simulated moving bed chromatography",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.08.004",
     "publication date": "01-2016",
     "abstract": "Simulated moving bed chromatography (SMBC) has emerged as a significant separation technology in the process industry. SMB operating parameters are chosen to satisfy various performance objectives such as maximization of purity or productivity and the choice of the objective is generally guided by process economics. From an industrial perspective, the SMB must be operated flexibly, so that the same unit can be operated to satisfy different objectives. Transiting from one objective to another entails large transition periods, resulting in an economic loss. We propose use of optimal transitions as an approach to minimizing transition time, reducing use of feed and desorbent during transition as well as reduction in off-specification product relative to a non-optimal, step change approach. Optimal transitions can also be used in recovering from feed upset scenarios. The above methods are demonstrated using simulations on a benchmark SMBC process for separation of glucose and fructose using Ca++ exchange resin.",
     "keywords": ["Simulated moving bed", "Chromatography", "Nonlinear programming", "Optimal transitions"]},
    {"article name": "Modeling and simulation of VMD desalination process by ANN",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.08.019",
     "publication date": "01-2016",
     "abstract": "In this work, an artificial neural network (ANN) model based on the experimental data was developed to study the performance of vacuum membrane distillation (VMD) desalination process under different operating parameters such as the feed inlet temperature, the vacuum pressure, the feed flow rate and the feed salt concentration. The proposed model was found to be capable of predicting accurately the unseen data of the VMD desalination process. The correlation coefficient of the overall agreement between the ANN predictions and experimental data was found to be more than 0.994. The calculation value of the coefficient of variation (CV) was 0.02622, and there was coincident overlap between the target and the output data from the 3D generalization diagrams. The optimal operating conditions of the VMD process can be obtained from the performance analysis of the ANN model with a maximum permeate flux and an acceptable CV value based on the experiment.",
     "keywords": ["Vacuum membrane distillation", "Desalination", "Artificial neural network", "Simulation", "Modeling"]},
    {"article name": "Fault diagnosis of chemical processes with incomplete observations: A comparative study",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.08.018",
     "publication date": "01-2016",
     "abstract": "An important problem to be addressed by diagnostic systems in industrial applications is the estimation of faults with incomplete observations. This work discusses different approaches for handling missing data, and performance of data-driven fault diagnosis schemes. An exploiting classifier and combined methods were assessed in Tennessee\u2013Eastman process, for which diverse incomplete observations were produced. The use of several indicators revealed the trade-off between performances of the different schemes. Support vector machines (SVM) and C4.5, combined with k-nearest neighbourhood (kNN), produce the highest robustness and accuracy, respectively. Bayesian networks (BN) and centroid appear as inappropriate options in terms of accuracy, while Gaussian na\u00efve Bayes (GNB) is sensitive to imputation values. In addition, feature selection was explored for further performance enhancement, and the proposed contribution index showed promising results. Finally, an industrial case was studied to assess informative level of incomplete data in terms of the redundancy ratio and generalize the discussion.",
     "keywords": ["Fault diagnosis", "Missing data", "Incomplete observations", "Classification", "Imputation", "Machine learning"]},
    {"article name": "Operational optimization of networks of compressors considering condition-based maintenance",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.08.008",
     "publication date": "01-2016",
     "abstract": "The paper presents a mixed integer linear programming model which deals with the optimal operation and maintenance of networks of compressors of chemical plants. This optimization model considers condition-based maintenance which involves the degradation of the condition of the compressors. The paper focuses on online and offline washing, two different cleaning procedures which reduce the extra power used by the compressors due to fouling. The state-of-the-art has demonstrated the optimal schedule of the maintenance of a single compressor neglecting the interactions between operation and maintenance of more than one compressor. The suggested optimization model studies a compressor station with multiple compressors and provides their optimal schedule and the best decisions for their washing. Different case scenarios examine the influence of different types of washing methods on the total costs of operation and maintenance. The paper demonstrates the benefits of the optimization and demonstrates that maintenance and operation have to be examined simultaneously and not separately, in contrast to common industrial practice and previous approaches in the literature.",
     "keywords": ["Mixed integer programming", "Condition based maintenance", "Washing schedule", "Compressor washing", "Fouling"]},
    {"article name": "Population balance discretization for growth, attrition, aggregation, breakage and nucleation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.08.011",
     "publication date": "01-2016",
     "abstract": "This paper presents a new discretization method to solve one-dimensional population balance equations (PBE) for batch and unsteady/steady-state continuous perfectly mixed systems. The numerical technique is valid for any size change mechanism (i.e., growth, aggregation, attrition, breakage and nucleation occurring alone or in combination) and different discretization grids.The developed strategy is based on the moving pivot technique of Kumar and Ramkrishna and the cell-average method of Kumar et al. A novel contribution is proposed to numerically handle the growth and attrition terms, for which a new representation of the number density function within each size class is developed. This method allows describing the number particle fluxes through the class interfaces accurately by preserving two sectional population moments.By comparing the numerical particle size distributions with analytical solutions of one-dimensional PBEs (including different size change mechanisms and particle-size dependent kinetics), the accuracy of the proposed numerical method was proved.",
     "keywords": ["Population balance equation", "Particles size distribution", "Discretization method", "Size change mechanisms"]},
    {"article name": "Spatio-temporal frequency response analysis of forced slip velocity effect on solute concentration oscillations in a reverse osmosis membrane channel",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.08.016",
     "publication date": "01-2016",
     "abstract": "A spatio-temporal frequency response analysis is developed based on the multi-dimensional Fourier transform, which decomposes spatio-temporal input and output signals into travelling waves of different spatial wavenumbers and temporal frequencies. A spatio-temporal pulse test is also developed to allow for simultaneous input of multiple wavenumber\u2013frequency combinations. This analysis is applied to a membrane channel, where the input is an effective streamwise fluid velocity at the wall that varies along the membrane length and in time. The combined effect of the temporal frequencies and spatial wavenumbers of the slip velocity on the resulting solute concentration oscillations, which have the potential for reducing the susceptibility of the membrane to fouling, is analysed at Reynolds numbers of 280 and 560, and Schmidt numbers of 600 and 1200. Frequencies up to 800\u00a0Hz and wavenumbers up to 500\u00a0m\u22121 are studied. It is found that for each wavenumber there is a corresponding temporal frequency that results in a maximum amplitude ratio. Developing a spatio-temporal frequency response profile for such a system helps the design of control input profiles that decrease fouling in membrane systems.",
     "keywords": ["Spatio-temporal frequency response", "Fourier transforms", "Boundary condition control", "Forced slip velocity", "Concentration polarisation"]},
    {"article name": "Model predictive control with non-uniformly spaced optimization horizon for multi-timescale processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.08.010",
     "publication date": "01-2016",
     "abstract": "Many chemical processes exhibit disparate timescale dynamics with strong coupling between fast, moderate and slow variables. To effectively handle this issue, a model predictive control (MPC) scheme with a non-uniformly spaced optimization horizon is proposed in this paper. This approach implements the time intervals that are small in the near future but large in the distant future, allowing the fast, moderate and slow dynamics to be included in the optimization whilst reducing the number of decision variables. A sufficient condition for ensuring stability for the proposed MPC is developed. The proposed approach is demonstrated using a case study of an industrial paste thickener control problem. While the performance of the proposed approach remains similar to a conventional MPC, it reduces the computational complexity significantly.",
     "keywords": ["Model predictive control", "Non-uniformly spaced optimization horizon", "Multi-timescale processes", "Stability", "Dissipativity"]},
    {"article name": "Numerical simulations of gas\u2013liquid flow in thermal sorption processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.09.003",
     "publication date": "01-2016",
     "abstract": "Thermal storage systems, used, e.g., for domestic heating, must be able to compensate the mismatch between supply and demand. The most efficient techniques for thermal storage are based on sorption storage processes. Usually in sorption, the adsorption process occurs in combination with a solid state adsorbent, whereas absorption takes place in a liquid/gas system. During such sorption processes the flow behavior of the carrier medium is crucial for the efficiency of a falling film absorber. In this work the hydrodynamics of the falling liquid film in two geometrical setups, namely on an inclined plane and over two horizontal parallel tubes, is studied. For the simulation the Eulerian\u2013Eulerian model of the software ANSYS CFX and the interFoam application of the open source software OpenFOAM were used. The numerical results of the two geometries were compared with each other and also with existing data from literature to predict the performance of a sorption storage regarding the specific wetted area and the needed height for gravity driven film absorption.",
     "keywords": ["Numerical simulation", "Two-phase flow", "Volume of fluid", "Liquid film", "Wetted area"]},
    {"article name": "Fleet sizing in chemical supply chains using agent-based simulation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.08.015",
     "publication date": "01-2016",
     "abstract": "Fleet sizing is an essential element of chemical supply chain management. It is expensive to purchase and maintain tank cars. Further, the sizing decision is not an isolated one and is closely linked to fleet routing, inventory management and other aspects of logistics and supply chain management. Traditional methods to determine the size of fleets rely on coarse models of the supply chain. In this paper, we present a detailed agent-based simulation model of a multisite chemical supply chain that brings out the intricacies of the tank car fleet sizing problem. The model explicitly takes into account the independent decision making of the different entities as well as the interactions across operations such as replenishment planning and order assignment. Our simulation studies show that different fleet sizes and routing policies can have significant impact on the overall performance of the supply chain including on customer satisfaction and plant performance.",
     "keywords": ["Supply chain management", "Fleet sizing", "Simulation", "Agent-based model", "Optimization", "Specialty chemicals"]},
    {"article name": "Optimal design for flexible operation of the post-combustion CO2 capture plant with uncertain economic factors",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.09.002",
     "publication date": "01-2016",
     "abstract": "The ultimate benefit of flexible operation of the post-combustion CO2 capture (PCC) plant depends on the ability to optimally balance between many competing factors, including the additional capital investment and operating cost savings. In this work, a large number of scenarios are constructed by considering combinations of possible realizations of the uncertain economic factors such as energy cost profile, emission penalty and value of captured CO2. Then, the design choices like the size of the storage tanks and the regeneration capacity are optimized by minimizing an overall cost averaged over all the scenarios. The optimal design problem is naturally formulated as a two-stage stochastic program. This multi-scenario optimal design is compared with the design that minimizes the overall cost for just a single nominal scenario as well as the design that minimizes the cost averaged over the worst-case scenarios.",
     "keywords": ["Post-combustion CO2 capture", "Flexible operation", "Optimal design", "Uncertainties", "Two-stage stochastic programming"]},
    {"article name": "Price model of electrical energy for PSE applications",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.08.013",
     "publication date": "01-2016",
     "abstract": "The electrical energy (EE) price plays a significant role in the economic assessment of industrial processes. PSE/CAPE applications are often based on economic evaluations/optimizations where EE price and its possible dynamic evolution are important input data. Planning, scheduling, on-line optimization (dynamic) conceptual design, and feasibility studies are some of the applications where short-, medium-, and long-term predictions of EE quotations are involved. The paper discusses the main issues that contribute to EE quotations such as geographical, meteorological, seasonal, political, social, and financial terms. EE prices show a significant dependency on crude oil quotations with a time-delay of about a quarter/season. An econometric model comprising both linear and periodic components with an implicit stochastic term delivered by the reference component is first commented, then identified, and finally validated respect to real EE quotations. The paper provides also a discussion on how to forecast EE prices ranging from short- to long-term horizons.",
     "keywords": ["Electrical energy price", "Econometric model", "Industrial processes", "Price forecast", "Short-, long-term predictions", "Crude oil quotations"]},
    {"article name": "Numerical simulation of gas-diffusion-electrodes with moving gas\u2013liquid interface: A study on pulse-current operation and electrode flooding",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.09.005",
     "publication date": "01-2016",
     "abstract": "In gas-diffusion-electrodes of electrochemical systems, the interface between gas and liquid electrolyte can move with operation time. It is challenging to mathematically assess moving interfaces, especially if the transient and spatial distribution of species are of interest. We mathematically model a gas-diffusion-electrode and apply a finite volume method with moving grid to solve the model equations. The step-size of the finite volume method is coupled to the moving gas\u2013liquid interface, which is coupled to the current density applied. In detail, we study pulse-current operation and flooding of the electrode, and investigate parameters that influence the oxygen distribution in the electrode. The results obtained emphasize the benefit of the moving grid method applied. This method is able to assess the accurate diffusion resistance for oxygen in the gas-diffusion-electrode. Based on this work, possible limitations of gas-diffusion-electrodes can be derived for their usage in metal air batteries.",
     "keywords": ["Mathematical model", "Gas-diffusion-electrode", "Finite volume method", "Moving grid", "Zinc air battery", "Metal air battery"]},
    {"article name": "Optimization of naphtha purchase price using a price prediction model",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.08.012",
     "publication date": "01-2016",
     "abstract": "In order to meet company needs, various models of naphtha price forecasting and optimization models of average naphtha purchase price have been developed. However, these general models are limited in their ability to predict future trends as they only include quantitative data. Furthermore, naphtha price predictions based on fluctuation trends have not been published in the literature. Thus, we developed a system dynamics (SD) model considering time-series data, mathematical formulations, and qualitative factors. The results obtained from our model were compared with the published literature. The best result of the SD is the European naphtha forecasting price model, and the forecasting accuracy percentage shows 92.82%. Furthermore, a nonlinear programming (NLP) model was developed to optimize the purchase price by considering the naphtha price of the forecasting models. In addition, the average optimization value was approximately 45.07\u00a0USD/ton cheaper than that of the heuristic approach.",
     "keywords": ["Purchase price optimization", "Artificial neural network", "Forecasting model", "System dynamics", "Heuristics"]},
    {"article name": "Global optimization of multiphase flow networks using spline surrogate models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.08.022",
     "publication date": "01-2016",
     "abstract": "A general modelling framework for optimization of multiphase flow networks with discrete decision variables is presented. The framework is expressed with the graph and special attention is given to the convexity properties of the mathematical programming formulation that follows. Nonlinear pressure and temperature relations are modelled using multivariate splines, resulting in a mixed-integer nonlinear programming (MINLP) formulation with spline constraints. A global solution method is devised by combining the framework with a spline-compatible MINLP solver, recently presented in the literature. The solver is able to globally solve the nonconvex optimization problems. The new solution method is benchmarked with several local optimization methods on a set of three realistic subsea production optimization cases provided by the oil company BP.",
     "keywords": ["Nonlinear flow networks", "Petroleum production optimization", "Mixed-integer nonlinear programming", "Branch-and-bound", "Splines"]},
    {"article name": "Human immunomodulation and initial HIV spread",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.09.004",
     "publication date": "01-2016",
     "abstract": "We use published data in order to build up a networked mathematical framework aiming at providing: (a) a predictive understanding on how distinct stressors and immunosenescence may potentially affect the natural inflammatory response mechanisms and (b) new insights to developing early interventions that seek to exploit natural immune processes. Relying on fifty fundamental assumptions on HIV immunopathogenesis, the model simulations suggested that: (a) from a translational perspective, forthcoming physiological transitions in the systemic inflammation process do exist with conditions for bifurcation between the uninfected and the infected state being seriously impacted by immunomodulation and (b) the required therapy efficacy for pre-exposure prophylaxis may be decisively affected by immunomodulation and by the drug class used. Whereas unsuitable to make quantitative predictions due to limited experimental data and the complexity in vivo, this modeling effort paves the way for assessing the impact of personalized medicine for global epidemics within complex systems thinking.",
     "keywords": ["Ab antibody", "antibody", "AHI acute HIV infection", "acute HIV infection", "AIDS acquired immunodeficiency syndrome", "acquired immunodeficiency syndrome", "APC antigen-presenting cell", "antigen-presenting cell", "CTE critical therapy efficiency", "critical therapy efficiency", "DC dendritic cell", "dendritic cell", "FDC follicular dendritic cell", "follicular dendritic cell", "Fe iron", "iron", "GLN genital lymph node", "genital lymph node", "HAART highly active antiretroviral therapy", "highly active antiretroviral therapy", "HIV human immunodeficiency virus", "human immunodeficiency virus", "IL-2 interkeukin-2", "interkeukin-2", "IL-10 interkeukin-10", "interkeukin-10", "IL-12 interkeukin-12", "interkeukin-12", "IFN-\u03b3 interferon-\u03b3", "interferon-\u03b3", "LP lamina propria", "lamina propria", "LT lymphoid tissues", "lymphoid tissues", "RTI reverse transcriptase inhibitor", "reverse transcriptase inhibitor", "NK natural killer (cell)", "natural killer (cell)", "O.E. old elderly", "old elderly", "PB peripheral blood", "peripheral blood", "PI protease inhibitor", "protease inhibitor", "PVL peak viral load", "peak viral load", "PVT peak viremia time", "peak viremia time", "RNA ribonucleic acid", "ribonucleic acid", "Se selenium", "selenium", "SPT set-point time", "set-point time", "SPVL set-point viral load, T4, helper CD4+ T-cell", "set-point viral load, T4, helper CD4+ T-cell", "T4SP set point CD4+ T-cell count", "set point CD4+ T-cell count", "T8 CD8+ T-cell", "CD8+ T-cell", "Tr regulatory CD4+ T-cell", "regulatory CD4+ T-cell", "VA vitamin A", "vitamin A", "Zn zinc", "zinc", "Y.E. young elderly", "young elderly", "AIDS", "Immune system", "Infectious disease", "Nutrition", "Prophylaxis", "Therapy"]},
    {"article name": "Comprehensive Fischer\u2013Tropsch reactor model with non-ideal plug flow and detailed reaction kinetics",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.08.017",
     "publication date": "01-2016",
     "abstract": "This paper presents a detailed first principle Fischer\u2013Tropsch reactor model including detailed heat transfer calculations and detailed reaction kinetics. The model is based on a large number of components and chemical reactions. The model is tuned to a fixed bed nearplug flow reactor but can also be applied to slurry and micro-channel reactors.The presented model is based on a cascade of ideally stirred reactors. This modelling approach is novel for Fischer\u2013Tropsch reactors and has the advantage of being able to represent none-ideal reactors. Using a large number of components and reactions makes it possible to better represent the product slate than with conventional modelling based on distribution models.The results of the simulations emphasise that temperature control is important. Global conversion and product yields are dependent on operating conditions especially the temperature. The model is used to calculate the dimensions of an industrial reactor from a laboratory scale reactor.",
     "keywords": ["ASF Anderson Flory Schultz", "Anderson Flory Schultz", "BtL biomass to liquids", "biomass to liquids", "CSTR continuously stirred tank reactor", "continuously stirred tank reactor", "CtL coal to liquids", "coal to liquids", "GtL gas to liquids", "gas to liquids", "SRK Soave Redlich Kwong", "Soave Redlich Kwong", "Fischer\u2013Tropsch", "Reactor", "Kinetics", "Simulation", "Heat transfer"]},
    {"article name": "Techno-economic assessment of CO2 bio-fixation using microalgae in connection with three different state-of-the-art power plants",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.09.001",
     "publication date": "01-2016",
     "abstract": "Large-scale microalgae cultivations for CO2 bio-sequestration from power plants can be an alternative process to conventional technologies if the capital investments associated with carbon capture, transport and storage can be reduced or avoided. In order to counterbalance the costs required to operate massive microalgae cultivations, it is necessary to create additional revenues through biomass sales. This manuscript examines the techno-economics of microalgae cultivations for the integration with three power plant technologies using an artificial neural network model. The economics are estimated using the net present value approach. The assessment is carried out at photosynthesis efficiencies ranging from 2% to 6%. The sensitivity assessment shows microalgae selling prices in the range of $440\u20131028/t at a photosynthetic efficiency of 4% for low and high cost scenarios in order to achieve an electricity price similar to that from a conventional power plant without a CO2 capture and storage.",
     "keywords": ["CO2 bio-fixation", "Microalgae cultivation", "Photosynthetic efficiency", "Solar radiation", "Artificial neural networks"]},
    {"article name": "Polygeneration of hydrogen and power based on coal gasification integrated with a dual chemical looping process: Thermodynamic investigation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.09.010",
     "publication date": "01-2016",
     "abstract": "This paper assesses, from a thermodynamic perspective, the conversion of coal to power and hydrogen through gasification simultaneously with a dual chemical looping processes, namely chemical looping air separation (CLAS) and water\u2013gas shift with calcium looping CO2 absorption (WGS-CaL). CLAS offers an advantage over other mature technologies in that it can significantly reduce its capital cost. WGS-CaL is an efficient method for hydrogen production and CO2 capture. The three major factors, oxygen to coal (O/C), steam to coal (S/C) and CaO to coal (Ca/C) were analyzed. Moreover, the comparisons of this suggested process and the traditional processes including integrated gasification combined cycle (IGCC), integrated gasification combined cycle with carbon capture and storage (IGCC-CCS) and integrated gasification combined cycle with calcium-based chemical looping (IGCC-CaL) were discussed. And, the exergy destruction analysis of this suggested process has also been calculated.",
     "keywords": ["Polygeneration of hydrogen and power", "Coal gasification", "Dual chemical looping process", "Simulation"]},
    {"article name": "Integration of wind, solar and biomass over a year for the constant production of CH4 from CO2 and water",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.09.006",
     "publication date": "01-2016",
     "abstract": "In this paper we optimize the combination of biomass, wind and solar energy for the constant production of synthetic methane. Biomass is used for the production of power and/or hydrogen. Photovoltaic solar and wind energy are used to obtain power. Water is electrolyzed generating oxygen and hydrogen, which is used to synthesize methane with CO2. The model is formulated as an MINLP. The optimization suggests the production of power using solar energy complemented with biomass. Biomass is processed using indirect gasification and steam reforming. The hydrogen is produced from water electrolysis. The investment and production costs are 175\u00a0M\u20ac and 0.38\u00a0\u20ac/Nm3 respectively. A sensitivity analysis shows that biomass is preferred for prices and investment below 50\u00a0\u20ac/t and 1500\u00a0\u20ac/kW. Solar energy is used for high cost of biomass if solar incidence is above 1200\u00a0kWh/m2\u00a0yr. Wind use is restricted to low solar incidence and wind velocities above 9\u00a0m/s.",
     "keywords": ["Solar energy", "Biomass", "Wind power", "Synthetic methane", "Process integration", "Hydrogen"]},
    {"article name": "Reconstruction of a distribution from a finite number of its moments: A comparative study in the case of depolymerization process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.09.008",
     "publication date": "01-2016",
     "abstract": "The resolution of the population balance equation (PBE) using moment-based methods offers a high computational efficiency however, information on the time evolution of the probability density function (PDF) is out of reach. For this, several PDF reconstruction methods using a finite number of moments are proposed in the literature. In this contribution, three different methods (i.e. beta kernel density function based method, spline based technique and the maximum entropy based approach) are tested and compared to the analytical solution of a depolymerization process. The maximum entropy method gives the most accurate approximations using only a set of six moments. This method is combined with the quadrature method of moments (QMOM) for a simultaneous reconstruction during the PBE resolution. A three nodes and a four nodes quadrature are tested. The results show that the quality of the reconstruction is highly dependent on the accuracy of the computed moments.",
     "keywords": ["Moment problem", "Population balance", "Reconstruction methods", "Maximum entropy"]},
    {"article name": "GMM and optimal principal components-based Bayesian method for multimode fault diagnosis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.09.013",
     "publication date": "01-2016",
     "abstract": "Principal component analysis (PCA) serves as the most fundamental technique in multivariate statistical process monitoring. However, other than determining contributions to a fault from each variable based on the pre-selected major principal components (PCs), the PCA-based fault diagnosis with an optimal selection of PCs is seldom investigated. This paper presents a novel Gaussian mixture model (GMM) and optimal principal components (OPCs)-based Bayesian method for efficient multimode fault diagnosis. First, the GMM and Bayesian inference is utilized to identify the operating mode, and then local PCA model is established in each mode. Second, given that the various principal components (PCs) may contain distinct fault signatures, the behavior of each PC in local PCA is examined and the OPCs are selected through stochastic optimization algorithm. Based on the OPCs, a Bayesian diagnosis system is then formulated to identify the fault statuses in a probability manner. Performance of GMM\u2013OPC Bayesian diagnosis is examined through a numerical example and the Tennessee Eastman challenge process. The efficiency and feasibility are demonstrated.",
     "keywords": ["Gaussian mixture model", "Principal component analysis", "Bayesian method", "Process monitoring", "Fault diagnosis"]},
    {"article name": "Fast arbitrary order moments and arbitrary precision solution of the general rate model of column liquid chromatography with linear isotherm",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.09.009",
     "publication date": "01-2016",
     "abstract": "Algorithms and software are presented for efficiently computing reference solutions of the general rate model with proven error bounds. Moreover, algorithms and software are presented for efficiently computing moments of arbitrary order. The methods are based on numerical inverse Laplace transform, and support both quasi-stationary and dynamic linear binding models. The inlet concentration profiles are treated in a most general way using piecewise cubic polynomials. Algorithmic differentiation obviates manual derivation of the required derivatives. Arbitrary precision arithmetics are applied for minimizing numerical roundoff errors, and several convergence acceleration techniques are evaluated. The implemented software package is freely available as open source on GitHub.",
     "keywords": ["Arbitrary precision solution", "Arbitrary order moments", "Numerical inverse Laplace transform", "Error analysis", "Algorithmic differentiation", "Convergence acceleration"]},
    {"article name": "Hybrid optimization technique for cyclic steam stimulation by horizontal wells in heavy oil reservoir",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.09.016",
     "publication date": "01-2016",
     "abstract": "With the rapid depletion of conventional oil resource, economical and efficient exploitation of heavy oil reservoirs is one of the most effective ways to meet future energy demand. This paper establishes an efficient parametric optimization method for cyclic steam stimulation (CSS) by horizontal wells in heavy oil reservoirs. The net present value (NPV) of a CSS project is maximized by optimizing operational parameters using a new hybrid optimization technique. This hybrid technique is developed by integrating uniform design (UD) into the initialization process of conventional particle swarm optimization (PSO). Case study reveals that initializing PSO with UD can improve the quality of initial particles in conventional PSO and thus speed up its convergence rate. Simulation results indicate that parametric optimization using the hybrid technique is able to obtain the best CSS development strategy for heavy oil reservoirs on both technical and economic sides.",
     "keywords": ["Parametric optimization", "Cyclic steam stimulation", "Hybrid Optimization technique", "Particle swarm optimization", "Uniform design method", "Heavy oil reservoir"]},
    {"article name": "A toolbox using the stochastic optimization algorithm MIPT and ChemCAD for the systematic process retrofit of complex chemical processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.08.023",
     "publication date": "01-2016",
     "abstract": "Global optimization techniques using powerful algorithms have led to a wide range of applications to increase the efficiency of chemical processes. Nevertheless, the performance for optimization of process models is limited by a certain complexity, especially accounting for existing processes (retrofit). Due to the great combinatorial diversity of possible alternatives a systematic approach is essential. The local integration of modifications in the overall process leads to changes in internal streams. Therefore new operating points have to be found and resulting effects on the plant performance have to be evaluated. An optimization framework for the purpose of retrofitting using a rigorous process simulation tool is proposed to fulfill this task. Here, flowsheet simulation software packages are offering a high performance for the prediction of new operation points for following units, and for units affected by recycle streams. An optimization approach using flowsheet simulation software and the stochastic optimization algorithm Molecular-Inspired Parallel Tempering (MIPT) implemented in the programming software Matlab\u2122 is presented. Both of these programs are linked via OPC (OLE for process control), a standard communication platform. The toolbox provides a quick evaluation of the process by searching for the global optimum. The MIPT algorithm is suitable for large optimization problems and can handle constraints and infeasibilities. The usage of a rigorous process simulator is providing a high accuracy of the thermodynamic results which is necessary to evaluate the influence of the new process design. Furthermore, a simulation model of the industrial plant can directly be used for the optimization. A complex multicomponent separation process with recycle streams is used to demonstrate the advantages of the proposed toolbox. To simplify the user input a graphical user interface was programmed. The results of a sensitivity analysis and the optimization for different feed compositions are presented.",
     "keywords": ["Optimization", "Molecular-Inspired Parallel Tempering", "Multicomponent separation process", "Retrofit", "Stochastic algorithm", "ChemCAD\u2122"]},
    {"article name": "A discrete-time scheduling model for continuous power-intensive process networks with various power contracts",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.09.019",
     "publication date": "01-2016",
     "abstract": "Increased volatility in electricity prices and new emerging demand side management opportunities call for efficient tools for the optimal operation of power-intensive processes. In this work, a general discrete-time model is proposed for the scheduling of power-intensive process networks with various power contracts. The proposed model consists of a network of processes represented by Convex Region Surrogate models that are incorporated in a mode-based scheduling formulation, for which a block contract model is considered that allows the modeling of a large variety of commonly used power contracts. The resulting mixed-integer linear programming model is applied to an illustrative example as well as to a real-world industrial test case. The results demonstrate the model's capability in representing the operational flexibility in a process network and different electricity pricing structures. Moreover, because of its computational efficiency, the model holds much promise for its use in a real industrial setting.",
     "keywords": ["Production scheduling", "Demand side management", "Process networks", "Power contracts", "Mixed-integer linear programming"]},
    {"article name": "Definition and validation of a patient-individualized physiologically-based pharmacokinetic model",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.09.018",
     "publication date": "01-2016",
     "abstract": "Pharmacokinetic modeling based on a mechanistic approach is a promising tool for drug concentration prediction in living beings. The development of a reduced physiologically-based pharmacokinetic model (PBPK model), is performed by lumping organs and tissues with comparable characteristics respect to drug distribution phenomena. The proposed reduced model comprises eight differential equations and 18 adaptive parameters. To improve the quality of the PBPK model these adaptive parameters are alternatively: (i) individualized according to literature correlations on the physiological features of each patient; (ii) assigned as constants based on the features of either human body or drug properties; (iii) regressed respect to experimental data.The model predictive capability is validated with experimental blood concentrations of remifentanil, an analgesic drug, administered via bolus injection with four doses (2, 5, 15, 30\u00a0\u03bcg/kg) to mixed groups of patients. Concentration profiles for the four simulated doses reveal a rather good consistency with experimental data.",
     "keywords": ["Pharmacokinetic models", "Physiologically based modeling", "Personalized parameters", "Biodistribution", "Model reduction and lumping", "Remifentanil"]},
    {"article name": "Flexibility analysis of process supply chain networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.07.016",
     "publication date": "01-2016",
     "abstract": "One of the key fundamentals for organizations to remain competitive in the present economic climate is to effectively manage their supply chains under uncertainty. The notion of supply chain flexibility attempts to characterize the ability of a supply chain to perform satisfactorily in the face of uncertainty. However, limited quantitative analysis is available. In this work, we utilize a flexibility analysis framework developed within the context of process operations and design to characterize supply chain flexibility. This framework also provides a quantitative mapping to various types of flexibility discussed in the operations research and management science literature. Two case studies are included to illustrate the application of this framework for analyzing the flexibility of existing supply chain processes, as well as utilizing it in supply chain design.",
     "keywords": ["Supply chain", "Flexibility analysis", "Mixed-integer programming", "Flexibility index"]},
    {"article name": "Medium-term maintenance turnaround planning under uncertainty for integrated chemical sites",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.09.007",
     "publication date": "01-2016",
     "abstract": "Plant maintenance poses extended disruptions to production. Maintenance effects are amplified when the plant is part of an integrated chemical site, as production levels of adjacent plants in the site are also significantly influenced. A challenge in dealing with turnarounds is the difficulty in predicting their duration, due to discovery work and delays. This uncertainty in duration affects two major planning decisions: production levels and maintenance manpower allocation. The latter must be decided several months before the turnarounds occur. We address the scheduling of a set of plant turnarounds over a medium-term of several months using integer programming formulations. Due to the nature of uncertainty, production decisions are treated through stochastic programming ideas, while the manpower aspect is handled through a robust optimization framework. We propose combined robust optimization and stochastic programming formulations to address the problem and demonstrate, through an industrial case study, the potential for significant savings.",
     "keywords": ["Maintenance scheduling", "Mixed-integer linear programming", "Uncertainty", "Stochastic programming", "Robust optimization"]},
    {"article name": "A comparative simulation study of power generation plants involving chemical looping combustion systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.10.002",
     "publication date": "01-2016",
     "abstract": "This work presents a simulation study on both energy and economics of power generation plants with inherent CO2 capture based on chemical looping combustion technologies. Combustion systems considered include a conventional chemical looping system and two extended three-reactor alternatives (exCLC and CLC3) for simultaneous hydrogen production. The power generation cycles include a combined cycle with steam injected gas turbines, a humid air turbine cycle and a simple steam cycle. Two oxygen carriers are considered in our study, iron and nickel. We further analyze the effect of the pressure reaction and the turbine inlet temperature on the plant efficiency. Results show that plant efficiencies as high as 54% are achieved by the chemical looping based systems with competitive costs. That value is well above the efficiency of 46% obtained by a conventional natural gas combined cycle system under the same conditions and simulation assumptions.",
     "keywords": ["Chemical looping combustion", "Power generation plants", "Thermal efficiency", "Economic evaluation"]},
    {"article name": "A synchronous cellular automaton model of mass transport in porous media",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.10.001",
     "publication date": "01-2016",
     "abstract": "In this work we present a fully synchronous coarse grained cellular automaton model for large-scale simulations at molecular level. The model is based on Margolus partitioning scheme, which was generalized as to describe quantitatively diffusion, adsorption and directed flow in porous media. Our aim is to create conceptually simple and computationally efficient framework to model the mass transport in porous materials with large representative volume. This work focuses on the fundamental aspects of the generalized Margolus cellular automaton. We exemplify the model by solving several diffusion problems, studying the monolayer adsorption, chromatography on disordered porous structures and chemical transformation in a system with phase separation. The results indicate that the model reflects the essential features of these phenomena. Absence of round-off errors, fully synchronous way of implementation, autonomous physically meaningful time scale and ease-to-handle boundary conditions make this model a promising framework for study various transport phenomena in porous structures.",
     "keywords": ["Cellular automata", "Porous media", "Adsorption", "Chromatography"]},
    {"article name": "Bifurcation control of high-dimensional nonlinear chemical processes using an extended washout-filter algorithm",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.09.011",
     "publication date": "01-2016",
     "abstract": "This paper presents a design framework to modify the bifurcation characteristics and stability of high-dimensional, nonlinear, chemical processes within specified operating regions. Portions of solution branches are stabilized and oscillatory process dynamics in the vicinity of Hopf bifurcation points (HBPs) are attenuated. A new optimization algorithm is introduced to circumvent the limitations of traditional washout filters for feedback control. This controller is used to operate a high-dimensional, nitroxide-mediated, radical polymerization (NMRP) in a continuous-stirred-tank reactor (CSTR). The algorithm adjusts the eigenvalues of the model Jacobian matrix to relocate the HBPs. It is shown to permit flexible modifications of the bifurcation characteristics, providing acceptable performance for set-point tracking and disturbance rejection, and stabilizing solution branches in specified regions.",
     "keywords": ["Bifurcation control", "Nonlinear system", "Nitroxide-mediated, radical polymerization (NMRP)", "Washout filters"]},
    {"article name": "Synthesis and design of new hybrid configurations for biobutanol purification",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.10.009",
     "publication date": "01-2016",
     "abstract": "The development of new technologies for biobutanol production by fermentation has resulted in higher butanol concentrations, less by-products and higher volumetric productivities during fermentation. These new technology developments have the potential to provide a production process that is economically viable in comparison to the petrochemical pathway for butanol production. New alternative hybrid configurations based on liquid\u2013liquid extraction and distillation for the biobutanol purification were presented. The alternatives are designed and optimized minimizing two objective functions: the total annual cost (TAC) as an economical index and the eco-indicator 99 as an environmental function. All the new configurations presented reduced the TAC compared to the traditional hybrid configuration, in particular a thermally coupled alternative exhibited a 24.5% reduction of the TAC together with a 11.8% reduction of the environmental indicator. Also intensified sequences represented a promising option in the reduction of the TAC but with some penalty in the eco-indicator.",
     "keywords": ["Biobutanol", "Biofuels", "Process synthesis", "Process optimization"]},
    {"article name": "Efficient multi-product multi-BOM batch scheduling for a petrochemical blending plant with a shared pipeline network",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.09.015",
     "publication date": "01-2016",
     "abstract": "We present an effective scheduling heuristic for realistic production planning in a petrochemical blending plant. The considered model takes into account orders spanning a multi-product portfolio with multiple bills of materials per product, that need to be scheduled on shared production facilities including a complex pipeline network. Capacity constraints, intermediate storage restrictions, due dates, and the dedication of resources to specific product families have to be respected. The primary objective of the heuristic is to minimize the total order tardiness. Secondary objectives include the minimization of pipeline cleaning operations, the minimization of lead times, and the balanced utilization of filling units.The developed algorithm is based on a dynamic prioritization-based greedy search that schedules the orders sequentially. The proposed method can schedule short to mid-term operations and evaluate different plant configurations or production policies on a tactical level. We demonstrate its performance on various real-world inspired scenarios for different scheduling strategies.Our heuristic was used during the construction phase of a new blending plant and was instrumental in the optimal design of the plant.",
     "keywords": ["Batch scheduling", "Chemical blending plant", "Heuristic"]},
    {"article name": "A reactive optimization strategy for the simultaneous planning, scheduling and control of short-period continuous reactors",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.09.017",
     "publication date": "01-2016",
     "abstract": "The efficient and economic operation of processing systems ideally requires a simultaneous planning, scheduling and control framework. Even when the optimal simultaneous solution of this problem can result in large scale optimization problems, such a solution can represent economic advantages making feasible its computation using optimization decomposition and/or few operating scenarios. After reducing the complexity of the optimal simultaneous deterministic solution, it becomes feasible to take into account the effect of model and process uncertainties on the quality of the solution. In this work we consider those changes in product demands that take place once the process is already under continuous operation. Therefore, a reactive strategy is proposed to meet the new product demands. Based on an optimization formulation for handling the simultaneous planning, scheduling, and control problem of continuous reactors, we propose a heuristic strategy for dealing with unexpected events that may appear during operation of a plant. Such a strategy consists of the rescheduling of the products that remain to be manufactured after the given disturbance hits the process. Such reactive strategy for dealing with planning, scheduling and control problems under unforeseen events is tested using two continuous chemical reaction systems.",
     "keywords": ["Planning", "Scheduling", "Control", "Optimization"]},
    {"article name": "Preprocessing and tightening methods for time-indexed MIP chemical production scheduling models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.10.003",
     "publication date": "01-2016",
     "abstract": "We propose a series of preprocessing algorithms for the generation of strong valid inequalities for time-indexed, discrete and continuous, mixed-integer programming scheduling models for problems in network production environments. Specifically, starting from time- and inventory-related instance data, the proposed algorithms use constraint propagation techniques to calculate parameters that are used to bound the number of times subsets of tasks can be executed in a feasible solution. We also extend some of the propagation ideas to generate three classes of new tightening constraints. The proposed methods result in tightening constraints expressed in terms of assignment binary variables (Xijt\u00a0=\u00a01 if task i is assigned to start on unit j at time point t) which are present in all time-indexed MIP models, therefore they are applicable to all time-indexed models accounting for a wide range of processing features. Finally, the methods are shown to lead to up to two orders of magnitude reduction in computational time when optimal solutions are found and significantly improve optimality gap when a time limit is enforced.",
     "keywords": ["Constraint propagation", "Strong valid inequalities", "Preprocessing"]},
    {"article name": "Optimal design of batch mass exchange networks with multipurpose exchange units",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.10.004",
     "publication date": "01-2016",
     "abstract": "A novel mathematical model for simultaneous optimization of batch mass exchange networks with multipurpose mass exchange units that can be shared by more than one match in different periods is presented in this work. It can be shown that both utility cost and capital investment can be reduced simultaneously with the use of multipurpose mass exchangers and mass storage tanks. Specifically, state-space superstructure that does not contain any structural simplification is proposed to capture the entire characteristics of the network configuration and a mixed-integer nonlinear optimization model is then formulated accordingly to generate the optimal batch operating policies and the corresponding flowsheet. Two examples are presented in this paper to demonstrate the validity and advantages of the proposed approach.",
     "keywords": ["Batch process", "Mass exchange network", "Simultaneous optimization", "Multipurpose mass exchanger", "Mixed-integer nonlinear optimization"]},
    {"article name": "A decomposition methodology for dynamic modeling of cold box in offshore natural gas liquefaction process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.09.020",
     "publication date": "01-2016",
     "abstract": "Natural gas liquefaction process using mixed and/or cascade refrigerant is popular in onshore LNG (liquefied natural gas) plant. Similar attempt has been adopted for FLNG (floating LNG) but still needed for the improvement of the process to enhance its efficiency as well as reliability. The dynamic modeling of cold box which is a core equipment in LNG/FLNG plant enabling to liquefy natural gas is crucial in order to develop or improve a liquefaction process concerning operability and controllability. A decomposition methodology for dynamic modeling of cold box in the case of lack of internal design data at early design stage is presented. The proposed methodology is validated through the industrial application of offshore natural gas liquefaction process and expected to be extensively applied to the various process designs which require dynamic simulation of cold box unit.",
     "keywords": ["Cold box", "LNG", "FLNG", "Liquefaction", "Dynamic simulation", "Decomposition approach"]},
    {"article name": "First-principles models and sensitivity analysis for the lignocellulosic biomass-to-methanol conversion process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.05.012",
     "publication date": "01-2016",
     "abstract": "The paper illustrates the redefinition of the concept of biorefinery and its application onto the biogas field. The proposed integrated framework merges in a synergistic way the second and third generation biorefineries with the aim of reducing the CO2 emissions and maximizing the utilization of biomass potential. The specific focus of the paper is on the mathematical modelling of the main conversion steps of the lignocellulosic residuals into biosyngas by means of thermal treatment (pyrolysis, combustion, gasification) and, next, into biomethanol (heterogeneously catalyzed synthesis). The models are implemented in user-friendly programmes and adopted to estimate biomass conversion and process yield. A final study of sensitivity analysis is provided to open the way for the next process optimization based on detail models.",
     "keywords": ["Biorefinery", "Biogas", "Biomethanol", "Biomass", "Gasification"]},
    {"article name": "New a priori and a posteriori probabilistic bounds for robust counterpart optimization: I. Unknown probability distributions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.09.014",
     "publication date": "01-2016",
     "abstract": "Optimization problems often have a subset of parameters whose values are not known exactly or have yet to be realized. Nominal solutions to models under uncertainty can be infeasible or yield overly optimistic objective function values given the actual parameter realizations. Worst-case robust optimization guarantees feasibility but yields overly conservative objective function values. The use of probabilistic guarantees greatly improves the performance of robust counterpart optimization. We present new a priori and a posteriori probabilistic bounds which improve upon existing methods applied to models with uncertain parameters whose possible realizations are bounded and subject to unspecified probability distributions. We also provide new a priori and a posteriori bounds which, for the first time, permit robust counterpart optimization of models with parameters whose means are only known to lie within some range of values. The utility of the bounds is demonstrated through computational case studies involving a mixed-integer linear optimization problem and a linear multiperiod planning problem. These bounds reduce the conservatism, improve the performance, and augment the applicability of robust counterpart optimization.",
     "keywords": ["Robust counterpart optimization", "Optimization under uncertainty", "Probabilistic bounds", "Mathematical modeling"]},
    {"article name": "Optimal scenario reduction framework based on distance of uncertainty distribution and output performance: II. Sequential reduction",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.05.010",
     "publication date": "01-2016",
     "abstract": "In this paper, a novel sequential scenario reduction framework for general optimization problem is proposed. The proposed method extends the previous work (Li and Floudas, 2014) and aims to tackle optimization problems with a large number of uncertain parameters and a huge number of scenarios generated from the factorial combination. The proposed method first ranks the uncertain parameters based on their effects on the optimal objective using global sensitivity analysis. Then, the parameters are sequentially considered in generating uncertainty scenarios. This method can essentially reduce the computational efforts needed for evaluating the objective values of all scenarios, which is often impractical for a huge number of scenarios. Criteria for quantifying the quality of scenario reduction are also proposed based on robust optimization and scenario optimization. Case studies are presented to illustrate the sequential scenario reduction framework and the results verify the efficiency of the proposed approach.",
     "keywords": ["Optimal scenario reduction", "Sequential framework", "Uncertainty", "Mixed integer linear optimization"]},
    {"article name": "Inventory pinch gasoline blend scheduling algorithm combining discrete- and continuous-time models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.08.005",
     "publication date": "01-2016",
     "abstract": "This work introduces multi-period inventory pinch-based algorithm to solve continuous-time scheduling models (MPIP-C algorithm), a three level method which combines discrete-time approximate scheduling with continuous-time detailed scheduling and with inventory pinch-based optimization of operating states. When applied to gasoline blending, the top level computes optimal recipes for aggregated blends over periods initially delineated by inventory pinch points. Discrete-time middle level uses fixed blend recipes to compute an approximate schedule, i.e. what, when, and how much to produce; it also allocates swing storage and associated product shipments with specific storage. Continuous-time model at the third level computes when exactly to start/stop an operation (blend, tank transfer, shipment). MPIP-C algorithm solves linear or nonlinear problems 2\u20133 orders of magnitude faster than full-space models.",
     "keywords": ["Inventory pinch", "Nonlinear gasoline blending", "Discrete-time models", "Continuous-time models"]},
    {"article name": "Improved continuous-time model for gasoline blend scheduling",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.08.003",
     "publication date": "01-2016",
     "abstract": "This work introduces a reduced-size continuous-time model for scheduling of gasoline blends. Previously published model has been modified by (i) introducing new model features (penalty for deliveries in order to reduce sending material from different product tanks to the same order, product and blender-dependent minimum setup times, maximum delivery rate from component tanks, threshold volume for each blend), (ii) by reducing the number of integer variables, and (iii) by adding lower bounds on the blend and switching costs, which significantly improve convergence. Nonlinearities are introduced by ethyl RT-70 equations for octane blending. Medium-size linear problems (two blenders, more than 20 orders, 5 products) are solved to optimality within one or two minutes. Previously unsolved large scale blending problems (more than 35 orders, 5 product, 2 or 3 blenders) have also been solved to less than 0.5% optimality gap.",
     "keywords": ["Gasoline blend scheduling", "Continuous-time model", "Reduced number of discrete variables", "Nonlinear blending models"]},
    {"article name": "Model-based integration of control and operations: Overview, challenges, advances, and opportunities",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.04.011",
     "publication date": "12-2015",
     "abstract": "We provide a systematic review of current progress in integrated control and operations. Recently, an increasing number of integrated methods have been presented in the literature. These methods optimize the entire system by simultaneously solving the control and operation problems. Their advantages have been demonstrated in comparison to the conventional sequential method, which solves the subproblems independently and neglects the interaction among the subproblems. We give an in-depth review of these integrated methods in terms of the integration scopes, model formulations, solution algorithms, and implementation strategies to address uncertainty. Though significant progress has been made, research on integrated methods is still in its infant stage. It is still mathematically challenging to solve a complex integrated problem composed of binary/logic variables from operation problems and nonlinear differential-algebraic equations inherited from control problems. We discuss some major challenges concerning complexity, heterogeneity, and uncertainty, and suggest a number of promising directions for future research.",
     "keywords": ["Scheduling", "Planning", "Dynamic Optimization", "Control", "Integration", "Optimization"]},
    {"article name": "Simulation and optimization of multi-component organic Rankine cycle integrated with post-combustion capture process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.03.021",
     "publication date": "12-2015",
     "abstract": "A multi-component working fluid organic Rankine cycle (ORC) with advanced configuration is proposed and optimized in this paper. The proposed ORC utilizes the wasted heat of a CO2 capture process as a heat source, and waste heat utilization is optimized through heat integration. The ORC employs advanced configurations: multi component working fluid, a cold energy recuperating in multi stream cryogenic heat exchanger (MSCHE), and a vapor recondensation process (VRP), thus, its power generation efficiency is much higher than that of conventional ORCs that utilize wasted heat. Process optimization is achieved through exergy evaluation. The results indicate that the proposed cycle is able to produce 304\u00a0kJ per kg liquefied natural gas (LNG), and its corresponding second-law efficiency is approximately 46.2%. With the power generation of the ORC, the power de-rate caused by the CO2 capture process installation is completely compensated and produces more electricity compared with the original power plant.",
     "keywords": ["ORC", "LNG", "Regasification", "CO2 capture", "Exergy optimization", "Multi-component working fluid"]},
    {"article name": "The importance of proper economic criteria and process modeling for single- and multi-objective optimizations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.02.008",
     "publication date": "12-2015",
     "abstract": "This paper provides an overview of the influences that different economic objectives have on the efficiencies of those optimal process designs obtained by using single- and multi-objective optimizations. Optimizations of monetary criteria, like the profit, lead to operationally and environmentally more efficient but economically less attractive designs than optimization of non-monetary economic objectives, like the internal rate of return. The net present value produces compromise designs with intermediate efficiencies and environmental impacts. These differences are significant only if the processes\u2019 mathematical models are sufficiently accurate for establishing appropriate trade-offs between investment and cash flow. The Pareto curves obtained by different economic objectives vary regarding the maximum environmental impacts and in the intervals of the environmental indicators. The composed criteria that combine the economic and environmental indicators into one single objective produce smaller differences between optimum designs that are closer to those designs with minimum possible environmental impacts.",
     "keywords": ["Engineering economics", "Objective function", "Net present value", "Single-objective optimization", "Multi-objective optimization"]},
    {"article name": "Simulation and assessment of an integrated acid gas removal process with higher CO2 capture rate",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.01.008",
     "publication date": "12-2015",
     "abstract": "An integrated acid gas removal process with high CO2 capture rate is proposed, which is featured by Rectisol\u00ae process heat integrated with CO2 multistage compression. Detailed modeling and simulation of the integrated process are conducted. Three key factors that contribute to CO2 capture rate have been identified, which also serve as the foundation to process utility and investment analysis. Results revealed that three factors that affect the CO2 capture rate are: the temperature of rich solvent after heating by compression section, the pressure of low pressure flash, and the rich solvent input ratio between CO2 desorption and H2S concentration column. The capture rate of 80\u201390% is determined as the optimal range. The optimal energy consumption is around 0.93\u00a0GJ/tCO2, while the conventional process requires an energy consumption of 1.07\u00a0GJ/tCO2. The optimal operation cost is close to 24 US$/tCO2, which is 6.1% less than that of the conventional process.",
     "keywords": ["Acid gas removal", "Rectisol\u00ae technology", "Process simulation", "Carbon capture, Process integration"]},
    {"article name": "Shortcut assessment of alternative distillation sequence schemes for process intensification",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.05.011",
     "publication date": "12-2015",
     "abstract": "Finding good process schemes is a difficult task due to its search among multiple alternatives. In literature, several methods are successfully implemented as computer tools, but their use is limited to their developers. There are several useful rules of thumb and heuristics providing useful guidelines, but sometimes these are contradictory. Some authors tried to rank the various alternatives, defining heuristic equations to provide a quantitative parameter to choose among possible solutions. In this paper, mathematical model of distillation columns is simplified, assuming infinite number of stages. The resulting proposed equation has a great similarity to one of the previous heuristic ones. A simple equation not relying on heuristics and easy to use in calculation is provided to evaluate the distillation sequence energy efficiency (DSE) for each alternative. This allows to quantify the advantages derived from process intensification for a given feed composition. The proposed equation is dimensionless, as the Carnot efficiency is used instead of the temperature difference between distillate and bottoms. On the other hand, intermediate results provide also useful information. For instance, the Carnot efficiency of each column indicates when a heat pump or enhanced distillation would be useful. The recovery efficiency for each particular compound allows an easy comparison between alternatives, considering variations on the feed composition. The new equation is verified comparing its results with cases already solved in literature using different methods. The results show that all the methods in literature are able to provide the best sequence, except the heuristics-based ones that are not providing an overall sequence evaluation. The novelty of the proposed DSE method resides in its ease of application, compared to nowadays available methods, and requires only the feed composition and products boiling points.",
     "keywords": ["Thermally coupled distillation", "Divided wall column", "Separation efficiency", "Carnot efficiency"]},
    {"article name": "Optimum sizing of supply equipment for time varying demand",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.05.004",
     "publication date": "12-2015",
     "abstract": "The sizing of supply equipment to meet a time varying demand is an important engineering problem. Optimal sizing of various supply equipment can reduce the overall cost of the supply system significantly. In this paper, the screening curve methodology, originally proposed for planning electrical power system, is extended to address various process system related problems: cost optimal sizing of various pumps to satisfy time varying water demand, ideal mix of various lighting options for a given lighting load, etc. These examples illustrate that the proposed methodology is a simple, versatile, and powerful tool for appropriately sizing various equipment to satisfy time varying demands during grassroots design. During debottlenecking, supply system is expanded; new supply equipment are installed along with appropriate utilisation of existing supply equipment. A methodology is proposed to address expansion planning of various supply equipment during debottlenecking and demonstrated using an example of debottlenecking an air conditioning system.",
     "keywords": ["Screening curve", "Optimisation", "Time varying demand", "System planning", "Expansion planning"]},
    {"article name": "Simulation and process integration for tert-amyl-methyl ether (TAME) synthesis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.05.020",
     "publication date": "12-2015",
     "abstract": "This paper proposes an extended approach to develop a new sustainable process to produce tert-amyl-methyl ether (TAME) using as feedstock enriched C5 fraction (LCN \u2013 light cracking naphtha) from fluid catalytic cracking (FCC). To the best of our knowledge, up to now, different authors developed the separation section without considering all possible options. The main contribution is to bring together for comparison different separation techniques of the given mixture and to develop new configurations for the separation section of the plant. In this respect, pressure swing is combined with liquid\u2013liquid separation. Existing technologies consider methanol (MeOH) separation from reactor effluent only by water extraction, combined with distillation. Conceptual design based on residual curve maps (RCM) analysis, considered in this paper, reveals new possibilities to use pressure swing, eventually combined with liquid\u2013liquid separation. Thus, compared to other results reported in literature, new separation sequences are proposed for TAME synthesis reactor effluent separation, in the frame of an extended and detailed analysis for the whole process.To underline process characteristics, three case studies, with those different configurations are presented and analysed using Aspen HYSYS\u00ae v8.4. Main details are obtained using process simulation, process integration and environmental impact computer tools. In the first case study, classical MeOH separation using water extraction is considered. The second case study is based only on pressure swing distillation to separate the azeotropes between hydrocarbons and methanol. In the third case study, pressure swing distillation is combined with separation based on hydrocarbon\u2013methanol liquid\u2013liquid phase equilibrium. Using process simulation results, setup with Aspen HYSYS\u00ae v8.4, heat integration analysis, performed with SPRINT\u00ae v2.8, is accomplished to exploit energy savings. Environmental impact calculations are performed using WAR algorithm, considering different fuel types for utilities generation. Results show that the elimination of water in separation section and the use of liquid\u2013liquid phase separation ensure lower energy consumption (overall heat recovery in case study 3 is 9.87\u00a0MW, compared to 7.47\u00a0MW for case study 2) and better environmental performance. Economic indicators calculated with Aspen Process Economic Analyzer\u00ae allow identification of attractive process changes, for the new proposed process configuration.",
     "keywords": ["TAME synthesis and separation", "Methanol recovery", "Environmental impact", "Pressure swing distillation", "Liquid\u2013liquid equilibrium", "Techno-economical evaluation"]},
    {"article name": "Automated process design of acid gas removal units in natural gas processing",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.05.030",
     "publication date": "12-2015",
     "abstract": "This study looks at the design of the acid gas removal unit (AGRU) for natural gas processing. For the purpose of enhancing energy efficiency a number of different structural options are considered including multiple feeds, semi-lean and pump around modifications in addition to modification of operational parameters. Previous studies in this area have considered the comparison of different individual configurations but there has been a lack of research considering the simultaneous optimization of equipment configuration. Hence, in this study a superstructure-based optimization approach is used to simultaneously identify the most appropriate arrangement and operating conditions while the maximum energy recovery potential is also realized with the aid of energy composite curves (ECC). This methodology is applied to a case study where it is shown that the optimal configuration contains a combination of pump around and semi-lean process modifications allowing a 15.9% reduction of utility costs.",
     "keywords": ["Acid gas removal unit", "Process design", "Optimization", "Energy recovery"]},
    {"article name": "Thermo-environomic optimisation strategy for fuel decarbonisation process design and analysis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.04.018",
     "publication date": "12-2015",
     "abstract": "To meet the CO2 reduction targets and ensure sustainable energy supply, the development and deployment of cost-competitive innovative low-carbon energy technologies is essential. To design and evaluate the competitiveness of such complex integrated energy conversion systems, a systematic thermo-environomic optimisation strategy for the consistent modelling, comparison and optimisation of fuel decarbonisation process options is developed. The environmental benefit and the energetic and economic costs are assessed for several carbon capture process options. The performance is systematically compared and the trade-offs are assessed to support decision-making and identify optimal process configurations with regard to the polygeneration of H2, electricity, heat and captured CO2. The importance of process integration in the synthesis of efficient decarbonisation processes is revealed. It appears that different process options are in competition when a carbon tax is introduced. The choice of the optimal configuration is defined by the priorities given to the different thermo-environomic criteria.",
     "keywords": ["CO2 capture and storage", "Biomass", "Power plant", "Process design", "Energy integration", "Multi-objective optimisation"]},
    {"article name": "Influence of process operating conditions on solvent thermal and oxidative degradation in post-combustion CO2 capture",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.05.003",
     "publication date": "12-2015",
     "abstract": "The CO2 post-combustion capture with amine solvents is modeled as a complex system interconnecting process energy consumption and solvent degradation and emission. Based on own experimental data, monoethanolamine degradation is included into a CO2 capture process model. The influence of operating conditions on solvent loss is validated with pilot plant data from literature. Predicted solvent consumption rates are in better agreement with plant data than any previous work, and pathways are discussed to further refine the model. Oxidative degradation in the absorber is the largest cause of solvent loss while thermal degradation does not appear as a major concern. Using a single model, the process exergy requirement decreases by 10.8% and the solvent loss by 11.1% compared to our base case. As a result, this model provides a practical tool to simultaneously minimize the process energy requirement and the solvent consumption in post-combustion CO2 capture plants with amine solvents.",
     "keywords": ["Post-combustion CO2 capture", "Monoethanolamine thermal and oxidative degradation", "Process modeling", "Plant design", "Integrated experimental and modeling study"]},
    {"article name": "Time-optimal operation of multi-component batch diafiltration",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.05.029",
     "publication date": "12-2015",
     "abstract": "We study the minimum-time operation of batch multi-component diafiltration processes. We employ the technique of Pontryagin's minimum principle to derive the candidates for an optimal operation. The optimal operation is defined as a state-feedback strategy. Simulations and numerical optimizations are applied to confirm the optimality of the proposed time-optimal operation. Obtained results are evaluated on two case studies, a typical multi-component diafiltration processes. Standard operational approaches are compared to the optimal operation and the resulting improvements show attractivity of the proposed approach.",
     "keywords": ["Optimal control", "Multi-component systems", "Pontryagin's minimum principle", "Diafiltration"]},
    {"article name": "Operation and modeling of RO desalination process in batch mode",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.05.022",
     "publication date": "12-2015",
     "abstract": "The performance of a batch reverse osmosis (RO) desalination process in terms of permeate quantity and salinity as a function of feed pressure and feed salinity is evaluated by using laboratory experiments and process modeling. Special attention is paid to the water and salt permeability constants (Kw, Ks) which affect the permeate and salt flux across the membrane. Kw and Ks are found to be strongly pressure-dependent for the batch system which is in-line with earlier observations for continuous RO systems. However, the most important findings of this work are the dependence of Kw and Ks on feed salinity, something that have never been observed or reported in the literature. In order to better qualify these observations, further experiments with the batch system are conducted with a constant feed salinity so that the operating condition resembles that of a continuous RO process.",
     "keywords": ["Reverse osmosis", "Batch system", "Pilot plant", "Performance assessment"]},
    {"article name": "Process synthesis involving multi-period operations by the P-graph framework",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.04.037",
     "publication date": "12-2015",
     "abstract": "The P-graph (process graph) framework is an effective tool for process-network synthesis (PNS). Here we extended it to multi-period operations. The efficacy of the P-graph methodology has been demonstrated by numerous applications. The unambiguous representation of processes and the availability of the axioms defining combinatorially feasible structures facilitates the development of efficient algorithms to determine maximal structures, solution structures, and optimal structure for processes. However, it is presumed that single-period operation prevails. It implies that the operating conditions and the load of each operating unit remain unchanged, i.e., steady-state operation. This is usually true for the chemical industry but often not in agriculture or where seasonal effects are important. The current work proposes a multi-period operation wherein the load of operating units varies from period to period to accommodate demand, assuming operating conditions remain steady in each period. A modeling technique is proposed to represent operating units in the multi-period operation. The different periods are connected by \u201cfictitious\u201d streams, which ensures, that the unit is sized properly.",
     "keywords": ["Process-network synthesis", "P-graph", "Multi-period operation", "Mathematical modeling", "Optimization"]},
    {"article name": "Decision support for ranking Pareto optimal process designs under uncertain market conditions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.06.009",
     "publication date": "12-2015",
     "abstract": "Considering the uncertainty of economic conditions, multi-objective optimisation can be favoured to single-objective optimisation for process design. However, from the Pareto sets generated by multi-objective optimisation it is not obvious to identify the best one, given that each solution is optimal with regard to the selected objectives. A method taking into account the economic parameters uncertainty to support decision making based on the Pareto-optimal solutions is proposed. It uses a Monte-Carlo simulation to define the probability of each of the Pareto optimal configuration to be in the list of the best configurations from the economical point of view. For a given economic context defined the most probable best configurations are identified. The proposed method is applied to two cases: the CO2 capture in power plants and synthetic natural gas production from biomass resources. The results allow to identify the most attractive system designs and give recommendations for the process engineers.",
     "keywords": ["Decision making", "Economic conditions", "Multi-objective optimization", "Process design", "CO2 capture", "SNG production"]},
    {"article name": "Comparative study of coal, natural gas, and coke-oven gas based methanol to olefins processes in China",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.03.007",
     "publication date": "12-2015",
     "abstract": "Traditional olefins production mainly depends on oil. In view of the short supply of oil, feedstocks are expanded to coal, natural gas, coke-oven gas, and methanol in China. In this paper, a comparative study of alternative olefins production is conducted from aspects of techno-economic feasibility and environmental friendliness. Results show that coal-to-olefins has a significant cost advantage. However, it suffers from low energy efficiency and serious CO2 emissions. To address these problems, this study proposes and analyses coal-to-olefins with CO2 capture, coal and natural gas-to-olefins, and coal and coke-oven gas-to-olefins. The two co-feed systems ensure great reduction of CO2 emissions and significant improving energy efficiency. They should be actively developed in regions with rich coal and gas. While in regions with rich coal and lean gas, coal-to-olefins with CO2 capture should be developed in large scale. This paper also provides several suggestions on planning these olefins production routes in China.",
     "keywords": ["Coal", "Natural gas", "Coke-oven gas", "Methanol", "Olefins", "Techno-economic analysis"]},
    {"article name": "Robust chemical product design via fuzzy optimisation approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.01.007",
     "publication date": "12-2015",
     "abstract": "Traditionally, the design of new chemical products for specific applications is done by using a combination of design heuristics, experimental studies and expert judgements. In addition to the conventional methods, chemical products can also be designed by using computer-aided molecular design (CAMD) techniques. Based on CAMD, optimal chemical products can be designed by identifying the molecule with the best properties that correspond with the target functionalities of the product. In general, the optimality of product property (termed as property superiority) is the only factor considered while designing optimal products by using CAMD techniques. However, it is noted that property prediction models are developed with certain accuracy and uncertainties. As the accuracy of property prediction models (termed as property robustness) can affect the effectiveness of CAMD techniques in predicting the product property, the effects of property prediction uncertainty have to be considered while applying CAMD techniques. This paper presents a systematic fuzzy optimisation based molecular design methodology. The methodology is developed for the design of optimum molecules used in chemical processes by considering and optimising both property superiority and robustness. Property superiority is quantified by property optimality. Meanwhile, property robustness is expressed by the standard deviation of the property prediction model, which is a measure of average variation between the experimental data and estimated values of product property using property prediction model. Fuzzy optimisation approach is extended in this work to address and trade off property superiority and robustness simultaneously. Molecular design technique is adapted in this work to identify the optimal molecular structure which satisfies multiple product specification. To illustrate the proposed method, a case study is presented where optimal solution is selected based on how much the solution satisfied the criteria of property superiority and robustness.",
     "keywords": ["Product design", "Inverse design techniques", "Fuzzy optimisation", "Property prediction uncertainties"]},
    {"article name": "Mathematical modelling of the pre-oxidation of a uranium carbide fuel pellet",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.05.001",
     "publication date": "12-2015",
     "abstract": "Uranium carbide is a candidate fuel for future nuclear reactors. However, for it to be implemented in a closed fuel cycle, an outline for its reprocessing is necessary. One proposed method is to oxidise the uranium carbide into uranium oxide which can then be reprocessed using current infrastructure. A mathematical model describing the heat and mass transfer processes involved in such an oxidation has been constructed. The available literature was consulted for reaction coefficients and information on reaction products. A stable and convergent numerical solution has been developed using a combination of finite-difference approximations of the differential equations. Completion times of approximately 3\u201330\u00a0h are predicted given a spherical pellet with a radius of 9.35\u00a0mm under varying initial conditions. The transient temperature distribution throughout the system is predicted, with a maximum temperature of 1458\u00a0\u00b0C observed from an initial temperature of 500\u00a0\u00b0C at an oxygen concentration of 3.15\u00a0mol\u00a0m\u22123.",
     "keywords": ["Uranium carbide", "Oxidation", "Finite-difference techniques", "Heat and mass transfer"]},
    {"article name": "Kinetic models based on analysis of the dissolution of copper, zinc and brass from WEEE in a sodium persulfate environment",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.05.002",
     "publication date": "12-2015",
     "abstract": "The purpose of this study is to provide an accurate kinetic description for the dissolution process of metals found in WEEE, such copper, zinc, and brass, using persulfate as a leaching oxidant. The factors affecting the performance and efficiency of the leaching process, such as stirring speed, persulfate concentration and temperature were separately investigated. It was observed that the leaching rate of the metals increased with the increase of temperature and persulfate concentration. We propose three models which accurately describe the variation in the surface of the solid for two kinds of geometries, a rectangular block, in the case of pure zinc dissolution, and a cylindrical rod for pure copper and brass dissolution. The apparent activation energy for the leaching of pure copper and zinc has been evaluated using Arrhenius expression and has been determined to be 5.2\u00a0\u00d7\u00a0104\u00a0J\u00a0mol\u22121 for copper and 3.5\u00a0\u00d7\u00a0104\u00a0J\u00a0mol\u22121 for zinc, in the range between 30 and 60\u00a0\u00b0C. The third model which describes the dissolution of brass has provided the best results.",
     "keywords": ["WEEE", "Copper", "Zinc", "Brass", "Leaching process", "Kinetic models"]},
    {"article name": "Process control of a dropwise additive manufacturing system for pharmaceuticals using polynomial chaos expansion based surrogate model",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.07.014",
     "publication date": "12-2015",
     "abstract": "The paper presents a dropwise additive manufacturing process for pharmaceutical products (DAMPP) as an alternative to conventional methods. This mini manufacturing process for the production of personalized pharmaceutical products utilizes drop-on-demand (DoD) printing technology for the deposition of active pharmaceutical ingredient (API) onto edible substrates. Here we present a process control framework for DAMPP, including on-line monitoring, automation and closed loop control, in order to produce individual dosage forms with the desired critical quality attributes, including formulation composition, drop size, deposit morphology and dissolution performance. In order to achieve desired product morphology, a surrogate model based on polynomial chaos expansion is developed to relate the critical process parameters to deposit morphology using dissolution data of the active pharmaceutical ingredient. The proposed process control strategy can effectively mitigate variations in the dissolution profiles due to variable dosage amounts and enable the application of the DoD system for the production of individualized dosage regimens.",
     "keywords": ["Process control", "Supervisory control", "Polynomial chaos expansion", "Surrogate modeling", "Pharmaceutical process", "Drop on demand"]},
    {"article name": "Computer-aided modelling template: Concept and application",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.02.010",
     "publication date": "12-2015",
     "abstract": "Modelling is an important enabling technology in modern chemical engineering applications. A template-based approach is presented in this work to facilitate the construction and documentation of the models and enable their maintenance for reuse in a wider application range. Based on a model decomposition technique which identifies generic steps and workflow involved, the computer-aided template concept has been developed. This concept is implemented as a software tool, which provides a user-friendly interface for following the workflow steps and guidance through the steps providing additional information and comments on model construction, storage and future use/reuse. The application of the tool is highlighted with a multi-scale modelling case study involving a catalytic membrane fixed bed reactor and a two-phase system for oxidation of unsaturated acid with hydrogen peroxide. Both case studies reflect different aspects of template creation and use with respect to model development.",
     "keywords": ["Computer-aided modelling framework", "Modelling template", "Catalytic fixed bed reactor", "Oxidation of unsaturated fatty acids"]},
    {"article name": "Semantic algorithm for Industrial Symbiosis network synthesis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.04.031",
     "publication date": "12-2015",
     "abstract": "The paper introduces a semantic algorithm for building Industrial Symbiosis networks. Built around ontology modelling of knowledge in the domain of Industrial Symbiosis, the algorithm enables the acquisition of the explicit knowledge from the user through ontology instantiation and input/output matching based on semantic relevance between the participants. Formation of innovative Industrial Symbiosis networks is enabled by decomposition of properties characterising respective resources and solutions, the process optimised for set environmental criteria. The proposed algorithm is implemented as a web service. The potential of the algorithm is demonstrated by several case studies using real-life data.",
     "keywords": ["Industrial Symbiosis", "Ontology", "Semantic matching", "Optimisation", "Network synthesis"]},
    {"article name": "Evolutionary algorithm for de novo molecular design with multi-dimensional constraints",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.06.012",
     "publication date": "12-2015",
     "abstract": "An evolutionary approach for solving molecular design problems with descriptors of varying dimensionality has been developed. Spatial fragment based descriptors are employed to generate candidate solutions within a population, which is evolved through the application of genetic operators toward an improved fitness. The candidate molecules are represented as graphs, and as such, customized operators of crossover and mutation have been developed to be compatible with this representation. The search space is conveniently represented through limitations on the occurrence of each fragment, as defined by the chosen data set, and the spatial capabilities of this space are captured through an initial conformational analysis. This spatial information is compressed and utilized to generate conformational space estimations throughout the algorithm, which expedites the search for solution graphs. The effect of various user determined input parameters is considered and exemplified through a case study involving the identification of solvents falling within a desired boiling point range, as estimated by a multi-dimensional property model.",
     "keywords": ["Molecular design", "Genetic algorithm", "Descriptors"]},
    {"article name": "Formulating the optimization problem when using sequential quadratic programming applied to a simple LNG process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.06.003",
     "publication date": "11-2015",
     "abstract": "Sequential quadratic programming (SQP) may be very efficient compared with other techniques for the optimization of simple processes for the liquefaction of natural gas (LNG), and can be combined with process evaluation using commercial flowsheet simulators. However, the level of success is dependent on the formulation of the problem. In this work, effects of varying different aspects of the optimization problem formulation is investigated, such as variable selection, formulae for the estimation of derivatives, initial values, variable bounds, and formulation of constraints. Especially the formulation of the constraint for the temperature difference between the hot and cold composite curve is essential. The commonly used minimum temperature difference constraint should generally not be employed in gradient based optimization. Recommendations regarding optimization of simple LNG processes using SQP and flowsheet simulators are provided.",
     "keywords": ["LNG", "PRICO", "Optimization", "Sequential quadratic programming", "Formulation of constraints"]},
    {"article name": "Optimal drug infusion profiles using a Particle Swarm Optimization algorithm",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.05.026",
     "publication date": "11-2015",
     "abstract": "The dynamic optimization of the administration of therapeutic drugs in simulated patients is proposed. The approach is based on a non-linear discontinuous cardiorespiratory model, which has been conceived to simulate the effect of inotropic and vasoactive drugs as well as anesthetic agents. A stochastic technique (Particle Swarm Optimization), within the context of the control vector parameterization approach, is adopted to identify the infusion profiles of various drugs in order to track, as close as possible, the set-points on several variables of medical interest. Two different medical procedures are investigated in order to test the efficiency and robustness of the algorithm: a congestive heart failure and the unclamping of an aortic vessel. Due to the conflicting nature of the different objectives, compromise solutions are obtained in all cases.",
     "keywords": ["Dynamic optimization", "Cardiorespiratory model", "Drug infusion", "Particle Swarm Optimization"]},
    {"article name": "A geometrically based grid refinement technique for multiphase flows",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.05.031",
     "publication date": "11-2015",
     "abstract": "An adaptive mesh refinement technique developed for the solution of scalar problems is extended to the simulation of two-phase flow problems, as a means of reducing the computational runtime associated with such problems. The methodology, involving the adaptive partition of the domain into uniformly discretised regions, is extended to systems of equations without increase in algorithmic complexity. By application first to the simpler case of the Euler equations of gas dynamics, the technique is shown to handle shocks without loss of accuracy and to result in significant CPU runtime reductions of over 90%. Application to more complex two-phase flow problems, including the flashing flow during the decompression of a pipeline, also show dramatic increase in computational performance.",
     "keywords": ["Multi-phase flow", "Adaptive Mesh Refinement", "Pipeline decompression", "Carbon Capture and Storage"]},
    {"article name": "Combined use of MILP and multi-linear regression to simplify LCA studies",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.06.002",
     "publication date": "11-2015",
     "abstract": "Life cycle assessment (LCA) has become the prevalent approach for quantifying the environmental impact of products over their entire life cycle. Unfortunately, LCA studies require large amounts of data that are difficult to collect in practice, which makes them expensive and time consuming. This work introduces a method that simplifies standard LCA studies by using proxy metrics that are identified following a systematic approach. Our method, which combines multi-linear regression and mixed-integer linear programming, builds in an automatic manner simplified multi-linear regression models of impact that predict (with high accuracy) the damage in different environmental categories from a reduced number of proxy metrics. Our approach was applied to data retrieved from ecoinvent. Numerical results show that few indicators suffice to describe the environmental performance of a process with high accuracy. Our findings will help develop general guidelines for simplified LCA studies that will focus on quantifying a reduced number of key indicators.",
     "keywords": ["Multi-linear regression", "Streamlined LCA analysis", "Environmental impact prediction", "Mixed-integer linear programming, Life cycle assessment"]},
    {"article name": "Dynamic modeling of an industrial diesel hydroprocessing plant by the method of continuous lumping",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.06.005",
     "publication date": "11-2015",
     "abstract": "Diesel hydroprocessing is an important refinery process which consists of hydrodesulfurization to remove the undesired sulfur from the oil feedstock followed by hydrocracking and fractionation to obtain diesel with desired properties. Due to the new emission standards to improve the air quality, there is an increasing demand for the production of ultra low sulfur diesel fuel. This paper is addressing the development of a reliable dynamic process model which can be used for real-time optimization and control purposes to improve the process conditions of existing plants to meet the low-sulfur demand. The overall plant model consists of a hydrodesulfurization (HDS) model for the first two reactor beds followed by a hydrocracking (HC) model for the last cracking bed. The models are dynamic, non-isothermal, pseudo-homogeneous plug flow reactor models. Reaction kinetics are modeled using the method of continuous lumping which treats the reaction medium as a continuum of species whose reactivities depend on the true boiling point of the mixture. The key modeling parameters are estimated using industrial data. Steady-state and dynamic model predictions of the reactor bed temperatures, sulfur removal, and diesel production match closely the plant data.",
     "keywords": ["Ultra low sulfur diesel", "Hydrodesulfurization", "Continuous lumping", "Dynamic reactor modeling", "Parameter estimation"]},
    {"article name": "A multi-thread parallel computation method for dynamic simulation of molecular weight distribution of multisite polymerization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.05.027",
     "publication date": "11-2015",
     "abstract": "Molecular weight distribution (MWD) is an important quality index of polymer products. Many methods have been proposed to dynamically simulate the MWD of polymerization, but these methods are normally designed for serial computations. In this paper, a multi-thread parallel computation method was proposed for multisite free-radical polymerization. Analysis of the relationship among different subtasks revealed a combined parallel strategy by fully exploiting the parallel feature of the process. A good performance was obtained to accelerate the dynamic simulation of MWD based on Flory method. We theoretically analyzed the speedup ratio (SR) and parallel efficiency (PE). Results showed that software algorithm and hardware configuration exhibited a good match. The efficiency of the proposed parallel method was presented through industrial slurry processes that used high-density polyethylene (HDPE).",
     "keywords": ["Multi-thread parallel computation", "Multisite polymerization", "Molecular weight distribution", "Dynamic simulation"]},
    {"article name": "Computational strategies for large-scale MILP transshipment models for heat exchanger network synthesis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.05.015",
     "publication date": "11-2015",
     "abstract": "Determining the minimum number of units is an important step in heat exchanger network synthesis (HENS). The MILP transshipment model (Papoulias and Grossmann, 1983) and transportation model (Cerda and Westerberg, 1983) were developed for this purpose. However, they are computationally expensive when solving for large-scale problems. Several approaches are studied in this paper to enable the fast solution of large-scale MILP transshipment models. Model reformulation techniques are developed for tighter formulations with reduced LP relaxation gaps. Solution strategies are also proposed for improving the efficiency of the branch and bound method. Both approaches aim at finding the exact global optimal solution with reduced solution times. Several approximation approaches are also developed for finding good approximate solutions in relatively short times. Case study results show that the MILP transshipment model can be solved for relatively large-scale problems in reasonable times by applying the approaches proposed in this paper.",
     "keywords": ["Heat exchanger network synthesis (HENS)", "Transshipment model", "Mixed-integer linear programming", "Computational strategies", "Model reformulation"]},
    {"article name": "A general modular framework for the integrated optimal management of an industrial gases supply-chain and its production systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.06.007",
     "publication date": "11-2015",
     "abstract": "A general modular methodology for the simultaneous optimization of the supply-chain network and the production systems of a general industrial gas producer is developed and implemented in a C++ program. The formulation and solution algorithm are specifically designed to be able to work on-line and to determine the optimal assignments of production site output to customer demand in the supply-chain and the corresponding optimal operating conditions for the production plants in integrated fashion. Here, the production network is not simply modelled as a set of product sources, rather the model is detailed enough to allow effective and feasible optimization of the entire system. Moreover, the proposed approach can be easily combined with the rolling horizon technique to mitigate the uncertainties in demand. The modelling strategies, employed for the supply-chain network and the production sites, along with the solution approach, adopted for the resulting optimization problem, are detailed. BzzMath library classes are used to meet the computational efficiency requirements for on-line applications. The effectiveness of the proposed methodology is demonstrated on a case study involving a portion of the real supply-chain and network of production facilities of Linde Gas Italia S.r.l.",
     "keywords": ["Supply-chain optimization", "Industrial planning strategies", "Air separation units", "Linear programming", "Process systems engineering"]},
    {"article name": "On the tuning of predictive controllers: Application of generalized Benders decomposition to the ELOC problem",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.06.004",
     "publication date": "11-2015",
     "abstract": "This work investigates the computational procedures used to obtain global solution to the economic linear optimal control (ELOC) problem. The proposed method employs the generalized Benders decomposition (GBD) algorithm. Compared to the previous branch and bound approach, a naive application of GBD to the ELOC problem will improve computational performance, due to less frequent calls to computationally slow semi-definite programming (SDP) routines. However, the reverse-convex constraints of the original problem will reappear in the relaxed master problem. In response, a convexification of the relaxed master constraints has been developed and proven to preserve global solution characteristics. The result is a multi-fold improvement in computational performance. A technological benefit of decomposing the problem into steady-state and dynamic parts is the ability to utilize nonlinear steady-state models, since the relaxed master problem is free of SDP type constraints and can be solved using any global nonlinear programming algorithm.",
     "keywords": ["Stochastic control", "Optimization", "Generalized Benders decomposition", "Linear matrix inequalities"]},
    {"article name": "A robust possibilistic programming approach for pharmaceutical supply chain network design",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.06.008",
     "publication date": "11-2015",
     "abstract": "In this paper, a bi-objective mixed integer linear programming (BOMILP) model is developed for a pharmaceutical supply chain network design (PSCND) problem. The model helps to make several decisions about the strategic issues such as opening of pharmaceutical manufacturing centers and main/local distribution centers along with optimal material flows over a mid-term planning horizon as the tactical decisions. It aims to concurrently minimize the total costs and unfulfilled demands as the first and second objective functions. Since the critical parameters are tainted with great degree of epistemic uncertainty, a robust possibilistic programming approach is used to handle uncertain parameters. In order to verify and analyze the proposed model, it is tested on a real case study and managerial insights are provided.",
     "keywords": ["Pharmaceutical supply chain network design", "Robust possibilistic programming", "Multi-period location-allocation"]},
    {"article name": "Development and validation of a two phase CFD model for tubular biodiesel reactors",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.06.010",
     "publication date": "11-2015",
     "abstract": "The use of biodiesel as an alternative to diesel has gained increasing momentum over the past 15 years. To meet this growing demand there is a need to optimise the transesterification reactor at the heart of the biodiesel production system. Assessing the performance of innovative reactors is difficult due to the liquid\u2013liquid reaction mixture that is affected by mass transfer, reaction kinetics and component solubility. This paper presents a Computational Fluid Dynamic model of a tubular reactor developed in ANSYS CFX that can be used to predict the onset of mixing via turbulent flow. In developing the model an analysis of the reaction mixture is provided before the presentation of experimental data, which includes flow visualisation results and temperature dependant viscosity and density data for each phase. The detailed data and model development procedure represents an advancement in the modelling of the two phase transesterification reaction used in biodiesel production.",
     "keywords": ["Computational Fluid Dynamics", "Two phase flow", "Biodiesel", "FAME", "Glycerol"]},
    {"article name": "Water and energy integration: A comprehensive literature review of non-isothermal water network synthesis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.06.011",
     "publication date": "11-2015",
     "abstract": "Synthesis of non-isothermal water networks consisting of water-usage, wastewater treatment, and heat exchanger networks has been recognised as an active research field in process systems engineering. However, only brief overviews of this important field have so far been provided within the literature. This work presents a systematic and comprehensive review of papers published over the last two decades and highlights possible future directions within this field. This review can be useful for researchers and engineers interested in water and energy integration within process water networks using systematic methods based on pinch analysis, mathematical programming, and their combination. We believe that this research field will continue to be active in the near future due to the importance of simultaneous optimisation of process, water and energy integration for achieving profitability and sustainability within process industries.",
     "keywords": ["Water networks", "Non-isothermal water networks", "Pinch analysis", "Mathematical programming", "Review"]},
    {"article name": "Optimal scheduling of single stage batch plants with direct heat integration",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.07.006",
     "publication date": "11-2015",
     "abstract": "This paper addresses the multi-objective optimization problem arising in the operation of heat integrated batch plants, where makespan and utility consumption are the two conflicting objectives. A new continuous-time MILP formulation with general precedence variables is proposed to simultaneously handle decisions related to timing, product sequencing, heat exchanger matches (selected from a two-stage superstructure) and their heat loads. It features a complex set of timing constraints to synchronize heating and cooling tasks, derived from Generalized Disjunctive Programming. Through the solution of an industrial case study from a vegetable oil refinery, we show that major savings in utilities can be achieved while generating the set of Pareto optimal solutions through the \u025b-constraint method.",
     "keywords": ["Energy efficiency", "Heat exchanger networks", "Short-term scheduling", "Multi-objective optimization", "Mathematical modeling", "Mixed-integer linear programming"]},
    {"article name": "Interactive NBI and (E)NNC methods for the progressive exploration of the criteria space in multi-objective optimization and optimal control",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.07.004",
     "publication date": "11-2015",
     "abstract": "A wide range of problems arising from real world applications present multiple and conflicting objectives to be simultaneously optimized. However, this multi-objective nature is too often neglected. Multi-objective optimization proved to be a powerful tool to correctly describe the trade-offs among conflicting objectives in a set of optimal solutions known as the Pareto set. This paper introduces an interactive method to solve multi-objective problems based on geometric considerations. The method returns a wider Pareto set, at a negligible computational cost, when compared to existing methods. The interactivity also allows the decision-maker to explore only relevant parts of the Pareto set. The extreme solutions yield insightful considerations on the generation of the scalarization parameters for the Normal Boundary Intersection and the Enhanced Normalized Normal Constraints methods. The proposed method is applied to: (i) three scalar multi-objective problems and (ii) the multi-objective optimal control of a tubular and a fed-batch reactor.",
     "keywords": ["Multi-objective optimization", "Optimal control", "(Enhanced) Normalized Normal Constraint", "Normal Boundary Intersection", "Nonlinear optimization"]},
    {"article name": "A numerical study on biomass fast pyrolysis process: A comparison between full lumped modeling and hybrid modeling combined with CFD",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.07.007",
     "publication date": "11-2015",
     "abstract": "This study focuses on process modeling and simulation of a biomass fast pyrolysis system. For the simulation of the biomass fast pyrolysis process, two types of simulation models were developed: lumped model and hybrid model. Employing the above models, the effect of reaction temperature on reaction rate and final product yields were analyzed. It was found that the hybrid model exhibited a peculiar characteristic of displaying multiphase reacting flow occurring in fluidized bed. This behavior of hybrid model could have attributed for the difference in product yields of the models (hybrid and lumped). For the yields of the tar and NCG, hybrid model prediction was very consistent with the experimental results than the lumped model. However, for char yield, the results of both the lumped model and the hybrid model were close to that of experimental results.",
     "keywords": ["Biomass", "Fast pyrolysis CFD", "Process analysis", "Hybrid model"]},
    {"article name": "A new methodology combining total site analysis with exergy analysis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.07.010",
     "publication date": "11-2015",
     "abstract": "Total site analysis allows determining the potential heat transfer between multiple plants to obtain further energy savings to the process integration in industrial plants. Exergy analysis offers a clear understanding of the integration of thermodynamic systems in process integration. A new methodology that combines total site analysis and exergy analysis is presented. The methodology allows for simultaneous use of both thermodynamic systems and heat transfer networks. The combination of the two types of utilities allows for better exploitation of the plants\u2019 energy profile. In addition, the methodology allows specifying the networks\u2019 and thermodynamic systems characteristics and number. The capacities of this methodology are tested on a case study where different combinations of systems are studied to determine their behavior with variable parameters.",
     "keywords": ["Total site analysis", "Heat integration", "Exergy analysis", "Heat transfer networks", "Shaft work targeting"]},
    {"article name": "Some efficient approaches for multi-objective constrained optimization of computationally expensive black-box model problems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.07.013",
     "publication date": "11-2015",
     "abstract": "Multi-objective constrained optimization problems which arise in many engineering fields often involve computationally expensive black-box model simulators of industrial processes which have to be solved with limited computational time budget, and hence limited number of simulator calls. This paper proposes two heuristic approaches aiming to build proxy problem models, solvable by computationally efficient optimization methods, in order to quickly provide a sufficiently accurate approximation of the Pareto front. The first approach builds a multi-objective mixed-integer linear programming (MO-MILP) surrogate model of the optimization problem relying on piece-wise linear approximations of objectives and constraints obtained through brute-force sensitivity computation. The second approach builds a multi-objective nonlinear programming (MO-NLP) surrogate model using curve fitting of objectives and constraints. In both approaches the desired number of approximated solutions of the Pareto front are generated by applying the \u025b-constraint method to the multi-objective surrogate problems. The proposed approaches are tested for the cost vs. life cycle assessment (LCA)-based environmental optimization of drinking water production plants. The results obtained with both approaches show that a good quality approximation of Pareto front can be obtained with a significantly smaller computational time than with a state-of-the-art metaheuristic algorithm.",
     "keywords": ["Multi-objective optimization", "Life cycle assessment", "Environmental engineering", "Expensive black-box model"]},
    {"article name": "Integration potential, resource efficiency and cost of forest-fuel-based biorefineries",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.07.011",
     "publication date": "11-2015",
     "abstract": "A multidisciplinary study of the implementation potential of a biorefinery, using forestry residues as feedstock, is performed by assessing techno-economic factors, system integration and feedstock supply. The process is based on biochemical conversion of logging residues to produce ethanol, biogas, pellets, heat and electricity. Nine models were designed in Aspen Plus based on the available feedstock and the required co-products. Focus was on the product ratio of pellets and heat. The net present value of the plants was calculated and thermal integration with district-heating systems in areas with regional feedstock availability was investigated. Also co-location with pulp and paper mills in Sweden was investigated to replace fossil fuels with pellets. Seven of the nine models showed a positive net present value assuming an 11% discount rate and 30% corporate tax. Five counties in Sweden were identified as potential feedstock suppliers to a biorefinery processing 200\u00a0kt\u00a0dry\u00a0feedstock/y.",
     "keywords": ["DHS district heating system", "district heating system", "DM dry matter", "dry matter", "APEA Aspen Process Economic Analyzer", "Aspen Process Economic Analyzer", "WWT waste-water treatment", "waste-water treatment", "SSF simultaneous saccharification and fermentation", "simultaneous saccharification and fermentation", "WIS water-insoluble solids", "water-insoluble solids", "FPU filter paper units", "filter paper units", "CHP combined heat and power", "combined heat and power", "LHVs lower heating values", "lower heating values", "NPV net present value", "net present value", "CF cash flow", "cash flow", "MESP minimum ethanol selling price", "minimum ethanol selling price", "HMF hydroxymethylfurfural", "hydroxymethylfurfural", "Biorefinery", "Forestry residues", "Ethanol", "Biogas", "Techno-economic assessment", "Integration potential"]},
    {"article name": "Model predictive control for the self-optimized operation in wastewater treatment plants: Analysis of dynamic issues",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.07.003",
     "publication date": "11-2015",
     "abstract": "This paper describes a procedure to find the best controlled variables in an economic sense for the activated sludge process in a wastewater treatment plant, despite the large load disturbances. A novel dynamic analysis of the closed loop control of these variables has been performed, considering a nonlinear model predictive controller (NMPC) and a particular distributed NMPC-PI control structure where the PI is devoted to control the process active constraints and the NMPC the self-optimizing variables. The well-known self-optimizing control methodology has been applied, considering the most important measurements of the process. This methodology provides the optimum combination of measurements to keep constant with minimum economic loss. In order to avoid nonfeasible dynamic operation, a preselection of the measurements has been performed, based on the nonlinear model of the process and evaluating the possibility of keeping their values constant in the presence of typical disturbances.",
     "keywords": ["Self-optimizing control", "Process optimization", "Activated sludge process", "Model predictive control"]},
    {"article name": "Numerical strategies for the bifurcation analysis of perfectly stirred reactors with detailed combustion mechanisms",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.07.008",
     "publication date": "11-2015",
     "abstract": "The paper introduces a numerical tool based on a predictor\u2013corrector continuation algorithm to obtain the bifurcation analysis of a perfectly stirred reactor with detailed reaction mechanisms.Each step of the continuation algorithm is reviewed and adapted to handle reaction mechanisms with hundreds of species and thousands of reactions. Particularly, the adoption of a Broyden solver in the predictor\u2013corrector algorithm and a new formulation of the test functions are proposed. The implementation in Matlab and the adoption of the CANTERA Toolbox, make the tool easily applicable to reaction mechanisms available in CHEMKIN format.To validate and demonstrate the capability of the tool, the full equilibrium curves have been obtained for three different cases, having increasing number of species and reactions: methane\u2013air (GRIMech.1.2), simple surrogates of Jet-A in air (JetSurF2.0) and a ternary surrogate of Jet-A in air (CRECK). The tool gets performances that make affordable the computations even with desktop computers.",
     "keywords": ["Bifurcation analysis", "Parametric continuation", "Detailed kinetic mechanism", "Jet fuels", "Ignition and extinction limits", "Cantera"]},
    {"article name": "Extending explicit and linearly implicit ODE solvers for index-1 DAEs",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.07.002",
     "publication date": "11-2015",
     "abstract": "Nonlinear differential-algebraic equations (DAE) are typically solved using implicit stiff solvers based on backward difference formula or RADAU formula, requiring a Newton\u2013Raphson approach for the nonlinear equations or using Rosenbrock methods specifically designed for DAEs. Consistent initial conditions are essential for determining numeric solutions for systems of DAEs. Very few systems of DAEs can be solved using explicit ODE solvers. This paper applies a single-step approach to system initialization and simulation allowing for systems of DAEs to be solved using explicit (and linearly implicit) ODE solvers without a priori knowledge of the exact initial conditions for the algebraic variables. Along with using a combined process for initialization and simulation, many physical systems represented through large systems of DAEs can be solved in a more robust and efficient manner without the need for nonlinear solvers. The proposed approach extends the usability of explicit and linearly implicit ODE solvers and removes the requirement of Newton\u2013Raphson type iteration.",
     "keywords": ["Differential algebraic equations", "Initialization", "Explicit solvers", "Consistent initial conditions", "Single-step solution"]},
    {"article name": "Reception, mixture, and transfer in a crude oil terminal",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.07.012",
     "publication date": "11-2015",
     "abstract": "The aim of this work is the modeling and validation of a process of reception, mixture and delivery of crude oil in a terminal. Each tank at the terminal receives different oil qualities from different cargos. Constraints are imposed to ensure material balance and operating rules. Inventory levels in the tanks are discretized. Crude oil should be transferred to the refinery according to a schedule of volumes and qualities. Linear constraints with an adjustment term for composition discrepancies are formulated to force the concentration in a tank to be equal to the concentration of the outlet volume. The problem consists in finding an optimized schedule that meets the constraints. A MILP model is proposed and solved for specific cases. In order to achieve good results in an affordable time, the rolling horizon strategy (RHS) is applied to determine the optimal schedule of crude oil operations over a time horizon.",
     "keywords": ["Crude oil scheduling", "Integer programming", "Planning", "Inventory management", "Rolling horizon"]},
    {"article name": "Solving power-constrained gas transportation problems using an MIP-based alternating direction method",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.07.005",
     "publication date": "11-2015",
     "abstract": "We present a solution algorithm for problems from steady-state gas transport optimization. Due to nonlinear and nonconvex physics and engineering models as well as discrete controllability of active network devices, these problems lead to difficult nonconvex mixed-integer nonlinear optimization models. The proposed method is based on mixed-integer linear techniques using piecewise linear relaxations of the nonlinearities and a tailored alternating direction method. Most other publications in the field of gas transport optimization only consider pressure and flow as main physical quantities. In this work, we additionally incorporate heat power supplies and demands as well as a mixing model for different gas qualities. We demonstrate the capabilities of our method on Germany's largest transport networks and hereby present numerical results on the largest instances that were ever reported in the literature for this problem class.",
     "keywords": ["Nonconvex mixed-integer nonlinear optimization", "Alternating direction methods", "Piecewise linear relaxations", "Gas transport networks", "Heat power supply and demand"]},
    {"article name": "Multi-objective optimisation using surrogate models for the design of VPSA systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.07.009",
     "publication date": "11-2015",
     "abstract": "Vacuum/pressure swing adsorption is an attractive and often energy efficient separation process for some applications. However, there is often a trade-off between the different objectives: purity, recovery and power consumption. Identifying those trade-offs is possible through use of multi-objective optimisation methods but this is computationally challenging due to the size of the search space and the need for high fidelity simulations due to the inherently dynamic nature of the process. This paper presents the use of surrogate modelling to address the computational requirements of high fidelity simulations needed to evaluate alternative designs. We present SbNSGA-II ALM, surrogate based NSGA-II, a robust and fast multi-objective optimisation method based on kriging surrogate models and NSGA-II with the Active Learning MacKay (ALM) design criterion. The method is evaluated by application to an industrially relevant case study: a two column six step system for CO2/N2 separation. A 5 times reduction in computational effort is observed.",
     "keywords": ["Carbon capture", "Kriging", "Multi-objective optimisation", "Surrogate modelling", "Vacuum/pressure swing adsorption", "Dynamic simulation"]},
    {"article name": "Optimal procurement contract selection with price optimization under uncertainty for process networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.07.015",
     "publication date": "11-2015",
     "abstract": "In this work, we propose extending the production planning decisions of a chemical process network to include optimal contract selection under uncertainty with suppliers and product selling price optimization. We use three quantity-based contract models: discount after a certain purchased amount, bulk discount, and fixed duration contracts. We propose the use of general regression models to describe the relationship between selling price, demand, and possibly other predictors, such as economic indicators. For illustration purposes, we consider three demand-response models (i.e., selling price as a function of demand) that are typically encountered in the literature: linear, constant-elasticity, and logit. We develop a mixed-integer nonlinear two-stage stochastic programming that accounts for uncertainty in both supply (e.g., raw material spot market price) and demand (random nature of the residuals of the regression models) for the planning of the process network. The proposed method is illustrated with two numerical examples of chemical process networks.",
     "keywords": ["Optimal contract selection", "Price optimization", "Uncertainty", "Process network production planning"]},
    {"article name": "Evaluation of hydrodynamic behavior of the perforated gas distributor of industrial gas phase polymerization reactor using CFD-PBM coupled model",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.07.001",
     "publication date": "11-2015",
     "abstract": "A 2D computational fluid dynamics (Eulerian\u2013Eulerian) multiphase flow model coupled with a population balance model (CFD-PBM) was implemented to investigate the fluidization structure in terms of entrance region in an industrial-scale gas phase fluidized bed reactor. The simulation results were compared with the industrial data, and good agreement was observed. Two cases including perforated distributor and complete sparger were applied to examine the flow structure through the bed. The parametric sensitivity analysis of time step, number of node, drag coefficient, and specularity coefficient was carried out. It was found that the results were more sensitive to the drag model. The results showed that the entrance configuration has significant effect on the flow structure. While the dead zones are created in both corners of the distributors, the perforated distributor generates more startup bubbles, heterogeneous flow field, and better gas\u2013solid interaction above the entrance region due to jet formation.",
     "keywords": ["PBE population balance equation", "population balance equation", "PBM population balance model", "population balance model", "KTGF kinetic theory of granular flow", "kinetic theory of granular flow", "Sc specularity coefficient", "specularity coefficient", "Computational fluid dynamics", "Population balance model", "Gas phase fluidized bed polymerization reactor", "Direct quadrature method of moments", "Perforated distributor"]},
    {"article name": "Optimal processing pathway selection for microalgae-based biorefinery under uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.08.002",
     "publication date": "11-2015",
     "abstract": "We propose a systematic framework for the selection of optimal processing pathways for a microalgae-based biorefinery under techno-economic uncertainty. The proposed framework promotes robust decision making by taking into account the uncertainties that arise due to inconsistencies among and shortage in the available technical information. A stochastic mixed integer nonlinear programming (sMINLP) problem is formulated for determining the optimal biorefinery configurations based on a superstructure model where parameter uncertainties are modeled and included as sampled scenarios. The solution to the sMINLP problem determines the processing technologies, material flows, and product portfolio that are optimal with respect to all the sampled scenarios. The developed framework is implemented and tested on a specific case study. The optimal processing pathways selected with and without the accounting of uncertainty are compared with respect to different objectives.",
     "keywords": ["Biofuels", "Microalgal biorefinery", "Uncertainty analysis", "Stochastic mixed integer nonlinear programming (sMINLP)", "Decision-making under uncertainty"]},
    {"article name": "A multi-scale framework for CO2 capture, utilization, and sequestration: CCUS and CCU",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.04.034",
     "publication date": "10-2015",
     "abstract": "We present a multi-scale framework for the optimal design of CO2 capture, utilization, and sequestration (CCUS) supply chain network to minimize the cost while reducing stationary CO2 emissions in the United States. We also design a novel CO2 capture and utilization (CCU) network for economic benefit through utilizing CO2 for enhanced oil recovery. Both the designs of CCUS and CCU supply chain networks are multi-scale problems which require decision making at material, process and supply chain levels. We present a hierarchical and multi-scale framework to design CCUS and CCU supply chain networks with minimum investment, operating and material costs. While doing so, we take into consideration the selection of source plants, capture processes, capture materials, CO2 pipelines, locations of utilization and sequestration sites, and amounts of CO2 storage. Each CO2 capture process is optimized, and the best materials are screened from large pool of candidate materials. Our optimized CCUS supply chain network can reduce 50% of the total stationary CO2 emission in the U.S. at a cost of $35.63 per ton of CO2 captured and managed. The optimum CCU supply chain network can capture and utilize CO2 to make a total profit of more than 555 million dollars per year ($9.23 per ton). We have also shown that more than 3% of the total stationary CO2 emissions in the United States can be eliminated through CCU networks at zero net cost. These results highlight both the environmental and economic benefits which can be gained through CCUS and CCU networks. We have designed the CCUS and CCU networks through (i) selecting novel materials and optimized process configurations for CO2 capture, (ii) simultaneous selection of materials and capture technologies, (iii) CO2 capture from diverse emission sources, and (iv) CO2 utilization for enhanced oil recovery. While we demonstrate the CCUS and CCU networks to reduce stationary CO2 emissions and generate profits in the United States, the proposed framework can be applied to other countries and regions as well.",
     "keywords": ["Carbon capture and storage", "CO2 utilization", "CO2 sequestration", "CCUS", "CCU", "Optimization"]},
    {"article name": "Making processes work",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.03.022",
     "publication date": "10-2015",
     "abstract": "Chemical processes and their flow-sheets are systems and as with all systems one cannot optimise each part alone and expect to get an optimal process. One also has to take into account the connections and interactions between the parts of the systems in order to achieve the global optimum. In this paper fundamental thermodynamics will be used to show how to achieve an optimal solution. A coal-to-liquids (CTL) process will be used to illustrate the method.The overall material balance for a process will be looked at first. This material balance must include the constraints such as the energy (heat and work) balance and thus must include the feed streams that supply utilities, such as heat and electricity to the process. The best material balance ensures that as much of the feed material ends up in the product. In the examples discussed, focus will be on ensuring that as much of the carbon in the feed as possible ends up in the hydrocarbon product; the carbon from the feed that does not report to the product is emitted from the process as CO2 which is undesirable for a number of reasons.The resulting overall material balance is then regarded as the process target, since it is the \u201cbest\u201d material balance. Furthermore, the manner in which energy (heat and work) is added or removed from a process, affects the material balance by introducing irreversibilities. The greater these irreversibilities are, the further the process operates from the process target, implying that the process produces more CO2 per mole of product produced.Many processes, such as CTL, require substantial quantities of work to be added. It is shown that this may be done by designing the overall process such that the process itself is effectively a heat engine. Thus heat at high temperature is added in an endothermic, high temperature sub-process (e.g. gasification) and (less) heat is rejected at a lower temperature from an exothermic, low temperature sub-process (e.g. Fischer\u2013Tropsch synthesis). Just as in a heat engine, there is a relationship between the values of the high and low temperatures, the quantities of heat flowing in and out of the sub processes and the amount of work added to the overall process. One can note that any stream has an enthalpy and a temperature and these two together can be used to describe the work content of this stream.The Carnot temperature for each sub-process is defined as the temperature at which the heat added to the sub-process takes with it the work content required by the sub-process. The bigger the difference between the actual operating temperature and the Carnot temperature, the more irreversible the process is and the further away the process operates from the process target.A CTL process has been chosen to apply the methods in order to obtain the process target and the overall material balances for different options. It is shown that there are different ways of arranging the heat engine for CTL, for example indirect or direct liquefaction, and that the direct route has higher carbon efficiency than the indirect route. However it is shown that one can use the ideas in the paper to synthesise a new route for CTL where rather than gasifying to syngas, one gasifies to hydrogen and carbon dioxide followed by the FT synthesis reaction. In this way one can show that this indirect CTL route is nearly as efficient as the direct route.",
     "keywords": ["Process synthesis", "Process efficiency", "Coal-to-liquid", "Entropy analysis", "Carnot engine", "Heat engine"]},
    {"article name": "Challenges and opportunities in modeling pharmaceutical manufacturing processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.03.018",
     "publication date": "10-2015",
     "abstract": "The pharmaceutical industry currently faces economic and regulatory challenges associated with process development. Process modeling tools can play a role in developing robust and economically efficient manufacturing processes. However pharmaceutical companies have lagged behind other specialty chemical manufacturers in the adoption of these tools. This is in part due to the challenges associated with modeling solids-based processes. However recent advances in the modeling of particulate processes have created opportunities to apply process modeling tools, including flowsheet modeling, to pharmaceutical manufacturing processes. This work will provide an overview of the challenges associated with modeling particulate processes and discuss recent developments in pharmaceutical process modeling. In addition, a case study involving a continuous feeding and blending process will be presented where many of the modeling approaches discussed in this work are applied.",
     "keywords": ["Pharmaceutical manufacturing", "Solids-based process modeling", "Flowsheet modeling", "Continuous processing"]},
    {"article name": "Design of integrated biorefineries",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.05.021",
     "publication date": "10-2015",
     "abstract": "The paper outlines a systems approach with capabilities to address common complexities and practicalities in the design of real-life integrated biorefineries. The approach favors a decomposition of the problem into process synthesis, process integration and flowsheeting. The synthesis of paths introduces a graph representation sufficiently generic to model the general problem. Likewise, the development of product portfolios offers a generic cascade representation that combines thermodynamics with mathematical programming. The methodology renders high-throughput capacity and has been exploited to review large combinations of paths through exhaustive screening, subsequently leading to significant savings in capital and operating costs. The paper highlights results from the approach as it has configured the operation and the evolution of existing pilots and demos. The methodology is being extended to address strategic decisions and the better integration of the biorefinery concept. The paper explains limitations and opportunities of existing methods and tools, highlighting the scope for future developments and applications.",
     "keywords": ["Biorefineries", "Integration", "Synthesis", "Thermodynamics", "Value chain analysis"]},
    {"article name": "Process systems engineering studies for the synthesis of catalytic biomass-to-fuels strategies",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.04.007",
     "publication date": "10-2015",
     "abstract": "The goal of this paper is to show how chemical process synthesis and analysis studies can be coupled with experimental heterogeneous catalysis studies to identify promising research directions for the development of strategies for the production of renewable fuels. We study five catalytic biomass-to-fuels strategies that rely on production of platform chemicals, such as levulinic acid and fermentable sugars. We first integrate catalytic conversion subsystems with separation subsystems to generate complete conversion strategies, and we then develop the corresponding process simulation models based on experimental results. Our analyses suggest that catalytic biomass-to-fuel conversion strategies could become economically competitive alternatives to current biofuel production approaches as a result of iterative experimental and computational efforts.",
     "keywords": ["PSE process systems engineering", "process systems engineering", "LA levulinic acid", "levulinic acid", "FA formic acid", "formic acid", "SA sulfuric acid", "sulfuric acid", "GVL \u03b3-valerolactone", "\u03b3-valerolactone", "BL sec-butyl levulinate", "sec-butyl levulinate", "BF sec-butyl formate", "sec-butyl formate", "BA butyl acetate", "butyl acetate", "GGE gallon of gasoline equivalent", "gallon of gasoline equivalent", "SBP 2-sec-butyl phenol", "2-sec-butyl phenol", "NREL National Renewable Energy Laboratory", "National Renewable Energy Laboratory", "HEN heat exchanger network", "heat exchanger network", "MSP minimum selling price", "minimum selling price", "EOS equation of state", "equation of state", "MT metric tons", "metric tons", "Lignocellulosic biofuels", "Process synthesis", "Technoeconomic evaluation", "Heterogeneous catalysis"]},
    {"article name": "Product design \u2013 Molecules, devices, functional products, and formulated products",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.04.013",
     "publication date": "10-2015",
     "abstract": "Chemical product design is a multidisciplinary and diverse subject. This article provides an overview of product design while focusing on product conceptualization. Four product types are considered \u2013 molecular products, formulated products, devices and functional products. For molecular products, computer-aided design tools are used to predict the physicochemical properties of single molecules and blends. For formulated products, an integrated experiment-modeling approach is used to generate the formula with the specified product attributes. For devices and functional products, conceptual product design is carried out by modeling the product based on thermodynamics, kinetics and transport processes, by performing experiments, and by decision making based on rule-based methods The results are product specifications in terms of the type of ingredients, composition, and the structure, form, shape or configuration of the product.",
     "keywords": ["Molecular design", "Conceptual product design", "Formulated products", "Chemical devices", "Functional products"]},
    {"article name": "A systematic framework for the design, simulation and optimization of personalized healthcare: Making and healing blood",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.03.008",
     "publication date": "10-2015",
     "abstract": "We review the key building blocks of a design framework for modeling and optimizing biomedical systems under development in the Biological Systems Engineering Laboratory and the Centre for Process Systems Engineering at Imperial College. The framework features the following components: (i) in vitro environment, where model parameters can be obtained and new setups can be tested; (ii) in silico environment, including a simulation module for representing relevant physical or biological processes, and an optimization module, for calculating improved in vitro or in vivo outcomes; (iii) in vivo environment, from which organ and patient-specific parameters are collected and which can also implement personalized suggestions for improved outcomes. Two applications in the area of healthy and diseased blood are thoroughly discussed to exemplify the framework's characteristics. We discuss progress in the different areas and the way in which they are connected and finally propose a hybrid in vitro/in silico/in vivo platform.",
     "keywords": ["Biomedical design framework", "Red blood cell production", "Bioreactor design", "Chemotherapy modeling and optimization for leukemia", "Cell cycle", "Environmental stress"]},
    {"article name": "Future opportunities and challenges in the design of new energy conversion systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.04.029",
     "publication date": "10-2015",
     "abstract": "In this perspective, an overview of the key challenges and opportunities in the design of new energy systems is presented. Recent shifts in the prices of natural energy resources combined with growing environmental concerns are creating a new set of challenges for process design engineers. Due to the massive scale and impact of energy conversion processes, some of the best solutions to the energy crisis lie in the design of new process systems which address these new problems. In particular, many of the most promising solutions take a big-picture approach by integrating many different processes together to take advantage of synergies between seemingly unrelated processes. This paper is an extended version of a paper published as part of the proceedings of the 8th International Conference on the Foundations of Computer-Aided Process Design (FOCAPD 2014) Adams (2014).",
     "keywords": ["Energy conversion systems", "Solid oxide fuel cells", "Chemical looping combustion", "Polygeneration", "Oxyfuel combustion", "Synthetic fuels"]},
    {"article name": "From process integration to process intensification",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.03.011",
     "publication date": "10-2015",
     "abstract": "In this paper, we establish a connection between process integration and process intensification. Focusing on processes with material recycle, we use an asymptotic analysis to demonstrate that intensification represents a limit case of tight integration through significant material recycling. Based on this result, we propose a novel avenue for discovering intensification opportunities at the process design stage. Subsequently, we investigate the dynamics and control implications of the transition from process integration to process intensification. We demonstrate that, for the same steady-state performance, the dynamic response of an integrated process is slower than that of its intensified equivalent. Also, we provide a theoretical justification for existing empirical arguments concerning the loss of control degrees of freedom caused by process intensification. The theoretical developments are applied on a reaction\u2013separation\u2013recycle process example.",
     "keywords": ["Process intensification", "Process integration", "Process synthesis", "Process control"]},
    {"article name": "Challenges and opportunities in computer-aided molecular design",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.03.009",
     "publication date": "10-2015",
     "abstract": "In this paper, the significant development, current challenges and future opportunities in the field of chemical product design using computer-aided molecular design (CAMD) tools are highlighted. With the gaining of focus on the design of novel and improved chemical products, the traditional heuristic based approaches may not be effective in designing optimal products. This leads to the vast development and application of CAMD tools, which are methods that combine property prediction models with computer-assisted search in the design of various chemical products. The introduction and development of different classes of property prediction methods in the overall product design process is discussed. The exploration and application of CAMD tools in numerous single component product designs, mixture design, and later in the integrated process-product design are reviewed in this paper. Difficulties and possible future extension of CAMD are then discussed in detail. The highlighted challenges and opportunities are mainly about the needs for exploration and development of property models, suitable design scale and computational effort as well as sustainable chemical product design framework. In order to produce a chemical product in a sustainable way, the role of each level in a chemical product design enterprise hierarchy is discussed. In addition to process parameters and product quality, environment, health and safety performance are required to be considered in shaping a sustainable chemical product design framework. On top of these, recent developments and opportunities in the design of ionic liquids using molecular design techniques have been discussed.",
     "keywords": ["Chemical product design", "Computer-aided molecular design", "Ionic liquids", "Property estimation methods", "Sustainable chemical product design"]},
    {"article name": "A perspective on process synthesis: Challenges and prospects",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.05.007",
     "publication date": "10-2015",
     "abstract": "This paper gives the author's perspective on some of the open questions and opportunities in process synthesis focusing on separation systems as the application. Driven by energy and environmental concerns and challenged by introduction of new raw materials, this author anticipates significant advances in: (1) novel approaches that integrate experimental studies and process synthesis activities, and multi-scale and surrogate models for accurately capturing the behavior of these unconventional mixtures, (2) systematic generation of alternatives for processing these mixtures, and (3) global, robust, and stochastic optimization for identifying the optimum alternative. This paper is an extended version of a conference paper (Cremaschi, 2014) presented at the 8th International Conference on Foundations of Computer-Aided Process Design.",
     "keywords": ["Process synthesis", "Distillation sequences", "Separation systems", "Surrogate models"]},
    {"article name": "Challenges and opportunities in integration of design and control",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.03.019",
     "publication date": "10-2015",
     "abstract": "Process synthesis and design of plant operation are related topics but current industrial practice solves these problems sequentially. The implication of this sequential strategy may result in design of processing systems which are very hard to control. This paper presents a discussion on drivers for an integrated approach and outlines the challenges in formulation of such a multi-objective synthesis problem. This discussion is viewed in relation to some of the changing trends in the industry. Significant results have been published which in different ways seek to handle the integrated problem. Further, advancements in control algorithms and software have widened the range of feasible operation and control for strongly interconnected production systems. In light of these advances in different areas of the field, recommendations for further research and initiatives for development of an integrated approach are given with focus on how new results on the short term can improve industrial practice.",
     "keywords": ["Process design", "Process synthesis", "Process control", "Process integration"]},
    {"article name": "Education for sustainability: Developing a taxonomy of the key principles for sustainable process and product design",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.03.010",
     "publication date": "10-2015",
     "abstract": "Design for sustainability as an independent field of study is both multidisciplinary and cross-cutting. It encompasses engineering, natural science, economics, finance, political science, social science and the humanities. It concerns governments, corporations and consumers. Although not normally considered design topics, the effects of manufactured products and energy usage on society and the environment are increasingly impacting process design choices. Because of the numerous groups and constituencies involved, sustainability is a difficult concept to define. However, from a design perspective, professional competency in sustainability is becoming an important prerequisite for the production of economically viable products. This contribution proposes an outline for a sustainability taxonomy, including the key concepts that define professional competency in design and engineering for sustainability. This contribution is an extended version of a paper originally published in the Proceedings of the 8th International Conference on the Foundations of Computer Aided Process Design Conference, Cle Elum, Washington, 2014.",
     "keywords": ["Sustainability", "Process design", "Engineering education", "Sustainability taxonomy"]},
    {"article name": "Supply chain design and optimization: Challenges and opportunities",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.03.015",
     "publication date": "10-2015",
     "abstract": "Optimal supply chain design is vital to the success of industrial concerns now more than ever before. This paper reviews some principal research opportunities and challenges in the field of supply chain design. The growing area of enterprise-wide optimization and the increasing importance of energy and sustainability issues provide plentiful opportunities for supply chain design research. However, modeling, algorithmic, and computational challenges arise from these research opportunities. There are three major technical challenge areas where knowledge gaps can be addressed in supply chain design, namely multi-scale challenges, multi-objective and sustainability challenges, and multi-player challenges. This paper provides an overview of opportunity areas, a description of relevant technical challenges, and a perspective on how these challenges might be addressed in supply chain design. Illustrative examples are presented to illuminate avenues for future research.",
     "keywords": ["Supply chains", "EWO", "Energy and sustainability", "Multi-scale modeling and optimization", "Life cycle optimization"]},
    {"article name": "Robust autothermal microchannel reactors",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.03.013",
     "publication date": "10-2015",
     "abstract": "Autothermal microchannel reactors are intensified process units that bring significant energy efficiency benefits over their conventional counterparts. Efficiency gains are obtained, however, at the cost of operational challenges. These stem from the loss of control handles that is inherent to combining several unit operations in a single physical device. In this paper, we investigate the impact of two recently proposed reactor design concepts (a segmented catalyst macromorphology and an embedded layer of phase change material) aimed at improving the steady state energy distribution and, respectively, preventing the advent of hotspots during transient operation, on reactor dynamics and control. Using an autothermal microchannel reactor coupling steam methane reforming with methane catalytic combustion as a prototype system, we demonstrate through rigorous simulations that these design innovations have a synergistic effect, resulting in superior steady-state performance and excellent disturbance rejection ability.",
     "keywords": ["Catalytic plate reactors", "Microchannel reactors", "Process intensification", "Process control"]},
    {"article name": "Simultaneous process optimization and heat integration based on rigorous process simulations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.04.033",
     "publication date": "10-2015",
     "abstract": "This paper introduces a simultaneous process optimization and heat integration approach, which can be used directly with the rigorous models in process simulators. In this approach, the overall process is optimized utilizing external derivative-free optimizers, which interact directly with the process simulation. The heat integration subproblem is formulated as an LP model and solved simultaneously during optimization of the flowsheet to update the minimum utility and heat exchanger area targets. A piecewise linear approximation for the composite curve is applied to obtain more accurate heat integration results. This paper describes the application of this simultaneous approach for three cases: a recycle process, a separation process and a power plant with carbon capture. Case study results indicate that this simultaneous approach is relatively easy to implement and achieves higher profit and lower operating cost and, in the case of the power plant example, higher net efficiency than the sequential approach.",
     "keywords": ["Heat integration", "Simulation based optimization", "Simultaneous approach", "Piecewise linear approximation", "Carbon capture"]},
    {"article name": "Conceptual design and optimization of chemical processes under uncertainty by two-stage programming",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.05.016",
     "publication date": "10-2015",
     "abstract": "This contribution presents a method and a tool for modelling and optimizing process superstructures in the early phase of process design where the models of the processing units and other data are inaccurate. To adequately deal with this uncertainty, we employ a two-stage formulation where the operational parameters can be adapted to the realization of the uncertainty while the design parameters are the first-stage decisions. The uncertainty is represented by a set of discrete scenarios and the optimization problem is solved by stage decomposition. The approach is implemented in the computer tool FSOpt (Flow sheet Superstructure Optimization) FSOpt provides a flexible environment for the modelling of the unit operations and the generation of superstructures and algorithms for the translation of the superstructure into non-linear programming models.The approach is applied to two case studies, the hydroformylation of dodec-1-ene and the separation of an azeotropic mixture of water and formic acid.",
     "keywords": ["Chemical processes", "Process design", "Mathematical modelling", "Optimization", "Uncertainty", "Evolutionary algorithm"]},
    {"article name": "Sustainable process synthesis\u2013intensification",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.04.030",
     "publication date": "10-2015",
     "abstract": "Chemical industry is facing global challenges such as the need to find sustainable production processes. Process intensification as part of process synthesis has the potential to find truly innovative and more sustainable solutions. In this paper, a computer-aided, multi-level, multi-scale framework for synthesis, design and intensification of processes, for identifying more sustainable alternatives is presented. Within the framework, a three stage work-flow has been implemented where, in the first \u201csynthesis\u201d stage an optimal processing route is synthesized through a network superstructure optimization approach and related synthesis tools. In the second, \u201cdesign\u201d stage, the processing route from the first stage is further developed and a base case design is established and analyzed. In the third, \u201cinnovation\u201d stage, more sustainable innovative solutions are determined. The application of the framework is illustrated through a case study related to the production of di-methyl carbonate, which is an important bulk chemical due to its multiplicity of uses.",
     "keywords": ["Process synthesis", "Process design", "Sustainable design", "Sustainability", "Process intensification", "Systematic framework"]},
    {"article name": "Process synthesis, design and analysis using a process-group contribution method",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.04.019",
     "publication date": "10-2015",
     "abstract": "This paper describes the development and application of a process-group contribution method to model, simulate and synthesize chemical processes. Process flowsheets are generated in the same way as atoms or groups of atoms are combined to form molecules in computer aided molecular design (CAMD) techniques. The fundamental pillars of this framework are the definition and use of functional process-groups (building blocks) representing a wide range of process operations, flowsheet connectivity rules to join the process-groups to generate all the feasible flowsheet alternatives and flowsheet property models like energy consumption, atom efficiency, environmental impact to evaluate the performance of the generated alternatives. In this way, a list of feasible flowsheets are quickly generated, screened and selected for further analysis. Since the flowsheet is synthesized and the operations in the flowsheet designed through predictive models to match a set of design targets, optimal solution of a given synthesis problem is guaranteed.",
     "keywords": ["Process synthesis", "Toluene hydrodealkylation", "Process-groups", "Group-contribution", "CAMD"]},
    {"article name": "A unifying framework for optimization-based design of integrated reaction\u2013separation processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.03.014",
     "publication date": "10-2015",
     "abstract": "The determination of the best flowsheet and the development of a conceptual design of a chemical reaction\u2013separation process is a complex task. Although model-based methods for process design have been proposed in the last decades, only a few contributions address the simultaneous design and optimization of integrated reaction\u2013separation processes. This paper presents a systematic and optimization-based approach for the design of such processes. Shortcut methods are utilized to screen alternative flowsheet structures. For the most promising alternatives a rigorous optimization of the entire flowsheet is executed to determine the best alternative. The incremental refinement of this framework allows for the systematic generation and numerically robust evaluation of every reaction\u2013separation process. Consequently, it helps to shorten the time for developing new and innovative processes and improves the chances to find the best process design. The suggested methodology is illustrated by means of a case study considering ethyl tert-butyl ether (ETBE) production.",
     "keywords": ["Conceptual design", "Reaction\u2013separation process synthesis", "Shortcut evaluation", "ETBE production process"]},
    {"article name": "Using GREENSCOPE indicators for sustainable computer-aided process evaluation and design",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.04.020",
     "publication date": "10-2015",
     "abstract": "Manufacturing sustainability can be increased by educating those who design, construct, and operate facilities, and by using appropriate tools for process evaluation and design. The U.S. Environmental Protection Agency's GREENSCOPE methodology and tool, for evaluation and design of chemical processes, suits these purposes. This work describes example calculations of GREENSCOPE indicators for the oxidation of toluene and puts them into context with best- and worst-case limits. Data available from the process is transformed by GREENSCOPE into understandable information which describes sustainability. An optimization is performed for various process conversions, with results indicating a maximum utility at intermediate conversions. Lower conversions release too much toluene through a purge stream; higher conversions lead to the formation of too many byproducts. Detailed results are elucidated through the context of best- and worst-case limits and graphs of total utility and GREENSCOPE indicator values, which are calculated within an optimization framework for the first time.",
     "keywords": ["Chemical", "Process", "Sustainability", "Optimization", "GREENSCOPE"]},
    {"article name": "Computer-aided molecular design in the continuous-molecular targeting framework using group-contribution PC-SAFT",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.04.008",
     "publication date": "10-2015",
     "abstract": "Computer-aided molecular design allows generating novel fluids fulfilling a set of target properties. An integrated design of fluid and process directly employs a process-based objective function. In this work, we solve the integrated process and fluid design problem using the continuous-molecular targeting computer-aided molecular design (CoMT\u2013CAMD) framework. CoMT\u2013CAMD exploits the molecular picture underlying the PC-SAFT equation of state. In the simultaneous optimization of process and fluid, relaxed pure component parameters allow for an efficient optimization. The result is a hypothetical optimal target fluid. In previous work, fluids showing similar performance as the target fluid were obtained from a mapping onto a database. Here, we integrate computer-aided molecular design to realize the actual design of novel fluids. The resulting method for fluid design is based on a group-contribution method for the PC-SAFT parameters (GPC-SAFT) and applied to the design of working fluids for Organic Rankine cycles and solvents for CO2 capture.",
     "keywords": ["Integrated fluid and process design", "Computer-aided molecular design", "PC-SAFT"]},
    {"article name": "A systematic methodology for optimal mixture design in an integrated biorefinery",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.04.032",
     "publication date": "10-2015",
     "abstract": "Biomass is a sustainable source of energy which can be utilised to produce value-added products such as biochemical products and biomaterials. In order to produce a sustainable supply of such value-added products, an integrated biorefinery is required. An integrated biorefinery is a processing facility that integrates multiple biomass conversion pathways to produce value-added products. To date, various biomass conversion pathways are available to convert biomass into a wide range of products. Due to the large number of available pathways, various systematic screening tools have been developed to address the process design aspect of an integrated biorefinery. Process design however, is often inter-linked with product design as it is important to identify the optimal molecule (based on desired product properties) prior to designing its optimal production routes. In cases where the desired product properties cannot be met by a single component chemical product, a mixture of chemicals would be required. In this respect, product and process design decisions would be a challenging task for an integrated biorefinery. In this work, a novel two-stage optimisation approach is developed to identify the optimal conversion pathways in an integrated biorefinery to convert biomass into the optimal mixtures in terms of target product properties. In the first stage, the optimal mixture is designed via computer-aided molecular design (CAMD) technique. CAMD technique is a reverse engineering approach which predicts the molecules with optimal properties using property prediction models. Different classes of property models such as group contribution (GC) models and quantitative structure property relationship (QSPR) are adapted in this work. The main component of the mixture is first determined from the target product properties. This is followed by the identifying of additive components to form an optimal mixture with the main component based on the desired product properties. Once the optimal mixture is determined, the second stage identifies the optimal conversion pathways via superstructural mathematical optimisation approach. With such approach, the optimal conversion pathways can be determined based on different optimisation objectives (e.g. highest product yield, lowest environmental impact etc.). To illustrate the proposed methodology, a case study on the design of fuel additives as a mixture of different molecules from palm-based biomass is presented. With the developed methodology, optimal fuel additives are designed based on optimal target properties. Once the optimal fuel additives are designed, the optimal conversion pathways in terms of highest product yield and economic performance that convert biomass into the optimal fuel additives are identified.",
     "keywords": ["Product design", "Integrated biorefinery", "Inverse design techniques", "Mixture design", "Integrated product and process design"]},
    {"article name": "Multivariate characterization, modeling, and design of ionic liquid molecules",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.04.009",
     "publication date": "10-2015",
     "abstract": "Ionic liquids that have tailored structures with an array of unique functional properties can have important applications in areas such as CO2 capture and sequestration, sulfur removal from fuels, energy storage, biomass pretreatment, and chemical separations. Within a computer-aided molecular design (CAMD) framework, a characterization based method was combined with chemometric techniques in a reverse problem formulation to design ionic liquid (IL) structures corresponding to particular physical properties. Infrared spectra generated from density functional theory (DFT) simulations were used for capturing information on molecular architecture and calibration of latent variable property models to synthesize ILs in a logical and systematic methodology.",
     "keywords": ["Ionic liquid design", "Latent variable models", "Principal component analysis", "Density functional theory", "QSPR", "CAMD"]},
    {"article name": "Optimization of sub-ambient separation systems with embedded cubic equation of state thermodynamic models and complementarity constraints",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.04.038",
     "publication date": "10-2015",
     "abstract": "A previously developed equation-based flowsheet optimization framework is extended and applied to design sub-ambient separation systems for oxy-fired coal power systems with carbon capture. Unlike most commercial flowsheet design and optimization tools, the proposed methods use exact derivatives and large-scale nonlinear programming algorithms to solve large flowsheet design problems with many degrees of freedom, including the simultaneous design of air separation units (ASUs) and their accompanying multistream heat exchangers. Emphasis is placed on additional model improvements regarding thermodynamic calculations. In order to maintain differentiability, complementarity constraints are used to model switches, including vanishing and reappearing phases. Nevertheless, these complementarity constraints may construct trivial phase equilibrium solutions, and a procedure based on embedded bubble and dew points calculations is proposed to avoid them. Furthermore, additional complementarity constraints for the cubic equation of state model are proposed to ensure correct phase identification in the supercritical region. Finally, the efficacy of these new models are demonstrated by optimization of the CO2 processing unit and compression train for an oxy-fired power plant.",
     "keywords": ["Air separation units", "Multistream heat exchangers", "Mathematical programs with complementarity constraints", "Cubic equations of state", "Oxycombustion power systems"]},
    {"article name": "Parallel optimization by means of a Spectral-Projected-Gradient approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.04.010",
     "publication date": "10-2015",
     "abstract": "The judicious exploitation of the inherent optimization capabilities of the Spectral-Projected-Gradient method (SPG) is proposed. SPG was implemented in order to achieve efficiency. The novel adjustments of the standard SPG algorithm showed that the parallel approach proves to be useful for optimization problems related to process systems engineering. Efficiency was achieved without having to relax the problems because the original model solutions were obtained in reasonable time.",
     "keywords": ["Optimization", "Parallel programming", "Non-linear problems", "Efficiency"]},
    {"article name": "Data clustering for model-prediction discrepancy reduction \u2013 A case study of solids transport in oil/gas pipelines",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.04.027",
     "publication date": "10-2015",
     "abstract": "The minimum fluid-flow velocity to ensure particle transport in pipelines is an essential design and operation consideration for oil and gas production. This flow velocity is difficult to estimate due to complex nature of the physical processes. It has been shown that the predictions of different, alternative models may vary several orders of magnitude for the same inputs. This paper introduces a systematic approach to reduce this discrepancy using data clustering, model selection, and cluster identification techniques. The approach is tested using 772 experimental data points (published in open literature), and the results show that the average of the error percentages between the predictions and experimental velocities are reduced from several orders of magnitude to 37%.",
     "keywords": ["k-Means clustering", "Particle transport", "Threshold velocity", "Model selection"]},
    {"article name": "Policy effects on microgrid economics, technology selection, and environmental impact",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.03.012",
     "publication date": "10-2015",
     "abstract": "This paper deals with the impact of policy decisions on optimal microgrid design. A generic system is considered consisting of solar photovoltaics, wind turbine, microturbines, electric boiler, gas-fired boiler, and a battery bank. The microgrid is grid-connected and designed to supply both heat and power. An optimal design is found to minimize the cost of energy supply over a 20 year lifespan. The optimal design is analyzed under a variety of policy scenarios such as emission taxation, emission reduction, and minimum system autonomy.",
     "keywords": ["Microgrid", "Optimization", "Public policy", "Renewable power"]},
    {"article name": "Bayesian estimation of parametric uncertainties, quantification and reduction using optimal design of experiments for CO2 adsorption on amine sorbents",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.04.028",
     "publication date": "10-2015",
     "abstract": "Uncertainty quantification plays a significant role in establishing reliability of mathematical models, while applying to process optimization or technology feasibility studies. Uncertainties, in general, could occur either in mathematical model or in model parameters. In this work, process of CO2 adsorption on amine sorbents, which are loaded in hollow fibers is studied to quantify the impact of uncertainties in the adsorption isotherm parameters on the model prediction. The process design variable that is most closely related to the process economics is the CO2 sorption capacity, whose uncertainty is investigated. We apply Bayesian analysis and determine a utility function surface corresponding to the value of information gained by the respective experimental design point. It is demonstrated that performing an experiment at a condition with a higher utility has a higher reduction of design variable prediction uncertainty compared to choosing a design point at a lower utility.",
     "keywords": ["Bayesian inference", "Adaptive metropolis", "Parallel propagation", "Optimal experimental design", "CO2 adsorption"]},
    {"article name": "Enhanced-efficiency operating variables selection for vapor compression refrigeration cycle system",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.05.005",
     "publication date": "09-2015",
     "abstract": "In this paper, a novel enhanced-efficiency selection of operating variables based on self-optimizing control (SOC) method for the vapor compression refrigeration cycle (VCC) system is proposed. An objective function is proposed to maximize the energy efficiency of the VCC system while meeting with the demand of indoor thermal comfort. With the detailed analysis of operating variables, three unconstrained degrees of freedom are selected among all the candidate operating variables. Then two SOC methods are applied to determine the optimal individual controlled variables (CVs) and measurement combinations as CVs. The model predictive control (MPC) method based controllers and PID controllers are designed for different sets of CVs, and the experimental results indicate that the proposed selection of CVs can achieve a good trade-off between optimal (or near optimal) stable operation and enhanced-efficiency of the synthesized control structure.",
     "keywords": ["Vapor compression refrigeration cycle", "Operating variables", "Self-optimizing control", "Model predictive control", "Energy efficiency"]},
    {"article name": "A branch-and-price approach to evaluate the role of cross-docking operations in consolidated supply chains",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.04.039",
     "publication date": "09-2015",
     "abstract": "Supply-chain management and optimization aims at reducing costs and inventories. One way to increase the supply-chain efficiency is to use cross-docking for consolidating shipments from different suppliers. Cross-docking is a warehousing strategy used in logistics that consists on moving goods from suppliers to customers through a cross-dock facility. The employment of this strategy must be carefully evaluated because sometimes transportation requests can be better directly moved from source-sites to destination. A realistic problem studying the convenience of direct delivery, avoiding some cross-docking transfers, is here discussed. An efficient methodology for finding (near)optimal solutions is also described. The methodology is based on the use of column generation embedded into an incomplete branch-and-price tree. The approach provides (near)optimal solutions by solving the column generation sub-problems without necessarily considering all unexplored nodes in the search-tree. Finally, we show computational results on numerous test problems and on four configurations of the addressed case study.",
     "keywords": ["Supply chain", "Cross-docking", "Direct delivery", "Column generation", "Branch-and-price", "Decomposition"]},
    {"article name": "Elastic net with Monte Carlo sampling for data-based modeling in biopharmaceutical manufacturing facilities",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.05.006",
     "publication date": "09-2015",
     "abstract": "Biopharmaceutical manufacturing involves multiple process steps that can be challenging to model. Oftentimes, operating conditions are studied in bench-scale experiments and then fixed to specific values during full-scale operations. This procedure limits the opportunity to tune process variables to correct for the effects of disturbances. Generating process models has the potential to increase the flexibility and controllability of the biomanufacturing processes. This article proposes a statistical modeling methodology to predict the outputs of biopharmaceutical operations. This methodology addresses two important challenging characteristics typical of data collected in the biopharmaceutical industry: limited data availability and data heterogeneity. Motivated by the final aim of control, regularization methods, specifically the elastic net, are combined with sampling techniques similar to the bootstrap to develop mathematical models that use only a small number of input variables. This methodology is evaluated on an antibody manufacturing dataset.",
     "keywords": ["Data-based modeling", "Elastic net", "Lasso", "Monte Carlo methods", "Biopharmaceutical manufacturing", "Regularization methods"]},
    {"article name": "A mixed-integer dynamic optimization approach for the optimal planning of distributed biorefineries",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.05.008",
     "publication date": "09-2015",
     "abstract": "The implementation of supply chains based on biomass conversion requires the exploration of various aspects, including the selection of processing technologies, configuration of the supply chain, portfolio of products as well as the feedstock selection. One important feature of this system is that the composition of the available biomass changes drastically through the year because this depends significantly on the climatic conditions; this way, the dynamic behavior of this process is an important issue that must be considered. This study presents a dynamic optimization model for the optimal planning of a distributed biorefinery system taking into account the time dependence of the involved variables and parameters. In addition, this paper incorporates a model predictive control methodology to obtain the behavior of the storages and orders of the supply chain; where the objective function is the difference between the required and satisfied demands in the markets. Therefore, this study considers relevant issues, which include the multiple available biomass feedstocks at various harvesting sites, the availability and seasonality of biomass resources, potential geographical locations for processing plants that produce multiple products using diverse production technologies, economies of scale for the production technologies, demands and prices of multiple products in each consumer, locations of storage facilities and a number of transportation modes between the supply chain components. The model was applied to a case study for a distributed biorefinery system in Mexico. Results show that is possible to get the configuration and the behavior of the supply chain considering its dynamic behavior in a rigorous way; furthermore, the solutions obtained by the model illustrate that the supply chains based on biomass conversion are seriously affected by the availability of bioresources over the time.",
     "keywords": ["Biorefinery supply chain", "Model predictive control", "Dynamic optimization", "Optimal planning"]},
    {"article name": "Application of MHE to large-scale nonlinear processes with delayed lab measurements",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.04.015",
     "publication date": "09-2015",
     "abstract": "The paper addresses nonlinear estimation problems on nonlinear processes containing several lab measurements sampled slowly and with long delay, which is the usual case in industrial polymerization applications. A moving horizon estimation algorithm is developed to compute the theoretical optimal solution given the multi-rate measurements. In this algorithm, the MHE window is recalculated as the new lab measurement becomes available. Simulation studies on a polymerization process with plant model mismatch are performed. Observability analysis and estimation results of MHE with and without lab measurements show that lab measurements help identify the disturbances and can improve the performance of both estimation and closed-loop control.",
     "keywords": ["Moving horizon estimation", "Nonlinear state estimation", "Multi-rate sampling measurements", "Observability", "Polymer process"]},
    {"article name": "Optimal design of closed-loop supply chain networks with multifunctional nodes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.05.009",
     "publication date": "09-2015",
     "abstract": "This paper introduces a general mathematical programming framework that employs an innovative generalized supply chain network (SCN) composition coupled with forward and reverse logistics activities. Generalized echelon will have the ability to produce/distribute all forward materials/products and recover/redistribute simultaneously all the returned which are categorized with respect to their quality zone. The work addresses a multi-product, multi-echelon and multi-period Mixed-Integer Linear Programming (MILP) problem in a closed-loop supply chain network design solved to global optimality using standard branch-and-bound techniques. Further, the model aims to find the optimal structure of the network in order to satisfy market demand with the minimum overall capital and operational cost. Applicability and robustness of the proposed model are illustrated by using a medium real case study from a European consumer goods company whereas its benefits are valued through a comparison with a counterpart model that utilizes the mainstream fixed echelon network structure.",
     "keywords": ["Closed Loop Supply chain network design", "Generalized networks", "Generalized nodes", "Network design flexibility"]},
    {"article name": "A grid-based facilities allocation approach with safety and optimal heat exchanger networks synthesis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.05.014",
     "publication date": "09-2015",
     "abstract": "A new approach is presented to determine optimal layout of facilities where toxic releases may occur in an existing or new facility. The land area is divided in equally sized rectangular grids, where each grid contains up to one facility surrounded by streets. Some facilities may produce hot and/or cold streams and the associated heat exchangers network (HEN) is simultaneously optimized with the layout problem. The three dimensions of geographical allocation points for each generated stream are included in the model. No additional cost for geographical allocation of heating and cooling services is considered since every facility is expected to contain these services regardless of their use in the HEN. The toxic effect is estimated via probit functions and its associated risk reduction results in providing safety to the combined HEN-facility layout problem. The grid-based allocation eliminates numerical difficulties appearing with conventional non-overlapping and Euclidian distance equations.",
     "keywords": ["Heat exchanger networks", "MINLP", "Grid-allocation", "Toxic releases", "Facility layout", "Safety"]},
    {"article name": "Optimal experimental design for identification of transport coefficient models in convection\u2013diffusion equations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.04.036",
     "publication date": "09-2015",
     "abstract": "Methods for the careful design of optimal experiments for the identification of the structure and parameters of transport models often strongly depend on a-priori knowledge about the unknown model. However, this kind of knowledge is usually poor for complex systems. We propose a novel procedure that is less sensitive with respect to poor a-priori knowledge; it relies on an optimization problem to maximize the information content of the measurement data for the purpose of model identification. Specifically, based on existing model-based methods, optimal design of experiments is addressed in the context of three-dimensional, time-dependent transport problems by introducing experiment design variables and the transport coefficient as degrees of freedom of the optimization. The problem is solved by means of an iterative strategy that \u2013 by sequentially designing a series of experiments \u2013 strives to adjust the settings of the experimental conditions by exploiting the results from previous experiments.The key methodical ingredient of the novel procedure is the use of incremental model identification introduced previously. The suggested procedure is illustrated by means of an extensive numerical case study for a convection\u2013diffusion equation originating from the modeling and simulation of energy transport in laminar wavy film flow.",
     "keywords": ["Distributed parameter systems", "Modeling and identification", "Inverse problem", "Parameter estimation", "Model selection", "Optimal experimental design (OED)"]},
    {"article name": "Combine operations research with molecular biology to stretch pharmacogenomics and personalized medicine\u2014A case study on HIV/AIDS",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.05.017",
     "publication date": "09-2015",
     "abstract": "The dramatic reduction in morbidity and mortality associated with the use of highly active antiretroviral therapy has created new challenges for clinicians: AIDS has become a chronic, potentially life-threatening, condition in many clinical instances. In this paper, a novel system engineering approach based on mixed-integer linear programming (MILP) is presented to support HIV/AIDS clinicians when formulating real-world therapeutic plans for heavily treatment-experienced patients under variable settings. Our results suggest that, while current practices (standard protocols and/or subjective recommendations based on the clinician's experience) can generally provide satisfactory management of drug resistance in the short-term, optimization-based therapy planning has a far greater potential to achieve this goal over expanded time horizons thereby changing paradigms and rethinking best practices in the HIV/AIDS clinical arena. Moreover, the ability of this methodology to address other viral pathologies (e.g., hepatitis B and C virus) can make this work appeal to a broader audience.",
     "keywords": ["Therapy", "Public health", "Mixed-integer programming", "Large-scale optimization"]},
    {"article name": "Improved infinite horizon LQ tracking control for injection molding process against partial actuator failures",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.05.018",
     "publication date": "09-2015",
     "abstract": "Focusing on injection molding processes with partial actuator failures, a new design of infinite horizon linear quadratic control is introduced. A new state space process model is first derived through input\u2013output process data. Furthermore, an improved infinite horizon linear quadratic control scheme, whereby the process state variables and tracking error can both be regulated separately, is proposed to show enhanced control performance against partial actuator failures and unknown disturbances. Under the circumstances of actuator faults, the closed-loop system is indeed a process with uncertain parameters. Hence, a sufficient condition is proposed to guarantee robust stability is presented using Lyapunov theory. The proposed concepts are illustrated in an injection velocity control case study to show the effectiveness.",
     "keywords": ["Injection molding process", "Actuator failures", "State space models", "Infinite horizon linear quadratic control"]},
    {"article name": "Generalized capital investment planning of oil-refineries using MILP and sequence-dependent setups",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.05.013",
     "publication date": "09-2015",
     "abstract": "Due to quantity times quality nonlinear terms inherent in the oil-refining industry, performing industrial-sized capital investment planning (CIP) in this field is traditionally done using linear (LP) or nonlinear (NLP) models whereby a gamut of scenarios are generated and manually searched to make expand and/or install decisions. Though mixed-integer nonlinear (MINLP) solvers have made significant advancements, they are often slow for large industrial applications in optimization; hence, we propose a more tractable approach to solve the CIP problem using a mixed-integer linear programming (MILP) model and input\u2013output (Leontief) models, where the nonlinearities are approximated to linearized operations, activities, or modes in large-scaled flowsheet problems. To model the different types of CIP's known as revamping, retrofitting, and repairing, we unify the modeling by combining planning balances with scheduling concepts of sequence-dependent changeovers to represent the construction, commission, and correction stages explicitly in similar applications such as process design synthesis, asset allocation and utilization, and turnaround and inspection scheduling. Two motivating examples illustrate the modeling, and a retrofit example and an oil-refinery investment planning problem are also highlighted.",
     "keywords": ["Oil-refinery production", "Investment planning", "Process design synthesis", "Sequence-dependent changeovers"]},
    {"article name": "A general spatio-temporal model of energy systems with a detailed account of transport and storage",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.05.019",
     "publication date": "09-2015",
     "abstract": "This paper presents a general spatio-temporal model of energy systems comprising technologies for generation/conversion, transport and storage and infrastructures for transport. The model determines the optimal network structure (e.g. location and size of technologies and their interconnections through transport infrastructures) and its operation (e.g. rate of utilisation of technologies and transport flows) considering simultaneously the short-term dynamics and a long-term planning horizon.Here, we address one of the main challenges of solving a large scale MILP model: tractability. This issue is mainly caused by the need to include a wide range of time scales in the model: yearly (or decadal) intervals to include investment decisions; seasonal intervals to account for e.g. seasonal variations in demand and availability of resources; and hourly (or shorter) intervals to model the dynamics of storage technologies and to account for intermittency of renewable resources and demand. To exacerbate the problem, the spatial aspects also need to be fine enough to locate and size the technologies properly and to model the transport of resources, which depend on the location of demand and availability of resources. The model uses an efficient representation of time that exploits periodicity in system properties via a non-uniform hierarchical time discretisation. A decomposition method is also proposed wherein the large problem is broken down into 3 sub-problems that are then solved iteratively until the objective function is no longer improved. These methods significantly improve the computational efficiency without sacrificing temporal and spatial detail.The applicability of the model is illustrated using a case study in which the least-cost design and operation of a hydrogen network is determined such that the hourly transport demand of the different regions of an island is met by the intermittent and remotely located wind energy.",
     "keywords": ["Energy systems", "Energy storage and transport", "Resource-Technology Network", "Spatio-temporal modelling", "Optimisation", "Hydrogen networks"]},
    {"article name": "Systematic retrofitting methodology for pharmaceutical drug purification processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.05.024",
     "publication date": "09-2015",
     "abstract": "A systematic retrofitting methodology supported by real data implementation was developed to facilitate the optimization of pharmaceutical drug purification processes. The methodology consists of five tasks: (I) understand the plant through measurements, (II) create a process model, (III) adapt the model for optimization, (IV) optimize the process model, and (V) interpret the outcome of the task. A novelty of this methodology is the use of path-flow decomposition as a tool to provide a visual representation of the process, thus facilitating its understanding and the creation of the process model. Furthermore, a decision-tree diagram furnishes detailed and specific support during the creation of the process model, in particular for a dissolution\u2013filtration\u2013crystallization process. Identification of constraints and optimization variables ensure the applicability of the methodology to pharmaceutics. Incorporation of measured process data improves the reliability of the methodology, making it applicable in real case studies. The methodology was applied to an industrial case study.",
     "keywords": ["Path-flow decomposition", "Process simulation", "MINLP", "Thermally unstable drug", "Sterilization"]},
    {"article name": "A new method to detect and quantify correlated alarms with occurrence delays",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.05.028",
     "publication date": "09-2015",
     "abstract": "This paper proposes a new method to detect correlated alarms and quantify the correlation level to improve the management of industrial alarm systems. The method is mainly composed of three parts. First, a so-called occurrence delay is defined as the main cause leading to erroneous conclusions from existing methods to detect correlated alarms. In order to tolerate the presence of occurrence delays, a mechanism is presented to generate continuous-valued pseudo alarm signals. Second, a novel approach is given to estimate the correlation delay between alarm signals, so that the correlation delay can be separated from occurrence delays to obtain real occurrence delays (ROD). Third, a statistical test based on the ROD is proposed to determine whether two alarm signals are correlated or not, and the Pearson's correlation coefficient is applied to quantify the correlation level. Numerical examples and industrial case studies are provided to support the proposed method.",
     "keywords": ["Alarm management", "Correlated alarms", "Occurrence delays", "Correlation delays", "Pearson's correlation coefficient"]},
    {"article name": "An incremental approach using local-search heuristic for inventory routing problem in industrial gases",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.05.023",
     "publication date": "09-2015",
     "abstract": "In this paper we solve the inventory routing problem (IRP) occurring in industrial gas distribution where liquefied industrial gases are distributed to customers that have cryogenic tanks to store the gases on-site. We consider a multi-period inventory routing problem with multiple products assuming deterministic demand rates and the proposed model is formulated as a linear mixed-integer program. We propose an incremental approach based on decomposing the set of customers in the original problem into sub-problems. The smallest sub-problem consists of the customer that needs to be delivered most urgently along with a set of its neighbors. We solve each sub-problem with the number of customers growing successively by providing the solution of the previously solved sub-problem as an input. Each sub-problem is then solved with a randomized local-search heuristic method. We also propose an objective function that drives the local-search heuristics toward a long-term optimal solution. The main purpose of this paper is to develop a solution methodology appropriate for large-scale real-life problem instances particularly in industrial gas distribution.",
     "keywords": ["Vendor managed inventory", "Liquefied gases", "Industrial gases distribution", "Inventory routing problem", "Bulk gas distribution", "Local-search heuristic"]},
    {"article name": "Construction of analytical solutions and numerical methods comparison of the ideal continuous settling model",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.05.025",
     "publication date": "09-2015",
     "abstract": "Continuous sediment has a wide application in chemical engineering process, such as wastewater treatment, water reuse, mineral waste manage and processing. This paper provides analytical solutions of the ideal continuous settling model by using the method of characteristics. The analytical solutions are compared with experiment data to show that the solutions accurately predict the sediment height and concentration as a function of time and loading conditions. Additionally, three alternative methods, using finite differences are compared to the analytical solutions, and their accuracy and efficiency are evaluated. It is shown that all three methods are reliable for solving sedimentation problems but have varying efficiency. Method YRD is the most accurate but also has the greatest computation and implementation cost. Method SG is the least accurate but is the easiest to implement with lowest computation cost. Method G is a compromise between the two methods, providing acceptable accuracy and low computation cost.",
     "keywords": ["Continuous sedimentation", "Method of characteristics", "Hyperbolic partial differential equation", "Numerical methods"]},
    {"article name": "Lagrangian decomposition approach to scheduling large-scale refinery operations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.04.021",
     "publication date": "08-2015",
     "abstract": "This work focuses on the scheduling of refinery operations from crude oil processing to the blending and dispatch of finished products. A new algorithm for Lagrangian decomposition (LD) is proposed and applied to realistic large scale refinery scheduling problem to evaluate its efficiency. A novel strategy is presented to formulate restricted relaxed sub-problems based on the solution of the Lagrangian relaxed sub-problems that take into consideration the continuous process characteristic of the refinery. This new algorithm, referred to as restricted Lagrangian decomposition algorithm, the best lower bound is obtained amongst the restricted-relaxed sub-problems and relaxed sub-problems in each iteration. The goal of the decomposition is to produce better solutions for those integrated scheduling problems that cannot be solved in reasonable computation times. The application of the proposed algorithm results in substantial reduction in CPU solution time, duality gap, and the total number of iterations compared to classical LD.",
     "keywords": ["Refinery operations scheduling", "Blend scheduling", "Lagrangian decomposition", "Mixed-integer linear programming"]},
    {"article name": "Improving scenario decomposition algorithms for robust nonlinear model predictive control",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.04.024",
     "publication date": "08-2015",
     "abstract": "This paper deals with the efficient computation of solutions of robust nonlinear model predictive control problems that are formulated using multi-stage stochastic programming via the generation of a scenario tree. Such a formulation makes it possible to consider explicitly the concept of recourse, which is inherent to any receding horizon approach, but it results in large-scale optimization problems. One possibility to solve these problems in an efficient manner is to decompose the large-scale optimization problem into several subproblems that are iteratively modified and repeatedly solved until a solution to the original problem is achieved. In this paper we review the most common methods used for such decomposition and apply them to solve robust nonlinear model predictive control problems in a distributed fashion. We also propose a novel method to reduce the number of iterations of the coordination algorithm needed for the decomposition methods to converge. The performance of the different approaches is evaluated in extensive simulation studies of two nonlinear case studies.",
     "keywords": ["Economic model predictive control", "Uncertainty", "Robust control", "Distributed computing", "Optimization"]},
    {"article name": "Reduction of complex energy-integrated process networks using graph theory",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.04.025",
     "publication date": "08-2015",
     "abstract": "This paper focuses on the analysis of complex (multi-loop) energy-integrated process networks. Simple (single-loop) energy-integrated networks (comprising of large energy recycle or throughput) with two-time scale dynamics are the building blocks for such complex networks. The modular structure of these complex networks lends them to a graph theoretic analysis, whereby weak and strong connections between process units arising from time scale separation are identified from structural information. Subsequently, a graph-theoretic framework for network analysis and control is developed, and connecting links are built to an equivalent analysis using singular perturbations. The proposed analysis framework is illustrated via application to a representative complex process network.",
     "keywords": ["Graph theory", "Hierarchical control", "Integrated plant control"]},
    {"article name": "A time scale-bridging approach for integrating production scheduling and process control",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.04.026",
     "publication date": "08-2015",
     "abstract": "In this paper, we propose a novel framework for integrating scheduling and nonlinear control of continuous processes. We introduce the time scale-bridging model (SBM) as an explicit, low-order representation of the closed-loop input\u2013output dynamics of the process. The SBM then represents the process dynamics in a scheduling framework geared towards calculating the optimal time-varying setpoint vector for the process control system. The proposed framework accounts for process dynamics at the scheduling stage, while maintaining closed-loop stability and disturbance rejection properties via feedback control during the production cycle. Using two case studies, a CSTR and a polymerization reactor, we show that SBM-based scheduling has significant computational advantages compared to existing integrated scheduling and control formulations. Moreover, we show that the economic performance of our framework is comparable to that of existing approaches when a perfect process model is available, with the added benefit of superior robustness to plant-model mismatch.",
     "keywords": ["Production scheduling", "Nonlinear control", "Integrated scheduling and control"]},
    {"article name": "Simultaneous synthesis of a heat exchanger network with multiple utilities using utility substages",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.04.005",
     "publication date": "08-2015",
     "abstract": "Heat exchanger network synthesis (HENS) has progressed by using mathematical programming-based simultaneous methodology. Although various considerations such as non-isothermal mixing and bypass streams are applied to consider real world alternatives in modeling phase, many challenges are faced because of its properties within non-convex mixed-integer nonlinear programming (MINLP). We propose a modified superstructure, which contains a utility substage for use in considering multiple utilities in a simultaneous MINLP model. To improve model size and convergence, fixed utility locations according to temperature and series connections between utilities are suggested. The numbers of constraints, discrete, and continuous variables show that overall model size decreases compared with previous research. Thus, it is possible to expand the feasible search area for reaching the nearest global solution. The model's effectiveness and applications are exemplified by several literature problems, where it is used to deduce a network superior to that of any other reported methodology.",
     "keywords": ["Heat exchanger network synthesis (HENS)", "Multiple utilities", "Mixed-integer nonlinear programming (MINLP)", "Mathematical programming", "Discrete variable"]},
    {"article name": "Predictive models and operation guidance system for iron ore pellet induration in traveling grate\u2013rotary kiln process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.04.035",
     "publication date": "08-2015",
     "abstract": "Thermal state of iron ore pellets in industrial traveling grate\u2013rotary kiln process cannot be revealed straightforward, which is unfavorable for field operations. In this study, coupled predictive models of pellet thermal state within traveling grate and rotary kiln were established. Based on the calculated temperature profiles, predictive model of pellet compression strength was also established to assist in process optimization. All the models proposed were validated by the industrial data collected from a domestic plant, and the results show that grate model possesses a high accuracy, kiln model is considered to be accurate to within 10\u201315% of actual values, and strength model can identify the variation of pellet strength caused by the thermal changes. The proposed models were embodied into an operation guidance system developed for a large-scale pelletizing plant, and the system running results illustrate that the predictive models and expertise rules established can optimize the process very well.",
     "keywords": ["Predictive models", "Iron ore pellets", "Traveling grate", "Rotary kiln", "Operation guidance"]},
    {"article name": "Exergy analysis in ProSimPlus\u00ae simulation software: A focus on exergy efficiency evaluation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.02.014",
     "publication date": "08-2015",
     "abstract": "On industrial sites, the promotion of best practices to enable an efficient utilization of energy has emerged as one of the major point of focus. Among the different approaches existing to improve industrial processes, the exergy analysis, although limited to the academic world, has been shown to be a powerful tool for improving energy efficiency of thermal and chemical systems. The purpose of this paper is then to present the use of the ProSimPlus\u00ae modelling and simulation environment as an exergy analysis computer-aided tool. Expressions implemented in the simulator for computing exergies in its various forms are presented. The adopted approach for calculating exergy efficiency in a systematic way is also exposed; it combines the fuel-product concepts to the transit exergy concept. ProsimPlus\u00ae exergy module's capabilities are illustrated through the example of an ammonia production plant.",
     "keywords": ["Exergy analysis", "Exergy efficiency", "Process integration", "Process simulator"]},
    {"article name": "Planning and scheduling of steel plates production. Part I: Estimation of production times via hybrid Bayesian networks for large domain of discrete variables",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.02.005",
     "publication date": "08-2015",
     "abstract": "Knowledge of the production loads and production times is an essential ingredient for making successful production plans and schedules. In steel production, the production loads and the production times are impacted by many uncertainties, which necessitates their prediction via stochastic models. In order to avoid having separate prediction models for planning and for scheduling, it is helpful to develop a single prediction model that allows us to predict both production loads and production times. In this work, Bayesian network models are employed to predict the probability distributions of these variables. First, network structure is identified by maximizing the Bayesian scores that include the likelihood and model complexity. In order to handle large domain of discrete variables, a novel decision-tree structured conditional probability table based Bayesian inference algorithm is developed. We present results for real-world steel production data and show that the proposed models can accurately predict the probability distributions.",
     "keywords": ["Estimation of production time", "Hybrid Bayesian network", "Structure learning", "Large domain of discrete variables", "Decision tree", "Steel plate production"]},
    {"article name": "An example of Pareto dominance for dimensionality reduction in multi-objective optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.04.023",
     "publication date": "08-2015",
     "abstract": "In this Letter to the Editor, we propose some changes to an example of Pareto dominance by Copado-M\u00e9ndez et al., since the original errors in the article change the understanding of the reader. The proposed changes come from a comparison with a previous work of Brockhoff and Zitzler. Sustained on this comparison, the example of Pareto dominance is reconstructed. The original article is an excellent work, very sound and useful. We want to contribute to a better understanding of the audience.",
     "keywords": null},
    {"article name": "Efficient ant colony optimization for computer aided molecular design: Case study solvent selection problem",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.04.004",
     "publication date": "07-2015",
     "abstract": "In this paper, we propose a novel computer-aided molecular design (CAMD) methodology for the design of optimal solvents based on an efficient ant colony optimization (EACO) algorithm. The molecular design problem is formulated as a mixed integer nonlinear programming (MINLP) model in which a solvent performance measure is maximized (solute distribution coefficient) subject to structural feasibility, property, and process constraints. In developing the EACO algorithm, the better uniformity property of Hammersley sequence sampling (HSS) is exploited. The capabilities of the proposed methodology are illustrated using a real world case study for the design of an optimal solvent for extraction of acetic acid from waste process stream using liquid\u2013liquid extraction. The UNIFAC model based on the infinite dilution activity coefficient is used to estimate the mixture properties. New solvents with better targeted properties are proposed.",
     "keywords": ["Ant colony optimization", "Group contribution method", "Computer aided molecular design", "Hammersley sequence sampling", "Oracle penalty function", "UNIFAC"]},
    {"article name": "Solid oxide fuel cell reactor analysis and optimisation through a novel multi-scale modelling strategy",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.04.006",
     "publication date": "07-2015",
     "abstract": "The simulation of a solid oxide fuel cell (SOFC) that incorporates a detailed user-developed model was performed within the commercial flowsheet simulator Aspen Plus. It allows modification of the SOFC's governing equations, as well as the configuration of the cell's fuel-air flow pattern at the flowsheet level. Initially, the dynamic behaviour of single compartment of a cell was examined with a 0D model, which became the building block for more complex SOFC configurations. Secondly, a sensitivity analysis was performed at the channel (1D) scale for different flow patterns. Thirdly, the effect of fuel and air flow rates on the predominant distributed variables of a cell was tested on a 2D assembly. Finally, an optimisation study was carried out on the 2D cell, leading to a robust, optimal air distribution profile that minimises the internal temperature gradient. This work forms the foundation of future stack and system scale studies.",
     "keywords": ["Solid oxide fuel cell", "Aspen Plus", "Multi-scale", "Optimisation", "Modelling"]},
    {"article name": "Fully coupled LES-DEM of particle interaction and agglomeration in a turbulent channel flow",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.04.003",
     "publication date": "07-2015",
     "abstract": "Coupled large eddy simulation and the discrete element method are applied to study turbulent particle\u2013laden flows, including particle dispersion and agglomeration, in a channel. The particle\u2013particle interaction model is based on the Hertz\u2013Mindlin approach with Johnson\u2013Kendall\u2013Roberts cohesion to allow the simulation of van der Waals forces in a dry air flow. The influence of different particle surface energies, and the impact of fluid turbulence, on agglomeration behaviour are investigated. The agglomeration rate is found to be strongly influenced by the particle surface energy, with a positive relationship observed between the two. Particle agglomeration is found to be enhanced in two separate regions within the channel. First, in the near-wall region due to the high particle concentration there driven by turbophoresis, and secondly in the buffer region where the high turbulence intensity enhances particle\u2013particle interactions.",
     "keywords": ["Large eddy simulation", "Discrete element method", "Two-phase flow", "Agglomeration", "Channel"]},
    {"article name": "Initialization strategies for optimization of dynamic systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.04.016",
     "publication date": "07-2015",
     "abstract": "For dynamic optimization applications, real-time solution reliability is improved if there is an initialized prior solution that is sufficiently close to the intended solution. This paper details several initialization strategies that are useful for obtaining an initial solution. Methods include warm start from a prior solution, linearization, structural decomposition, and an incremental unbounding of decision variables that leads up to solving the originally intended problem. Even when initialization is not required to solve a dynamic optimization problem, a staged initialization approach sometimes leads to an overall faster solution time when compared to a single optimization attempt. Several challenging optimization problems are detailed that include a high-index differential and algebraic equation pendulum model, a standard reactor model used in many benchmark tests, a tethered aerial vehicle, and smart grid energy storage. These applications are representative of a larger class of applications resulting from the simultaneous approach to optimization of dynamic systems.",
     "keywords": ["Initialization", "Decomposition", "Differential algebraic equations", "Dynamic optimization", "Large-scale", "Smart grid energy system"]},
    {"article name": "Data-driven individual and joint chance-constrained optimization via kernel smoothing",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.04.012",
     "publication date": "07-2015",
     "abstract": "We propose a data-driven, nonparametric approach to reformulate (conditional) individual and joint chance constraints with right-hand side uncertainty into algebraic constraints. The approach consists of using kernel smoothing to approximate unknown true continuous probability density/distribution functions. Given historical data for continuous univariate or multivariate random variables (uncertain parameters in an optimization model), the inverse cumulative distribution function (quantile function) and the joint cumulative distribution function are estimated for the univariate and multivariate cases, respectively. The approach relies on the construction of a confidence set that contains the unknown true distribution. The distance between the true distribution and its estimate is modeled via \u03d5-divergences. We propose a new way of specifying the size of the confidence set (i.e., the \u03d5-divergence tolerance) based on point-wise standard errors of the smoothing estimates. The approach is illustrated with a motivating and an industrial production planning problem with uncertain plant production rates.",
     "keywords": ["90C15", "90C90", "Process systems engineering", "Data-driven chance constraint", "Kernel smoothing", "\u03d5-Divergence"]},
    {"article name": "Predictive control with multiobjective optimization: Application to a sludge drying operation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.04.017",
     "publication date": "07-2015",
     "abstract": "The main objective of this study is to develop an offline tuning of the operating input parameters for a sludge drying operation, by using multiobjective optimization techniques combined with a predictive control method. The manipulated variables concerned are the temperature and the relative humidity of the drying air (Tair, RHair). The optimal time for the reversal operation of the product is also investigated. The optimization procedure is coupled to a one-dimensional numerical model that allows the simulation of moisture content and temperature field evolutions in the product during the drying step. A genetic algorithm is used to identify the two manipulated variables, at each step time, by minimizing simultaneously three objective functions over a finite horizon. These objective functions are linked to penalties concerning the heating and dehumidifying of the outside air used for the drying stage and to a global moisture content gap relative to a drying target. First, the heat and mass transfer model is validated for the drying step of a plate sample of sludge, with a reversal operation. Afterwards, the optimization procedure is carried out, and the results are discussed in terms of an energetic analysis.",
     "keywords": ["Offline predictive control", "Multiobjective optimization", "Multiphysics modeling", "Operating input parameters"]},
    {"article name": "Generic mathematical programming formulation and solution for computer-aided molecular design",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.04.022",
     "publication date": "07-2015",
     "abstract": "This short communication presents a generic mathematical programming formulation for computer-aided molecular design (CAMD). A given CAMD problem, based on target properties, is formulated as a mixed integer linear/non-linear program (MILP/MINLP). The mathematical programming model presented here, which is formulated as an MILP/MINLP problem, considers first-order and second-order molecular groups for molecular structure representation and property estimation. It is shown that various CAMD problems can be formulated and solved through this model.",
     "keywords": ["Molecular design", "CAMD", "Chemical structure", "Group contribution", "MILP/MINLP"]},
    {"article name": "Optimization of a reactive distillation process with intermediate condensers for silane production",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.04.014",
     "publication date": "07-2015",
     "abstract": "This work presents a reactive distillation column for the catalytic disproportionation of trichlorosilane to silane which includes three consecutive reversible reactions. This reaction system is however characterized by a large distinction in the boiling points of the components, which make the reactive distillation extremely favored. Nevertheless, the normal reactive distillation column possesses the shortage of high refrigeration requirement. By removing heat at temperature higher than that at the condenser a superstructure representation, rigorous simulations, and optimization problems were combined to derive optimal reactive distillation columns which can realize heat integration between stages and utilities at several refrigeration conditions. An iterative simulation-optimization procedure was proposed to consider temperature changes in stages due to heat integration. The results showed that the installation of two inter-condensers results in the best option with economic savings up to 56%.",
     "keywords": ["Reactive distillation", "Silanes", "Intermediate condensers", "Optimization"]},
    {"article name": "A combined canonical variate analysis and Fisher discriminant analysis (CVA\u2013FDA) approach for fault diagnosis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.03.001",
     "publication date": "06-2015",
     "abstract": "This paper proposes a combined canonical variate analysis (CVA) and Fisher discriminant analysis (FDA) scheme (denoted as CVA\u2013FDA) for fault diagnosis, which employs CVA for pretreating the data and subsequently utilizes FDA for fault classification. In addition to the improved handling of serial correlations in the data, the utilization of CVA in the first step provides similar or reduced dimensionality of the pretreated datasets compared with the original datasets, as well as decreased degree of overlap. The effectiveness of the proposed approach is demonstrated on the Tennessee Eastman process. The simulation results demonstrate that (i) CVA\u2013FDA provides better and more consistent fault diagnosis than FDA, especially for data rich in dynamic behavior; and (ii) CVA\u2013FDA outperforms dynamic FDA in both discriminatory power and computational time.",
     "keywords": ["Fault diagnosis", "Canonical variate analysis", "Fisher discriminant analysis", "Dynamic FDA", "Tennessee Eastman process", "Process monitoring"]},
    {"article name": "A unified data-driven design framework of optimality-based generalized iterative learning control",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.03.003",
     "publication date": "06-2015",
     "abstract": "This paper proposes a unified design framework for data-driven optimality-based generalized iterative learning control (DDOGILC), including data-driven optimal ILC (DDOILC), data-driven optimal point-to-point ILC (DDOPTPILC), and data-driven optimal terminal ILC (DDTILC). First, a dynamical linearization in the iteration domain is developed. Then three specific DDOGILC approaches are proposed. Both design and analysis of the controller only require the measured I/O data without relying on any explicit model information. The optimal learning gain can be updated iteratively, which makes the proposed DDOGILC more adaptable to the changes in the plant. Furthermore, the proposed DDOPTPILC and DDOTILC only depend on the tracking error at specific points, and thus they can deal with the scenario when the system outputs are measured only at some time instants. Moreover, the proposed DDOPTPILC and DDOTILC approaches do not need to track the unnecessary output reference points so that the convergence performance is improved.",
     "keywords": ["Data-driven control", "Norm optimal design", "Iterative learning control", "Point-to-point iterative learning control", "Terminal iterative learning control", "Nonlinear discrete-time systems"]},
    {"article name": "Nonlinear ill-posed problem analysis in model-based parameter estimation and experimental design",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.03.002",
     "publication date": "06-2015",
     "abstract": "Discrete ill-posed problems are often encountered in engineering applications. Still, their sound analysis is not yet common practice and difficulties arising in the determination of uncertain parameters are typically not assigned properly. This contribution provides a tutorial review on methods for identifiability analysis, regularization techniques and optimal experimental design. A guideline for the analysis and classification of nonlinear ill-posed problems to detect practical identifiability problems is given. Techniques for the regularization of experimental design problems resulting from ill-posed parameter estimations are discussed. Applications are presented for three different case studies of increasing complexity.",
     "keywords": ["Ill-posed problems", "Ill-conditioning analysis", "Singular value decomposition", "Identifiability problems", "Parameter subset selection", "Tikhonov regularization"]},
    {"article name": "Pattern-based supply network planning in the pharmaceutical industry",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.03.006",
     "publication date": "06-2015",
     "abstract": "Manufacturing in process industries mostly takes place in multi-level supply networks consisting of several plants which are located at different sites and linked by complex logistics relations and material flows. Thus, a key issue is the coordination of plant operations considering the supply of intermediates between various plants. Time-consuming setup and cleaning activities require production to be carried out in campaigns. We propose an application-oriented approach for supply network planning in the pharmaceutical industry which is based on a novel aggregation scheme and a disjunctive graph representation of the problem. A mixed-integer linear programming model is proposed based on a continuous representation of time. Our approach generates a comprehensive schedule which coordinates the production activities, including the campaigns in the network. Numerical results show that our approach yields solutions for small- and medium-sized problem instances very quickly and is even capable of coping with the complexity of real world problems.",
     "keywords": ["Supply network planning", "Process industry", "Aggregation techniques", "State-task-networks"]},
    {"article name": "Multi-bucket optimization for integrated planning and scheduling in the perishable dairy supply chain",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.03.020",
     "publication date": "06-2015",
     "abstract": "This paper considers a dairy industry problem on integrated planning and scheduling of set yoghurt production. A mixed integer linear programming formulation is introduced to integrate tactical and operational decisions and a heuristic approach is proposed to decompose time buckets of the decisions. The decomposition heuristic improves computational efficiency by solving big bucket planning and small bucket scheduling problems. Further, mixed integer linear programming and constraint programming methodologies are combined with the algorithm to show their complementary strengths. Numerical studies using illustrative data with high demand granularity (i.e., a large number of small-sized customer orders) demonstrate that the proposed decomposition heuristic has consistent results minimizing the total cost (i.e., on average 8.75% gap with the best lower bound value found by MILP) and, the developed hybrid approach is capable of solving real sized instances within a reasonable amount of time (i.e., on average 92% faster than MILP in CPU time).",
     "keywords": ["Integrated planning and scheduling", "Mixed integer linear programming", "Constraint programming", "Hybrid approach", "Perishability", "Dairy supply chain"]},
    {"article name": "Deconstructing principal component analysis using a data reconciliation perspective",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.03.016",
     "publication date": "06-2015",
     "abstract": "Data reconciliation (DR) and principal component analysis (PCA) are two popular data analysis techniques in process industries. Data reconciliation is used to obtain accurate and consistent estimates of variables and parameters from erroneous measurements. PCA is primarily used as a method for reducing the dimensionality of high dimensional data and as a preprocessing technique for denoising measurements. These techniques have been developed and deployed independently of each other. The primary purpose of this article is to elucidate the close relationship between these two seemingly disparate techniques. This leads to a unified framework for applying PCA and DR. Further, we show how the two techniques can be deployed together in a collaborative and consistent manner to process data. The framework has been extended to deal with partially measured systems and to incorporate partial knowledge available about the process model.",
     "keywords": ["Data reconciliation", "Principal component analysis", "Model identification", "Estimation", "Denoising"]},
    {"article name": "A decision support system for the operational production planning and scheduling of an integrated pulp and paper mill",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.03.017",
     "publication date": "06-2015",
     "abstract": "Production planning and scheduling in the process industry in general and in the pulp and paper (P&P) sector in particular can be very challenging. Most practitioners, however, address those activities relying only on spreadsheets, which is time-consuming and sub-optimal. The literature has reported some decision support systems (DSSs) that are far from the state-of-the-art with regard to optimization models and methods, and several research works that do not address industrial issues. We contribute to reduce that gap by developing and describing a DSS that resulted from several iterations with a P&P company and from a thorough review of the literature on process systems engineering. The DSS incorporates relevant industrial features (which motivated the development of a specific model), exhibits important technical details (such as the connection to existing systems and user-friendly interfaces) and shows how optimization can be integrated in real world applications, enhanced by key pre- and post-optimization procedures.",
     "keywords": ["Decision support system", "Lot-sizing and scheduling", "MIP-based heuristics", "Pulp and paper industry", "Continuous production"]},
    {"article name": "Agent assisted interactive algorithm for computationally demanding multiobjective optimization problems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.03.004",
     "publication date": "06-2015",
     "abstract": "We generalize the applicability of interactive methods for solving computationally demanding, that is, time-consuming, multiobjective optimization problems. For this purpose we propose a new agent assisted interactive algorithm. It employs a computationally inexpensive surrogate problem and four different agents that intelligently update the surrogate based on the preferences specified by a decision maker. In this way, we decrease the waiting times imposed on the decision maker during the interactive solution process and at the same time decrease the amount of preference information expected from the decision maker. The agent assisted algorithm is not specific to any interactive method or surrogate problem. As an example we implement our algorithm for the interactive NIMBUS method and the PAINT method for constructing the surrogate. This implementation was applied to support a real decision maker in solving a two-stage separation problem.",
     "keywords": ["Multiple objective programming", "Interactive methods", "Agent-based optimization", "Surrogate problem", "NIMBUS", "PAINT"]},
    {"article name": "Steady-state optimization of chemical processes with guaranteed robust stability and controllability under parametric uncertainty and disturbances",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.04.001",
     "publication date": "06-2015",
     "abstract": "A new methodology is proposed for the steady-state optimal design of chemical processes under parametric uncertainty and disturbances. The methodology allows the integration of uniform constraints for robust controllability and stability in an optimization problem by using the Routh\u2013Hurwitz test and zero dynamics based method. The underlying mathematical problem is difficult to solve because it involves infinite stability and controllability constraints. We developed an algorithm where an infinite number of constraints can be implemented as several relaxation problems that are solved iteratively. Additionally, the dynamic simulation results under parametric uncertainty and disturbances are also used to estimate the bound of the state perturbations rather than assumptions based on experience, which may lead to overly conservative or non-implementable design. To illustrate the methodology, three different examples are presented and robustly stable and controllable designs are obtained.",
     "keywords": ["Robust stability", "Controllability", "Steady-state optimization", "Parametric uncertainty", "Disturbance"]},
    {"article name": "Coupling of smoothed particle hydrodynamics and finite volume method for two-dimensional spouted beds",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.04.002",
     "publication date": "06-2015",
     "abstract": "A coupled method with smoothed particle hydrodynamics (SPH) and finite volume method (FVM) is proposed in this work for the simulation of the particle dynamics in two-dimensional spouted beds. Based on the pseudo-fluid model, SPH is used for discrete phase to trace the movement of each individual particle and FVM for continue phase to compute the turbulent fluid. Two phases are coupled through effects of drag force, gas pressure and volume fraction of each phase. A two-dimensional tapered-based spouted bed is chosen as a case study to demonstrate the performance of the SPH\u2013FVM coupled algorithm. The simulation results show a good agreement with the experimental data and other simulation results by the two-fluid model and discrete element method in the literature. The spouted shape, time-averaged particle velocities and particle vertical velocities in the spout are analyzed and the distribution of gas flow field and turbulent kinetic energy are then discussed. It indicates that the present method is more suitable to study the fluidization within the spouted beds.",
     "keywords": ["Smoothed particle hydrodynamics", "Finite volume method", "Coupled method", "Spouted bed", "Particle dynamics", "Fluidization"]},
    {"article name": "Alternative mixed-integer linear programming models of a maritime inventory routing problem",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.03.005",
     "publication date": "06-2015",
     "abstract": "A single product maritime inventory routing problem is addressed in this paper by exploring the use of continuous and discrete time models. We first present a continuous time model based on time slots for single docks, which is enhanced by reformulating the time assignment constraints. Next, we present a model based on event points to handle parallel docks. A discrete time is also presented based on a single commodity fixed-charge network flow problem (FCNF). All the models are solved for multiple randomly generated instances of different problems to compare their computational efficiency, and to illustrate the solutions obtained.",
     "keywords": ["Maritime transportation", "Inventory routing", "Maritime scheduling", "Mixed-integer linear programming"]},
    {"article name": "Topological preservation techniques for nonlinear process monitoring",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.02.002",
     "publication date": "05-2015",
     "abstract": "This work proposes a novel approach for the offline development and online implementation of data-driven process monitoring (PM) using topological preservation techniques, specifically self-organizing maps (SOM). Previous topological preservation PM applications have been restricted due to the lack of monitoring and diagnosis tools. In the proposed approach, the capabilities of SOM are further extended so that all aspects of PM can be performed in a single environment. First for fault detection, using SOM's vector quantization abilities, an SOM-based Gaussian mixture model (GMM) is proposed to define the normal region. For identification, an SOM-based contribution plot is proposed to identify the variables most responsible for the fault. This is done by analyzing the residual of the faulty point and an SOM model of the normal region used in fault detection. The data points are projected on the model by locating the best matching unit (BMU) of the point. Finally, for fault diagnosis a procedure is formulated involving the concept of multiple self-organizing maps (MSOM), creating a map for each fault. This allows the ability to include new faults without directly affecting previously characterized faults. A Tennessee Eastman Process (TEP) application is performed on dynamic faults such as random variations, sticky valves and a slow drift in kinetics. Previous studies of the TEP have considered particular feed-step-change faults. Results indicate an excellent performance when compared to linear and nonlinear distance preservation techniques and standard nonlinear SOM approaches in fault diagnosis and identification.",
     "keywords": ["Process monitoring", "Fault detection", "Fault diagnosis", "Self-organizing maps", "Gaussian mixture model"]},
    {"article name": "Analytical solution of isothermal semi-batch reactor with proceeding irreversible second-order reaction of Class II: A\u00a0+\u00a0B\u00a0\u2192\u00a0PROD",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.02.001",
     "publication date": "05-2015",
     "abstract": "A formulation of the time dependent molar balance equations, one of which is a nonlinear Bernoulli differential equation, is a preliminary step to modeling an isothermal semi-batch reactor with irreversible second-order reactions. The molar balance equations obtained in this fashion are classified as first order nonlinear ODEs. This paper presents a methodology to replace the nonlinear ODE that characterizes the molar composition of the reactants with an equivalent, linear ODE derived from a Bernoulli ODE to obtain an analytical solution. This approach leads to a general solution. The analytical results using this methodology are subsequently compared to numerical solutions to verify the accuracy of the model.",
     "keywords": ["Modeling of a semi-batch reactor", "2nd order kinetics of Class II", "Analytical solution", "Bernoulli ODE"]},
    {"article name": "The optimization and prediction of properties for crude oil blending",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.02.006",
     "publication date": "05-2015",
     "abstract": "Research into the properties of mixed crude oils has been the most important research for refineries and oil transportation. Because the desirable nature of mixed crude oils is a powerful guarantee for normal operations and corporate earnings. In this paper, the optimization and prediction model for Shanbei crude oil blending is established. The objectives were to improve the total yield of fractions and the compatibility and to reduce the viscosity of the mixed crude oils. After solving the model, two applicable proportions for mixed crude oils were given. With these proportions, two samples of mixed oils were prepared and distilled. The test results showed that the model was valid. The study of the viscosity and compatibility index was introduced to the optimization model because forecasting those properties in mixed crude oils has a certain significance.",
     "keywords": ["Optimization and prediction", "Crude oil blending", "Compatibility", "Dynamic viscosity", "Fouling and coking"]},
    {"article name": "Review and classification of recent observers applied in chemical process systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.01.019",
     "publication date": "05-2015",
     "abstract": "Observers are computational algorithms designed to estimate unmeasured state variables due to the lack of appropriate estimating devices or to replace high-priced sensors in a plant. It is always important to estimate those states prior to developing state feedback laws for control and to prevent process disruptions, process shutdowns and even process failures. The diversity of state estimation techniques resulting from intrinsic differences in chemical process systems makes it difficult to select the proper technique from a theoretical or practical point of view for design and implementation in specific applications. Hence, in this paper, we review the applications of recent observers to chemical process systems and classify them into six classes, which differentiate them with respect to their features and assists in the design of observers. Furthermore, we provide guidelines in designing and choosing the observers for particular applications, and we discuss the future directions for these observers.",
     "keywords": ["Review", "Observer", "State estimation", "Chemical process"]},
    {"article name": "The heterogeneous vehicle routing and truck scheduling problem in a multi-door cross-dock system",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.02.003",
     "publication date": "05-2015",
     "abstract": "Cross-docking is a logistics technique applied by many industrial firms to get substantial savings in two warehousing costly functions like storage and order picking. Incoming shipments are unloaded from inbound trucks on a cross-dock terminal with minimal storage space and directly transferred to outbound vehicles that carry them to their destinations. The major decisions at the operational level are the vehicle routing and scheduling, the dock door assignment and the truck scheduling at the cross-dock. Because such decisions are interdependent, all of them are simultaneously considered in the so-called vehicle routing problem with cross-docking (VRPCD). Previous contributions on VRPCD assume that pickup and delivery tasks are accomplished by a homogeneous vehicle fleet, and they mostly ignore the internal transportation of goods through the cross-dock. This work introduces a new rigorous mixed-integer linear programming (MILP) formulation for the VRPCD problem to determine the routing and scheduling of a mixed vehicle fleet, the dock door assignment, the truck docking sequence and the travel time required to move the goods to the assigned stack door all at once. To improve the computational efficiency of the branch-and-cut search, an approximate sweep-based model is developed by also considering a set of constraints mimicking the sweep algorithm for allocating nodes to vehicles. Numerous heterogeneous VRPCD examples involving up to 50 transportation requests and a heterogeneous fleet of 10 vehicles with three different capacities were successfully solved using the proposed approaches in acceptable CPU times.",
     "keywords": ["Vehicle routing", "Cross-docking", "Truck scheduling", "Logistics"]},
    {"article name": "Fault detection and diagnosis with parametric uncertainty using generalized polynomial chaos",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.02.009",
     "publication date": "05-2015",
     "abstract": "This paper presents a new methodology to identify and diagnose intermittent stochastic faults occurring in a process. A generalized polynomial chaos (gPC) expansion representing the stochastic inputs is employed in combination with the nonlinear mechanistic model of the process to calculate the resulting statistical distribution of measured variables that are used for fault detection and classification. A Galerkin projection based stochastic finite difference analysis is utilized to transform the stochastic mechanistic equation into a coupled deterministic system of equations which is solved numerically to obtain the gPC expansion coefficients. To detect and recognize faults, the probability density functions (PDFs) and joint confidence regions (JCRs) of the measured variables to be used for fault detection are obtained by substituting samples from a random space into the gPC expansions. The method is applied to a two dimensional heat transfer problem with faults consisting of stochastic changes combined with step change variations in the thermal diffusivity and in a boundary condition. The proposed methodology is compared with a Monte Carlo (MC) simulations based approach to illustrate its advantages in terms of computational efficiency as well as accuracy.",
     "keywords": ["Stochastic faults", "Fault isolation", "Diagnosability", "Uncertainty analysis", "Computational efficiency"]},
    {"article name": "Multi-objective optimization methodology to size cogeneration systems for managing flares from uncertain sources during abnormal process operations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.02.012",
     "publication date": "05-2015",
     "abstract": "Flaring is common practice in industries to reduce the risk during abnormal situations, to maintain the product quality or to operate safely during process start up and shut down. Due to its large negative impacts on the environment and society, various protocol and steps, i.e., Kyoto protocol, the United Nations Environment Programme, have been created for future mitigation. There is significant amount of heating value lost during flaring events. A cogeneration (COGEN) system can use waste flare streams as fuel to generate heat and power within a process. The objective of this work is to develop an optimization framework for sizing a COGEN unit to manage flares from uncertain sources by minimizing the overall cost and emissions of greenhouse gases. Multi-objective trade-offs between the economic, environmental, and energetic aspects are presented through Pareto fronts for a base case ethylene plant using a stochastic optimization technique based on genetic algorithm.",
     "keywords": ["Flare minimization", "Cogeneration", "Optimal sizing", "Process integration", "Greenhouse gas emissions", "Uncertainty"]},
    {"article name": "Electrochemical model of a lithium-ion battery implemented into an automotive battery management system",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.02.007",
     "publication date": "05-2015",
     "abstract": "This paper presents the development of an electrochemical model that can be implemented into automotive battery management systems (BMSs). Compared with empirical models, the electrochemical model features more accurate state estimates over a broader and longer use of the battery. In this work, model implementation schemes are devised to make the electrochemical model uncomplicated enough to be embedded into the BMS. A nonlinear system of partial differential equations in the model is discretized into a linearized system of algebraic equations (AEs). A solver selected to evaluate the resulting system of AEs is modified for its application to the BMS. As the BMS is preoccupied by its existing tasks, the reformulated equations and optimized solver are reorganized such that the limited computational resources of the BMS are appropriately exploited. The electrochemical model is consequently implemented into the BMS, predicting battery behaviors in 1\u00a0s intervals while occupying a 14\u00a0kB RAM.",
     "keywords": ["Lithium-ion battery", "Battery model", "Battery management system", "Electric vehicle"]},
    {"article name": "Improved Big-M reformulation for generalized disjunctive programs",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.02.013",
     "publication date": "05-2015",
     "abstract": "In this work, we present a new Big-M reformulation for Generalized Disjunctive Programs. Unlike the traditional Big-M reformulation that uses one M-parameter for each constraint, the new approach uses multiple M-parameters for each constraint. Each of these M-parameters is associated with each alternative in the disjunction to which the constraint belongs. In this way, the proposed MINLP reformulation is at least as tight as the traditional Big-M, and it does not require additional variables or constraints. We present the new Big-M, and analyze the strength in its continuous relaxation compared to that of the traditional Big-M. The new formulation is tested by solving several instances with an NLP-based branch and bound method. The results show that, in most cases, the new reformulation requires fewer nodes and less time to find the optimal solution.",
     "keywords": ["Disjunctive programming", "Mixed-integer programming", "Big-M"]},
    {"article name": "Simultaneous design of water reusing and rainwater harvesting systems in a residential complex",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.02.011",
     "publication date": "05-2015",
     "abstract": "This paper introduces an optimization formulation to design residential water systems that satisfy the water demands in a housing complex involving rainwater harvesting, storage and distribution as well as the simultaneous design of water networks for recycling, reusing, regenerating and storing reclaimed water. The design task is considered as a multi-objective optimization problem where one objective is the minimization of the fresh water consumption and the other objective is the minimization of the total annual cost. The proposed model accounts for the variability in the water demands through the different hours of the day and for the different seasons of the year. The seasonal dependence of the rainwater has also been considered in the optimization model. A case study for the city of Morelia in Mexico is presented. The results show that significant reductions can be obtained in the total fresh water consumption and in the total cost.",
     "keywords": ["Rainwater harvesting", "Reclaimed water recycling and reusing", "Residential complex", "Optimization"]},
    {"article name": "Optimization of steel production scheduling with complex time-sensitive electricity cost",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.02.004",
     "publication date": "05-2015",
     "abstract": "Energy-intensive industries can take advantage of process flexibility to reduce operating costs by optimal scheduling of production tasks. In this study, we develop an MILP formulation to extend a continuous-time model with energy-awareness to optimize the daily production schedules and the electricity purchase including the load commitment problem. The sources of electricity that are considered are purchase on volatile markets, time-of-use and base load contracts, as well as onsite generation. The possibility to sell electricity back to the grid is also included. The model is applied to the melt shop section of a stainless steel plant. Due to the large-scale nature of the combinatorial problem, we propose a bi-level heuristic algorithm to tackle instances of industrial size. Case studies show that the potential impact of high prices in the day-ahead markets of electricity can be mitigated by jointly optimizing the production schedule and the associated net electricity consumption cost.",
     "keywords": ["Scheduling", "Steel plant", "Energy optimization", "Demand-side management", "Continuous-time models"]},
    {"article name": "Separating an azeotropic mixture of toluene and ethanol via heat integration pressure swing distillation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.02.016",
     "publication date": "05-2015",
     "abstract": "A procedure is suggested to separating a minimum-boiling azeotrope of toluene and ethanol via heat integration pressure swing distillation (PSD), and an optimized separation configuration is obtained via taking the minimization of the total annual cost (TAC) as an objective function. The result demonstrates that PSD with heat integration is more economical than conventional PSD without heat integration. Based on steady-state simulation results, several control structures were explored using Aspen Dynamics. The results indicate that the composition/temperature cascade control structure and the pressure-compensated temperature control of a PSD process with partial heat integration with stage 21 selected as the control stage in the low pressure column can handle disturbances well. As for the PSD with full heat integration, stage 20 of the low pressure column can act as the control stage because of its more efficient controllability under feed flow rate and feed composition disturbances.",
     "keywords": ["Toluene", "Ethanol", "Pressure-swing distillation", "Heat integration", "Dynamic simulation"]},
    {"article name": "Flow regimes in T-shaped micro-mixers",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.02.017",
     "publication date": "05-2015",
     "abstract": "The different flow regimes occurring in T-mixers are investigated by means of direct numerical simulations. Three different values of the aspect ratio of the inlet channels, \u03bai, that is their width to height ratio are considered, namely \u03bai\u00a0=\u00a00.75, 1 and 2. For the configurations with \u03bai\u00a0=\u00a00.75 and 1, the same behavior as previously described in the literature, is found. In particular, as the Reynolds number is increased, the flow evolves from vortical to engulfment steady regimes, then to unsteady asymmetric and symmetric periodic regimes, until, finally, it becomes chaotic. All the critical values of the Reynolds number, at which the transitions between the different regimes occur, are found to be very similar for \u03bai\u00a0=\u00a00.75 and 1, while some differences are highlighted in the vorticity dynamics and characteristic frequencies of the unsteady regimes. The observed scenario is completely different for \u03bai\u00a0=\u00a02. Indeed, in this case, the flow evolves directly from the vortical regime to an unsteady symmetric behavior, with a vorticity dynamics that is significantly different from those observed for the other aspect ratios.",
     "keywords": ["Micro-reactors", "Engulfment", "Flow instabilities", "Laminar flow"]},
    {"article name": "Impact of shape representation schemes used in discrete element modelling of particle packing",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.02.015",
     "publication date": "05-2015",
     "abstract": "In different computer models, shape is represented using different methodologies, to varying degrees of precision. This paper examines two approaches to shape representation, and their effects on accuracy in the context of cylindrical particle packing. Two discrete element method (DEM) based software packages are used. A X-ray CT scan of a packed bed provides the experimental measurements for comparison. Eight sphere-composite representations of the same cylindrical pellet were tested. Two of these gave results that quantitatively follow experimental measurements. A range of factors that in theory could affect accuracy of the simulation results are examined, including edge roundedness, surface roughness and restitutional behaviour as a function of sphere-composite representations. The conclusion is that, for packing at least, matching the object's overall shape and dimensions is not enough. Only when a high enough resolution is applied to corners and edges, could the sphere-composite approach possibly match the experimental data quantitatively.",
     "keywords": ["Bulk packing fraction", "Local packing fraction", "Pellet orientation distribution", "Sphere-composite approach", "Voxel-based approach", "Cylinder packing"]},
    {"article name": "Multi-objective optimization of SNG production from microalgae through hydrothermal gasification",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.01.013",
     "publication date": "05-2015",
     "abstract": "The conversion of microalgae biomass into biofuels is a quite well explored field of research. Due to high photosynthetic efficiency, microalgae are considered as a potential feedstock for next-generations biofuel conversion processes.This paper addresses the thermochemical conversion of highly diluted microalgae feedstock into synthetic natural gas (SNG) through supercritical hydrothermal gasification. The complete conversion chain is modeled including the cultivation phase, settling ponds, centrifuges, catalytic hydrothermal gasification with salt separation unit and SNG purification system. Thermodynamic, economic and environmental models are considered for each process step, in order to solve a Mixed Integer Non Linear Programming (MINLP) optimization problem.The problem is solved by applying a two steps decomposition approach, using Multi Objective Evolutionary Algorithm with Mixed Integer Linear Programming (MILP). It is finally demonstrated that coupling microalgae cultivation systems with hydrothermal gasification (HTG) and waste energy recovery utilities leads to high energy/exergy efficiencies, emissions reduction and globally better sustainable processes.",
     "keywords": ["BM biomass", "biomass", "GWP Global Warming Potential", "Global Warming Potential", "HHV higher heating value", "higher heating value", "HP heat pump", "heat pump", "HTG hydrothermal gasification", "hydrothermal gasification", "HRT hydraulic retention time", "hydraulic retention time", "LCIA Life Cycle Impact Assessment", "Life Cycle Impact Assessment", "LCI life cycle inventory", "life cycle inventory", "LHV lower heating value", "lower heating value", "MER Minimum Energy Requirement", "Minimum Energy Requirement", "MILP mixed integer linear programming", "mixed integer linear programming", "MINLP mixed integer non linear programming", "mixed integer non linear programming", "MM molecular mass", "molecular mass", "MOO multi-objective optimization", "multi-objective optimization", "NG natural gas", "natural gas", "NGCC Natural Gas Combined Cycle", "Natural Gas Combined Cycle", "OPEX operating expenses or operating costs", "operating expenses or operating costs", "SCWG Supercritical Water Gasification", "Supercritical Water Gasification", "SNG synthetic natural gas", "synthetic natural gas", "SMR steam methane reforming", "steam methane reforming", "WGS water\u2013gas shift", "water\u2013gas shift", "Microalgae", "Hydrothermal gasification", "Synthetic natural gas", "Process design", "Thermo-economic optimization"]},
    {"article name": "A multi-compartment population balance model for high shear granulation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.01.009",
     "publication date": "04-2015",
     "abstract": "This work extends the granulation model published by Braumann et al. (2007) to include multiple compartments in order to account for mixture heterogeneity encountered in powder mixing processes. A stochastic weighted algorithm is adapted to solve the granulation model which includes simultaneous coalescence and breakage. Then, a new numerical method to solve stochastic reactor networks is devised. The numerical behaviour of the adapted stochastic weighted algorithm is compared against the existing direct simulation algorithm. Lastly, the performance of the new compartmental model is then investigated by comparing the predicted particle size distribution against an experimentally measured size distribution. It is found that the adapted stochastic weighted algorithm exhibits superior performance compared to the direct simulation algorithm and the multi-compartment model produces results with better agreement with the experimental results compared to the original single-compartment model.",
     "keywords": ["Granulation", "Stochastic weighted algorithm", "Compartmental model", "Breakage", "Coalescence"]},
    {"article name": "Optimization of the various modes of flexible operation for post-combustion CO2 capture plant",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.12.017",
     "publication date": "04-2015",
     "abstract": "One option to mitigate the adverse effect of power plant output loss from adding a CO2 capture plant is to operate it in flexible modes in which the capture level and/or regeneration rate are dynamically varied in response to varying electricity market demand and price. This can help the plant meet peak electricity demand and improve its overall profit. However, the benefit is offset by higher capital costs and/or CO2 emission penalty. Various modes of flexible operation including capture level reduction and solvent storage have been optimized for a given post-combustion capture system with typical daily electrical energy price patterns and the results are compared with those from a fixed point operation. Effects of varying storage capacities and energy price patterns have also been evaluated. Simultaneous use of the two flexible modes is also optimized and the result showed significantly higher cost savings compared to the individual uses.",
     "keywords": ["MEA absorption", "Post-combustion CO2 capture", "Solvent storage", "Flexible operation", "Optimization", "ECP energy cost profile", "energy cost profile", "CLR capture level reduction", "capture level reduction", "CLRSS simultaneous capture level reduction and solvent storage", "simultaneous capture level reduction and solvent storage", "FPO fixed point operation", "fixed point operation", "SS solvent storage", "solvent storage"]},
    {"article name": "Effects of thermodynamically coupled reaction diffusion in microalgae growth and lipid accumulation: Model development and stability analysis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.01.011",
     "publication date": "04-2015",
     "abstract": "This study investigates and presents the effects of thermodynamically coupled nonisothermal reaction-diffusion processes on microalgae growth, substrate consumption and neutral lipid production in a pond or wastewater treatment plant. The non-stirred chemostat hypothesis and linear nonequilibrium thermodynamics theory are applied to formulate the model equations that account the bulk phase compositions and temperature, resistances to the heat and mass transfers, and cross effects due to the thermodynamic coupling of heat and mass flow in the presence of chemical reaction. Nondimensional forms of the model equations are numerically solved. Bulk phase concentrations and temperatures, external resistances to heat and substrate transfers, and thermodynamic coupling may generate substantial number of new parameters that control the evolution and stability in microalgal growth and lipid production that are important for biofuels. Instabilities due to perturbations in nutrient concentrations may lead to spatial structures where the wavenumber plays important role in reaction diffusion systems.",
     "keywords": ["Microalgae growth", "Lipid accumulation", "Reaction-diffusion systems", "Thermodynamic coupling", "External resistances", "Stability analysis"]},
    {"article name": "Computational strategies for improved MINLP algorithms",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.01.015",
     "publication date": "04-2015",
     "abstract": "In order to improve the efficiency for solving MINLP problems, we present in this paper three computational strategies. These include multiple-generation cuts, hybrid methods and partial surrogate cuts for the Outer Approximation and Generalized Benders Decomposition. The properties and convergence of the strategies are analyzed. Based on the proposed strategies, five new MINLP algorithms are developed, and their implementation is discussed. Results of numerical experiments for benchmark MINLP problems are reported to demonstrate the efficiency of the proposed methods.",
     "keywords": ["Mixed Integer Nonlinear Programming (MINLP)", "Outer Approximation (OA)", "Generalized Benders Decomposition (GBD)", "Cutting Plane"]},
    {"article name": "On the dynamic optimization of methane production in anaerobic digestion via extremum-seeking control approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.01.018",
     "publication date": "04-2015",
     "abstract": "This paper proposes an extremum-seeking control approach based on sliding mode to achieve the dynamic optimization of methane outflow rate in anaerobic digestion processes. Open-loop analysis for a two-population model shown that the system becomes unstable due at the accumulation of volatile fatty acids (VFA). Then the controller is designed to achieve the regulation of VFA concentration close to the optimal set-point while maximizing the methane production. The control law is based on a variable-structure feedback to iteratively extremize the methane outflow rate and converges to the neighborhood of the optimum with sliding mode motions. In contrast with previous works on extremum-seeking control with sliding mode, the control scheme includes an observer-based uncertain estimator which computes the unknown terms related to the growth kinetics and the inlet composition. Practical stabilizability for the closed-loop system around to unknown optimal set-point is analyzed. Numerical experiments illustrate the effectiveness of the proposed approach.",
     "keywords": ["Anaerobic digestion process", "Extremum-seeking control", "Sliding mode", "Robust variable-structure control"]},
    {"article name": "Derivative-free methods applied to daily production optimization of gas-lifted oil fields",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.01.014",
     "publication date": "04-2015",
     "abstract": "In the oil industry, computer simulators are routinely applied on day to day operations and in what-if analyses. With the increasing complexity of operations, engineers are relying on simulation to synthesize models for mathematical optimization or else applying derivative-free methods for simulation-based optimization, which requires only the sampling of the objective function. This work adapts an algorithm based on augmented Lagrangian and derivative-free trust-region algorithms to handle hard constraints found in production optimization. The effectiveness of the proposed method is assessed in an oil production network and compared with mathematical optimization based on piecewise-linear models.",
     "keywords": ["Derivative-free optimization", "Petroleum production optimization", "Trust region", "Augmented Lagrangian", "Simulation"]},
    {"article name": "A MILP algorithm for utilities pre-design based on the Pinch Analysis and an exergy criterion",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.12.010",
     "publication date": "04-2015",
     "abstract": "Designing Heat Exchangers Network for heat recovery is a challenge currently solved with optimization algorithms. However, making a tradeoff between HEN and utilities rely on how relevant are the proposed utilities. In this paper we propose a preselection algorithm for utilities, focused on chillers, heat pumps, Organic Rankine Cycles (ORC) units and Combined Heat & Power (CHP) units. The Pinch Analysis is used to provide input data such as the Grand Composite Curve (GCC). A MILP algorithm based on an exergy criterion automatically preselects and predesigns utilities from the GCC. The formulation of the optimization problem is described, and so the degrees of freedom for the user. Finally, an example from Food & Drink industry shows the performance of this algorithm. Fast and accurate, this algorithm has been implemented in a software named CERES to prepare the results for the HEN design, which is not described in this article.",
     "keywords": ["Energy integration", "Exergy", "MILP optimization", "Utility design"]},
    {"article name": "Refinery continuous-time crude scheduling with consideration of long-distance pipeline transportation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.01.012",
     "publication date": "04-2015",
     "abstract": "In reality, crudes from unloading storage tanks at a docking berth may experience long-distance pipeline transportation to refinery charging tanks before their processing by crude distillation units. Such pipeline transportations cause significant transport delay and crude holdup that will substantially affect plant production performance. Unfortunately, current short-term crude scheduling studies have never systematically considered these issues. In this work, a new continuous-time crude scheduling model has been developed, which addresses the long-distance pipeline transportation and other realistic considerations such as brine settling and multiple jetties for crude unloading. The feeding of crudes into pipeline, and crude movements inside pipeline, as well as crude discharging to the receiving charging tanks are combined with the continuous time formulation of crude scheduling. The efficacy of the developed scheduling model has been demonstrated by three case studies including one industrial size example. An outer-approximation (OA) based iterative algorithm (Karuppiah et al., 2008) is implemented to successfully solve the case studies.",
     "keywords": ["Crude scheduling", "Long-distance pipeline transportation", "MINLP"]},
    {"article name": "A comparative study of a direct discretization and an operator-splitting solver for population balance systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.01.010",
     "publication date": "04-2015",
     "abstract": "A direct discretization approach and an operator-splitting scheme are applied for the numerical simulation of a population balance system which models the synthesis of urea with a uni-variate population. The problem is formulated in axisymmetric form and the setup is chosen such that a steady state is reached. Both solvers are assessed with respect to the accuracy of the results, where experimental data are used for comparison, and the efficiency of the simulations. Depending on the goal of simulations, to track the evolution of the process accurately or to reach the steady state fast, recommendations for the choice of the solver are given.",
     "keywords": ["Population balance systems", "Direct discretization", "Operator-splitting", "Urea synthesis", "Uni-variate population"]},
    {"article name": "Optimizing scheduling of refinery operations based on piecewise linear models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.01.022",
     "publication date": "04-2015",
     "abstract": "Optimizing scheduling is an effective way to improve the profit of refineries; it usually requires accurate models to describe the complex and nonlinear refining processes. However, conventional nonlinear models will result in a complex mixed integer nonlinear programming (MINLP) problem for scheduling. This paper presents a piecewise linear (PWL) modeling approach, which can describe global nonlinearity with locally linear functions, to refinery scheduling. Specifically, a high level canonical PWL representation is adopted to give a simple yet effective partition of the domain of decision variables. Furthermore, a unified partitioning strategy is proposed to model multiple response functions defined on the same domain. Based on the proposed PWL partitioning and modeling strategy, the original MINLP can be replaced by mixed integer linear programming (MILP), which can be readily solved using standard optimization algorithms. The effectiveness of the proposed strategy is demonstrated by a case study originated from a refinery in China.",
     "keywords": ["Optimization", "Piecewise linear programming", "Piecewise linear representation", "Refinery scheduling", "Unified simplicial partition"]},
    {"article name": "Correntropy based data reconciliation and gross error detection and identification for nonlinear dynamic processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.01.005",
     "publication date": "04-2015",
     "abstract": "Measurement information in dynamic chemical processes is subject to corruption. Although nonlinear dynamic data reconciliation (NDDR) utilizes enhanced simultaneous optimization and solution techniques associated with a finite calculation horizon, it is still affected by different types of gross errors. In this paper, two algorithms of data processing, including correntropy based NDDR (CNDDR) as well as gross error detection and identification (GEDI), are developed to improve the quality of the data measurements. CNDDR's reconciliation and estimation are accurate in spite of the presence of gross errors. In addition to CNDDR, GEDI with a hypothesis testing and a distance\u2013time step criterion identifies types of gross errors in dynamic systems. Through a case study of the free radical polymerization of styrene in a complex nonlinear dynamic chemical process, CNDDR greatly decreases the influence of the gross errors on the reconciled results and GEDI successfully classifies the types of gross errors of the measured data.",
     "keywords": ["Chemical processes", "Correntropy", "Data reconciliation", "Instrumentation", "Optimization", "Systems engineering"]},
    {"article name": "An optimization model for evaluating the economic impact of availability and maintenance notions during the synthesis and design of a power plant",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.01.020",
     "publication date": "04-2015",
     "abstract": "In this paper, we introduce an optimization strategy in order to comprehensively quantify the impact of availability and maintenance notions during the early stages of synthesis and design of a new natural gas combined cycle power plant. A detailed state-space approach is thoroughly discussed, where influence of maintenance funds on each component's repair rate is directly assessed.In this context, analysis of the reliability characteristics of the system is centered at two designer-adopted parameters, which largely influence the obtained results: the number of components which may fail independently at the same time, and the number of simultaneous failure/repair events.Then, optimal solutions are evaluated as the availability-related parameters and the amount of resources assigned for maintenance actions are varied across a wide range of feasible values, which enable obtaining more accurate and detailed estimations of the expected economic performance for the project when compared with traditional economic evaluation approaches.",
     "keywords": ["NGCC power plant", "Optimal design", "Availability", "Maintenance", "Economic optimization", "State-space approach"]},
    {"article name": "Modeling and simulation of mixing in water-in-oil emulsion flow through a valve-like element using a population balance model",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.01.017",
     "publication date": "04-2015",
     "abstract": "Emulsion flows are very common in natural processes as well as in several engineering areas, such as in the process of desalting crude oil that occurs in refineries. This kind of flow is described as a polydispersed multiphase flow. In this work, we evaluated the behavior of water-in-oil emulsion flowing through a duct with an element used to mimic the effect of a globe valve. An Eulerian multi-fluid approach was employed by solving the population balance equation coupled with computational fluid dynamics. Coalescence and breakage models recently developed were extended to this inhomogeneous model. A bivariate population balance problem was also solved to demonstrate the mixing caused by the valve-like element. The simulated results showed good agreement with the available experimental data for the Sauter and DeBroukere mean diameters.",
     "keywords": ["CFD", "Multiphase flow", "Population balance", "Coalescence", "Breakage", "OpenFOAM"]},
    {"article name": "An optimization-based approach to extract faceted crystal shapes from stereoscopic images",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.01.016",
     "publication date": "04-2015",
     "abstract": "The size and shape of particles crucially influences the characteristics of solid products. Until recently these quantities were evaluated using light microscopy. However, extracting the three-dimensional shape of a faceted crystal from a single image is a formidable computer vision challenge. In this work we combine stereoscopic imaging devices (e.g., commercial stereoscopic microscopes or the stereoscopic flow through cell that continuously draws samples from a crystallizer (Schorsch et al., 2014)) with a model-based approach in which parametric polytopes are used to describe faceted crystals (Hours et al., 2014). In the shape reconstruction algorithm these parametric polytopes are scaled and rotated until their projections closely match the measured stereoscopic images, which is formulated as a nonlinear optimization problem. The proposed approach is assessed using simulated images and experimental data. We also assess in which cases the proposed approach does or does not provide advantages over concepts using generic particle shapes (Schorsch et al., 2012).",
     "keywords": ["Crystallization", "Particle shape", "Particulate processes", "Stereoscopic imaging", "Faceted crystals", "Nonlinear optimization"]},
    {"article name": "Stability analysis and passivity properties for a class of chemical reactors: Internal entropy production approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.01.021",
     "publication date": "04-2015",
     "abstract": "In this contribution, the stability and passivity properties of a class of chemical reactors are addressed from a thermodynamical point of view. For this purpose, a thermodynamical consistent model is derived from a generic gas reactor model whose rate is based on the reaction progress at the mesoscopic scale. It is shown that the internal entropy production may be used as a candidate Lyapunov function to prove the isolated system stability properties and as a storage function to emphasize the passivity properties when the chemical reactor interacts with the surroundings.",
     "keywords": ["Chemical reactor", "Stability", "Passivity", "Thermodynamics and stability", "Internal entropy production"]},
    {"article name": "A general framework for data reconciliation\u2014Part I: Linear constraints",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.12.004",
     "publication date": "04-2015",
     "abstract": "This paper presents a new method, based on Bayesian reasoning, for the reconciliation of data from arbitrary probability distributions. The main idea is to restrict the joint prior probability distribution of the involved variables with model constraints to get a joint posterior probability distribution. This paper covers the case of linearly constrained variables, with the focus on equality constraints. The procedure is demonstrated with the help of three simple graphical examples. Because in general the posterior probability density function cannot be calculated analytically, it is sampled with a Markov chain Monte Carlo (MCMC) method. From this sample the density and its moments can be estimated, along with the marginal densities, moments and quantiles. The method is tested on several artificial examples from material flow analysis, using an independence Metropolis\u2013Hastings sampler.",
     "keywords": ["Data reconciliation", "Nonnormal distributions", "Linear equality constraints", "Markov chain Monte Carlo (MCMC)", "Independence Metropolis\u2013Hastings sampler", "Material flow analysis (MFA)"]},
    {"article name": "Reliability of complex chemical engineering processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.12.013",
     "publication date": "03-2015",
     "abstract": "This paper presents a stochastic performance modelling approach that can be used to optimise design and operational reliability of complex chemical engineering processes. The framework can be applied to processes comprising multiple units, including the cases where closed form process performance functions are unavailable or difficult to derive from first principles, which is often the case in practice. An interface that facilitates automated two-way communication between Matlab\u00ae and process simulation environment is used to generate large process responses. The resulting constrained optimisation problem is solved using both Monte Carlo Simulation (MCS) and First Order Reliability Method (FORM); providing a wide range of stochastic process performance measures. Adding such capabilities to traditional deterministic process simulators provides a more informed basis for selecting optimum design factors; giving a simple way of enhancing overall process reliability and cost-efficiency. Two case study systems are considered to highlight the applicability and benefits of the approach.",
     "keywords": ["Chemical process reliability", "Design optimisation", "Uncertainty modelling", "Stochastic analysis", "Process performance simulation"]},
    {"article name": "Design and optimization of modified non-sharp column configurations for quaternary distillations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.12.006",
     "publication date": "03-2015",
     "abstract": "The possible structural changes of a non-sharp quaternary distillation configuration are considered. For the reference configuration composed of four columns, different alternatives are generated following the process intensification principle to reduce the number of equipment units. The intensified systems with three or two columns are obtained, including the dividing wall columns. Simulator Aspen Plus V8.0 was used to design and simulate all the systems for a hydrocarbon mixture. The intensified structures showed relevant energy savings compared to the reference case. The most promising alternatives were optimized by means of the differential evolution (DE) method minimizing the total annual cost (TAC). It was observed that the intensified systems were able to reduce both the energy consumption and the number of equipment units. The best intensified system has a TAC of 11.98% lower than the optimized reference case.",
     "keywords": ["Non-sharp separation", "Process synthesis", "Process intensification", "Multicomponent distillation", "Stochastic optimization", "Dividing wall column"]},
    {"article name": "A new group contribution method for mineral concentration processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.12.009",
     "publication date": "03-2015",
     "abstract": "In this work, a group contribution model to predict the behavior of mineral concentration circuits is presented. The new model is an expansion and modification of an existing model in the literature (Sep\u00falveda et al., 2014). The modifications extend the number of process groups from 35 to 143, which makes it possible to extend the number of concentration circuits that can be represented from 1492 to over 274 million circuits and to increase the maximum number of stages from 6 to 9. The errors observed between the fitting and the prediction results of concentration circuits that were not included in the fitting verifies that the new model could be extremely useful in the design of mineral concentration circuits.",
     "keywords": ["Group contribution", "Process design", "Concentration circuit", "Flotation", "Mineral"]},
    {"article name": "Heuristic solution approaches to the pharmaceutical R&D pipeline management problem",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.12.014",
     "publication date": "03-2015",
     "abstract": "The paper presents two heuristic approaches, a shrinking horizon multiple two-stage stochastic programming (MTSSP) decomposition algorithm and a knapsack decomposition algorithm (KDA), for solving multistage stochastic programmes (MSSPs) with endogenous uncertainty, specifically focusing on pharmaceutical research and development (R&D) pipeline management problem. The MTSSP decomposition algorithm decomposes the problem into a series of two-stage stochastic programmes, which are solved as resources become available. The KDA decomposes the MSSP into a series of knapsack problems, which are created and solved at key decision points on a rolling horizon fashion. Based on the results of the six case studies, both the MTSSP decomposition algorithm and the KDA generate implementable solutions that are within three percent of the rigorous MSSP solution obtained by CPLEX 12.51. Both methods showed several orders of magnitude decrease in the CPU times compared to ones that were required to solve the rigorous MSSP.",
     "keywords": ["Endogenous uncertainty", "Multistage stochastic programming", "Pharmaceutical R&D pipeline management", "Knapsack decomposition"]},
    {"article name": "On the use of filters to facilitate the post-optimal analysis of the Pareto solutions in multi-objective optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.12.012",
     "publication date": "03-2015",
     "abstract": "Multi-objective optimization (MOO) has emerged recently as a useful technique in the design and planning of engineering systems because it allows identifying alternatives leading to significant environmental savings. MOO models typically contain an infinite number of Pareto solutions, from which decision-makers should choose the best one according to their preferences. An approach is here presented that identifies and retains for further inspection a reduced set of Pareto solutions showing better overall performance. The capabilities of our approach are illustrated through its application to the design of reverse osmosis desalination plants considering simultaneously the unitary production cost and a set of environmental impacts in several damage categories. Our method reduces significantly the number of Pareto points, thereby facilitating the decision-making process in MOO.",
     "keywords": ["Multi-objective optimization", "Pareto filters", "Decision-making", "Desalination"]},
    {"article name": "Input variable scaling for statistical modeling",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.12.016",
     "publication date": "03-2015",
     "abstract": "Input variable scaling is one of the most important steps in statistical modeling. However, it has not been actively investigated, and autoscaling is mostly used. This paper proposes two input variable scaling methods for improving the accuracy of soft sensors. One method statistically derives the input variable scaling factors; the other one uses spectroscopic data of a material whose content is estimated by the soft sensor. The proposed methods can determine the scales of the input variables based on their importance in output estimation. Thus, it can reduce the negative effects of input variables which are not related to an output variable. The effectiveness of the proposed methods was confirmed through a numerical example and industrial applications to a pharmaceutical and a distillation processes. In the industrial applications, the proposed methods improved the estimation accuracy by up to 63% compared to conventional methods such as autoscaling with input variable selection.",
     "keywords": ["Statistical model", "Soft sensor", "Input variable scaling", "Pharmaceutical process", "Distillation process"]},
    {"article name": "A dynamic intraparticle model for fluid\u2013solid adsorption kinetics",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.01.001",
     "publication date": "03-2015",
     "abstract": "The fluid\u2013solid adsorption batch kinetics is surely one of the most popular topic in the chemical engineering science. The water purification from pollutant components, such as metals and organic compounds, can be considered one of the main application of this field. Even if the topic is of a great scientific and industrial interest, the modeling of the mentioned systems is by now far to be reliable. As a matter of fact, most of the models reported in the literature are based on semi empirical approaches that describe the adsorption experimental data on the basis of equilibria and the kinetic terms. In this paper, a new modeling approach is proposed for adsorption kinetics investigation performed in batch reactors with a fluid\u2013solid system. In particular, the mass balances have been developed by taking into account for both the external and internal mass transfer diffusion limitations, solving the dynamic partial differential equations (PDEs) system along the radius of the sorbent particles, considering both the fluid and solid phases that constitute the sorbent particle. From a numerical point of view, the solution of this type of problem is very challenging because it involves the simultaneous solution of many PDEs, ODEs and AEs. Here, physical parameters have to be evaluated either from existing correlations or by direct measurements. This fact makes the model predictable. In order to test the model, some Cu(II) and Pb(II) adsorption tests, taken from literature, using different kind of silica have been interpreted. The presented model can be considered of great interest, as it is the starting point for designing continuous adsorption columns for water purification.",
     "keywords": ["Dynamic intraparticle model", "Adsorption", "Kinetics", "Mass transfer", "Water treatment", "Modeling"]},
    {"article name": "Application of the bio-inspired Krill Herd optimization technique to phase equilibrium calculations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.12.008",
     "publication date": "03-2015",
     "abstract": "The Krill Herd optimization technique, which is based on the simulated herding behaviour of the krill crustacean, is applied to calculations involving phase equilibrium and phase stability, as the application of this emerging technique is extremely limited in the literature. In this work, the Krill Herd algorithm (KH)1 and the modified L\u00e9vy-flight Krill Herd algorithm (LKH)2 has been applied to phase stability (PS)3 and phase equilibrium calculations in non-reactive (PE)4 and reactive (rPE)5 systems, where global minimization of the total Gibbs energy is necessary. Several phase stability and phase equilibrium systems were considered for the analysis of the performance of the technique that includes both vapour and liquid phase conditions.The Krill Herd algorithm was found to reliably determine the desired global optima in PS, PE and rPE problems with generally higher success rates and lower computing time requirements than previously applied metaheuristic techniques such as those involving swarm intelligence and genetic and evolutionary algorithms.",
     "keywords": ["Phase equilibrium", "Stability analysis", "Metaheuristic optimization"]},
    {"article name": "Optimal robust optimization approximation for chance constrained optimization problem",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.01.003",
     "publication date": "03-2015",
     "abstract": "Chance constraints are useful for modeling solution reliability in optimization under uncertainty. In general, solving chance constrained optimization problems is challenging and the existing methods for solving a chance constrained optimization problem largely rely on solving an approximation problem. Among the various approximation methods, robust optimization can provide safe and tractable analytical approximation. In this paper, we address the question of what is the optimal (least conservative) robust optimization approximation for the chance constrained optimization problems. A novel algorithm is proposed to find the smallest possible uncertainty set size that leads to the optimal robust optimization approximation. The proposed method first identifies the maximum set size that leads to feasible robust optimization problems and then identifies the best set size that leads to the desired probability of constraint satisfaction. Effectiveness of the proposed algorithm is demonstrated through a portfolio optimization problem, a production planning and a process scheduling problem.",
     "keywords": ["Chance constraint", "Robust optimization", "Uncertainty set", "Optimal approximation"]},
    {"article name": "Heat integration of intermittently available continuous streams in multipurpose batch plants",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.12.003",
     "publication date": "03-2015",
     "abstract": "Presented in this paper is a mathematical technique for simultaneous heat integration and process scheduling in multipurpose batch plants. Taking advantage of the intermittent continuous behavior of process streams during transfer from one processing unit to another, as determined by the recipe, the presented formulation aims to maximize the coincidence of availability of hot and cold stream pairs with feasible temperature driving forces, while taking into consideration process scheduling constraints. Contrary to similar contributions in published literature, time is treated as one of the key optimization variables instead of a parameter fixed a priori. Heat integration during stream transfer has the added benefit of shortened processing time, which invariably improves throughput, as more batches are likely to be processed within a given time horizon, compared to conventional heating and cooling in situ. Application of the proposed model to a case study shows improvements of more than 30% in energy savings and up to 15% in product output.",
     "keywords": ["Batch plants", "Scheduling", "Heat integration", "Superstructure", "Mathematical optimization"]},
    {"article name": "Fast evaluation of univariate aggregation integrals on equidistant grids",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.12.011",
     "publication date": "03-2015",
     "abstract": "A variety of production processes in chemistry and biotechnology are concerned with particles dispersed in an environmental phase. The particle distribution is mathematically described by the solution of population balance equations of integro-differential type. We are concerned with the aggregation process: it invokes an integral term that is usually numerically expensive to evaluate and often dominates the total simulation cost. We will expose the algorithmic details of an efficient approach based on a separable approximation of the aggregation kernel and a subsequent fast Fourier transformation. This approach reduces the originally quadratic complexity to an almost optimal complexity O ( n log n ) in the dimension of the approximation space. We include numerical tests illustrating its application to representative aggregation kernels from the literature. While originally developed in the context of a discretization with piecewise constant functions, we illustrate how these ideas can be applied in the setting of the popular sectional methods.",
     "keywords": ["Population balance equation", "Aggregation", "Convolution", "Separable kernel approximation", "FFT"]},
    {"article name": "Effect of normal paraffins separation from naphtha on reaction kinetics for olefins and aromatics production",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.01.002",
     "publication date": "03-2015",
     "abstract": "The objective of this study is to investigate the effect of the normal paraffins (n-paraffins) separation by the simulated moving-bed (SMB) on reaction kinetics of the naphtha thermal cracking (NTC) and catalytic reforming (NCR) for the olefins and aromatics production, respectively. First a process simulation of the SMB unit integrated to the petrochemical complex (PCC) was performed. Chemical reaction kinetics of NTC and NCR were proposed and validated. And a retrofit PCC with the SMB unit (rPCC) was compared to a conventional PCC (cPCC) in terms of products composition, flow rate, and energy consumption. It was found that olefins and aromatics yields of NTC and NCR can increase by 14\u00a0wt% (41\u00a0kt/yr) and 11\u00a0wt% (127\u00a0kt/yr) for the naphtha capacity of 2475\u00a0kt/yr, respectively. However, the total energy consumption of rPCC increased by about 67.8\u00a0MW (or 25%) because of the desorbent recovery in the SMB unit.",
     "keywords": ["ATC annualized total cost", "annualized total cost", "BTX benzene, toluene, and xylene", "benzene, toluene, and xylene", "CCR continuous catalyst regenerative", "continuous catalyst regenerative", "cPCC conventional petrochemical complex", "conventional petrochemical complex", "CR cyclic regenerative", "cyclic regenerative", "H-HC heavy hydrocarbons", "heavy hydrocarbons", "L-HC light hydrocarbons", "light hydrocarbons", "NCR naphtha catalytic reforming", "naphtha catalytic reforming", "NHT naphtha hydro-treating", "naphtha hydro-treating", "NTC naphtha thermal cracking", "naphtha thermal cracking", "n-paraffins normal-paraffins", "normal-paraffins", "non n-paraffins iso-paraffins, naphthenes, and aromatics", "iso-paraffins, naphthenes, and aromatics", "i-paraffins iso-paraffins", "iso-paraffins", "PCC petrochemical complex", "petrochemical complex", "PFD process flow diagram", "process flow diagram", "rPCC retrofit petrochemical complex with SMB unit", "retrofit petrochemical complex with SMB unit", "SMB simulated moving-bed", "simulated moving-bed", "SR semi-regenerative", "semi-regenerative", "Simulated moving-bed (SMB)", "Reaction kinetics", "Naphtha", "Thermal cracking", "Catalytic reforming", "Petrochemical complex (PCC)"]},
    {"article name": "Model-based experimental screening for DOC parameter estimation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.01.004",
     "publication date": "03-2015",
     "abstract": "In the current study a parameter estimation method based on data screening by sensitivity analysis is presented. The method applied Multivariate Data Analysis (MVDA) on a large transient data set to select different subsets on which parameters estimation was performed. The subset was continuously updated as the parameter values developed using Principal Component Analysis (PCA) and D-optimal onion design. The measurement data was taken from a Diesel Oxidation Catalyst (DOC) connected to a full scale engine rig and both kinetic and mass transport parameters were estimated. The methodology was compared to a conventional parameter estimation method and it was concluded that the proposed method achieved a 32% lower residual sum of squares but also that it displayed less tendencies to converge to a local minima. The computational time was however significantly longer for the evaluated method.",
     "keywords": ["Parameter estimation", "D-optimal design", "Diesel Oxidation Catalyst", "Multivariate Data Analysis", "Engine rig experiments"]},
    {"article name": "Approximate ODE models for population balance systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.12.015",
     "publication date": "03-2015",
     "abstract": "We propose an approximate polynomial method of moments for a class of first-order linear PDEs (partial differential equations) of hyperbolic type, involving a filtering term with applications to population balance systems with fines removal terms. The resulting closed system of ODEs (ordinary differential equations) represents an extension to a recently published method of moments which utilizes least-square approximations of factors of the PDE over orthogonal polynomial bases. An extensive numerical analysis has been carried out for proof-of-concept purposes. The proposed modeling scheme is generally of interest for control and optimization of processes with distributed parameters.",
     "keywords": ["Method of moments", "Population balance equation", "Least-square approximation", "Method of characteristics"]},
    {"article name": "The multi-period optimisation of an amine-based CO2 capture process integrated with a super-critical coal-fired power station for flexible operation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2015.01.006",
     "publication date": "03-2015",
     "abstract": "In this work, we present a model of a super-critical coal-fired power plant integrated with an amine-based CO2 capture process. We use this model to solve a multi-period dynamic optimisation problem aimed at decoupling the operation of the power plant from the efficiency penalty imposed by the CO2 capture plant, thus providing the power plant sufficient flexibility to exploit price variation within an electricity market. We evaluate four distinct scenarios: load following, solvent storage, exhaust gas by-pass and time-varying solvent regeneration. The objective is to maximise the decarbonised power plant's short run marginal cost profitability. It is found that while the solvent storage option provides a marginal improvement of 4% in comparison to the load following scenario, the exhaust gas bypass scenario results in a profit reduction of 17% whereas the time-varying solvent regeneration option increases the profitability of the power plant by 16% in comparison to the reference scenario.",
     "keywords": ["Flexible CCS", "Post-combustion CO2 capture", "Dynamic optimisation", "Dynamic process modelling", "SAFT-VR", "Multi-scale modelling"]},
    {"article name": "Municipal solid waste to liquid transportation fuels \u2013 Part II: Process synthesis and global optimization strategies",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.10.007",
     "publication date": "03-2015",
     "abstract": "This paper investigates the production of liquid transportation fuels from municipal solid waste (MSW). A comprehensive process synthesis superstructure is utilized that incorporates a novel mathematical model for MSW gasification. The production of liquid products proceeds through a synthesis gas intermediate that can be converted into Fischer\u2013Tropsch hydrocarbons or methanol. The methanol can be converted into either gasoline or olefins, and the olefins may subsequently be converted into gasoline and distillate. Simultaneous heat, power, and water integration is included within the process synthesis framework to minimize utilities costs. A rigorous deterministic global optimization branch-and-bound strategy is utilized to minimize the overall cost of the waste-to-liquids (WTL) refinery and determine the optimal process topology. Several case studies are presented to illustrate the process synthesis framework and the nonconvex mixed-integer nonlinear optimization model presented in this paper. This is the first study that explores the possibility of liquid fuels production from municipal solid waste utilizing a process synthesis approach within a global optimization framework. The results suggest that the production of liquid fuels from MSW is competitive with petroleum-based processes. The effect that the delivered cost of municipal solid waste has on the overall cost of liquids production is also investigated parametrically.",
     "keywords": ["Municipal solid waste", "Process synthesis", "Global optimization", "Mathematical modeling", "Mixed-integer nonlinear optimization"]},
    {"article name": "Simulation-based optimization framework for multi-echelon inventory systems under uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.10.008",
     "publication date": "02-2015",
     "abstract": "Inventory optimization is critical in supply chain management. The complexity of real-world multi-echelon inventory systems under uncertainties results in a challenging optimization problem, too complicated to solve by conventional mathematical programing methods. We propose a novel simulation-based optimization framework for optimizing distribution inventory systems where each facility is operated with the (r, Q) inventory policy. The objective is to minimize the inventory cost while maintaining acceptable service levels quantified by the fill rates. The inventory system is modeled and simulated by an agent-based system, which returns the performance functions. The expectations of these functions are then estimated by the Monte-Carlo method. Then the optimization problem is solved by a cutting plane algorithm. As the black-box functions returned by the Monte-Carlo method contain noises, statistical hypothesis tests are conducted in the iteration. A local optimal solution is obtained if it passes the test on the optimality conditions. The framework is demonstrated by two case studies.",
     "keywords": ["Simulation-based optimization", "Supply chain management", "Agent-based modeling", "Cutting plane algorithm", "Statistical hypothesis tests"]},
    {"article name": "An algorithm for optimal waste heat recovery from chemical processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.11.003",
     "publication date": "02-2015",
     "abstract": "We describe a computer algorithm designed to calculate the optimal energy extraction in the form of heat used for steam raising from chemical processes. The concepts are illustrated using chemical plant stream data in a process with multiple distillation columns. Pinch analysis is first applied to find the grand composite curve (GCC) of the problem, which is then used by the algorithm to determine the maximum mass flow rate of steam that can be produced from process waste heat. An analysis of the effects of the minimum temperature of approach \u0394Tmin on the optimal steam raising result is also conducted, and it is found that, in general, a higher \u0394Tmin will reduce the percentage heat recovery from the process.",
     "keywords": ["Pinch analysis", "Steam raising", "Optimization", "Waste heat recovery", "Chemical process design"]},
    {"article name": "Variant and invariant states for chemical reaction systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.10.009",
     "publication date": "02-2015",
     "abstract": "Models of chemical reaction systems can be quite complex as they typically include information regarding the reactions, the inlet and outlet flows, the transfer of species between phases and the transfer of heat. This paper builds on the concept of reaction variants/invariants and proposes a linear transformation that allows viewing a complex nonlinear chemical reaction system via decoupled dynamic variables, each one associated with a particular phenomenon such as a single chemical reaction, a specific mass transfer or heat transfer. Three aspects are discussed, namely, (i) the decoupling of reactions and transport phenomena in open non-isothermal both homogeneous and heterogeneous reactors, (ii) the decoupling of spatially distributed reaction systems such as tubular reactors, and (iii) the potential use of the decoupling transformation for the analysis of complex reaction systems, in particular in the absence of a kinetic model.",
     "keywords": ["Chemical reaction systems", "State decoupling", "Reaction variants", "Reaction invariants", "Reaction extents"]},
    {"article name": "Application of water pinch technology in minimization of water consumption at a refinery",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.11.004",
     "publication date": "02-2015",
     "abstract": "Water is the most significant entity that controls local and global development. The present research involves property integration technique, graphical and mathematical; to first establish accurate targets for maximum direct recycle of process sources, as well as minimum waste discharge and fresh water consumption at an oil refinery. Property-based water allocation network is designed for the process based on previously identified targets. Chemical oxygen demand (COD) and hardness properties are taken as pollutants (contaminants) and treated as single and double contaminant approach to dig out fresh water demand in the process. Both graphical and mathematical programming techniques exhibit the same water reduction of 149.0\u00a0m3/h (43.8%) and 208.0\u00a0m3/h (61.18%) for COD and hardness case, respectively, in single contaminant approach. While, a new double contaminant mathematical programming has shown a significant reduction of 39%, based on mixing rules of properties. Among the multiple water networks, one possible water allocation network is developed based on mass exchange.",
     "keywords": ["Minimization", "Water pinch", "Water management", "Pollution prevention", "Mathematical programming", "Process integration"]},
    {"article name": "Nonlinear model predictive control of an industrial polymerization process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.11.001",
     "publication date": "02-2015",
     "abstract": "Nonlinear model predictive control (NMPC) is used to maintain and control polymer quality at specified production rates because the polymer quality measures have strong interacting nonlinearities with different temperatures and feed rates. Polymer quality measures that are available from the laboratory infrequently are controlled in closed-loop using a NMPC to set the temperature profile of the reactors. NMPC results in better control of polymer quality measures at different production rates as compared to using the nonlinear process model with reaction kinetics to implement offline targets for reactor temperatures.",
     "keywords": ["Industrial nonlinear model predictive control", "Polymerization control", "Closed-loop control", "Polymer quality control"]},
    {"article name": "Steady-state multiplicity analysis of two-stage-riser catalytic pyrolysis processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.11.011",
     "publication date": "02-2015",
     "abstract": "New two-stage-riser fluidized catalytic pyrolysis (TSRFCP) for maximizing propylene yield technology is considered as an efficient route to moderate the propylene demand/supply gap and to lower the propylene price. The possibility of existence of complex nonlinear behavior associated with the TSRFCP process puts limitations on the supervision of this system. Based on the developed and validated model for the TSRFCP process, this paper focuses on the elucidation of multiple steady states and relevant (in) stability characteristic over a wide range of operating condition. First, graphic analysis of heat generation/removal curves demonstrates that the TSRFCP process has at least one steady state and a maximum of three output steady states under the considered operating conditions and uncertainties such as cooling water flow rates and Conradson carbon residue. Then, operating maps revealing topologies between important input and output variables can disclose detailed nonlinear behavior (input/output multiplicity). Moreover, depending on the choice of the input variable and the relevant operating/design condition, input multiplicity may exist. In short, these results can guide the succeeding control structure selection for realistic TSRFCP processes.",
     "keywords": ["TSRFCP processes", "Input multiplicity", "Multiple steady states", "(In) stability", "Operating maps", "Nonlinear dynamics"]},
    {"article name": "A new algorithm to find all alternate optimal flux distributions of a metabolic network",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.11.006",
     "publication date": "02-2015",
     "abstract": "Finding all optimal solutions for a metabolic model is the challenge of metabolic modeling, but there is no practical algorithm for large scale models. A two-phase algorithm is proposed here to systematically identify all optimal solutions. In phase 1, the model is reduced using the FVA approach; in phase 2, all optimal solutions are searched by the addition of a binary variable to convert the model to an MILP problem. The proposed approach proved itself to be a more tractable method for large scale metabolic models when compared with the previously proposed algorithm. The algorithm was implemented on a metabolic model of Escherichia coli (iJR904) to find all optimal flux distributions. The model was reduced from 1076 to 80 fluxes and from 998 to 54 equations and the MILP problem was solved, resulting in 30,744 various flux distributions. For the first time, this number of optimal solutions has been reported.",
     "keywords": ["Flux balance analysis", "Metabolic network", "Multiple optimal solutions", "Flux variability", "Mixed-integer linear programming"]},
    {"article name": "Simulation of a plant scale reactive distillation column for esterification of acetic acid",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.11.007",
     "publication date": "02-2015",
     "abstract": "Most of the reactive distillation (RD) simulation studies rely upon very scarce laboratory scale experimental data available in literature. In the present work, an industrial scale RD finishing column for ethyl acetate production is simulated using rate based model of Aspen Plus\u2122. Available activity coefficient data could not predict composition profiles of the industrial column. Therefore, a new set of NRTL model parameters is established for this system using the vapor\u2013liquid equilibrium data available in literature. This new set of data was used in the non-equilibrium, rate based model of Aspen Plus\u2122. Baring the variation in concentration profile due to fluctuations caused by pseudo-steady-state behavior of industrial unit, the predicted concentration profile closely matches the plant data. The simulation results for stage efficiencies, component generation rates, and the effect of the organic reflux rate on ethanol conversion and ethyl acetate purity are also presented and discussed.",
     "keywords": ["Ethyl acetate production", "Reactive distillation (RD)", "Plant data", "Liquid phase activity", "Rate-based model", "Rigorous simulation"]},
    {"article name": "Exploitation of the control switching structure in multi-stage optimal control problems by adaptive shooting methods",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.11.009",
     "publication date": "02-2015",
     "abstract": "The adaptive switching structure approach is generalized from single-stage problems and single-shooting to multi-stage problems and multiple-shooting. This generalization is based on previous work on the exploitation and detection of the control switching structure and wavelet-based control grid refinement for single-stage problems. Here, single-shooting is employed to transcribe the multi-stage optimal control problem (OCP) into a nonlinear programming problem. The proposed multi-stage formulation is also capable to represent the transcription of single-stage OCP stemming from multiple-shooting. Thus, the previously reported adaptive multiple-shooting approach is extended by an adaptation of the switching structure. Finally, a new stopping criterion is introduced that measures the intermediate constraint violation at the optimal solution.The proposed adaptive switching structure detection is illustrated for a multi-stage and a multiple-shooting problem using the Williams\u2013Otto semi-batch reactor. A solution of user-specified accuracy in the objective and the path-constraints can be obtained using only few decision variables.",
     "keywords": ["Dynamic optimization", "Control grid refinement", "Multi-stage optimal control", "Structure detection", "Multiple-shooting", "Single-shooting"]},
    {"article name": "Constrained non-linear optimisation of a process for liquefaction of natural gas including a geometrical and thermo-hydraulic model of a compact heat exchanger",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.12.002",
     "publication date": "02-2015",
     "abstract": "A great deal of effort has been put into improving natural gas liquefaction processes, and a number of new process configurations have been described. Recent literature has identified a need for more realistic heat exchanger models to obtain optimum design and operating conditions that do not compromise safety, or that are unrealistic. Here we describe a concept for finding the design and operating conditions of a single mixed-refrigerant process which gives minimum power consumption under given space or weight constraints. We use a sophisticated heat exchanger modelling framework that takes into account system geometry and resolves the details of the heat exchanger through conservation equations coupled with accurate models of thermo-physical properties. First, we find the feasible region which does not compromise safety with Ledinegg instabilities. We then identify the optimal operating conditions for a specific design within this region, before identifying the process design that requires least power consumption. We illustrate how this differs from a purely thermodynamic optimisation, and discuss our key results.",
     "keywords": ["Natural gas liquefaction", "Heat exchanger", "Process", "Design", "Optimisation", "Thermodynamics"]},
    {"article name": "A combined first-principles and data-driven approach to model building",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.11.010",
     "publication date": "02-2015",
     "abstract": "We address a central theme of empirical model building: the incorporation of first-principles information in a data-driven model-building process. By enabling modelers to leverage all available information, regression models can be constructed using measured data along with theory-driven knowledge of response variable bounds, thermodynamic limitations, boundary conditions, and other aspects of system knowledge.We expand the inclusion of regression constraints beyond intra-parameter relationships to relationships between combinations of predictors and response variables. Since the functional form of these constraints is more intuitive, they can be used to reveal hidden relationships between regression parameters that are not directly available to the modeler. First, we describe classes of a priori modeling constraints. Next, we propose a semi-infinite programming approach for the incorporation of these novel constraints. Finally, we detail several application areas and provide extensive computational results.",
     "keywords": ["Regression", "Surrogate models", "Semi-infinite programming"]},
    {"article name": "Iterative improvement of parameter estimation for model migration by means of sequential experiments",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.12.001",
     "publication date": "02-2015",
     "abstract": "Determining an optimal design for estimation of parameters of a class of complex models expected to be built at a minimum cost is a growing trend in science and engineering. We adopt a scale-bias adjustment migration strategy for integrating base and new models based on similar nature underlying processes. Further, we propose a Bayesian sequential algorithm for obtaining the statistically most informative data about the migrated model for use in parameter estimation. The benefits of the proposed strategy over traditional approaches presented in recent reported work are demonstrated using Monte Carlo simulations.",
     "keywords": ["Model migration", "Bayesian", "Model-based design", "Sequential algorithm"]},
    {"article name": "Two stage stochastic bilevel programming model of a pre-established timberlands supply chain with biorefinery investment interests",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.11.005",
     "publication date": "02-2015",
     "abstract": "This work studies the supply allocation problem, using a Stackelberg game, for an established timberlands supply chain with an additional decision of new biorefinery investments. In a timberlands system, harvester and manufacturer decision makers have separate objectives to maximize their respective profits. This interaction is represented with a turn based Stackelberg game. The harvesters decide first on the quantity harvested, and the manufacturers decide on how much to utilize. This game is modeled with a bilevel mathematical program. The novel feature of this paper's bilevel formulation is the inclusion of parametric uncertainty in a two stage model. The first stage problem involves logistical decisions around biorefinery investments, such as location and capacity, while the second stage problem involves a bilevel timberlands model with parameter uncertainty. Studying this problem formulation revealed interesting insights for solving multiperiod problems with bilevel stages as well as the decision maker's behavior for the timberlands model.",
     "keywords": ["Timberlands supply chain modeling", "Bilevel programming", "Biofuels", "Optimization", "Uncertainty", "Two stage multiperiod"]},
    {"article name": "Numerical solution of mixed continuous\u2013discrete population balance models for depolymerization of branched polymers",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.11.008",
     "publication date": "02-2015",
     "abstract": "A simulation technique to describe the depolymerization of branched polymers via bivariate population balance modeling was developed. The polymers were characterized by two internal coordinates: the number of monomer units and branching bonds. Three commonly used mechanisms for depolymerization (random chain, end chain, and random debranching scission) were applied and formulated such that only physically possible polymers were created. The mechanisms and the population balance equation were formulated in a mixed continuous\u2013discrete manner. The population balance equation was solved using the Direct Quadrature Method of Moments (DQMOM). With this algorithm, the time evolution of the distribution with respect to the internal coordinate was computed. In addition, the algorithm was validated through comparison with Monte Carlo simulations. Notably, the accuracy of the mixed continuous\u2013discrete formulation was significantly higher that of the continuous formulation. However, DQMOM was found to be unsuitable for describing the temporal evolution of the distribution for random scission.",
     "keywords": ["Direct Quadrature Method of Moments (DQMOM)", "Bivariate population balance", "Breakage", "Polymer"]},
    {"article name": "CFD study on effect of channel confluence angle on fluid flow pattern in asymmetrical shaped microchannels",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.12.007",
     "publication date": "02-2015",
     "abstract": "Five asymmetrical shaped microchannels were used to investigate the effect of channel confluence angle, flow rate and flow rate ratio on the mixing performance in a passive mixing test. The flow characteristics for all geometries were studied experimentally and numerically using CFD technique. The results reveal that the mixing performance depends on the channel geometry, total flow rate and inlet flow rate ratios. The classical T-shape microchannel had the worst performance in the lower flow rate and in higher Re numbers its performance was placed between the other ones. Furthermore, the mixing effectiveness for all microchannels was defined and measured at various feed flow rates. The results indicate that the mixing effectiveness increases with increasing flow rate ratio and decrease of confluence angle. The reasons for the observed experimental results were analyzed by the predicted CFD flow patterns.",
     "keywords": ["Micromixer", "Confluence angle", "CFD modeling", "Channel shape"]},
    {"article name": "A semicontinuous approach for heterogeneous azeotropic distillation processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.12.005",
     "publication date": "02-2015",
     "abstract": "The separation of azeotropes has substantial energy and investment costs, and the available methods require high capital costs for reconstruction of process plants. As an alternative, a semicontinuous configuration that utilizes an existing plant with minor modifications has been explored. In this paper, a semicontinuous, heterogeneous azeotropic distillation process is proposed and acetic acid dehydration process is used as a case study. To carry out the simulation work, Aspen HYSYS\u00ae simulation software is used along with MATLAB\u00ae and an interface program to handle the mode-transition of the semicontinuous process. Sensitivity analyses on operating parameters are performed to identify the process limits. Comparisons are made to conventional heterogeneous azeotropic distillation, and dividing-wall distillation column on the annual cost. The results proved that the semicontinuous system is the best setup in terms of total annual costs and energy requirements.",
     "keywords": ["HAc dehydration", "Heterogeneous azeotropic distillation", "Dividing-wall distillation columns", "Process optimization", "Semicontinuous approach"]},
    {"article name": "A framework for efficient large scale equation-oriented flowsheet optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.05.013",
     "publication date": "01-2015",
     "abstract": "Despite the economic benefits of flowsheet optimization, many commercial tools suffer from long computational times, limited problem formulation flexibility and numerical instabilities. In this study, we address these challenges and present a framework for efficient large scale flowsheet optimization. This framework couples advanced process optimization formulations with state-of-the-art algorithms, and includes several notable features such as (1) an optimization-friendly formulation of cubic equation of state thermodynamic models; (2) a new model for distillation column optimization based on rigorous mass, equilibrium, summation and heat (MESH) equations with a variable number of trays that avoids integer variables; (3) improvements on the Duran\u2013Grossmann formulation for simultaneous heat integration and flowsheet optimization; and (4) a systematic initialization procedure based on model refinements and a tailored multi-start algorithm to improve feasibility and identify high quality local solutions.Capabilities of the framework are demonstrated on a cryogenic air separation unit synthesis study, including two thermally coupled distillation columns and accompanying multistream heat exchangers. A superstructure is formulated that includes several common ASU configurations in literature. As part of the optimization problem the solver selects the best topology in addition to operating conditions (temperatures, flowrates, etc.) for coal oxycombustion applications. The optimization problem includes up to 16,000 variables and 500 degrees of freedom, and predicts specific energy requirement of 0.18 to 0.25\u00a0kWh/kg of O2 depending on design assumptions. These results are compared to literature and plans to extend the framework to an entire coal oxycombustion power plant optimization study are discussed.",
     "keywords": ["Process optimization", "Heat integration", "Distillation", "Air separation unit", "Coal oxycombustion", "Mathematical programming with complementarity constraints"]},
    {"article name": "Embedded optimization for mixed logical dynamical systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.06.005",
     "publication date": "01-2015",
     "abstract": "Predictive control of hybrid systems is currently considered prohibitive using embedded computing platforms. To overcome this limitation for mixed logical dynamical systems of small to medium size, we propose to use (1) a standard branch-and-bound approach combined with a fast embedded interior point solver, (2) pre-processing heuristics, run once and offline, to significantly reduce the number of subproblems to be solved, and (3) relaxations of the original MPC problem that allow a trade off between computation time and closed-loop performance. A problem-specific ANSI C implementation of the proposed method can be automatically generated, and has a fixed memory footprint and a code size that is insignificantly larger than that of the subproblem solver. Two extensive numerical studies are presented, where problems with up to 60 binary variables are solved in less than 0.2\u00a0s with a performance deterioration of below 2% when compared to an optimal MPC scheme.",
     "keywords": ["Mixed logical dynamical systems", "Hybrid model predictive control", "Embedded mixed-integer real-time optimization", "Branch and bound"]},
    {"article name": "Efficient optimization-based design for the separation of heterogeneous azeotropic mixtures",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.03.012",
     "publication date": "01-2015",
     "abstract": "Model-based design of separation processes for heterogeneous azeotropic mixtures is a challenging task. The multiplicity of the solutions of equilibrium calculations and the discontinuity due to the potential switching between homogeneous and heterogeneous mixtures on a tray severely complicate the application of deterministic optimization. This paper presents a novel computational approach to reliably determine phase stability and the correct equilibrium solutions in every iteration of the optimization. The approach builds on the decomposition of the optimization problem into a generic superstructure model and an implicit model for equilibrium and enthalpy calculations, which is integrated into the optimization problem by means of an external function. The phase states and equilibrium solutions are determined by means of a reliable homotopy continuation algorithm. An additional reformulation of equilibrium solutions and enthalpy calculations allows overcoming the discontinuity problems. Different case studies illustrate the applicability and show the potential of the proposed method.",
     "keywords": ["Heteroazeotropic distillation", "Superstructure optimization", "Phase stability test", "Process design"]},
    {"article name": "Supply chain planning and scheduling integration using Lagrangian decomposition in a knowledge management environment",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.06.002",
     "publication date": "01-2015",
     "abstract": "The integration of planning and scheduling decisions in rigorous mathematical models usually results in large scale problems. In order to tackle the problem complexity, decomposition techniques based on duality and information flows between a master and a set of subproblems are widely applied. In this sense, ontologies improve information sharing and communication in enterprises and can even represent holistic mathematical models facilitating the use of analytic tools and providing higher flexibility for model building. In this work, we exploit this ontologies\u2019 capability to address the optimal integration of planning and scheduling using a Lagrangian decomposition approach. Scheduling/planning sub-problems are created for each facility/supply chain entity and their dual solution information is shared by means of the ontological framework. Two case studies based on a STN representation of supply chain planning and scheduling models are presented to emphasize the advantages and limitations of the proposed approach.",
     "keywords": ["Supply chain planning", "Process scheduling", "Ontology", "Decision-levels integration", "Lagrangian decomposition"]},
    {"article name": "Systematic network synthesis and design: Problem formulation, superstructure generation, data management and solution",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.03.007",
     "publication date": "01-2015",
     "abstract": "The developments obtained in recent years in the field of mathematical programming considerably reduced the computational time and resources needed to solve large and complex Mixed Integer Non Linear Programming (MINLP) problems. Nevertheless, the application of these methods in industrial practice is still limited by the complexity associated with the mathematical formulation of some problems. In particular, the tasks of design space definition and representation as superstructure, as well as the data collection, validation and handling may become too complex and cumbersome to execute, especially when large problems are considered. In an earlier work, we proposed a computer-aided framework for synthesis and design of process networks. In this contribution, we expand the framework by including methods and tools developed to structure, automate and simplify the mathematical formulation of the design problem. Furthermore, the models employed for the representation of the process alternatives included in the superstructure are refined, through the inclusion of the energy balance. Finally, the features of the framework are highlighted through the solution of two case studies focusing on food processing and biofuels.",
     "keywords": ["Process networks", "Synthesis and design", "Mathematical programming", "Computer-aided process engineering", "Vegetable oil refinery", "Bioethanol"]},
    {"article name": "Design of memetic algorithms for the efficient optimization of chemical process synthesis problems with structural restrictions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.08.006",
     "publication date": "01-2015",
     "abstract": "In Urselmann et al., 2011a, Urselmann et al., 2011b we presented a memetic algorithm (MA) for the design optimization of reactive distillation columns. The MA is a combination of a problem-specific evolutionary algorithm (EA) that optimizes the design variables and a mathematical programming (MP) method that solves the continuous sub-problems with fixed discrete decisions which are proposed by the EA to local optimality. In comparison to the usual superstructure formulation, the search space of the MA is significantly reduced without excluding feasible solutions. The algorithm computes many different local optima and can handle structural restrictions and discontinuous cost functions. In this contribution, a systematic procedure to modify the MA to solve more complex design problems is described and demonstrated using the example of a reactive distillation column with an optional side- or pre-reactor with structural restrictions on the number of streams. New concepts to handle connected and optional unit operations are proposed.",
     "keywords": ["Chemical process synthesis", "Memetic algorithms", "MINLP", "Optimization"]},
    {"article name": "Strategic planning optimization for natural gas to liquid transportation fuel (GTL) systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.04.010",
     "publication date": "01-2015",
     "abstract": "A strategic planning optimization model is proposed for a network of natural gas to liquids (GTL) systems, and it is solved using a rolling horizon strategy. The model formulation determines the strategic and tactical decisions of the GTL supply chain over a long time horizon. The decisions to build new GTL refineries may be made over the span of 30 years and their operations cover the span of 60 years. Multiple capacities of GTL refineries (i.e., 1, 5, 10, 50, and 200 thousand barrels per day) that produce gasoline, diesel, and kerosene commensurate to the United States demand ratio may exist in the network. The parameter inputs include the locations, availabilities, and prices of natural gas in the United States discretized by county, the delivery locations of fuel products, and the transportation costs of every input and output of the refinery, defined for each time period. Formulated as a large-scale mixed-integer linear optimization (MILP) model, the problem is solved using a rolling horizon strategy for tractability. Case studies on the state of Pennsylvania are presented for different planning schemes and their impact on the economic performance of the GTL network is discussed.",
     "keywords": ["Energy supply chain", "Strategic planning", "MILP", "GTL", "Transportation fuels"]},
    {"article name": "A dynamic programming based approach for explicit model predictive control of hybrid systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.06.003",
     "publication date": "01-2015",
     "abstract": "This work presents an algorithm for explicit model predictive control of hybrid systems based on recent developments in constrained dynamic programming and multi-parametric programming. By using the proposed approach, suitable for problems with linear cost function, the original model predictive control formulation is disassembled into a set of smaller problems, which can be efficiently solved using multi-parametric mixed-integer programming algorithms. It is also shown how the methodology is applied in the context of explicit robust model predictive control of hybrid systems, where model uncertainty is taken into account. The proposed developments are demonstrated through a numerical example where the methodology is applied to the optimal control of a piece-wise affine system with linear cost function.",
     "keywords": ["Multi-parametric programming", "Model predictive control", "Robust model predictive control", "Dynamic programming", "Hybrid systems"]},
    {"article name": "Long-term turnaround planning for integrated chemical sites",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.08.003",
     "publication date": "01-2015",
     "abstract": "An integrated chemical site involves a complex network of chemical plants. Typically, these plants interact closely, are dependent on each other for raw materials and demand for their products, and have the provision of intermediate storage tanks to help manage inventory at strategic points in the network. Disruptions in the operation of these plants can drastically affect flow of material in the site network. As a result, the choice of sequence and timing of planned periodic turnarounds, which are major disruptions, is important in order to minimize effects on profits and production. We investigate a discrete-time mixed-integer linear programming (MILP) model to perform turnaround optimization. The objective is to recommend potential schedules in order to minimize losses while satisfying network, resource, turnaround, demand, financial and other practical constraints. We propose general formulations to tackle this problem and study an industrial-size site network under various scenarios over a long-term horizon.",
     "keywords": ["Maintenance scheduling", "Turnaround planning", "Enterprise-wide optimization", "Mixed-integer linear programming"]},
    {"article name": "Designing a Total Site for an entire lifetime under fluctuating utility prices",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.07.004",
     "publication date": "01-2015",
     "abstract": "This paper describes a synthesis of Total Site in order to obtain additional energy savings by process-to-process heat integration. Enhanced Heat Integration and economically viable designs can be obtained by establishing an appropriate trade-off between the operating cost and the investment. The aim of this work was to improve the modeling of the Total Site by including proper pressure levels selection for intermediate utilities, preheating of intermediate utilities because of incomplete condensate recovery, pipeline layout design, and optimal pipe design with optimal pressure/temperature drops and optimal insulation thickness and heat losses during transportation along the pipes. Additionally, future utility prices are considered when synthesizing the Total Site as they are expected to influence the trade-off between investment and operating cost. A stochastic multi-period mixed-integer nonlinear programming model for the optimal synthesis of Total Site over its entire lifetime has been developed by including all the above-mentioned design aspects.",
     "keywords": ["Heat Integration", "Total Site", "Mathematical programming", "Utility price forecast", "Expected Net Present Value", "Pipeline design"]},
    {"article name": "Scheduling and energy \u2013 Industrial challenges and opportunities",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.05.024",
     "publication date": "01-2015",
     "abstract": "Recent developments in energy markets, such as the increasing share of inherently volatile renewable power in the energy supply mix and the need of reducing carbon emissions while improving the production efficiency, make the operating environment of process industries more dynamic and complex. At the same time, continued advances in the mathematical programming and IT technologies open up new opportunities to tackle the related operational scheduling problems in a more integrated way at an ever larger scale. This paper discusses the industrial challenges arising from the deregulation of the electricity markets and stronger presence of unpredictable renewable energy sources. It gives a brief overview of methods currently available followed by set of real industrial case studies. The paper concludes with a discussion of the main challenges and opportunities relevant to the presented examples.",
     "keywords": ["Scheduling", "Energy efficiency", "Demand-side Management", "Production planning", "Thermo-mechanical pulp", "Steel production"]},
    {"article name": "\u03f5-OA for the solution of bi-objective generalized disjunctive programming problems in the synthesis of nonlinear process networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.04.004",
     "publication date": "01-2015",
     "abstract": "There has been an increasing interest in multicriteria optimization (MCO) of nonlinear process network problems in recent years. Several mathematical models have been developed and solved using MCO methodologies including \u03f5-constraint, weighted sum, and minimum distance. In this paper, we investigate the bi-objective nonlinear network synthesis problem and propose an effective algorithm, \u03f5-OA, based on augmented \u03f5-constraint and logic-based outer approximation (OA). We provide theoretical characterization of the proposed algorithm and show that the solutions generated are efficient. We illustrate the effectiveness of our novel algorithm on two benchmark problems. The \u03f5-OA is compared to the straightforward use of OA with augmented \u03f5-constraint algorithm (\u03f5-con\u00a0+\u00a0OA), the augmented \u03f5-constraint without OA (\u03f5-MINLP), and the traditional \u03f5-constraint (T-\u03f5-con). Based on the results, our novel algorithm is very effective in solving the bi-objective generalized disjunctive programming problems in the synthesis of process networks.",
     "keywords": ["Generalized disjunctive programming", "Multiple criteria optimization", "Outer approximation", "Mixed-integer nonlinear programming", "\u03f5-Constraint method"]},
    {"article name": "MINLP model for the detailed scheduling of refined products pipelines with flow rate dependent pumping costs",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.05.012",
     "publication date": "01-2015",
     "abstract": "Multiproduct pipelines transport fuels from refineries to distant distribution terminals in batches. The energy needed to move the fluids through the pipeline is mainly associated with elevation gradients and friction head loss. Commonly, friction loss is the major term requiring pump stations to keep the flow moving, and it is strongly dependent on the fluid flow rate. Some studies have been carried out for reducing the pumping costs in multiproduct pipelines, but none of them has been focused on thoroughly considering the head loss due to friction along the pipeline. This paper introduces a novel MINLP continuous-time formulation for the detailed scheduling of single-source pipelines, rigorously tracking power consumption at every pipeline segment through nonlinear equations. Real-world case studies are successfully solved using GAMS\u2013DICOPT algorithm, which proves to be a useful tool for solving large-scale, nonlinear scheduling problems. Important reductions in the operation costs are achieved by keeping a more stable flow rate profile over the planning horizon.",
     "keywords": ["Multiproduct pipeline", "Detailed scheduling", "MINLP approach", "Friction head loss", "DICOPT solver"]},
    {"article name": "Investment planning in energy considering economic and environmental objectives",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.05.006",
     "publication date": "01-2015",
     "abstract": "This work proposes a linear disjunctive multiperiod optimization model for planning investments in energy sources considering two objectives, one economical (maximization of the net present value), and the other environmental (minimization of greenhouse gas emissions \u2013 GHG). The general goal of this approach is to provide an analysis tool for energy decision makers in planning investment considering different scenarios in GHG emanation. The decision variables of the model are the investment needs in money, capacity and time in order to satisfy 100% of the energy market for Argentina in the period 2010\u20132030. Two models are proposed, the first one considers the total amount of GHG released in the horizon time; and the other contemplates the amount of GHG year by year. Twenty scenarios are evaluated with both models. The results obtained are presented, which show the trade-offs between both objectives.",
     "keywords": ["Renewable energy", "Investment planning", "Multi-objective multiperiod optimization", "Environmental and Economic impact"]},
    {"article name": "Theoretical framework for formulating MIP scheduling models with multiple and non-uniform discrete-time grids",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.03.003",
     "publication date": "01-2015",
     "abstract": "We present a framework for the formulation of MIP scheduling models based on multiple and nonuniform discrete time grids. In a previous work we showed that it is possible to use different (possibly non-uniform) time grids for each task, unit, and material. Here, we generalize these ideas to account for general resources, and a range of processing characteristics such as limited intermediate storage and changeovers. Each resource has its own grid based on resource consumption and availability allowing resource constraints to be modeled more accurately without increasing the number of binary variables. We develop algorithms to define the unit-, task-, material-, and resource-specific grids directly from problem data. Importantly, we prove that the multi-grid formulation is able to find a schedule with the same optimal objective as the discrete-time single-grid model with an arbitrarily fine grid. The proposed framework leads to the formulation of models with reduced number of binary variables and constraints, which are able to find good solutions faster than existing models.",
     "keywords": ["Mixed-integer programming", "Process optimization", "Supply chain management"]},
    {"article name": "Integrated planning and scheduling under production uncertainties: Bi-level model formulation and hybrid solution method",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.02.023",
     "publication date": "01-2015",
     "abstract": "We propose a novel method for integrating planning and scheduling problems under production uncertainties. The integrated problem is formulated into a bi-level program. The planning problem is solved in the upper level, while the scheduling problems in the planning periods are solved under uncertainties in the lower level. The planning and scheduling problems are linked via service level constraints. To solve the integrated problem, a hybrid method is developed, which iterates between a mixed-integer linear programming solver for the planning problem and an agent-based reactive scheduling method. If the service level constraints are not met, a cutting plane constraint is generated by the agent-based scheduling method and appended to the planning problem which is solved to determine new production quantities. The hybrid method returns an optimality gap for validating the solution quality. The proposed method is demonstrated by two complicated problems which are solved efficiently with small gaps less than 1%.",
     "keywords": ["Planning", "Scheduling", "Service-level constraints", "Hybrid method", "Agent-based modeling and simulation."]},
    {"article name": "Optimal coupling of a biomass based polygeneration system with a concentrated solar power facility for the constant production of electricity over a year",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.11.006",
     "publication date": "01-2015",
     "abstract": "In this paper we address the integration of a polygeneration system based on biomass with a concentrated solar power facility for the constant production of electricity over a year long. The process is modelled as a superstructure embedding two different gasification technologies, direct and indirect, and two reforming modes, partial oxidation or steam reforming followed by gas cleaning and three alternatives for the syngas use, water gas shift reactor (WGSR) to produce hydrogen, a furnace for thermal energy production and an open Brayton cycle. We couple this system with a concentrated solar plant that uses tower technology, molten salts and a regenerative Rankine cycle. The problem is formulated as a multi-period mixed-integer non linear programming problem (MINLP). The optimal integration involves the use of indirect gasification, steam reforming and a Brayton cycle to produce 340\u00a0MW of electricity at 0.073\u00a0\u20ac/kWh and 97\u00a0kt/yr of hydrogen as a credit.",
     "keywords": ["Renewable energy", "Biofuels", "Solar energy", "Multiperiod optimization"]},
    {"article name": "Logic hybrid simulation-optimization algorithm for distillation design",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.03.016",
     "publication date": "01-2015",
     "abstract": "In this paper, we propose a novel algorithm for the rigorous design of distillation columns that integrates a process simulator in a generalized disjunctive programming formulation. The optimal distillation column, or column sequence, is obtained by selecting, for each column section, among a set of column sections with different number of theoretical trays. The selection of thermodynamic models, properties estimation, etc. is all in the simulation environment. All the numerical issues related to the convergence of distillation columns (or column sections) are also maintained in the simulation environment. The model is formulated as a Generalized Disjunctive Programming (GDP) problem and solved using the logic based outer approximation algorithm without MINLP reformulation. Some examples involving from a single column to thermally coupled sequence or extractive distillation shows the performance of the new algorithm.",
     "keywords": ["Distillation", "Generalized disjunctive programming", "Simulation", "Optimization"]},
    {"article name": "Tightening piecewise McCormick relaxations for bilinear problems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.03.025",
     "publication date": "01-2015",
     "abstract": "We address nonconvex bilinear problems where the main objective is the computation of a tight lower bound for the objective function to be minimized. This can be obtained through a mixed-integer linear programming formulation relying on the concept of piecewise McCormick relaxation. It works by dividing the domain of one of the variables in each bilinear term into a given number of partitions, while considering global bounds for the other. We now propose using partition-dependent bounds for the latter so as to further improve the quality of the relaxation. While it involves solving hundreds or even thousands of linear bound contracting problems in a pre-processing step, the benefit from having a tighter formulation more than compensates the additional computational time. Results for a set of water network design problems show that the new algorithm can lead to orders of magnitude reduction in the optimality gap compared to commercial solvers.",
     "keywords": ["Optimization", "Mathematical modeling", "Nonlinear programming", "Generalized Disjunctive Programming", "Water minimization"]},
    {"article name": "A stochastic programming approach for the Bayesian experimental design of nonlinear systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.06.006",
     "publication date": "01-2015",
     "abstract": "Several approaches for the Bayesian design of experiments have been proposed in the literature (e.g., D-optimal, E-optimal, A-optimal designs). Most of these approaches assume that the available prior knowledge is represented by a normal probability distribution. In addition, most nonlinear design approaches involve assuming normality of the posterior distribution and approximate its variance using the expected Fisher information matrix. In order to be able to relax these assumptions, we address and generalize the problem by using a stochastic programming formulation. Specifically, the optimal Bayesian experimental design is mathematically posed as a three-stage stochastic program, which is then discretized using a scenario based approach. Given the prior probability distribution, a Smolyak rule (sparse-grids) is used for the selection of scenarios. Two retrospective case studies related to population pharmacokinetics are presented. The benefits and limitations of the proposed approach are demonstrated by comparing the numerical results to those obtained by implementing a more exhaustive experimentation and the D-optimal design.",
     "keywords": ["Design of experiments", "Stochastic programming", "NLP", "MINLP"]},
    {"article name": "Gobal optimization of hybrid kinetic/FBA models via outer-approximation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.06.011",
     "publication date": "01-2015",
     "abstract": "Flux balance analysis (FBA) is a linear programming-based framework widely used to predict the behavior, in terms of the resulting flux distribution, of cellular organisms in different media. FBA models are constructed using only stoichiometric information, and for this reason they sometimes fail in predicting fluxes precisely. In this work, we formally define the concept of hybrid FBA/kinetic models, in which kinetic information of key processes is used to tighten the search space of standalone FBA formulations, thereby enhancing their predictive capabilities. This approach leads to non-linear non-convex models that may exhibit multiple local optima. To solve them to global optimality, we use a customized outer-approximation algorithm that exploits the structure of the kinetic equations. Numerical results show that our method enhances the quality of standalone FBA models, providing more accurate predictions.",
     "keywords": ["Metabolic engineering", "FBA", "GMA", "Global optimization", "Kinetic models"]},
    {"article name": "Optimal molecular design of working fluids for sustainable low-temperature energy recovery",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.04.009",
     "publication date": "01-2015",
     "abstract": "The practical development of sustainable energy sources is presently one of the key research topics due to its strong social and economic impact. Among the sources of sustainable energy, the use of low-temperature energy reservoirs has attracted interest. Normally, because of their low energy content, those processing streams are considered as non-profitable, especially when water is used as the working fluid in the thermodynamic cycles used for energy recovery from low-temperature energy sources. However, the efficiency of energy recovery can be increased using working fluids featuring low-temperature boiling points under the proper processing conditions. In this work we propose the optimal molecular design of a new family of organic fluids whose aim is to increase energy recovery from low-temperature energy sources. The design problem is cast as a mixed-integer non-linear programming problem, where binary variables are used to define the molecular structure of the working fluids and continuous variables permit the computation of physical and thermodynamic properties. The results indicate that optimal molecular design techniques permit the design of new organic compounds which increase energy recovery. Moreover, the toxicity of the new working fluids was reduced in comparison to other organic fluids used for the same purpose.",
     "keywords": ["Optimization", "Product design", "Energy recovery", "Sustainability"]},
    {"article name": "A MILP-based column generation strategy for managing large-scale maritime distribution problems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.04.008",
     "publication date": "01-2015",
     "abstract": "This paper presents a novel column generation algorithm for managing the logistics activities performed by a fleet of multi-parcel chemical tankers. In our procedure, for providing elementary routes, the conventional dynamic programming routes-generator is replaced by an efficient continuous-time MILP-slave problem. The performance of the decomposition method is evaluated by solving several examples dealing with the operations of a shipping company operating in the Asia Pacific Region. Computational results show that the proposed approach outperforms a pure exact optimization model and an alternative heuristic solution method reported in the literature.",
     "keywords": ["Tramp shipping", "Ship routing and scheduling", "Column generation", "MILP-slave-formulation"]},
    {"article name": "Bilevel optimization techniques in computational strain design",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.06.007",
     "publication date": "01-2015",
     "abstract": "Over the past decade a number of bilevel optimization techniques were introduced for computational strain design leading to the overproduction of biochemicals. In this paper, we provide an algorithm-centric description of the OptKnock and OptForce strain-design protocols, highlight their differences and demonstrate their application for a prototypical overproduction problem. The derivation of the equivalent MILP representation in both cases is described in detail along with provisions that lead to significantly improved performance. Comparison between the intervention strategies of OptKnock and OptForce for the overproduction of succinate in Escherichia coli reveal that while OptKnock couples succinate with biomass production, OptForce suggests interventions that improves the minimum production of succinate. Further, OptForce is more tractable as it identifies interventions from only the subset of reactions that must change in the overproducing strain. Overall, this paper highlights the computational challenges faced in strain design and the methodological choices explored by OptKnock and OptForce.",
     "keywords": ["Computational strain-design", "Bilevel problem", "MILP formulation"]},
    {"article name": "A generalization of the Branch-and-Sandwich algorithm: From continuous to mixed-integer nonlinear bilevel problems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.06.004",
     "publication date": "01-2015",
     "abstract": "We propose a deterministic global optimization algorithm for mixed-integer nonlinear bilevel problems (MINBP) by generalizing the Branch-and-Sandwich algorithm (Kleniati and Adjiman, 2014a). Advances include the removal of regularity assumptions and the extension of the algorithm to mixed-integer problems. The proposed algorithm can solve very general MINBP problems to global optimality, including problems with inner equality constraints that depend on the inner and outer variables. Inner lower and inner upper bounding problems are constructed to bound the inner optimal value function and provide constant-bound cuts for the outer bounding problems. To remove the need for regularity, we introduce a robust counterpart approach for the inner upper bounding problem. Branching is allowed on all variables without distinction by keeping track of refined partitions of the inner space for every refined subdomain of the outer space. Finite \u025b-convergence to the global solution is proved. The algorithm is applied successfully to 10 mixed-integer literature problems.",
     "keywords": ["Bilevel optimization", "Deterministic global optimization", "Branch-and-bound", "Optimal value function"]},
    {"article name": "Mixed integer polynomial programming",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.07.020",
     "publication date": "01-2015",
     "abstract": "The mixed integer polynomial programming problem is reformulated as a multi-parametric programming problem by relaxing integer variables as continuous variables and then treating them as parameters. The optimality conditions for the resulting parametric programming problem are given by a set of simultaneous parametric polynomial equations which are solved analytically to give the parametric optimal solution as a function of the relaxed integer variables. Evaluation of the parametric optimal solution for integer variables fixed at their integer values followed by screening of the evaluated solutions gives the optimal solutions.",
     "keywords": ["Mixed integer programming", "Polynomial programming", "Nonlinear inversion", "Multi-parametric programming"]},
    {"article name": "Optimization of VARICOL SMB processes using hybrid modeling and nonlinear programming",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.07.009",
     "publication date": "12-2014",
     "abstract": "The Varicol process exhibits continuous and discrete-time dynamics and can be represented by a hybrid model, where the continuous-time dynamics is described by mass-balance partial differential equations, whereas the discrete-time dynamics is described by a timed transition Petri net (TTPN). This hybrid model allows to simplify the solution of a normally difficult Mixed-Integer Non-Linear Programming problem for maximizing the productivity of a Varicol process subject to constraints on the desired purities of the extract and the raffinate components. Indeed, the coverability tree of the TTPN can be constructed to generate a set of candidate integer parameters, and the corresponding nonlinear problems are solved for a subset of candidates selected on the basis of an approximate evaluation using the (true moving bed) triangle theory. The optimal solution is the Varicol configuration (column sequence) for which the productivity is maximal. A repeatability and a robustness analysis show the feasibility of this approach.",
     "keywords": ["Simulated moving bed", "Chromatography", "Separation", "Optimization", "Process control"]},
    {"article name": "An exact solution approach based on column generation and a partial-objective constraint to design a cellulosic biofuel supply chain",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.07.011",
     "publication date": "12-2014",
     "abstract": "This study provides an exact solution method to solve a mixed-integer linear programming model that prescribes an optimal design of a cellulosic biofuel supply chain. An embedded structure can be transformed to a generalized minimum cost flow problem, which is used as a sub-problem in a column generation approach, to solve the linear relaxation of the mixed-integer program. This study proposes a dynamic programming algorithm to solve the sub-problem in O(m) time, generating improving path-flows. It proposes an inequality, called the partial objective constraint, which is based on the portion of the objective function associated with binary variables, to underlie a branch-and-cut approach. Computational tests show that the proposed solution approach solves most instances faster than a state-of-the-art commercial solver (CPLEX).",
     "keywords": ["Biomass/biofuel supply chain", "Embedded generalized flow problem", "Column generation", "Partial objective constraint", "Dynamic programming"]},
    {"article name": "Solving the heat and mass transfer equations for an evaporative cooling tower through an orthogonal collocation method",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.06.008",
     "publication date": "12-2014",
     "abstract": "In this paper, the orthogonal collocation technique is utilized to solve the Poppe method equations for heat and mass transfer in counter flowing wet-cooling towers. The six differential equations for unsaturated and supersaturated air from the Poppe method are simplified, yielding three differential equations that use the Heaviside function. The humidity ratio is demonstrated to be a finite power series at a normalized water temperature. The air enthalpy is expressed as a function of the normalized water temperature and the unknown coefficients of the expansion from the humidity ratio. The discrete formulation is solved using the Newton\u2013Raphson method using an explicit Jacobian. The proposed methodology is applied to eight examples, and the results are compared to the results obtained when the governing equations are integrated with the Dormand\u2013Prince method. The results indicate that the accuracy is similar between both techniques. However, the orthogonal collocation requires less CPU time.",
     "keywords": ["Cooling tower", "Poppe method", "Orthogonal collocation", "Dormand\u2013Prince", "Explicit Jacobian"]},
    {"article name": "Multi-scale product property model of polypropylene produced in a FBR: From chemical process engineering to product engineering",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.07.013",
     "publication date": "12-2014",
     "abstract": "A multi-scale product model has been built to characterize the polypropylene (PP) formation dynamics in a catalytic FBR. For the first time, the gas\u2013solid flow field, the morphological and molecular properties of particles, as well as their dynamics can be simultaneously obtained by solving the unique model that couples a CFD model, a population balance model (PBM) and moment equations. The quantitative relationships between the operating conditions and the multi-scale particle properties have been further established. The results demonstrate that the product model can be used to guide a multi-scale generalization of the polymer product from chemical process to product engineering.",
     "keywords": ["Polypropylene", "Fluidized bed reactor", "Multi-scale modeling", "Population balance model", "Method of moments", "End-use property"]},
    {"article name": "An MINLP model for the simultaneous integration of energy, mass and properties in water networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.07.008",
     "publication date": "12-2014",
     "abstract": "A model for the synthesis of water networks with a simultaneous integration of energy, mass and properties is presented. The model is formulated within a mixed-integer nonlinear programming framework where the objective function accounts for the minimization for the total annual cost satisfying energy, mass and property constraints for the water streams involved in the network. To accomplish this task, a new superstructure is proposed, in which a first stage for energy integration before mixing streams was considered, followed by a mass and property integration network, and placing finally a second energy integration network. Within this approach, the optimization model identifies when a stream can be used as a hot or a cold stream as part of the energy integration. The proposed approach was applied to two case studies, and the results show that there are significant advantages for the simultaneous implementation of the energy, mass and property integration strategies.",
     "keywords": ["Simultaneous optimization", "Water networks", "Energy integration", "Mass integration", "Property integration"]},
    {"article name": "On the impact of using volume as an independent variable for the solution of P\u2013T fluid-phase equilibrium with equations of state",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.06.009",
     "publication date": "12-2014",
     "abstract": "The constant pressure\u2013temperature (P\u2013T) flash plays an important role in the modelling of fluid-phase behaviour, and its solution is especially challenging for equations of state in which the volume is expressed as an implicit function of the pressure. We explore the relative merits of solving the P\u2013T flash in two ensembles: mole numbers, pressure and temperature, in which each free-energy evaluation requires the use of a numerical solver; and mole numbers, volume and temperature, in which a direct evaluation of the free-energy is possible. We examine the performance of two algorithms, HELD (Helmholtz free energy Lagrangian dual), introduced in Pereira et al. (2012), and GILD (Gibbs free energy Lagrangian dual), introduced here, for the fluid-phase equilibria of 8 mixtures comprising up to 10 components, using two equations of state. While the reliability of both algorithms is comparable, the computational cost of HELD is consistently lower; this difference becomes increasingly pronounced as the number of components is increased.",
     "keywords": ["Fluid-phase equilibria", "Phase stability", "Helmholtz free energy", "Gibbs free energy", "SAFT-VR equation of state", "Peng\u2013Robinson equation of state"]},
    {"article name": "Adaptive soft sensor modeling framework based on just-in-time learning and kernel partial least squares regression for nonlinear multiphase batch processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.07.014",
     "publication date": "12-2014",
     "abstract": "Batch processes are characterized by inherent nonlinearity, multiple phases and time-varying behavior that pose great challenges for accurate state estimation. A multiphase just-in-time (MJIT) learning based kernel partial least squares (KPLS) method is proposed for multiphase batch processes. Gaussian mixture model is estimated to identify different operating phases where various JIT-KPLS frameworks are built. By applying Bayesian inference strategy, the query data is classified into a particular phase with the maximal posterior probability, and thus the corresponding JIT-KPLS framework is chosen for online prediction. To further improve the predictive accuracy of the MJIT-KPLS algorithm, a hybrid similarity measure and an adaptive selection strategy are proposed for selecting local modeling samples. Moreover, maximal similarity replacement rule is proposed to update database. A procedure of input variable selection based on partial mutual information is also presented. The effectiveness of the MJIT-KPLS algorithm is demonstrated through application to industrial fed-batch chlortetracycline fermentation process.",
     "keywords": ["Adaptive soft sensor", "Batch process", "Kernel partial least squares", "Just-in-time learning", "Partial mutual information", "Chlortetracycline fermentation process"]},
    {"article name": "An algorithm for the identification and estimation of relevant parameters for optimization under uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.07.007",
     "publication date": "12-2014",
     "abstract": "Models are prone to errors, often due to uncertain parameters. For optimization under uncertainty, the larger the amount of uncertain parameters, the higher the computational effort and the possibility of obtaining unrealistic results. In this contribution it is assumed that not all uncertain parameters need to be regarded and focus should be laid on a subset. As a first step in the algorithm, a parameter estimation is carried out to determine expected values, followed by a linear-dependency analysis and a ranking of the uncertain parameters. Parameters with a high linear-dependency are fixed, while others are left uncertain. This is followed by a subset selection regarding the sensitivity of the parameters toward the model and toward a user-defined objective function. Thus, only parameters with the largest sensitivities are selected as uncertain parameters and considered for optimization under uncertainty. A case study is presented in which the algorithm is applied.",
     "keywords": ["Optimization under uncertainty", "Parameter estimation", "Identifiable parameter subset selection", "Monte Carlo simulation"]},
    {"article name": "Optimal operation strategy of batch vacuum distillation for sulfuric acid recycling process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.07.024",
     "publication date": "12-2014",
     "abstract": "Vacuum distillation techniques are widely used in food, biological, pharmaceutical, and wastewater treatment industries. Because of its operation at low temperatures, vacuum distillation prevents the thermal decomposition of materials and alleviates corrosion processes; however, condenser size can be dramatically increased because of reductions in mean temperature differences under the vacuum operation. In batch vacuum distillation processes, vapor generation rate and mean temperature differences are changed with time. In view of these characteristics of batch operation, this paper suggests a novel methodology to minimize the condenser size in batch vacuum distillation processes. The target process is a sulfuric acid recycling system in semiconductor manufacturing plants. In this paper, an equation-oriented dynamic model is established and optimization problem is formulated. By solving the nonlinear programming problem, the condenser size is dramatically reduced when operation time is fixed. In contrast, operation time is greatly shortened when the installed condenser surface area is fixed.",
     "keywords": ["Batch vacuum distillation", "Batch distillation dynamic modeling", "Optimal batch operation", "Sulfuric acid recycling", "Vacuum condenser"]},
    {"article name": "Synchronous and asynchronous decision making strategies in supply chains",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.07.005",
     "publication date": "12-2014",
     "abstract": "A supply chain is a network of entities involving an interplay of different individual behaviors. For an industrial scale supply chain, this network can be very complex. For a large scale supply chain, the structure is usually distributed with the different functions being performed both synchronously and asynchronously. A supply chain may consist of entities belonging to the same or different organizations. The behavior of the overall network evolves as a mixture of these synchronous and asynchronous business processes. In this work, we demonstrate the importance of capturing the synchronous and asynchronous decision-making strategies of different supply chain entities. We use agent based simulation models and embedded optimization models to study how the behavior of the network evolves as a result of the individual synchronous and asynchronous functions. Hybrid simulation based optimization is used to predict the optimal operation under the two scenarios.",
     "keywords": ["Synchronous and asynchronous decision-making", "Supply chain management", "Hybrid simulation based optimization"]},
    {"article name": "System model of a tablet manufacturing process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.07.026",
     "publication date": "12-2014",
     "abstract": "A novel process system model of a roll compaction tablet manufacturing process has been developed. The model couples together multiple unit operations, namely roller compaction, milling and tablet press, providing a framework to simulate cause and effect relationships between process parameters and product performance. Models for tablet tensile strength and dissolution are implemented to evaluate tablet performance. The models for the milling and the dissolution units are based on the solution of population balance equations. The model is validated by comparison with experimental data. This work demonstrates that it is possible to develop a model for tablet manufacturing that is based on fundamental process understanding and that the resulting simulation tool, requiring limited experimental calibration, can be used easily and effectively in the workflow of pharmaceutical product development.",
     "keywords": ["System modelling", "Pharmaceutical development", "Roller compaction", "Product performance", "Population balance modelling"]},
    {"article name": "Bioreactor-based fuel systems. I: Optimal production capacity considering start-up dynamics",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.07.006",
     "publication date": "12-2014",
     "abstract": "Traditional capacity sizing models based on production cost minimization do not consider the impact of facility construction and start-up time scale on the profitability of plant. For production facilities that link substantially different scales, such as biofuel production facilities that connect large numbers of small bioreactors to large scale processing equipment, the ramp-up to full production may be a significant consideration in the decision on facility scale. This paper presents a modified discounted cash flow method incorporating production growth during start-up, to determine the optimal production scale of systems that may operate at less than full capacity for a substantial period of time. For a case study of ethanol produced by blue-green algae, global sensitivity analysis using Sobols method indicates how exogenous and endogenous variables, including the ramp-up rate, affect the optimal scale and financial viability of the production system. Conditional-value-at-risk minimization shows how the optimal decision depends on the risk inclination of the decision-maker; a risk-neutral decision maker would choose a basic optimal production scale of 59\u00a0million liters (ML) of ethanol per year, while a risk-averse decision-maker might prefer to choose a capacity 30% less.",
     "keywords": ["Algal fuel", "Capacity design", "Uncertainty", "Global sensitivity analysis", "CVaR optimization"]},
    {"article name": "On the equivalence between the modifier-adaptation and trust-region frameworks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.07.028",
     "publication date": "12-2014",
     "abstract": "In this short note, the recently popular modifier-adaptation framework for real-time optimization is discussed in tandem with the well-developed trust-region framework of numerical optimization, and it is shown that the basic version of the former is a simplification of the latter when the problem is unconstrained. This relation is then exploited to propose a globally convergent modifier-adaptation algorithm using already developed trust-region theory. Cases when the two may not be equivalent and extensions to constrained problems are also discussed.",
     "keywords": ["Modifier adaptation", "Trust-region methods", "Real-time optimization"]},
    {"article name": "Fast distributed MPC based on active set method",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.08.001",
     "publication date": "12-2014",
     "abstract": "Modern chemical plants are characterized by their large-scale, strong interactions and the presence of constraints. With its ability to systematically handle these issues, distributed model predictive control (DMPC) is a promising approach for the control of such systems. However, the problem of how to efficiently solve the resulting distributed optimization problem is still an open question. This paper develops a novel fast DMPC approach based on a distributed active set method and offline inversion of the Hessian matrix to efficiently solve a constrained distributed quadratic program. A dual-mode optimization strategy based on the value of unconstrained optimal solution is developed to accelerate the computation of control action. The proposed method allows for the optimization to be terminated before convergence to cope with the fast sampling periods. Furthermore, a warm-start strategy based on the solution of the previous sampling instant is integrated with the approach to further improve convergence speed. The approach is highly parallelized as constraints can be checked in parallel. The approach is demonstrated using an academic example as well as a chemical process network control.",
     "keywords": ["Distributed model predictive control", "Parallel computing", "Fast MPC"]},
    {"article name": "Identification of probabilistic graphical network model for root-cause diagnosis in industrial processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.07.022",
     "publication date": "12-2014",
     "abstract": "Identification of faults in process systems can be based purely on measurement (e.g. PCA), or can exploit knowledge of process model structure to construct a causal network. This work introduces a method to identify most likely causal network in cases when process model is not known. An incidence matrix, showing location of measurements in the plant network structure, and historical process data are used to identify the optimal causal network structure by means of maximizing Bayesian scores for alternative causal networks. Causal subnetworks, corresponding to subgraphs of the process network, are identified by finding the most probable graph based on highest posterior probability of graph features computed via Markov Chain Monte Carlo simulation. Novel Bayesian contribution indices within the probabilistic graphical network are proposed to identify the potential root-cause variables. Application to Tennessee Eastman Chemical plant demonstrates that the presented method is significantly more accurate than the current methods.",
     "keywords": ["Probabilistic graphical model", "Cause\u2013effect relationship", "Structure learning", "Incidence matrix", "Markov chain Monte Carlo simulation", "Root-cause diagnosis"]},
    {"article name": "Ionic liquid effects on mass transfer efficiency in extractive distillation of water\u2013ethanol mixtures",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.08.002",
     "publication date": "12-2014",
     "abstract": "The relatively high viscosities of ionic liquids could reduce the mass transfer efficiency of the extractive distillation process. The rate-based model was adopted to analyze this phenomenon since it predicted the performance of an extractive distillation pilot plant using ionic liquids as solvent. For the water\u2013ethanol separation, three ionic liquids: 1-ethyl-3-methylimidazolium chloride, 1-ethyl-3-methylimidazolium acetate and 1-ethyl-3-methylimidazolium dicyanamide and the organic solvent ethylene glycol were used for the analysis. Simulations were conducted for sieve trays and Mellapak\u00ae 250Y. The results indicate that relatively high viscosities affect the mass transfer efficiency. However, the improvements in relative volatilities obtained from the ionic liquids help to overcome this effect. However, with high solvent viscosities (>65\u00a0mPa\u00a0s at T\u00a0=\u00a0353.15\u00a0K) it was not possible to overcome the reductions. Additionally, at higher distillate rates high relative volatilities yielded negative effects on mass transfer efficiency because of a decrease in vapor velocity.",
     "keywords": ["Rate-based model", "Extractive distillation", "Ionic liquids", "Mass transfer efficiency", "Water\u2013ethanol separation", "ASPEN\u00ae simulations"]},
    {"article name": "Plant-wide control system design: Primary controlled variable selection",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.08.004",
     "publication date": "12-2014",
     "abstract": "This work is focused on the development of a rigorous, model-based approach for the selection of primary controlled variables as part of a plant-wide control system design methodology. Controlled variables should be selected for their self-optimizing control performance and controllability while ensuring satisfactory performance in terms of dead-time and closed loop interactions. This work has considered both self-optimizing and control performance as well as has addressed issues related to loop-interactions and superstructure constraints. The new three-stage approach developed in this work results in a large-scale, constrained, mixed-integer multi-objective optimization problem. For solving this problem, a parallelized, bi-directional branch and bound algorithm with dynamic search strategies has been developed to solve the problem on large computer clusters. The proposed approach is then applied to an acid gas removal unit as part of an integrated gasification combined cycle power plant with CO2 capture.",
     "keywords": ["Primary controlled variable", "Self-optimizing", "Parallelized branch and bound", "AGR", "IGCC"]},
    {"article name": "Ideal adsorbed solution theory solved with direct search minimisation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.07.027",
     "publication date": "12-2014",
     "abstract": "The ideal adsorbed solution theory (IAST) is the most widespread theory for multicomponent adsorption interpretation. It postulates the existence of an adsorbed phase which behaves as a Raoult ideal solution. The theory results in a system of nonlinear algebraic equations which are solved to know the composition of the adsorbed mixture at equilibrium. In this paper an investigation on an alternative method for the IAST equations solution is proposed which is based on the minimisation of an objective function representing the iso-spreading pressure condition. This approach to the solution of the IAST equations reduces in some cases the computational effort and mitigates the issues of the currently adopted approaches (inversion of functions and initial guess). For binary systems, direct search minimisation approach is faster than the classic IAST equations solution approach up to 19.0 (Dual Langmuir isotherm) and 22.7 times (Toth isotherm). In ternary systems, this difference decreases to 10.4 (O\u2019Brien and Myers isotherm) times. Compared to FASTIAS approach, direct search minimisation is up to 4.2 times slower in ternary systems.",
     "keywords": ["Ideal adsorbed solution theory", "Adsorption equilibria", "Adsorption thermodynamics", "Solution algorithm"]},
    {"article name": "Analysis of finite difference discretization schemes for diffusion in spheres with variable diffusivity",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.05.022",
     "publication date": "12-2014",
     "abstract": "Two finite difference discretization schemes for approximating the spatial derivatives in the diffusion equation in spherical coordinates with variable diffusivity are presented and analyzed. The numerical solutions obtained by the discretization schemes are compared for five cases of the functional form for the variable diffusivity: (I) constant diffusivity, (II) temporally dependent diffusivity, (III) spatially dependent diffusivity, (IV) concentration-dependent diffusivity, and (V) implicitly defined, temporally and spatially dependent diffusivity. Although the schemes have similar agreement to known analytical or semi-analytical solutions in the first four cases, in the fifth case for the variable diffusivity, one scheme produces a stable, physically reasonable solution, while the other diverges. We recommend the adoption of the more accurate and stable of these finite difference discretization schemes to numerically approximate the spatial derivatives of the diffusion equation in spherical coordinates for any functional form of variable diffusivity, especially cases where the diffusivity is a function of position.",
     "keywords": ["Finite difference method", "Variable coefficient", "Diffusion", "Spherical geometry", "Method of lines"]},
    {"article name": "Plant-wide control system design: Secondary controlled variable selection",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.08.007",
     "publication date": "12-2014",
     "abstract": "This work is focused on the development of a rigorous, model-based approach for the selection of secondary controlled variables as part of a plant-wide control system design methodology. Secondary controlled variables should be easy to measure, easy to control, fast to respond to changes in the input variables, and lead to automatic, indirect control of the primary controlled variables. While much of the work on this subject has been based upon ad hoc approaches, here a systematic three-stage approach is proposed that addresses issues of controllability and economic performance of the control system. The first stage involves the generation of an initial set of candidate secondary controlled variables and the generation of selection constraints that are used to determine if manipulated variables can be used for control of candidate controlled variables. During the second stage, secondary controlled variables are selected to minimize integral absolute errors (IAEs) of the primary controlled variables subject to minimal loop interactions as determined by a relative gain array analysis. Finally, during the third stage, control performance of the secondary controlled variables is evaluated at off-design operations using a nonlinear process model. The proposed approach is then applied, as ongoing work in the application of plant-wide control, to an acid gas removal unit as part of an integrated gasification combined cycle power plant with CO2 capture.",
     "keywords": ["Secondary controlled variable", "IAE", "Parallelized branch and bound", "AGR", "IGCC"]},
    {"article name": "A probabilistic self-validating soft-sensor with application to wastewater treatment",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.08.008",
     "publication date": "12-2014",
     "abstract": "In the wastewater treatment plants (WWTPs), soft sensors are viewed as a simple signal estimator for hard-to-measure quantities. However, the presence of unreliable data, coupled with increasing demands for measurement quality assurance, has rendered inadequate such a simplistic view. In this paper, a probabilistic self-validating soft-sensor is proposed with the capability of performing self-diagnostics, self-reconstruction and online uncertainty measurement. In this framework, data collecting for soft-sensor modeling (easy-to-measure data) is validated by a Variational Bayesian Principal Component Analysis (VBPCA) model before performing a soft-sensor model construction. By integrating Relevant Vector Machine (RVM) as a predictive model, not only prediction values are obtained, but also the credibility of information for easy-to-measure and hard-to-measure quantities can be generated. The performance of the proposed soft-sensor is validated through two simulation studies of WWTPs with different process characteristics. The results suggest that the proposed strategy significantly improves the prediction performance.",
     "keywords": ["Soft sensor", "Wastewater", "Self-validation", "Variational Bayesian Principal Component Analysis", "Relevant Vector Machine"]},
    {"article name": "Adaptive monitoring of the process operation based on symbolic episode representation and hidden Markov models with application toward an oil sand primary separation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.08.009",
     "publication date": "12-2014",
     "abstract": "This paper presents a novel procedure for classification of normal and abnormal operating conditions of a process when multiple noisy observation sequences are available. Continuous time signals are converted to discrete observations using the method of triangular representation. Since there is a large difference in the means and variances of the durations and magnitudes of the triangles at different operating modes, adaptive fuzzy membership functions are applied for discretization. The expectation maximization (EM) algorithm is used to obtain parameters of the different modes for the durations and magnitudes assuming that states transit to each other according to a Markov chain model. Applying Hamilton's filter, probability of each state given new duration and magnitude is calculated to weight the membership functions of each mode previously obtained from a fuzzy C-means clustering. After adaptive discretization step, having discrete observations available, the combinatorial method for training hidden Markov models (HMMs) with multiple observations is used for overall classification of the process. Application of the method is studied on both simulation and industrial case studies. The industrial case study is the detection of normal and abnormal process conditions in the primary separation vessel (PSV) of an oil sand industry. The method shows an overall good performance in detecting normal and risky operating conditions.",
     "keywords": ["Symbolic episode representation", "Abnormal event detection", "EM algorithm", "Bayesian approach", "Fuzzy logic", "Hidden Markov model"]},
    {"article name": "Mixed integer optimal control of an intermittently aerated sequencing batch reactor for wastewater treatment",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.09.004",
     "publication date": "12-2014",
     "abstract": "Optimal aeration control strategies for sequencing batch reactors in WWT with bypass nitrification are hereby studied. The operation is defined alternating aerobic and anoxic phases with high frequency. The controlled variable, the aeration, can only adopt fixed values, on and off, leading to a discrete trajectory of bang\u2013bang type. The problem is to compute the number of switches and individual length of each aerobic and anoxic stage. This leads to a mixed integer nonlinear optimal control problem (MINTOC). The solution is challenging, since both integer and continuous variables ought to be considered in the optimization. In contrast to previous work, where optimization is performed based on the separation and independent solution of the integer and continuous problem, we apply an algorithm originally proposed by Sager (2005). The optimization program minimizes operation time and energy consumption. Effluent concentrations are considered as nonlinear constraints in accordance to environmental regulations.",
     "keywords": ["ASP activated sludge process", "activated sludge process", "BN bypass nitrification", "bypass nitrification", "IAP intermittent aeration profiles", "intermittent aeration profiles", "IPH integrated penalty homotopy", "integrated penalty homotopy", "(O)IAP (optimal) intermittent aeration profiles", "(optimal) intermittent aeration profiles", "PH penalty homotopy", "penalty homotopy", "RL relaxed solution", "relaxed solution", "SBR sequencing batch reactors", "sequencing batch reactors", "WWT wastewater treatment", "wastewater treatment", "COD chemical Oxygen demand", "chemical Oxygen demand", "Mixed integer optimal control", "Wastewater treatment", "Sequencing batch reactor", "Intermittent aeration", "Activated sludge process"]},
    {"article name": "Non-causal data-driven monitoring of the process correlation structure: A comparison study with new methods",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.09.003",
     "publication date": "12-2014",
     "abstract": "Current approaches for monitoring the process correlation structure lag significantly behind the effectiveness already achieved on the detection of changes in the mean levels of process variables. We demonstrate that this is true, even for well-known methodologies such as MSPC-PCA and related approaches. On the other hand, data-driven process monitoring approaches are typically non-causal and based on the marginal covariance between process variables. We also show that such global measure of association is unable, by design, to effectively discern changes in the local correlation structure of the system and propose, for the first time, the explicit use of partial correlations in process monitoring. As a second contribution, we introduce the use of sensitivity enhancing data transformations (SET) with the ability to maximize the detection ability of all monitoring procedures based on (partial or marginal) correlation, and show how they can be constructed. Results confirm the added-value of the proposed monitoring scheme.",
     "keywords": ["Process monitoring of the correlation structure", "Multivariate dynamic processes", "Sensitivity enhancing data transformations", "Partial correlation", "Marginal correlation"]},
    {"article name": "Model predictive temperature tracking in crystal growth processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.09.005",
     "publication date": "12-2014",
     "abstract": "The temperature gradients and distribution evolution within the crystal domain in Czochralski crystal growth process have important role in produced crystal's quality. Precise and tight regulation of temperature distribution and gradients is the most promising approach to ensure the crystal quality. In this work, the coupled crystal pulling dynamics and heat transfer models in Czochralski crystal growth with a time-varying boundary is considered. The moving boundary finite element method is used to obtain the optimal reference temperature trajectory associated with the reference crystal shape taking into account constraints on input and the temperature gradients. The obtained reference trajectory is used to implement a model predictive control strategy to track the reference temperature despite uncertainties in crystal domain geometry evolution and disturbances. Finally, the proposed method is implemented on a high fidelity finite element process model with non-planar interface and results are presented to validate the success of the proposed methodology.",
     "keywords": ["Parabolic partial differential equation", "Moving boundary domain", "Czochralski crystal growth", "Model predictive control", "Reference trajectory tracking"]},
    {"article name": "Moving boundary models for the growth of crystalline deposits from undetected leakages of industrial process liquors",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.08.011",
     "publication date": "12-2014",
     "abstract": "In this study, a computational model which simulates the growth of crystalline deposits from dripping salt solution is developed and validated. This problem is of interest to the nuclear industry where the morphology of deposited material impacts on its associated criticality risk. An existing model for simulating geological-stalagmite formations is adapted to the case of dripping salt-solutions which form thin films of fluid that precipitate out over time, forming accumulations. The implementation of a CFD Volume-of-Fluid multiphase model is developed such that the fluid-flow is coupled to the crystallisation kinetics and a moving-boundary model is used for describing the size and shape of growing crystalline deposits. The fluid-flow and forming accumulation are fully coupled, with the model able to account for solute diffusion and solvent evaporation. Results are in good agreement with experimental data for surrogate salt-solutions. Numerical results are presented to assess the sensitivity to process and environmental parameters.",
     "keywords": ["NNL National Nuclear Laboratory", "National Nuclear Laboratory", "CFD computational fluid dynamics", "computational fluid dynamics", "PUREX plutonium uranium redox extraction", "plutonium uranium redox extraction", "VOF volume of fluid", "volume of fluid", "UDF user defined function", "user defined function", "Crystallisation", "Mass transfer", "Moving boundary problem", "Nuclear safety", "Computational fluid dynamics"]},
    {"article name": "Game-theoretic modeling and optimization of multi-echelon supply chain design and operation under Stackelberg game and market equilibrium",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.08.010",
     "publication date": "12-2014",
     "abstract": "We propose a bilevel mixed-integer nonlinear programming (MINLP) model for the optimal design and planning of non-cooperative supply chains from the manufacturer's perspective. Interactions among the supply chain participants are captured through a single-leader\u2013multiple-follower Stackelberg game under the generalized Nash equilibrium assumption. Given a three-echelon superstructure, the lead manufacturer in the middle echelon first optimizes its design and operational decisions, including facility location, sizing, and technology selection, material input/output and price setting. The following suppliers and customers in the upstream and downstream then optimize their transactions with the manufacturer to maximize their individual profits. By replacing the lower level linear programs with their KKT conditions, we transform the bilevel MINLP into a single-level nonconvex MINLP, which is further globally optimized using an improved branch-and-refine algorithm. We also present two case studies, including a county-level biofuel supply chain in Illinois, to illustrate the application of the proposed modeling and solution methods.",
     "keywords": ["Supply chain optimization", "Game theory", "Stackelberg game", "Generalized Nash equilibrium", "Improved branch-and-refine algorithm", "Biofuel supply chain"]},
    {"article name": "Computer aided product design tool for sustainable product development",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.09.009",
     "publication date": "12-2014",
     "abstract": "A computer aided product design (CAPD) tool is proposed that finds mixtures matching target properties. Genetic algorithm crossover and mutation operators are completed with insertion or deletion operators adapted for side branches. A new substitution operator is devised for cyclic molecules. The mixture fitness is evaluated by a weighted sum of property performances. Molecules are represented by molecular graphs. They are split into molecular fragments which are built from polyatomic groups. Molecules or molecular fragments can be fixed, constrained or left free for building a new molecule. Building blocks are chemical functional groups or bio-sourced synthons. A specific coding of hydrogen-suppressed atoms is devised that can be used with various property estimation models where atom connectivity information is required. Illustration is provided through three case studies to find levulinic, glycerol and bio-based derivatives as substitute for chlorinated paraffin, methyl p-coumarate ester solvent and blanket wash solvent, respectively.",
     "keywords": ["Computer aided product design", "Genetic algorithm", "Molecular graph", "Bio-based molecule", "Sustainable product design"]},
    {"article name": "Integrated production scheduling and process control: A systematic review",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.09.002",
     "publication date": "12-2014",
     "abstract": "Production scheduling and process control have the common aim of identifying (economically) optimal operational decisions, and it is reasonable to expect that significant economic benefits can be derived from their integration. Yet, the scheduling and control fields have evolved quite independently of each other, and efforts aimed at integrating these two decision-making activities are quite recent. In this paper, we review progress made thus far in this direction. We identify key elements of control and scheduling, and carry out a systematic investigation of their use as building blocks for the formulation and solution of the integrated scheduling/control problem. On the basis of our review, we define several necessary directions for future development as well as a complement of promising applications.",
     "keywords": ["Production scheduling", "Process control", "Integrated scheduling and control"]},
    {"article name": "Annotated bibliography\u2014Use of optimization in LNG process design and operation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.09.010",
     "publication date": "12-2014",
     "abstract": "This paper provides a review of the literature on applications of optimization for design and operation of processes for liquefaction of natural gas and a few related subjects. The review takes the form of an annotated bibliography. A short summary of each of the 186 published works published up to and including 2013 is combined with a classification of the different contributions. While the main focus is design and optimization of processes for liquefaction of natural gas, a selection of publications considering optimization of other parts of the LNG value chain and optimization of other sub-ambient refrigeration processes is also included.",
     "keywords": ["LNG value chain", "Liquefaction", "Refrigeration", "Optimization", "Thermodynamics"]},
    {"article name": "Robustifying optimal experiment design for nonlinear, dynamic (bio)chemical systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.09.006",
     "publication date": "12-2014",
     "abstract": "Dynamic experiments that yield as much information as possible are highly valuable for estimating parameters in nonlinear dynamic processes. Techniques for model-based optimal experiment design enable to systematically design such experiments. However, these experiments depend on the current best estimate of the parameters, which are not necessarily the true values. Consequently, in real experiments (i) the information content can be lower than predicted and (ii) state constraints can be violated. This paper presents a novel, computationally tractable formulation that enables the robustification of optimally designed experiments with respect to (i) information content and (ii) constraint satisfaction. To this end, the objective function is the expected value of a scalar function of the Fisher information matrix, which is efficiently computed using the sigma point method. This approach already has a robustifying effect. The sigma point method also enables the efficient computation of constraints\u2019 variance\u2013covariance matrix, this can be exploited for further robustification.",
     "keywords": ["Optimal experiment design", "Robust optimal control", "Fisher information matrix", "Sigma point", "Parametric uncertainty"]},
    {"article name": "Heat exchanger network design of large-scale industrial site with layout inspired constraints",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.09.012",
     "publication date": "12-2014",
     "abstract": "This paper presents a systematic approach for the synthesis of the heat recovery network in total site using a Mixed Integer Linear Programming model. This model returns a near-to-optimal network configuration with minimum utility cost while allows to select geographically closest matches. The Heat Load Distribution is the subproblem of the network design and has been reported to be quite expensive to solve for large-scale problems. The computational complexity of HLD resides in the number of streams and the feasible networks. An additional challenge, raising particularly in industrial problems, has been the intermediate heat transfer network which aggravates the combinatorial complexity. The presented methodology deals with those difficulties by priority consideration based on the location of process units. It helps significantly reducing the computational time and also comes with a realistic network sketch with respect to the plant layout. Several examples are discussed along with a real industrial case study.",
     "keywords": ["Heat exchanger network (HEN)", "Heat load distribution (HLD)", "Mixed Integer Linear Programming (MILP)", "Large-scale industry", "Plant layout", "Piping cost"]},
    {"article name": "Fault propagation analysis of oscillations in control loops using data-driven causality and plant connectivity",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.09.017",
     "publication date": "12-2014",
     "abstract": "Oscillations in control loops are one of the most prevalent problems in industrial processes. Due to their adverse effect on the overall process performance, finding how oscillations propagate through the process units is of major importance. This paper presents a method for integrating process causality and topology which ultimately enables to determine the propagation path of oscillations in control loops. The integration is performed using a dedicated search algorithm which validates the quantitative results of the data-driven causality using the qualitative information on plant connectivity. The outcome is an enhanced causal model which reveals the propagation path. The analysis is demonstrated on a case study of an industrial paperboard machine with multiple oscillations in its drying section due to valve stiction.",
     "keywords": ["Causal analysis", "Fault propagation", "Control loops", "Paper machine", "Connectivity information", "Industrial application"]},
    {"article name": "Aromatization of propane: Techno-economic analysis by multiscale \u201ckinetics-to-process\u201d simulation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.10.001",
     "publication date": "12-2014",
     "abstract": "This paper addresses the techno-economic analysis of the propane aromatization process, by adopting a novel kinetics-to-process approach. The recent interest in this technological route derives from the development of new third generation biorefinery concepts, in which, algal oil is subjected to catalytic hydrodeoxygenation processes for the production of (Hydrotreated Renewable Jet) HRJ fuels. Beside biofuels, co-production of large amounts of propane is observed, which can be upgraded by a catalytic conversion to aromatics on zeolites. Kinetic studies of propane aromatization over H-ZSM-5 zeolite in a wide range of conversions are reported in the literature. Based on these results, a general kinetic model of propane aromatization has been developed. The revised kinetic scheme is then embedded in a process simulation, performed with the commercial code SimSci PRO/II by Schneider Electric. Basing on the process simulation and on available price assessments, a techno-economic analysis has been performed to show limits as well as potentialities of the proposed layout.",
     "keywords": ["Propane aromatization", "Multiscale simulation", "Kinetic modelling", "Process development", "Excel unit operation module", "Hydroprocessed renewable jet (HRJ) fuel"]},
    {"article name": "An adaptive moving grid method for solving convection dominated transport equations in chemical engineering",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.09.011",
     "publication date": "12-2014",
     "abstract": "Convection dominated processes in chemical engineering are frequently accompanied by steep propagating fronts. Numerical simulation of corresponding models with uniform fixed grids requires an excessive amount of grid points along the expected range of the front movement. In this contribution the implementation of an efficient adaptive grid method is presented and applied to two relevant spatially one-dimensional cases, the chlorination stage of the Deacon process and oxygen storage processes in a three-way catalyst. The algorithm exhibits a high accuracy with a much lower number of grid points and a therefore reduced computational effort as opposed to a fixed grid simulation. The present work demonstrates that the algorithm allows for a robust, simple, and fast implementation of the adaptive grid method in common simulation tools and, together with adequate supplementary material, aims to make the method readily accessible to the interested reader.",
     "keywords": ["Dynamic simulation", "Adaptive grid", "Deacon process", "Three-way catalyst", "MATLAB", "DIANA"]},
    {"article name": "CFD\u2013DEM simulation of the filtration performance for fibrous media based on the mimic structure",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.09.018",
     "publication date": "12-2014",
     "abstract": "In order to obtain the comprehensive filtration information, such as the deposition morphology of particles, evolutions of the instantaneous pressure drop and efficiency, 3-D mimic models of fibrous media had been established using a stochastic algorithm and computational fluid dynamics (CFD) - discrete element method (DEM) model was adopted to simulate the gas-solid flow characteristics in fibrous media exposed to particle loading in this work. Meanwhile, the simulation results had been validated by the macro-empirical model proposed in the previous literature. In addition, the influence of various parameters including porosity, face velocity and particle size on the filtration performance of fibrous media had also been investigated. It is concluded that simulation results of the CFD\u2013DEM model are good agreement with predictive values of the macro-empirical model. Moreover, the porosity of fibrous media, face velocity and particle size have an important effect on the filtration performance of the fibrous media.",
     "keywords": ["Fibrous media", "Mimic structure", "Filtration performance", "CFD\u2013DEM"]},
    {"article name": "Hierarchical scheduling and disturbance management in the process industry",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.09.020",
     "publication date": "12-2014",
     "abstract": "The integration of scheduling and control in the process industry is a topic that has been frequently discussed during the recent years, but many challenges remain in order to obtain integrated solutions that may be implemented at large-scale industrial sites. This paper introduces a general framework for production scheduling (PS) and detailed production scheduling (DPS) using a two-level hierarchical approach. The PS activity generates a monthly production schedule based on information on orders and forecasts, and the DPS activity handles disturbances in production on an hourly basis. The focus is on disturbances in the supply of utilities, which often cause great losses at process industrial sites. The research has been conducted in close collaboration with Perstorp, a world-leading company within several sectors of the specialty chemicals market. A specification list provided by Perstorp has been used as a starting point for formulating the PS and DPS activities as optimization problems. An example that is inspired by a real industrial site is presented to show how the PS and DPS may operate and how the integration of these two functions behaves.",
     "keywords": ["Production scheduling", "Hierarchical control", "Plant-wide disturbances", "Utilities", "Receding horizon control", "Process industry"]},
    {"article name": "Statistical process control based on Multivariate Image Analysis: A new proposal for monitoring and defect detection",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.09.014",
     "publication date": "12-2014",
     "abstract": "The monitoring, fault detection and visualization of defects are a strategic issue for product quality. This paper presents a novel methodology based on the integration of textural Multivariate Image Analysis (MIA) and multivariate statistical process control (MSPC) for process monitoring. The proposed approach combines MIA and p-control charts, as well as T2 and RSS images for defect location and visualization. Simulated images of steel plates are used to illustrate the monitoring performance of it. Both approaches are also applied on real clover images.",
     "keywords": ["Multivariate Image Analysis (MIA)", "ARL", "Control charts", "RSS image", "T2 image", "Wavelets"]},
    {"article name": "A rat\u2013human scale-up procedure for the endocrine system",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.09.015",
     "publication date": "12-2014",
     "abstract": "The main contribution of this work is to present a detailed scale-up procedure between human and rats models to more accurately predict what would happen in human beings, based on the experimental results obtained from rats. This procedure begins using the human model, given by Sorensen (1985). The proposed scale-up technique required to establish some assumptions, to do an intensive search in the literature about organs volumes and flow rates of body rats and a dedicated experimental work in the laboratory with these animals. Even though it is mainly focused on studying the endocrine system behavior to obtain a proper in in silico healthy rat it can be extended to study another body regions. Several simulation results with the obtained rat model are included and confronted with experimental data of ten healthy rats. The analogy between human and rat dynamic behavior after equivalent meal intakes are also discussed.",
     "keywords": ["Endocrine system model", "Scale-up", "Rats", "Humans", "Experiments"]},
    {"article name": "GPU based simulation of reactive mixtures with detailed chemistry in combination with tabulation and an analytical Jacobian",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.09.016",
     "publication date": "12-2014",
     "abstract": "Incorporating detailed chemistry in solvers still remains a daunting and intractable task due to the prohibitive computational cost. However the combination of advanced mathematical solution techniques such as tabulation and calculating the analytical Jacobian in combination with efficient computational techniques that use the graphical processing unit (GPU) to do calculations can drastically speedup the simulation. These techniques are not mutually exclusive as is demonstrated for an ordinary differential equation (ODE) problem describing the classical adiabatic, constant-volume ignition of an equimolar methane/air mixture. Acceleration with up to a factor 120 can be obtained with the new algorithm compared to the algorithm employed in the reference solver. Maximum speedup is obtained in the case where the analytical Jacobian and the rates are calculated using the GPU when using intrinsic compiler functions to calculate transcendental functions which are used to calculate thermodynamic and kinetic coefficients. This is because the GPU can calculate transcendental functions significantly more efficient than a CPU.",
     "keywords": ["Graphical processing unit", "Analytical Jacobian", "Ordinary differential equations", "Chemical kinetics", "Tabulation", "Efficiency"]},
    {"article name": "Global superstructure optimisation of red blood cell production in a parallelised hollow fibre bioreactor",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.10.004",
     "publication date": "12-2014",
     "abstract": "Recent work developed a novel, biomimetic, cost-effective three-dimensional hollow fibre bioreactor for growing healthy red blood cells ex vivo (Panoskaltsis et al., 2012). This bioreactor recapitulates architectural and functional properties of erythrocyte formation and thereby reduces the need for expensive growth factors by more than an order of magnitude. Individual experiments to empirically improve the bioreactor are intensive, so we propose global superstructure optimisation for bioreactor design. Our approach integrates topological design choices with operating conditions. Design choices include: number of parallelised bioreactors; number and type of hollow fibres; size and aspect ratio. Operating conditions are: feed concentrations; flowrate through the reactor. This manuscript quantitatively demonstrates, for the first time, the potential for ex vivo red blood cell production to compete openly against the transfusion market for rare blood. We discuss the potential of superstructure design not only on this individual bioreactor but also more generally on bioprocess optimisation.",
     "keywords": ["Red blood cell production", "Bioreactor design", "Bioreactor operation", "Bioprocess optimisation", "Global superstructure optimisation"]},
    {"article name": "Economic optimal control applied to a solar seawater desalination plant",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.10.005",
     "publication date": "12-2014",
     "abstract": "This paper discusses the formulation of an optimal control strategy taking into account economic objectives in the fresh water production process through a solar seawater desalination plant. It contributes both a linearised model of the solar-field dynamics and a simplified model of the produced distillate as a function of the outlet solar field water temperature. Then such linear models are used to design an economic receding horizon optimal controller. In particular, it comprises incomes related to the production of fresh water and the costs dealing with the electricity. Several simulations validate the proposed models and show the performance of the proposed economic optimal control strategy. In both cases, actual disturbances from physical experiments have been included in the simulations. Notice that the AQUASOL facility available at the Plataforma Solar de Almer\u00eda (Spain) has been considered in this work as testbed.",
     "keywords": ["Modelling", "Multi-effect distillation", "Receding horizon optimal control"]},
    {"article name": "An interior-point method for efficient solution of block-structured NLP problems using an implicit Schur-complement decomposition",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.09.013",
     "publication date": "12-2014",
     "abstract": "In this work, we address optimization of large-scale, nonlinear, block-structured problems with a significant number of coupling variables. Solving these problems using interior-point methods requires the solution of a linear system that has a block-angular structure at each iteration. Parallel solution is possible using a Schur-complement decomposition. In an explicit Schur-complement decomposition, the computational cost of forming and factorizing the Schur-complement is prohibitive for problems with many coupling variables. In this paper, we show that this bottleneck can be overcome by solving the Schur-complement equations implicitly, using a quasi-Newton preconditioned conjugate gradient method. This new algorithm avoids explicit formation and factorization of the Schur-complement. The computational efficiency of this algorithm is compared with the serial full-space approach, and the serial and parallel explicit Schur-complement approach. These results show that the PCG Schur-complement approach dramatically reduces the computational cost for problems with many coupling variables.",
     "keywords": ["Parallel computing", "Interior-point", "Nonlinear programming", "Schur-complement decomposition"]},
    {"article name": "Rapid and robust resolution of Underwood equations using convex transformations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.10.006",
     "publication date": "12-2014",
     "abstract": "In this work, a new method is proposed for solving Underwood's equations. Newton methods cannot be used without interval control, and may require many iterations or experience severe convergence problems if the roots are near poles and the initial guess is poor. It is shown that removing only one adjacent asymptote leads to convex functions, while removing both asymptotes leads to quasi convex functions which are close to linearity on wide intervals. Using a change of variable, a pair of convex functions is defined; at each point within the search interval one of the two functions is guaranteed to satisfy the monotonic convergence condition for Newton methods. The search interval is restricted to narrow solution windows (simple and costless) and a simple high quality initial guess can be obtained using their bounds. Two solution algorithms are presented: in the first one, Newton (including higher-order) methods can be safely applied without any interval control using the appropriate convex function; in the second one, Newton iterations are applied to a quasi-convex function, and convex functions are used only if an iterate goes out of its bounds. The algorithms are tested on several numerical examples, some of them recognized as very difficult in the literature. The proposed solution methods are simple, robust, very rapid (quadratic or super-quadratic convergence) and easy to implement. In most cases, convergence is obtained in 2\u20133 Newton iterations, even for roots extremely close to a pole.",
     "keywords": ["Underwood equations", "Roots", "Convergence", "Convexity", "Change of variables", "Newton method"]},
    {"article name": "Continuous baseload renewable power using chemical refrigeration cycles",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.10.002",
     "publication date": "12-2014",
     "abstract": "We propose a cycle to store and supply GWh of electricity from intermittent renewable energy by transforming carbon atoms between carbon dioxide and methane. Among hydrocarbons, methane is associated with the highest energy content per mole of carbon. Therefore, for a given electricity supply, it requires the least amount of carbon circulation. To minimize storage volumes, both carbon dioxide and methane are stored as liquids. When renewable energy is available, methane is synthesized from vaporizing carbon dioxide, and subsequently purified, liquefied and stored. When renewable energy is unavailable, liquid methane is vaporized and oxidized for electricity supply. The produced carbon dioxide is purified and liquefied for storage and subsequent usage. In each case, the electricity required for purification and liquefaction is minimized by integrating the vaporization and liquefaction steps of carbon dioxide and methane. The cycle can achieve \u223c55% storage efficiency with much reduced storage volumes compared to other options.",
     "keywords": ["Refrigeration", "Electric power", "Carbon dioxide", "Fuel cells", "Liquid methane", "Electrolysis"]},
    {"article name": "Integrated design and control of chemical processes \u2013 Part I: Revision and classification",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.05.010",
     "publication date": "12-2014",
     "abstract": "This work presents a comprehensive classification of the different methods and procedures for integrated synthesis, design and control of chemical processes, based on a wide revision of recent literature. This classification fundamentally differentiates between \u201cprojecting methods\u201d, where controllability is monitored during the process design to predict the trade-offs between design and control, and the \u201cintegrated-optimization methods\u201d which solve the process design and the control-systems design at once within an optimization framework. The latter are revised categorizing them according to the methods to evaluate controllability and other related properties, the scope of the design problem, the treatment of uncertainties and perturbations, and finally, the type the optimization problem formulation and the methods for its resolution.",
     "keywords": ["Process synthesis", "Process control", "Integrated design", "Controllability", "Process optimization"]},
    {"article name": "Integrated design and control of chemical processes \u2013 Part II: An illustrative example",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.09.019",
     "publication date": "12-2014",
     "abstract": "In this paper, several methodologies of integrated design are proposed and applied to the design of wastewater treatment plants and their control system, focusing on the activated sludge process, within a novel multiobjective framework. The scope of the problem considers both fixed plant layout and plant structure selection by defining a simple superstructure. The control strategy chosen is a linear Model Predictive Controller (MPC) with terminal penalty. The evaluation of the controllability has been performed using norm based indexes, and the robustness conditions for different uncertainty sources have been considered, in the frequency and time domains. The optimization strategies used are based on the integration of stochastic and deterministic methods, as well as genetic algorithms. The presented methodologies and their application to wastewater treatment plants can be considered as an illustrative example in the universe of integrated design techniques presented in the Part I article of this series.",
     "keywords": ["Integrated design", "Process synthesis", "Controllability", "Activated sludge process", "Model predictive control"]},
    {"article name": "Municipal solid waste to liquid transportation fuels \u2013 Part I: Mathematical modeling of a municipal solid waste gasifier",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.03.008",
     "publication date": "12-2014",
     "abstract": "This paper presents a generic gasifier model towards the production of liquid fuels using municipal solid waste (MSW) as a feedstock. The MSW gasification has been divided into three zones: pyrolysis, oxidation, and reduction. The pyrolysis zone has been mathematically modeled with an optimization based monomer model. Then, the pyrolysis, oxidation, and reduction zones are defined with different chemical reactions and equations in which some extents of these reactions are not known a priori. Using a nonlinear parameter estimation approach, the unknown gasification parameters are obtained to match the experimental gasification results in the best possible way. The results suggest that a generic MSW gasifier mathematical model can be obtained in which the average error is 8.75%. The mathematical model of the MSW gasifier is of major importance since it can be a part of a process superstructure towards the production of liquid transportation fuels.",
     "keywords": ["Municipal solid waste", "Gasification", "Parameter estimation", "Refuse derived fuel", "Mathematical modeling", "Optimization"]},
    {"article name": "Multi-objectives, multi-period optimization of district energy systems: II\u2014Daily thermal storage",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.10.016",
     "publication date": "12-2014",
     "abstract": "District heating is an efficient way of providing heat to urban areas. The use of storage tanks integrated with district heating network would permit to increase the annual utilization of base load equipment, avoiding over estimation of the size of backup equipment, and balancing the energy demand fluctuation during day and night.In the present work a multi-objective optimization model for sizing and operation optimization of district heating systems with heat storage tanks is presented. The model includes process design and energy integration techniques for optimizing the temperature intervals, the volume and the operation strategy of thermal storage tanks.The proposed model is demonstrated by means of two test cases. Results show that the efficiency, environmental impacts and total costs of an urban system can be improved after integrating the thermal storage by 4.7%, 5% and 2% respectively.",
     "keywords": ["Daily thermal storage", "District energy systems", "Mixed integer linear programming", "Evolutionary algorithm", "Multi-objective optimization", "CO2 mitigation"]},
    {"article name": "A chemical engineer's perspective on health and disease",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.09.007",
     "publication date": "12-2014",
     "abstract": "Chemical process systems engineering considers complex supply chains which are coupled networks of dynamically interacting systems. The quest to optimize the supply chain while meeting robustness and flexibility constraints in the face of ever changing environments necessitated the development of theoretical and computational tools for the analysis, synthesis and design of such complex engineered architectures. However, it was realized early on that optimality is a complex characteristic required to achieve proper balance between multiple, often competing, and objectives. As we begin to unravel life's intricate complexities, we realize that that living systems share similar structural and dynamic characteristics; hence much can be learned about biological complexity from engineered systems. In this article, we draw analogies between concepts in process systems engineering and conceptual models of health and disease; establish connections between these concepts and physiologic modelling; and describe how these mirror onto the physiological counterparts of engineered systems.",
     "keywords": ["Health", "Disease", "Homeostasis", "Systems engineering"]},
    {"article name": "Computer simulations and in vivo convection-enhanced delivery of fluorescent nanoparticles demonstrate variable distribution geometry",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.09.008",
     "publication date": "12-2014",
     "abstract": "Convection-enhanced delivery (CED) has emerged as a promising technique for bypassing the blood\u2013brain barrier to deliver therapeutic agents. However, animal studies and clinical trials that utilize the technique suggest that it may require further optimization before it can be safely used in humans. In particular, while volume of distribution in the target tissue can be controlled, the geometrical spread into a desired target region is highly variable from experiment to experiment. In the present paper we have sought to characterize the non-uniform distribution geometry using fluorescent nanoparticles in both a rat model and computer simulations.Using diffusion tensor imaging MRI data of the rat brain, we performed computer simulations of a 0.5\u00a0\u03bcL/min CED infusion. A step design catheter targeting the striatum was simulated to infuse 20\u00a0\u03bcL of infusate. Using the same infusion parameters, we then performed in vivo CED experiments where we infused fluorescently labeled polyethylene glycol-polylactide-polycaprolactone nanoparticles (FPNPs) into the rat striatum. Fluorescence microscopy was used to examine the distribution geometry histologically.The computer simulations demonstrated large variations in distribution patterns when catheter placement was shifted by only 1\u00a0mm. Animal infusions also exhibited highly irregular and variable distribution geometries despite the use of relatively small flow rates.Computer simulations and repeated in vivo infusions demonstrate the difficulty of achieving desired drug distribution in target tissue. We have proposed a calculation for sphericity which, along with the ubiquitous volume of distribution measure, may prove helpful in describing distribution geometry. Taken together, our results suggest that CED's limitations must be considered and further optimization may be required before the technique sees widespread use in humans.",
     "keywords": ["Convection-enhanced delivery", "Nanoparticles", "Drug delivery", "Blood\u2013brain barrier", "Brain cancer"]},
    {"article name": "A computer-aided framework for development, identification and management of physiologically-based pharmacokinetic models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.07.016",
     "publication date": "12-2014",
     "abstract": "The objective of this work is the development of a generic computer-aided modelling framework to support the development of physiologically-based pharmacokinetic models thereby increasing the efficiency and quality of the modelling process. In particular, the framework systematizes the modelling process by identifying the workflow involved and providing the required methods and tools for model documentation, construction, analysis, identification and discrimination. The application and benefits of the developed framework are demonstrated by a case study related to the whole-body physiologically-based pharmacokinetic modelling of the distribution of the drug cyclosporin A in rats and humans. Four alternative candidate models for rats are derived and discriminated based on experimental data. The model candidate that is best represented by the experimental data is scaled-up to a human being applying physiologically-based scaling laws and identifying model parameters that can be re-fitted by the limited experimental data accessible for humans using sensitivity and identifiability analysis techniques.",
     "keywords": ["Computer-aided modelling framework", "Pharmacokinetics", "Physiologically-based pharmacokinetic modelling", "Model identification", "Modelling software", "Sensitivity analysis"]},
    {"article name": "Model predictive control of anesthesia under uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.07.025",
     "publication date": "12-2014",
     "abstract": "This paper addresses inter- and intra-patient variability in the context of automated drug delivery during anesthesia. A combined strategy of model predictive control (MPC) and least squares online parameter estimation for the control of the hypnotic depth, measured by the Bispectral Index (BIS), under uncertainty is presented, where the uncertainty originates from patient variability. The parameter with the highest sensitivity, C50 the effect site concentration at 50% drug effect, is estimated online. The performance of the closed loop control design is shown for induction and maintenance of volatile anesthesia. In the maintenance phase, the control strategy is evaluated for predefined disturbances that are commonly occurring during surgery. The presented approach shows an improved performance compared to the nominal MPC controller under uncertainty.",
     "keywords": ["Anesthesia", "Patient variability", "Uncertainty", "Model predictive control", "Multi-parametric model predictive control", "Online parameter estimation"]},
    {"article name": "Comparison of different methods for predicting customized drug dosage in superovulation stage of in-vitro fertilization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.07.021",
     "publication date": "12-2014",
     "abstract": "In-vitro fertilization (IVF) is one of the highly pursued assisted reproductive technologies (ARTs) worldwide. Superovulation is the most crucial stage in IVF, since it involves injection of hormones externally to stimulate development and maturation of multiple oocytes. A model for the follicular dynamics as a function of injected hormones and patient characteristics has been developed and validated in our previous studies. Using the same model as a predictive tool along with the application of optimal control principles; the optimal dose and frequency of medication customized for each patient is predicted. The objective of successful superovulation is to obtain maximum number of mature oocytes/follicles within a particular size range, which is translated into mathematical form by using concepts from normal distribution. The problem is solved by different optimal control methods like the maximum principle and discretized non-linear programming. The results from both the approaches are compared and their advantages are discussed.",
     "keywords": ["Infertility", "Ovulation", "Normal distribution", "Maximum principle", "Non-linear programming"]},
    {"article name": "Quantitative analysis of robustness of dynamic response and signal transfer in insulin mediated PI3K/AKT pathway",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.07.018",
     "publication date": "12-2014",
     "abstract": "Robustness is a critical feature of signaling pathways ensuring signal propagation with high fidelity in the event of perturbations. Here we present a detailed quantitative analysis of robustness in insulin mediated PI3K/AKT pathway, a critical signaling pathway maintaining self-renewal in human embryonic stem cells. Using global sensitivity analysis, we identified robustness promoting mechanisms that ensure (1) maintenance of a first order or overshoot dynamics of self-renewal molecule, p-AKT and (2) robust transfer of signals from oscillatory insulin stimulus to p-AKT in the presence of noise. Our results indicate that negative feedback controls the robustness to most perturbations. Faithful transfer of signal from the stimulating ligand to p-AKT occurs even in the presence of noise, albeit with signal attenuation and high frequency cut-off. Negative feedback contributes to signal attenuation, while positive regulators upstream of PIP3 contribute to signal amplification. These results establish precise mechanisms to modulate self-renewal molecules like p-AKT.",
     "keywords": ["Robust dynamics", "Signal transfer efficiency", "PI3K/AKT pathway", "Self-renewal", "Global sensitivity analysis"]},
    {"article name": "A constrained wavelet smoother for pathway identification tasks in systems biology",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.07.019",
     "publication date": "12-2014",
     "abstract": "Metabolic time series data are being generated with increasing frequency, because they contain enormous information about the pathway from which the metabolites derive. This information is not directly evident, though, and must be extracted with advanced computational means. One typical step of this extraction is the estimation of slopes of the time courses from the data. Since the data are almost always noisy, and the noise is typically amplified in the slopes, this step can become a critical bottleneck. Several smoothers have been proposed in the literature for this purpose, but they all face the potential problem that smoothed time series data no longer correspond to a system that conserves mass throughout the measurement time period. To counteract this issue, we are proposing here a smoother that is based on wavelets and, through an iterative process, converges to a mass-conserving, smooth representation of the metabolic data. The degree of smoothness is user defined. We demonstrate the method with some didactic examples and with the analysis of actual measurements characterizing the glycolytic pathway in the dairy bacterium Lactococcus lactis. MATLAB code for the constrained smoother is available as a Supplement.",
     "keywords": ["Biochemical systems theory (BST)", "Dynamic flux estimation (DFE)", "Metabolic pathway analysis", "Parameter estimation", "Smoothing", "Wavelets"]},
    {"article name": "Identifying polymer structures for oral drug delivery \u2013 A molecular design approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.07.015",
     "publication date": "12-2014",
     "abstract": "A computer aided molecular design (CAMD) approach is proposed for generating molecular structures of polymer candidates which have the potential to be effective polymer carriers in drug delivery. A group contribution method has been used to predict properties such as glass transition temperature (Tg), expansion coefficient (\u03b1f) and water absorption (W). A mixed integer nonlinear optimization technique is used to successfully generate some novel polymer structures not available in the literature. The generated polymers are ranked according to a desirability curve. A solubility parameter analysis is done to further analyze and filter the generated polymers. The proposed new polymers need to be synthesized and analyzed to make them practical for use in oral drug delivery.",
     "keywords": ["Oral drug delivery", "Molecular design", "Structure\u2013property models", "Novel polymers", "Optimization", "Gastrointestinal track"]},
    {"article name": "Hydration free energies calculated using the AMBER ff03 charge model for natural and unnatural amino acids and multiple water models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.07.017",
     "publication date": "12-2014",
     "abstract": "In this work, we assess calculated hydration free energies of natural and unnatural amino acids compared to experimental hydration free energies of corresponding side-chain analogs using the ff03 charge model. Fine-grid, explicit water thermodynamic integration calculations using two widely used explicit water models (TIP3P and TIP4P-Ew) were performed on 19 natural amino acids and compared with experimental hydration free energies of corresponding side-chain analogs to establish expected accuracy levels for this charge model. Next, parameters previously derived for 17 unnatural amino acids and several new parameters optimized in this work were assessed using the same methodology. We found that the ff03 charge model is correlated with experimental hydration free energies but underestimates the solubilities of several polar natural and unnatural amino acids. Comparisons are presented with other forcefields and water models recently presented in the literature for both natural and modified amino acids.",
     "keywords": ["Unnatural amino acids", "Water", "Thermodynamic integration", "AMBER ff03", "Hydration free energies", "Solvation"]},
    {"article name": "Laying the foundations: An advisor's perspective",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.05.016",
     "publication date": "11-2014",
     "abstract": "Manfred Morari's years of graduate studies at the University of Minnesota (1974\u20131977), coincided with an intense period of fertile reexamination of control theory and its relevance to the industrial practice, that led to new questions and challenges, which defined Manfred's PhD research and determined the directions in which he would reshape the academic landscape and industrial practice. These questions and their resolution are today part of the mainstream: control structures for chemical plants; design of processes with desired operability, resiliency, and flexibility; design of robust controllers; inferential control. Manfred's impact is, by now, part of an exceptional personal record: groundbreaking papers and books, patented industrial devices and systems, and an influential family of academic off-springs. Here, the author will describe the control theory and practice milieu during Manfred's formative years, and underline certain factors (which unfortunately have weakened considerably in the last 30 years) that defined his successful professional trajectory.",
     "keywords": ["Process control", "Control structures for chemical plants", "Robust control", "Model-predictive control", "Process operability"]},
    {"article name": "An MPC-based control structure selection approach for simultaneous process and control design",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.08.014",
     "publication date": "11-2014",
     "abstract": "An optimization framework that addresses the simultaneous process and control design of chemical systems including the selection of the control structure is presented. Different control structures composed of centralized and fully decentralized predictive controllers are considered in the analysis. The system's dynamic performance is quantified using a variability cost function that assigns a cost to the worst-case closed-loop variability, which is calculated using analytical bounds derived from tests used for robust control design. The selection of the controller structure is based on a communication cost term that penalizes pairings between the manipulated and the controlled variables based on the tuning parameters of the MPC controller and the process gains. Both NLP and MINLP formulations are proposed. The NLP formulation is shown to be faster and converges to a similar solution to that obtained with the MINLP formulation. The proposed methods were applied to a wastewater treatment industrial plant.",
     "keywords": ["Simultaneous design and control", "Control structure selection", "Model predictive control"]},
    {"article name": "Evolution of concepts and models for quantifying resiliency and flexibility of chemical processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.12.013",
     "publication date": "11-2014",
     "abstract": "This paper provides a historical perspective and an overview of the pioneering work that Manfred Morari developed in the area of resiliency for chemical processes. Motivated by unique counter-intuitive examples, we present a review of the early mathematical formulations and solution methods developed by Grossmann and co-workers for quantifying Static Resiliency (Flexibility). We also give a brief overview of some of the seminal ideas by Manfred Morari and co-workers in the area of Dynamic Resiliency. Finally, we provide a review of some of the recent developments that have taken place since that early work.",
     "keywords": ["Flexibility", "Resiliency", "Design under uncertainty", "Dynamic resiliency"]},
    {"article name": "Optimal operation of heat exchanger networks with stream split: Only temperature measurements are required",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.03.020",
     "publication date": "11-2014",
     "abstract": "For heat exchanger networks with stream splits, we present a simple way of controlling the split ratio. We introduce the \u201cJ\u00e4schke Temperature\u201d, which for a branch with one exchanger is defined as TJ\u00a0=\u00a0(T\u00a0\u2212\u00a0T0)2/(Th\u00a0\u2212\u00a0T0), where T0 and T are the inlet and outlet temperatures of the split stream (usually cold), and Th is the inlet temperature of the other stream (usually hot). Assuming the heat transfer driving force is given by the arithmetic mean temperature difference, the J\u00e4schke Temperatures of all branches must be equal to achieve maximum heat transfer. The optimal controlled variable is the difference between the J\u00e4schke Temperatures of each branch, which should be controlled to zero. Heat capacity or heat transfer parameters are not needed, and no optimization is required to find the optimal setpoints for the controlled variables. Most importantly, our approach gives near-optimal operation for systems with logarithmic mean temperature difference as driving force.",
     "keywords": ["Heat exchanger networks", "Parallel systems", "Self-optimizing control", "Optimal operation"]},
    {"article name": "Optimal scenario reduction framework based on distance of uncertainty distribution and output performance: I. Single reduction via mixed integer linear optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.03.019",
     "publication date": "11-2014",
     "abstract": "Realistic decision making involves consideration of uncertainty in various parameters. While large number of scenarios brings significant challenge to computations, the scenario reduction aims at selecting a small number of representative scenarios that can capture the wide range of possible scenarios. A novel scenario reduction algorithm is proposed in this paper to incorporate the consideration of both input data and output performance of decision making. The proposed optimal scenario reduction algorithm, OSCAR, is formulated as a mixed integer linear optimization problem. It minimizes not only the probabilistic distance between the original and reduced input scenario distribution, but also minimizes the differences between the best, worst and expected performances of the output measure of the original and the reduced scenario distributions. The proposed method leads to reduced distribution not only closer to the original distribution in terms of the transportation distance, but also captures the performance of the output.",
     "keywords": ["Optimal scenario reduction", "Probabilistic distance", "Mixed integer linear optimization"]},
    {"article name": "Necessary and sufficient conditions for robust reliable control in the presence of model uncertainties and system component failures",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.07.023",
     "publication date": "11-2014",
     "abstract": "This paper provides necessary and sufficient conditions for several forms of controlled system reliability. For comparison purposes, past results on the reliability analysis of controlled systems are reviewed and several of the past results are shown to be either conservative or have exponential complexity. For systems with real and complex uncertainties, conditions for robust reliable stability and performance are formulated in terms of the structured singular values of certain transfer functions. The conditions are necessary and sufficient for the controller to stabilize the closed-loop system while retaining a desirable level of the closed-loop performance in the presence of actuator/sensor faults or failures, as well as plant-model mismatches. The resulting conditions based on the structured singular value are applied to the decentralized control for a high-purity distillation column and singular value decomposition-based optimal control for a parallel reactor with combined precooling. Tight polynomial-time bounds for the conditions can be evaluated by using available off-the-shelf software.",
     "keywords": ["Reliability analysis", "Robustness analysis", "Decentralized control", "Structured singular value", "Reliable control", "Decentralized integral control"]},
    {"article name": "A control-relevant approach to demand modeling for supply chain management",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.05.020",
     "publication date": "11-2014",
     "abstract": "The development of control-oriented decision policies for inventory management in supply chains has drawn considerable interest in recent years. Modeling demand to supply forecasts is an important component of an effective solution to this problem. Drawing from the problem of control-relevant parameter estimation, this paper presents an approach for demand modeling in a production-inventory system that relies on a specialized weight to tailor the emphasis of the fit to the intended purpose of the model, which is to provide forecasts to inventory management policies based on internal model control or model predictive control. A systematic approach to generate this weight function (implemented using data prefilters in the time domain) is presented and the benefits demonstrated on a series of representative case studies. The multi-objective formulation developed in this work allows the user to emphasize minimizing inventory variance, minimizing starts variance, or their combination, as dictated by operational and enterprise goals.",
     "keywords": ["Supply chain management", "Demand modeling", "Control-relevant model reduction", "Inventory control", "Internal model control", "Model predictive control"]},
    {"article name": "Predictive control with adaptive model maintenance: Application to power plants",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.03.011",
     "publication date": "11-2014",
     "abstract": "This paper describes a method for model predictive control (MPC) with model maintenance. A supervisor maintains the control model in real time by providing an estimate of disturbances and noise. A switch is triggered when the predictions of the control model deviate from disturbance estimate by more than a pre-determined amount. The predictive control algorithm described in the paper uses the innovation representation of a Markov\u2013Laguerre model. A Monte Carlo study and an experiment show that good models and stable control are obtained. A simulation study based on models of a boiler\u2013turbine unit shows that the algorithm can adapt to time-varying data. The performance is assessed using the area regulation (AR) test criterion currently adopted by PJM Interconnection LLC. The proposed adaptive MPC gives an AR test score of more than 90 with pressure fluctuations less than 3% even when the coal quality changes.",
     "keywords": ["Identification", "Adaptive control", "Model predictive control", "Laguerre model", "Supervision", "Robustness"]},
    {"article name": "On the design of exact penalty functions for MPC using mixed integer programming",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.07.001",
     "publication date": "11-2014",
     "abstract": "Soft constraints and penalty functions are commonly used in MPC to ensure that the optimization problem has a feasible solution, and thereby avoid MPC controller failure. On the other hand, soft constraints may allow for unnecessary violations of the original constraints, i.e., the constraints may be violated even if a valid solution that does not violate any constraints exists.The paper develops procedures for the minimizing (according to some norm) of the Lagrange multipliers associated with a given mp-QP problem, assumed to originate from an MPC problem formulation. To this end the LICQ condition is exploited in order to efficiently formulate the optimization problem, and thereby improve upon existing mixed integer formulations and enhance the tractability of the problem. The results are used to design penalty functions such that corresponding soft constraints are made exact, that is, the original (hard) constraints are violated only if there exists no solution where all constraints are satisfied.",
     "keywords": ["Soft constraints", "Exact penalty functions", "Lagrange multipliers", "Mixed integer programming", "LICQ"]},
    {"article name": "From robust model predictive control to stochastic optimal control and approximate dynamic programming: A perspective gained from a personal journey",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.10.014",
     "publication date": "11-2014",
     "abstract": "Developments in robust model predictive control are reviewed from a perspective gained through a personal involvement in the research area during the past two decades. Various min\u2013max MPC formulations are discussed in the setting of optimizing the \u201cworst-case\u201d performance in closed loop. One of the insights gained is that the conventional open-loop formulation of MPC is fundamentally flawed to address optimal control of systems with uncertain parameters, though it can be tailored to give conservative solutions with robust stability guarantees for special classes of problems. Dynamic programming (DP) may be the only general framework for obtaining closed-loop optimal control solutions for such systems. Due to the \u201ccurse of dimensionality (COD),\u201d however, exact solution of DP is seldom possible. Approximate dynamic programming (ADP), which attempts to overcome the COD, is discussed with potential extensions and future challenges.",
     "keywords": ["Robust model predictive control", "Min\u2013max control", "Approximate dynamic programming", "Multistage decision-making under uncertainty"]},
    {"article name": "Computational complexity and related topics of robustness margin calculation using \u03bc theory: A review of theoretical developments",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.09.018",
     "publication date": "11-2014",
     "abstract": "This paper provides a comprehensive overview of research related to computational complexity of structured singular value (a.k.a. \u03bc) problems. A survey of computational complexity results in \u03bc problems is followed by a concise introduction to computational complexity theory that is useful to characterize the inherent difficulty of solving an optimization. Results on the study for NP-hardness of \u03f5-approximation of \u03bc problems are discussed and conservatism of convex \u03bc upper-bounds including ones obtained from absolute stability theory is studied. NP-hardness of \u03bc computation and conservatism of convex upper-bounds open new research trends. In particular, we give an overview of polynomial-time model reduction methods and probabilistic randomized algorithms that have been active research topics since the mid-1990s.",
     "keywords": ["Mathematical systems theory", "Robust control", "Computational complexity"]},
    {"article name": "Nonlinear modeling, estimation and predictive control in APMonitor",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.04.013",
     "publication date": "11-2014",
     "abstract": "This paper describes nonlinear methods in model building, dynamic data reconciliation, and dynamic optimization that are inspired by researchers and motivated by industrial applications. A new formulation of the \u21131-norm objective with a dead-band for estimation and control is presented. The dead-band in the objective is desirable for noise rejection, minimizing unnecessary parameter adjustments and movement of manipulated variables. As a motivating example, a small and well-known nonlinear multivariable level control problem is detailed that has a number of common characteristics to larger controllers seen in practice. The methods are also demonstrated on larger problems to reveal algorithmic scaling with sparse methods. The implementation details reveal capabilities of employing nonlinear methods in dynamic applications with example code in both Matlab and Python programming languages.",
     "keywords": ["Advanced process control", "Differential algebraic equations", "Model predictive control", "Dynamic parameter estimation", "Data reconciliation", "Dynamic optimization"]},
    {"article name": "On-line state estimation of nonlinear dynamic systems with gross errors",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.11.018",
     "publication date": "11-2014",
     "abstract": "State estimation is a crucial part of the monitoring and/or control of all chemical processes. Among various approaches for this problem, moving horizon estimation (MHE) has the advantage of directly incorporating nonlinear dynamic models within a well-defined optimization problem. Moreover, advanced step moving horizon estimation (asMHE) substantially reduces the on-line computational expense associated with MHE. Previously, MHE and asMHE have both been shown to perform well when measurement noise follows some known Gaussian distribution. In this study we extend MHE and asMHE to consider measurements that are contaminated with large errors. Here standard least squares based estimators generate biased estimates even with relatively few gross error measurements. We therefore apply two robust M-estimators, Huber's fair function and Hampel's redescending estimator, in order to mitigate the bias of these gross errors on our state estimates. This approach is demonstrated on dynamic models of a CSTR and a distillation column. Based on this comparison we conclude that the asMHE formulation with the redescending estimator can be used to get fast and accurate state estimates, even in the presence of many gross measurement errors.",
     "keywords": ["State estimation", "Dynamic optimization", "Moving horizon estimation", "Gross error detection", "Nonlinear programming"]},
    {"article name": "Solving linear and quadratic programs with an analog circuit",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.01.011",
     "publication date": "11-2014",
     "abstract": "We present the design of an analog circuit which solves linear programming (LP) or quadratic programming (QP) problem. In particular, the steady-state circuit voltages are the components of the LP (QP) optimal solution. The paper shows how to construct the circuit and provides a proof of equivalence between the circuit and the LP (QP) problem. The proposed method is used to implement an LP-based Model Predictive Controller by using an analog circuit. Simulative and experimental results show the effectiveness of the proposed approach.",
     "keywords": ["Optimization", "MPC", "Linear programming", "Quadratic programming", "Analog computation", "Linear complementarity systems"]},
    {"article name": "Constrained dynamic programming of mixed-integer linear problems by multi-parametric programming",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.03.021",
     "publication date": "11-2014",
     "abstract": "This work addresses the topic of constrained dynamic programming for problems involving multi-stage mixed-integer linear formulations with a linear objective function. It is shown that such problems may be decomposed into a series of multi-parametric mixed-integer linear problems, of lower dimensionality, that are sequentially solved to obtain the globally optimal solution of the original problem. At each stage, the dynamic programming recursion is reformulated as a convex multi-parametric programming problem, therefore avoiding the need for global optimisation that usually arises in hard constrained problems. The proposed methodology is applied to a problem of mixed-integer linear nature that arises in the context of inventory scheduling. The example also highlights how the complexity of the original problem is reduced by using dynamic programming and multi-parametric programming.",
     "keywords": ["Dynamic programming", "Multi-parametric programming", "Mixed-integer linear programming"]},
    {"article name": "Design and in silico evaluation of an intraperitoneal\u2013subcutaneous (IP\u2013SC) artificial pancreas",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.02.024",
     "publication date": "11-2014",
     "abstract": "Prandial glucose regulation is a major challenge for the artificial pancreas using subcutaneous insulin (without a feedforward bolus) due to insulin's slow absorption-peak (50\u201360\u00a0min). Intraperitoneal insulin, with a fast absorption peak (20\u201325\u00a0min), has been suggested as an alternative to address these limitations. An artificial pancreas using intraperitoneal insulin was designed and evaluated on 100 in silico subjects and compared with two designs using subcutaneous insulin with and without a feedforward bolus, following the three meal (40\u201370\u00a0g-carbohydrates) evaluation protocol. The design using intraperitoneal insulin resulted in a significantly lower postprandial blood glucose peak (38\u00a0mg/dL) and longer time in the clinically accepted region (13%) compared to the design using subcutaneous insulin without a feedforward bolus and comparable results to a subcutaneous feedforward bolus design. This superior regulation with minimal user interaction may improve the quality of life for people with type 1 diabetes mellitus.",
     "keywords": ["Artificial pancreas", "Insulin pump", "Model predictive control", "Glucose sensor", "Zone model predictive control", "T1DM type 1 diabetes mellitus", "type 1 diabetes mellitus", "BG blood glucose", "blood glucose", "CSII continuous subcutaneous insulin infusion", "continuous subcutaneous insulin infusion", "AP artificial pancreas", "artificial pancreas", "IP intraperitoneal", "intraperitoneal", "IV intravenous", "intravenous", "SC subcutaneous", "subcutaneous", "CGM continuous glucose monitor", "continuous glucose monitor", "MPC model predictive control", "model predictive control", "PK pharmacokinetics", "pharmacokinetics", "PD pharmacodynamics", "pharmacodynamics", "CHO carbohydrate", "carbohydrate", "HBGI high blood glucose index", "high blood glucose index", "LBGI low blood glucose index", "low blood glucose index", "CVGA control variability grid analysis", "control variability grid analysis"]},
    {"article name": "Optimal design of sustainable water systems for cities involving future projections",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.05.026",
     "publication date": "10-2014",
     "abstract": "Water scarcity is one of the main concerns of several countries around the world. In this context, several approaches have been proposed for resource conservation and available water augmentation through specific actions such as process intensification and the use, reclamation, reuse, and recycle of alternative water sources. Nonetheless, there are no reported methodologies optimizing the multiannual planning of water usage, discharge, reclamation, storage and distribution in a macroscopic system considering natural and alternative water sources. In this paper, a multi-period mathematical programming model for the optimal planning of water storage and distribution in a macroscopic system is presented. The model addresses important factors such as population growth, change in the time value of money and change in the precipitation patterns. The proposed model is applied to the case of a Mexican city. The results show important advantages from the economic and sustainability points of view.",
     "keywords": ["Water management", "Water harvesting and reclamation", "Optimal water use", "Sustainable water use", "Optimal water storage and distribution in cities"]},
    {"article name": "Resource constrained project scheduling problem with setup times after preemptive processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.06.012",
     "publication date": "10-2014",
     "abstract": "In this paper, the preemptive resource constrained project scheduling problem with set up times is investigated. In this problem, a fixed setup time is required to restart when an process is preempted. The project contains activities inter-related by finish to start type precedence relations with a time lag of zero, which require a set of renewable resources. The problem formed in this way is an NP-hard. A mixed integer programming model is proposed for the problem and a parameters tuned meta-heuristic namely genetic algorithm is proposed to solve it. To evaluate the validation and performance of the proposed algorithm a set of 100 test problems is used. Comparative statistical results show that the proposed algorithm is efficiently capable to solve the problem.",
     "keywords": ["Project scheduling", "Preemption", "Set up time", "Genetic algorithm"]},
    {"article name": "Application of stepwise regression for dynamic parameter estimation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.06.013",
     "publication date": "10-2014",
     "abstract": "Dynamic parameter estimation in cases where it may be impossible to identify all the model parameters is considered. The objective is to obtain reliable estimates to the maximal number of physical parameters in a stable regression model where the modeling of the noise in the data is avoided. The modifications required in the stepwise regression algorithm to accommodate various nonlinear terms in the regression model are investigated and a new algorithm is presented. The algorithm considers the hierarchy among the parameters, the initial trends of the experimental data curves and the initial values of the state variables in order to establish a minimal initial set of parameters to be included in the model. Additional parameters are then added in a stepwise manner, while considering the hierarchy of the parameters and the associated reduction of the objective function value. The process continues as long as significant and physically feasible values for the parameters are obtained. The new method is demonstrated with several examples from the literature. Additional issues investigated include the proper combination of the simultaneous and sequential solution methods in the stepwise regression algorithm, the preferred method for the estimation of the derivatives and the effect of variable scaling.",
     "keywords": ["Stepwise regression", "Dynamic parameter estimation", "Regression analysis"]},
    {"article name": "Simultaneous production and distribution of industrial gas supply-chains",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.06.010",
     "publication date": "10-2014",
     "abstract": "In this paper, we propose a multi-period mixed-integer linear programming model for optimal enterprise-level planning of industrial gas operations. The objective is to minimize the total cost of production and distribution of liquid products by coordinating production decisions at multiple plants and distribution decisions at multiple depots. Production decisions include production modes and rates that determine power consumption. Distribution decisions involve source, destination, quantity, route, and time of each truck delivery. The selection of routes is a critical factor of the distribution cost. The main goal of this contribution is to assess the benefits of optimal coordination of production and distribution. The proposed methodology has been tested on small, medium, and large size examples. The results show that significant benefits can be obtained with higher coordination among plants/depots in order to fulfill a common set of shared customer demands. The application to real industrial size test cases is also discussed.",
     "keywords": ["Supply-chain optimization", "Industrial gases", "Production planning", "Inventory routing problem", "Multi-period model", "Mixed-integer linear programming"]},
    {"article name": "CFD study of heat transfer for oscillating flow in helically coiled tube heat-exchanger",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.07.001",
     "publication date": "10-2014",
     "abstract": "The heat transfer and pressure drop for oscillating flow in helically coiled tube heat-exchanger were numerically investigated based on the Navier\u2013Stokes equations. The correlation of the average Nussel number and average friction factor were proposed considering the frequency and the inlet velocity. The oscillating flow heat transfer problems are influenced by many factors. Hence we need an easy way to reduce the numbers of simulation or experiment. Therefore, the method of uniform design was adopted and the feasibility of this method was verified. The field synergy principle was used to explain the heat transfer enhancement of oscillating flow in helically coiled tube heat-exchanger. The result shows that the smaller the volume average field synergy angle in the helically coiled tube, the better the rate of heat transfer.",
     "keywords": ["Helically coiled", "CFD", "Heat transfer correlation", "Friction factor correlation", "Uniform design", "Field synergy principle"]},
    {"article name": "Simultaneous data reconciliation and gross error detection for dynamic systems using particle filter and measurement test",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.06.014",
     "publication date": "10-2014",
     "abstract": "Good dynamic model estimation plays an important role for both feedforward and feedback control, fault detection, and system optimization. Attempts to successfully implement model estimators are often hindered by severe process nonlinearities, complicated state constraints, systematic modeling errors, unmeasurable perturbations, and irregular measurements with possibly abnormal behaviors. Thus, simultaneous data reconciliation and gross error detection (DRGED) for dynamic systems are fundamental and important. In this research, a novel particle filter (PF) algorithm based on the measurement test (MT) is used to solve the dynamic DRGED problem, called PFMT-DRGED. This strategy can effectively solve the DRGED problem through measurements that contain gross errors in the nonlinear dynamic process systems. The performance of PFMT-DRGED is demonstrated through the results of two statistical performance indices in a classical nonlinear dynamic system. The effectiveness of the proposed PFMT-DRGED applied to a nonlinear dynamic system and a large scale polymerization process is illustrated.",
     "keywords": ["Chemical processes", "Data reconciliation", "Gross error detection", "Measurement test", "Particle filter", "System engineering"]},
    {"article name": "CFD modeling of bubbling fluidized beds using OpenFOAM\u00ae: Model validation and comparison of TVD differencing schemes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.07.002",
     "publication date": "10-2014",
     "abstract": "The two-fluid model with kinetic theory of granular flow is implemented into the open source CFD package OpenFOAM\u00ae. The effect of total variation diminishing (TVD) convection schemes is investigated by simulating two bubbling fluidized beds. Five TVD schemes are employed to discretize the convection terms of phase velocity and solid volume fraction. Simulated results of the two test cases give reasonable agreement with the experimental data in the literature. For the discretization of the phase velocity convection terms, the five schemes give quite similar time-averaged radial profiles of particle axial velocity. The predicted bubbles in the bed with a central jet are not influenced by the different schemes. For the discretization of the solid volume fraction convection terms, the limitedLinear01, Sweby01 and vanLeer01 schemes give the converged and reasonable solutions, whereas the SuperBee01 and MUSCL01 schemes diverge the solutions. When using the faceLimited gradient scheme the convection scheme becomes more diffusive.",
     "keywords": ["Fluidized bed", "CFD", "Two-fluid model", "TVD scheme", "OpenFOAM"]},
    {"article name": "Meta-modelling for fast analysis of CFD-simulated vapour cloud dispersion processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.07.003",
     "publication date": "10-2014",
     "abstract": "Released flammable chemicals can form an explosible vapour cloud, posing safety threat in both industrial and civilian environments. Due to the difficulty in conducting physical experiments, computational fluid dynamic (CFD) simulation is an important tool in this area. However, such simulation is computationally too slow for routine analysis. To address this issue, a meta-modelling approach is developed in this study; it uses a small number of simulations to build an empirical model, which can be used to predict the concentration field and the potential explosion region. The dimension of the concentration field is reduced from around 43,421,400 to 20 to allow meta-modelling, by using the segmented principal component transform-principal component analysis. Moreover, meta-modelling-based uncertainty analysis is explored to quantify the prediction variance, which is important for risk assessment. The effectiveness of the methodology has been demonstrated on CFD simulation of the dispersion of liquefied natural gas.",
     "keywords": ["CFD computational fluid dynamic", "computational fluid dynamic", "DoE design of experiments", "design of experiments", "FLACS flame acceleration simulator", "flame acceleration simulator", "GPR Gaussian process regression", "Gaussian process regression", "HSS Hammersley sequence sampling", "Hammersley sequence sampling", "LEL lower explosive limit", "lower explosive limit", "LNG liquefied natural gas", "liquefied natural gas", "MC Monte Carlo", "Monte Carlo", "PCA principal component analysis", "principal component analysis", "pdf probability density function", "probability density function", "RMSE root mean square error", "root mean square error", "SegPCT-PCA segmented principal component transform-principal component analysis", "segmented principal component transform-principal component analysis", "UEL upper explosive limit", "upper explosive limit", "Computational fluid dynamics", "Design of experiments", "Gaussian process regression", "Kriging", "Surrogate modelling", "Vapour cloud dispersion"]},
    {"article name": "Vehicle routing scheduling problem with cross docking and split deliveries",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.06.015",
     "publication date": "10-2014",
     "abstract": "One of the most important issues in managing a supply chain which makes the materials flow more efficiently from suppliers to customers is cross docking. In this paper a vehicle routing and scheduling problem in a network consisting of suppliers, customers and a cross dock is considered. A set of homogenous vehicles transfer products from suppliers to customers through a cross dock. Each vehicle has a limited capacity and both suppliers and customers must be visited within their time windows, also a customer can be visited more than once by different vehicles. A mixed integer non linear mathematical formulation of this problem is provided. A simulated annealing and a hybrid metaheuristic algorithm combining ant colony system and simulated annealing are proposed. These two proposed metaheuristics are implemented on different data sets. The experimental results show the superior performance of the hybrid algorithm compared with the proposed simulated annealing algorithm.",
     "keywords": ["Vehicle routing problem", "Scheduling", "Cross docking", "Simulated annealing", "Ant colony system", "Hybrid metaheuristic"]},
    {"article name": "The RAYMOND simulation package \u2014 Generating RAYpresentative MONitoring Data to design advanced process monitoring and control algorithms",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.07.010",
     "publication date": "10-2014",
     "abstract": "This work presents the RAYMOND simulation package for generating RAYpresentative MONitoring Data. RAYMOND is a free MATLAB package and can simulate a wide range of processes; a number of widely-used benchmark processes are available, but user-defined processes can easily be added. Its modular design results in large flexibility with respect to the simulated processes: input fluctuations resulting from upstream variability can be introduced, sensor properties (measurement noise, resolution, range, etc.) can be freely specified, and various (custom) control strategies can be implemented. Furthermore, process variability (biological variability or non-ideal behavior) can be included, as can process-specific disturbances.In two case studies, the importance of including non-ideal behavior for monitoring and control of batch processes is illustrated. Hence, it should be included in benchmarks to better assess the performance and robustness of advanced process monitoring and control algorithms.",
     "keywords": ["Process monitoring", "Process control", "Data generation", "Process simulation"]},
    {"article name": "CFD analysis on the catalyst layer breakage failure of an SCR-DeNOx system for a 350\u00a0MW coal-fired power plant",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.07.012",
     "publication date": "10-2014",
     "abstract": "Selective catalytic reduction as one of the secondary NOx control technologies is widely used in industrial sources including coal-fired power plants and large boilers. The performance of an SCR-DeNOx system is sensitive to the installment of its components such as turning vanes and hybrid grids. In this work, three-dimensional CFD simulations are carried out to analyze the breakage failure of an SCR-DeNOx system for a certain 350\u00a0MW coal-fired power plant. Research results are consistent with the phenomena that occur in the industrial application. It reveals that the breakage failure in the industrial application is likely to be caused by the inappropriate installation of the turning vane 3 locating closest to the catalyst, especially the angle of the turning vane 3. The analysis further shows that the lifetime or the breakage of the catalyst layers depends highly on the gas velocity, the fly ash distribution and its particle velocity.",
     "keywords": ["Computational Fluid Dynamics (CFD)", "Selective catalytic reduction (SCR)", "Flue gas denitrification (DeNOx)", "Turning vanes", "Failure analysis"]},
    {"article name": "Expectation Maximization method for multivariate change point detection in presence of unknown and changing covariance",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.06.016",
     "publication date": "10-2014",
     "abstract": "Data analysis plays an important role in system modeling, monitoring and optimization. Among those data analysis techniques, change point detection has been widely applied in various areas including chemical process, climate monitoring, examination of gene expressions and quality control in the manufacturing industry, etc. In this paper, an Expectation Maximization (EM) algorithm is proposed to detect the time instants at which data properties are subject to change. The problem is solved in the presence of unknown and changing mean and covariance in process data. Performance of the proposed algorithm is evaluated through simulated and experimental study. The results demonstrate satisfactory detection of single and multiple changes using EM approach.",
     "keywords": ["Change point detection", "Bayesian inference", "Expectation Maximization"]},
    {"article name": "Convergence criterion in optimization of stochastic processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.04.011",
     "publication date": "09-2014",
     "abstract": "A novel approach is developed and its applicability is demonstrated for defining convergence in optimization of stochastic processes for both design and control applications. The approach is computationally simple, does not require a priori knowledge, and does not get drawn into undesirable solutions from fortuitous good values.",
     "keywords": ["Stochastic", "Process", "Optimization", "Convergence criterion"]},
    {"article name": "Data-driven multi-stage scenario tree generation via statistical property and distribution matching",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.04.012",
     "publication date": "09-2014",
     "abstract": "This paper brings systematic methods for scenario tree generation to the attention of the Process Systems Engineering community. We focus on a general, data-driven optimization-based method for generating scenario trees that does not require strict assumptions on the probability distributions of the uncertain parameters. Using as a basis the Moment Matching Problem (MMP), originally proposed by H\u00f8yland and Wallace (2001), we propose matching marginal (Empirical) Cumulative Distribution Function information of the uncertain parameters in order to cope with potentially under-specified MMP formulations. The new method gives rise to a Distribution Matching Problem (DMP) that is aided by predictive analytics. We present two approaches for generating multi-stage scenario trees by considering time series modeling and forecasting. The aforementioned techniques are illustrated with a production planning problem with uncertainty in production yield and correlated product demands.",
     "keywords": ["90C15", "90C90", "Process Systems Engineering", "Stochastic Programming", "Scenario generation", "Distribution Matching Problem", "Time series forecasting", "Analytics"]},
    {"article name": "Adaptive discontinuous Galerkin methods for non-linear diffusion\u2013convection\u2013reaction equations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.05.002",
     "publication date": "09-2014",
     "abstract": "In this work, we apply the adaptive discontinuous Galerkin (DGAFEM) method to the convection dominated non-linear, quasi-stationary diffusion convection reaction equations. We propose an efficient preconditioner using a matrix reordering scheme to solve the sparse linear systems iteratively arising from the discretized non-linear equations. Numerical examples demonstrate effectiveness of the DGAFEM to damp the spurious oscillations and resolve well the sharp layers occurring in convection dominated non-linear equations.",
     "keywords": ["Non-linear diffusion\u2013convection reaction", "Discontinuous Galerkin", "Adaptivity", "Matrix reordering", "Preconditioning"]},
    {"article name": "Simulation based approach to optimal design of dividing wall column using random search method",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.05.001",
     "publication date": "09-2014",
     "abstract": "A systematic optimization method based on combination of radial basis function neural network (RBF-NN) and genetic algorithm (GA) is developed for optimal design of DWC. The RBF-NN is built by a series of rigorous simulations of DWC for different sets of stage numbers with available simulation software. Then, GA is applied to optimize the stage numbers of the DWC with an objective (cost) function evaluated with the results obtained by the RBF-NN. As a simulation based approach, the proposed method shows its strength in finding the optimal solution by just evaluating small portion of the possible combinations of the stage numbers, and thus can be known as a promising method for optimal design of DWC. Three case studies were solved to detect the optimal structure and their sensitivity to operational conditions was analyzed. The results were shown encouraging compared with those found in the literature.",
     "keywords": ["Distillation", "Dividing wall column", "Optimal design", "Radial basis function neural network", "Random search"]},
    {"article name": "Simulation and optimization on etherification of light gasoline in a process of two fixed-bed reactors and one distillation column",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.05.007",
     "publication date": "09-2014",
     "abstract": "The reaction of FCC light gasoline, being rich in C5 and C6 alkenes, exhibited etherification reactivity with methanol over the cation-exchange resin for increasing octane number. An etherification process of light gasoline in two reactors coupled with one distillation column was proposed and simulated. The axial dispersion model and rigorous algorithm were employed to calculate both reactors operated adiabatically and distillation column at steady state, respectively. Validation of the simulation was examined by the experimental data from pilot plant. Multi-objective optimization for maximizing the final conversion and minimizing the equipment cost and energy consumption concluded that optimizing the reaction volume and circulation ratios could obtain Pareto optimal set for selecting preferred design parameters by using non-dominated sorting genetic algorithm (NSGA-II).",
     "keywords": ["Etherification", "Light gasoline", "Methanol", "Simulation", "Multi-objective optimization", "NSGA-II"]},
    {"article name": "Strategic and tactical mathematical programming models within the crude oil supply chain context\u2014A review",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.05.008",
     "publication date": "09-2014",
     "abstract": "In today's business world, oil companies cannot be productive and competitive, and thus, will not survive without taking the supply chain management concepts into account. Consequently, the management of a crude oil supply chain (COSC) is increasingly receiving substantial importance. The growing number of papers and books on this topic is a further witness of this fact. To foster insight into issues intertwined with COSC problems, this paper is devoted to an extensive review of mathematical programming models in this context. The classification approach for this review is based on a taxonomy framework. In this framework, ongoing and emerging challenges surrounding the strategic and tactical decisions of COSC problems are investigated. As a main goal, the gaps of literature are analyzed to recommend possible research directions.",
     "keywords": ["Crude oil supply chain", "Strategic and tactical decisions", "Mathematical programming model", "Literature review"]},
    {"article name": "Integration of set point optimization techniques into nonlinear MPC for improving the operation of WWTPs",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.03.027",
     "publication date": "09-2014",
     "abstract": "Optimization and control strategies are necessary to keep wastewater treatment plants (WWTPs) operating in the best possible conditions, maximizing effluent quality with the minimum consumption of energy. In this work, a benchmarking of different hierarchical control structures for WWTPs that combines static and dynamic real time optimization (RTO) and nonlinear model predictive control (NMPC) is presented. The objective is to evaluate the enhancement of the operation in terms of economics and effluent quality that can be achieved when introducing NMPC technologies in the distinct levels of the multilayer structure. Three multilayer hierarchical structures are evaluated and compared for the N-Removal process considering the short term and long term operation in a rain weather scenario. A reduction in the operation costs of approximately 20% with a satisfactory compromise to effluent quality is achieved with the application of these control schemes.",
     "keywords": ["Nonlinear MPC", "Process control", "Hierarchical control", "Optimal operation", "Wastewater treatment"]},
    {"article name": "New continuous-time and discrete-time mathematical formulations for resource-constrained project scheduling problems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.05.009",
     "publication date": "09-2014",
     "abstract": "Two binary integer programming discrete-time models and two precedence-based mixed integer programming continuous-time formulations are developed for the resource-constrained project scheduling problem. The discrete-time models are based on the definition of binary variables that describe the processing state of every activity between two consecutive time points, while the continuous-time models are based on the concept of overlapping of activities, and the definition of a number of newly introduced sets. Our four mathematical formulations are compared with six representative literature models in 3240 benchmark problem instances. A detailed computational comparison assesses the performance of the mathematical models considered.",
     "keywords": ["Project scheduling", "Integer programming", "Scheduling", "Project management", "Resource constraints"]},
    {"article name": "Plant simulation and operation optimisation of SMR plant with different adjustment methods under part-load conditions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.05.011",
     "publication date": "09-2014",
     "abstract": "A plant simulation model of SMR (single mixed refrigerant cycle) plant which considers the performance variations of devices under different working conditions is established. It could be used to predict the performance variations of an SMR plant under part-load, and other working conditions. The operational optimisation based on plant simulation is conducted for SMR plant with different load adjustment facilities. The constraint condition uses a penalty function method and the Box method is applied for solution optimisation. The results show that the frequency conversion method could achieve the best adjustment performance and the composition optimisation is significant for the frequency conversion method, especially when the load is below 40%. The operational strategy giving the lowest energy consumption for SMR plant with different adjustment methods under part-load conditions was discussed and the efficiency of the MR compressor is regarded as the prime influence thereon.",
     "keywords": ["Load adjustment", "Throttle", "Frequency conversion", "Operational optimisation"]},
    {"article name": "Study on the number of decision variables in design and optimization of Varicol process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.05.004",
     "publication date": "09-2014",
     "abstract": "As a variant of simulated moving bed chromatography, Varicol process can realize flexible adsorbent allocation through asynchronous shifting of inlet and outlet ports. In this research, the switching strategies for achieving one certain average configuration were investigated. It was found the system performance depends on the total number of switches of the four ports during one switching period and Simple-Shifting-Scheme (SSS) with four switches brings out the best performance. More importantly, SSS could be different schemes, different by a parallel shifting, but all of them result in the same performance, demonstrating that only three instead of four switching times are decision variables while designing a Varicol process. To make the design of SSS more solid, a systematic method for derivation of SSS is proposed and applied on the optimization of a Varicol process for enantioseparation of 1,1\u2032-bi-2-naphthol and reduce the solvent consumption by 17% comparing with SMB.",
     "keywords": ["Simulated moving bed", "Varicol", "Switching strategy", "Design", "Simulation", "Optimization"]},
    {"article name": "Limitations in using Euler's formula in the design of heat exchanger networks with Pinch Technology",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.05.015",
     "publication date": "09-2014",
     "abstract": "Pinch Technology developed by Linnhoff and other workers has been widely adopted and considered to be one of the most successful techniques in process energy integration. The number of heat exchanger units is one of the most important aspects in the problem and Euler's formula has been applied in correlating the number of units and loops in the network. However, planarity of a graph is required for the application of Euler's formula, a fact that has been ignored in previous literature. It is demonstrated in this paper that Euler's formula cannot always be applied in Pinch Technology, by presenting examples of non-planar graphs resulting from thermodynamically feasible heat exchanger networks.",
     "keywords": ["Euler's formula", "Planar graphs", "Maximum energy recovery network", "Heat exchanger networks"]},
    {"article name": "Robust design and operations of hydrocarbon biofuel supply chain integrating with existing petroleum refineries considering unit cost objective",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.05.003",
     "publication date": "09-2014",
     "abstract": "This paper addresses the optimal design and planning of the advanced hydrocarbon biofuel supply chain with the unit cost objective. Benefited from the drop-in properties of advanced hydrocarbon biofuels, the supply chain takes advantage of the existing petroleum infrastructure, which may lead to significant capital and transportation savings. A mixed-integer linear programming model is proposed to simultaneously consider the supply chain design, integration strategy selection, and production planning. A robust optimization approach which tradeoffs the performance and conservatism is adopted to deal with the demand and supply uncertainty. Moreover, the unit cost objective makes the final products more cost-competitive. The resulting mixed-integer linear fractional programming model is solved by tailored optimization algorithm. County level cases in Illinois are analyzed and compared to show the advantage of the proposed optimization framework. The results show that the preconversion to petroleum-upgrading pathway is more economical when applying the unit cost objective.",
     "keywords": ["Biofuel supply chain", "Petroleum refinery", "Robust optimization", "Mixed-integer linear fractional programming"]},
    {"article name": "Coupled population balance\u2013CFD simulation of droplet breakup in a high pressure homogenizer",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.05.014",
     "publication date": "09-2014",
     "abstract": "A framework for a one-way coupling between population balance equation (PBE) and computational fluid dynamics (CFD) for emulsions undergoing breakup in a turbulent flow-regime is implemented in the open-source CFD package OpenFOAM. The PBE is discretized using a volume conservative scheme. This framework was applied to emulsification of food-grade vegetable oil-in-water emulsions in a Niro Soavi high pressure homogenizer (HPH) with different pressure drops and using oils of varying viscosities. Different breakage rate models were implemented, as well as the model of Becker et al. (2014) which is an extension of the Luo and Svendsen (1996) model to account for the dispersed phase viscosity. The Becker et al. (2014) model was found to give better predictions after three consecutive passes, without the need for additional empirically determined parameters. The multi-scale PBE\u2013CFD modelling approach allowed a detailed analysis of the breakup process in the gap and jet of the HPH valve.",
     "keywords": ["Population balance modelling", "Computational fluid dynamics", "Emulsification", "Multiscale modelling"]},
    {"article name": "Optimising chromatography strategies of antibody purification processes by mixed integer fractional programming techniques",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.05.005",
     "publication date": "09-2014",
     "abstract": "The strategies employed in chromatography steps play a key role in downstream processes for monoclonal antibody (mAb) manufacture. This work addresses the integrated optimisation of chromatography step sequencing and column sizing in mAb purification processes. Chromatography sequencing decisions include the resin selection at each typical step, while the column sizing decisions include the number of columns, the column diameter and bed height, and number of cycles per batch. A mixed integer nonlinear programming (MINLP) model was developed and then reformulated as a mixed integer linear fractional programming (MILFP) model. A literature approach, the Dinkelbach algorithm, was adopted as the solution method for the MILFP model. Finally, an industrially-relevant case study was investigated for the applicability of the proposed models and approaches.",
     "keywords": ["Biopharmaceutical manufacturing processes", "mAb", "Chromatography purification", "MINLP", "MILFP", "Dinkelbach algorithm"]},
    {"article name": "A mathematical programming model for optimal layout considering quantitative risk analysis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.05.019",
     "publication date": "09-2014",
     "abstract": "Safety and performance are important factors in the design and operation of chemical plants. This paper describes the formulation of a mixed integer nonlinear programming model for the optimization of plant layout with safety considerations. The model considers a quantitative risk analysis to take safety into account, and a bowtie analysis is used to identify possible catastrophic outcomes. These effects are quantified through consequence analyses and probit models. The model allows the location of facilities at any available point, an advantage over grid-based models. Two case studies are solved to show the applicability of the proposed approach.",
     "keywords": ["Optimization", "Mixed-integer programming", "Safety", "Quantitative risk analysis", "Plant layout"]},
    {"article name": "Computer modeling of surface interactions and contaminant transport in microstructures during the rinsing of patterned semiconductor wafers",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.05.018",
     "publication date": "09-2014",
     "abstract": "Rinsing microstructures on a patterned semiconductor wafer is modeled. The simulation results are presented for two cases when the surfaces of a trench as the microstructure are made of a single material, or two different materials. The dynamics of contaminant removal from the microstructure surfaces and its dependence on the geometrical structure, physical characteristics of the surfaces, and contaminant diffusivity are presented. The results show that in the case of a trench with two different materials, the cleaning dynamics of the trench bed strongly depends on the stacking order of the materials. When the upper material has a smaller desorption rate coefficient, the dynamics of contaminant transport develops a smaller rate at some point in time that depends on the thickness of the layers.",
     "keywords": ["Microstructures cleaning", "Rinse process", "Adsorption\u2013desorption", "Convection\u2013diffusion", "Transport phenomena"]},
    {"article name": "Studies for the development of a virtual permeameter",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.05.027",
     "publication date": "09-2014",
     "abstract": "A systematic analysis on the use of the lattice Boltzmann method (LBM) for predicting the permeability of packed beds is presented. A filtration rig is used to obtain permeability measurements of beds of glass beads, sand and crushed minerals. Subsequently, X-ray micro-tomography is employed to image bed samples for use as input to LBM calculations. Uncertainties in accuracy and reliability of predictions arising from pixel resolution, sample size and digitisation errors in the simulations are evaluated through sensitivity studies and assessments of the representativeness of the bed samples. For beds of spherical and near-spherical particles, any bed sample is capable of providing reproducible and reliable simulation results provided that the particles are adequately resolved within an LBM simulation. For more complex beds of polymorphous, polydisperse particles estimation of the permeability of representative samples of the entire bed is required before average results comparable with data are obtained.",
     "keywords": ["Permeability", "Packed beds", "Lattice Boltzmann method", "X-ray micro-tomography"]},
    {"article name": "Application of rolling horizon optimization to an integrated solid-oxide fuel cell and compressed air energy storage plant for zero-emissions peaking power under uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.06.001",
     "publication date": "09-2014",
     "abstract": "In this study, the application of a rolling horizon optimization strategy to an integrated solid-oxide fuel cell/compressed air energy storage plant for load-following is investigated. A reduced-order model of the integrated plant is used to simulate and optimize each optimization interval as a mixed integer non-linear program. Forecasting uncertainties are considered through the addition of measurement noise and use of stochastic Monte Carlo simulations. The addition of rolling horizon optimization gives significant reductions to the sum-of-squared-errors between the demand and supply profiles. A sensitivity analysis is used to show that increasing the forecasting and optimization horizon improves load tracking with diminishing returns. Incorporating white Gaussian noise to demand forecasts has a marginal impact on error, even when a relatively high noise power of is used. Consistently over- or under-predicting demand has a greater impact on the plant's load-tracking error. However, even under worst-case forecasting scenarios, using a rolling horizon optimization scheme provides a more than 50% reduction in error when compared to the original system. An economic objective function is formulated to improve gross revenue using electricity spot-prices, but results in a trade-off with load-following performance. The results indicate that the rolling horizon optimization approach could potentially be applied to future municipal-scale fuel cell/compressed air storage systems to achieve power levels which closely follow real grid power cycles using existing prediction models.",
     "keywords": ["CAES compressed air energy storage", "compressed air energy storage", "CCS carbon capture and sequestration", "carbon capture and sequestration", "EOS equation of state", "equation of state", "GT gas turbine", "gas turbine", "HHV higher-heating value", "higher-heating value", "HRSG heat recovery and steam generation", "heat recovery and steam generation", "IESO independent electricity systems operator", "independent electricity systems operator", "LCOE levelized cost of electricity", "levelized cost of electricity", "RHO rolling horizon optimization", "rolling horizon optimization", "SOFC solid oxide fuel cell", "solid oxide fuel cell", "SSE sum of squared error", "sum of squared error", "Solid oxide fuel cell", "Compressed air energy storage", "System design", "Peaking power", "Optimization", "Natural gas"]},
    {"article name": "Adaptive sequential sampling for surrogate model generation with artificial neural networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.05.021",
     "publication date": "09-2014",
     "abstract": "Surrogate models \u2013 simple functional approximations of complex models \u2013 can facilitate engineering analysis of complicated systems by greatly reducing computational expense. The construction of a surrogate model requires evaluation of the original model to gather the data necessary for building the surrogate. Sequential sampling procedures are proposed for determining and minimizing the required number of samples for efficient global surrogate construction. In this paper, two new adaptive sampling algorithms \u2013 one purely adaptive and one combining adaptive and space-filling characteristics \u2013 are proposed and compared to a purely space-filling approach. Our analysis suggests a mixed adaptive sampling approach for constructing surrogates for systems where the behavior of the underlying model is unknown. Results of the case study, optimization of carbon dioxide capture process with aqueous amines, revealed that the mixed adaptive sampling algorithm may reduce the required sample size by up to 40% compared to a purely space-filling design.",
     "keywords": ["Adaptive sampling", "Space-filling design", "Artificial neural networks", "Surrogate models"]},
    {"article name": "Modeling and simulation of an industrial falling film reactor using the method of lines with adaptive mesh. Study case: Industrial sulfonation of tridecylbenzene",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.05.023",
     "publication date": "09-2014",
     "abstract": "The method of lines with adaptive mesh was used to solve a falling film reactor model for the industrial sulfonation of tridecylbenzene. A model composed by three partial differential equations, three ordinary differential equations and several arithmetic ones was proposed. Using a discretization of 60 nodes in radial direction, the percentage of error respect to plant data in terms of exit liquid mean temperature and SO3 conversion was 0.03% and 1%, respectively. Temperature distribution in the liquid film aided to detect hot zones in the falling film near the entrance of the reactor, reaching an increase of ca. 36\u00a0K in the hottest point. The behavior of the reaction along the reactor was also studied based on the conversion profile and concentration distribution in the liquid film. The model allows performing a complete analysis of the reactor operation, making the model and its results a useful tool for the industry.",
     "keywords": ["Falling film reactor", "Sulfonation", "Method of lines", "Tridecylbenzene"]},
    {"article name": "Analysis and comparison of single period single level and bilevel programming representations of a pre-existing timberlands supply chain with a new biorefinery facility",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.05.025",
     "publication date": "09-2014",
     "abstract": "This research investigates the economic impact of a new biorefinery on an established timberlands system. This paper proposes that a single level model does not adequately represent the interactions in the supply chain. The timberlands system has two major decision makers: harvesters and manufacturers. Bilevel models are two-level turn-based models with each level representing a decision maker. The two levels are interconnected, but neither has control over the decisions of the other. The leader makes an initial decision, and the follower reacts; the leader can anticipate the follower's reaction. To demonstrate the value of this more complex representation, a single period bilevel and a single period single level model were formulated for a representative case study. The separation of objective functions in the bilevel problem yielded decisions different from those of the single level formulation. It was concluded that the bilevel model may more accurately represent the real system.",
     "keywords": ["Timberlands supply allocation", "Bilevel programming", "Biofuels", "Optimization", "Supply chain modeling"]},
    {"article name": "Improving lead time of pharmaceutical production processes using Monte Carlo simulation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.05.017",
     "publication date": "09-2014",
     "abstract": "Reliable product supply is one of the most critical missions of the pharmaceutical industry. The lead time, i.e. the duration between start and end of an activity, needs to be well managed in any production facility in order to make scheduling predictable, agile and flexible. We present a method for measuring and improving production lead time of pharmaceutical processes with a primary focus on Parenterals (i.e. injectables) production processes. Monte Carlo simulation is applied for quantifying the total lead time (TLT) of batch production as a probability distribution and sensitivity analysis reveals the ranking of sub-processes by impact on TLT. Based on these results, what-if analyses are performed to evaluate effects of investments, resource allocations and process improvements on TLT. An industrial case study was performed at a production site for Parenterals of F. Hoffmann-La Roche in Kaiseraugst, Switzerland, where the presented method supported analysis and decision-making of production enhancements.",
     "keywords": ["Monte Carlo simulation", "Sensitivity analysis", "Supply chain", "Pharmaceutical production", "Decision-making", "Industrial application"]},
    {"article name": "Quantifying the effectiveness of an alarm management system through human factors studies",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.03.013",
     "publication date": "08-2014",
     "abstract": "Alarm systems in chemical plants alert process operators to deviations in process variables beyond predetermined limits. Despite more than 30 years of research in developing various methods and tools for better alarm management, the human aspect has received relatively less attention. The real benefit of such systems can only be identified through human factors experiments that evaluate how the operators interact with these decision support systems. In this paper, we report on a study that quantifies the benefits of a decision support scheme called Early Warning, which predicts the time of occurrence of critical alarms before they are actually triggered. Results indicate that Early Warning is helpful in reaching a diagnosis more quickly; however it does not improve the accuracy of correctly diagnosing the root cause. Implications of these findings for human factors in process control and monitoring are discussed.",
     "keywords": ["Alarm management", "Process monitoring", "Prediction", "Process operators"]},
    {"article name": "Integration of modular process simulators under the Generalized Disjunctive Programming framework for the structural flowsheet optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.03.014",
     "publication date": "08-2014",
     "abstract": "The optimization of chemical processes where the flowsheet topology is not kept fixed is a challenging discrete-continuous optimization problem. Usually, this task has been performed through equation based models. This approach presents several problems, as tedious and complicated component properties estimation or the handling of huge problems (with thousands of equations and variables). We propose a GDP approach as an alternative to the MINLP models coupled with a flowsheet program. The novelty of this approach relies on using a commercial modular process simulator where the superstructure is drawn directly on the graphical use interface of the simulator. This methodology takes advantage of modular process simulators (specially tailored numerical methods, reliability, and robustness) and the flexibility of the GDP formulation for the modeling and solution. The optimization tool proposed is successfully applied to the synthesis of a methanol plant where different alternatives are available for the streams, equipment and process conditions.",
     "keywords": ["Process synthesis", "Generalized Disjunctive Programming", "Modular simulators", "Logic-based optimization algorithm"]},
    {"article name": "Modeling of a microreactor for propylene production by the catalytic dehydrogenation of propane",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.03.023",
     "publication date": "08-2014",
     "abstract": "A model of a microreactor was proposed to analyze the production of propylene by the propane dehydrogenation using a catalytic surface of V2O5/TiO2 doped with Rb. The reactor is a 50\u00a0mm length tube of 2\u00a0mm diameter whose wall is a catalytic surface, modeled using finite volume method in cylindrical coordinates over a tangential plane. Dehydrogenation kinetics is reported by Grabowski (2004). First, a mesh independence analysis was done to assure the adequate cell size. Second, a parametric analysis changing Reynolds number at different temperatures and propane-oxygen relations was done to find the Reynolds range to effectively use the reactor length (Re from 1 to 10). Then at the later Reynolds interval a parametric analysis involving temperature and composition was done to create productivity surfaces to find the highest productivity operation conditions. Finally, an analysis varying pressure at the maximum productivity conditions (Re\u00a0=\u00a01, T\u00a0=\u00a0500\u00a0K, C3H8/O2\u00a0=\u00a02) was developed.",
     "keywords": ["Microreactor", "Microreactor modeling", "Catalytic dehydrogenation", "Propane dehydrogenation", "Propylene production", "Vanadia-titania catalyst"]},
    {"article name": "Cognitive fault diagnosis in Tennessee Eastman Process using learning in the model space",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.03.015",
     "publication date": "08-2014",
     "abstract": "This paper focuses on the Tennessee Eastman (TE) process and for the first time investigates it in a cognitive way. The cognitive fault diagnosis does not assume prior knowledge of the fault numbers and signatures. This approach firstly employs deterministic reservoir models to fit the multiple-input and multiple-output signals in the TE process, which map the signal space to the (reservoir) model space. Then we investigate incremental learning algorithms in this reservoir model space based on the \u201cfunction distance\u201d between these models. The main contribution of this paper is to provide a cognitive solution to this popular benchmark problem. Our approach is not only applicable to fault detection, but also to fault isolation without knowing the prior information about the fault signature. Experimental comparisons with other state-of-the-art approaches confirmed the benefits of our approach. Our algorithm is efficient and can run in real-time for practical applications.",
     "keywords": ["Learning in the model space", "Tennessee Eastman Process", "Fault detection", "Cognitive fault diagnosis", "Reservoir computing", "One class learning"]},
    {"article name": "An online method to remove chattering and repeating alarms based on alarm durations and intervals",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.03.018",
     "publication date": "08-2014",
     "abstract": "Chattering and repeating alarms, which repeatedly make transitions between alarm and non-alarm states without operators\u2019 response, are the most common form of nuisance alarms encountered in industrial plants. The paper formulates two novel rules to detect chattering alarms caused by random noise and repeating alarms by regular patterns such as oscillation, and proposes an online method to effectively remove chattering and repeating alarms via the m-sample delay timer. Industrial examples are provided to support the formulated rules and to illustrate the effectiveness of the proposed method.",
     "keywords": ["Chattering alarms", "Repeating alarms", "Alarm duration", "Alarm interval", "Delay timers"]},
    {"article name": "Deterministic optimization of the thermal Unit Commitment problem: A Branch and Cut search",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.03.009",
     "publication date": "08-2014",
     "abstract": "This paper proposes a novel deterministic optimization approach for the Unit Commitment (UC) problem, involving thermal generating units. A mathematical programming model is first presented, which includes all the basic constraints and a set of binary variables for the on/off status of each generator at each time period, leading to a convex mixed-integer quadratic programming (MIQP) formulation. Then, an effective solution methodology based on valid integer cutting planes is proposed, and implemented through a Branch and Cut search for finding the global optimal solution. The application of the proposed approach is illustrated with several examples of different dimensions. Comparisons with other mathematical formulations are also presented.",
     "keywords": ["Energy optimization", "Unit Commitment problem", "Deterministic optimization", "Branch and Cut algorithm"]},
    {"article name": "Concurrent PLS-based process monitoring with incomplete input and quality measurements",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.03.022",
     "publication date": "08-2014",
     "abstract": "The process monitoring based on concurrent partial least square (CPLS) performs well on the monitoring of input and quality variables through five monitoring statistics. However, in practice, the case of missing variable is very common and the incomplete measurements will make it difficult to implement this monitoring method. Considering the presence of missing measurements occurring in both input and quality variables, this paper analyzes the influence of missing measurements on monitoring performance based on the assumption that input and quality variables satisfy multivariate Gaussian distribution under normal operation. The proposed method estimates the conditional distributions of missing variables, scores and residuals given the observable variables, and denotes monitoring statistics with these conditional distributions. Then, the probabilistic uncertain ranges of monitoring statistics are derived by calculating the general quadratic formulations of Gaussian-distributed missing variables. To determine the process operation in the presence of missing variables, the proposed method employs these uncertain ranges as monitoring statistics. Simulation examples illustrate feasibility of this proposed method and demonstrate its effectiveness.",
     "keywords": ["Process monitoring", "CPLS", "Incomplete measurement", "Monitoring statistic"]},
    {"article name": "Simultaneous regular and non-regular production scheduling of multipurpose batch plants: A real chemical\u2013pharmaceutical case study",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.03.017",
     "publication date": "08-2014",
     "abstract": "Regular and non-regular production can often be found in multipurpose batch plants, requiring two distinct operating strategies: campaign and short-term production. This paper proposes a solution approach for simultaneous scheduling of campaign and short-term products in multipurpose batch plants. Regular products follow a cyclic schedule and must cover several product deliveries during the scheduling horizon, while non-regular products have a non-cyclic schedule. The proposed approach explores the Resource-Task Network (RTN) discrete-time formulation. Moreover, a rolling horizon approach, and reformulation and branching strategies have been applied to deal with the computational complexity of the scheduling problem. Real case instances of a chemical\u2013pharmaceutical industry are solved, showing the applicability of the solution approach.",
     "keywords": ["Multipurpose batch plants", "Campaign and short-term scheduling", "Rolling horizon", "MILP models", "Resource-Task Network"]},
    {"article name": "A parallel Attainable Region construction method suitable for implementation on a graphics processing unit (GPU)",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.04.001",
     "publication date": "08-2014",
     "abstract": "A method for computing candidate Attainable Regions (ARs) suitable for implementation on a graphics processing unit (GPU) is discussed. This allows for both fast computation and complex candidate Attainable Regions to be considered. The method is tested on several problems of varying dimension with kinetics that contain multiple steady-states and temperature dependent kinetics. The computation of unbounded regions is also studied. The constructions obtained appear to approximate the AR boundary well and exhibit good convergence in a small number of iteration steps. A brief analysis of the convergence characteristics of the method is also provided and compared to the same algorithm implemented on a conventional multicore central processing unit (CPU).",
     "keywords": ["Attainable Regions", "Reactor networks", "GPGPU", "CUDA", "Chemical reactors", "Parallelization"]},
    {"article name": "Using surrogate models for efficient optimization of simulated moving bed chromatography",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.03.024",
     "publication date": "08-2014",
     "abstract": "A new approach of using computationally cheap surrogate models for efficient optimization of simulated moving bed (SMB) chromatography is presented. Two different types of surrogate models are developed to replace the detailed but expensive full-order SMB model for optimization purposes. The first type of surrogate is built through a coarse spatial discretization of the first-principles process model. The second one falls into the category of reduced-order modeling. The proper orthogonal decomposition (POD) method is employed to derive cost-efficient reduced-order models (ROMs) for the SMB process. The trust-region optimization framework is proposed to implement an efficient and reliable management of both types of surrogates. The framework restricts the amount of optimization performed with one surrogate and provides an adaptive model update mechanism during the course of optimization. The convergence to an optimum of the original optimization problem can be guaranteed with the help of this model management method. The potential of the new surrogate-based solution algorithm is evaluated by examining a separation problem characterized by nonlinear bi-Langmuir adsorption isotherms. By addressing the feed throughput maximization problem, the performance of each surrogate is compared to that of the standard full-order model based approach in terms of solution accuracy, CPU time and number of iterations. The quantitative results prove that the proposed scheme not only converges to the optimum obtained with the full-order system, but also provides significant computational advantages.",
     "keywords": ["Simulated moving bed (SMB) chromatography", "Surrogate models", "Reduced-order modeling", "Trust-region algorithm", "Optimization"]},
    {"article name": "Heat exchanger simulations involving phase changes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.04.002",
     "publication date": "08-2014",
     "abstract": "Heat exchangers in which phase changes occur are widely applied in industry. Condensing vapor streams and boiling liquid streams are important components in almost all flowsheets of chemical processes. The accurate modeling of these systems is important at the design stage so that heat exchangers with adequate heat-transfer area are purchased and installed.The purpose of this paper is to illustrate that some caution must be exercised in developing simulation models for heat exchangers when liquid is vaporized or vapor is condensed. Use of simple standard heat exchanger models can lead to incorrect differential temperature driving forces and gross under-estimation of heat-transfer area.",
     "keywords": ["Heat-exchanger design", "Phase changes", "Condensation", "Vaporization"]},
    {"article name": "MILP-based decomposition algorithm for dimensionality reduction in multi-objective optimization: Application to environmental and systems biology problems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.04.003",
     "publication date": "08-2014",
     "abstract": "Multi-objective optimization has recently gained wider interest in different domains of engineering and science. One major limitation of this approach is that its complexity grows rapidly as we increase the number of objectives. This work proposes a computational framework to reduce the dimensionality of multi-objective optimization (MOO) problems that identifies and eliminates in a systematic manner redundant criteria from the mathematical model. The method proposed builds on a mixed-integer linear programming (MILP) formulation introduced in a previous work by the authors. We illustrate the capabilities of our approach by its application to two case studies: the design of supply chains for hydrogen production and the multi-objective optimization of metabolic networks. Numerical examples show that our method outperforms other existing algorithms for dimensionality reduction.",
     "keywords": ["Dimensionality reduction", "Multi-objective optimization", "Enviromental system biology", "Divide-and-conquer", "MILP", "MOO"]},
    {"article name": "A workflow modeling system for capturing data provenance",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.04.006",
     "publication date": "08-2014",
     "abstract": "A workflow is an abstraction of the steps associated with the underlying work process and is typically modeled as a directed graph. The workflow concept under its various manifestations has been used to model applications in diverse areas, including project planning, manufacturing, scientific experiments, execution of computer software, and publishing. While the Open Provenance Model Core Specification had laid the foundation for defining the key concepts in a workflow, a simplified high level graphical representation of a workflow that is widely applicable is not available. In this paper we describe a novel general framework for building workflows and implementing the associated actions, which will facilitate understanding of work processes across multiple disciplines. As such, most work processes are organized hierarchically with well defined control and management responsibilities. This framework will facilitate integration and coordination of activities across associated domains. Additionally, it will act as a template to refer to the associated metadata as well as reference to access the instance data from archives of completed workflow cases. When a specific case is in progress, a finite state machine will guide the user through the steps and provide up to date information about the current state. We describe the main building blocks in the framework, their functionalities and illustrate the integration of workflows between an experimental and a scientific process.",
     "keywords": ["Workflow modeling", "Knowledge management", "Recipe management", "Data provenance"]},
    {"article name": "Investigation of flow through a computationally generated packed column using CFD and Additive Layer Manufacturing",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.04.005",
     "publication date": "08-2014",
     "abstract": "When analysing packed beds using CFD approaches, producing an accurate geometry is often challenging. Often a computational model is produced from non-invasive imaging of the packed bed using 3d MRI or \u03bc-CT. This work pioneers the exact reverse of this, by creating a physical bed from the computational model using Additive Layer Manufacturing (ALM). The paper focuses on both experimental analysis and computational analysis of packed columns of spheres. A STL file is generated of a packed column formed using a Monte-Carlo packing algorithm, and this is meshed and analysed using Computational Fluid Dynamics. In addition to this, a physical model is created using ALM on a 3d printer. This allows us to analyse the identical bed geometry both computationally and experimentally and compare the two. Pressure drop and flow patterns are analysed within the bed in detail.",
     "keywords": ["Packed Bed", "Pressure drop", "Porous media", "Fluid mechanics", "Computational Fluid Dynamics", "Simulation"]},
    {"article name": "Design and control of distillation processes for methanol\u2013chloroform separation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.03.026",
     "publication date": "08-2014",
     "abstract": "The binary mixture of methanol\u2013chloroform exhibits a minimum-boiling azeotrope with \u223c34\u00a0mol% methanol at 327\u00a0K under atmospheric pressure. In this paper, design and control of alternative distillation processes for separation of methanol\u2013chloroform azeotropic mixture are explored. The steady-state and dynamic simulations are carried out with Aspen Plus and Aspen Dynamics. The comparison in terms of steady-state design is done between homogeneous extractive distillation and pressure-swing distillation processes. The pressure-swing distillation process is found significantly more economical than the homogeneous extractive distillation process. Based on results, a heat-integrated pressure-swing distillation process is considered, and found economically feasible. Thus, the dynamic comparison is done between pressure-swing distillation systems with and without heat integration. The pressure-swing distillation process without heat integration can be controlled using a basic control structure, while the heat-integrated pressure-swing distillation system requires a pressure-compensated temperature control structure. Results show that dynamic controllabilities of both processes are quite similar.",
     "keywords": ["Process design", "Process control", "Pressure-swing distillation", "Extractive distillation", "Pressure compensated", "Azeotropes"]},
    {"article name": "A maximum-likelihood method for estimating parameters, stochastic disturbance intensities and measurement noise variances in nonlinear dynamic models with process disturbances",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.04.007",
     "publication date": "08-2014",
     "abstract": "An improved approximate maximum likelihood algorithm is developed for estimating measurement noise variances along with model parameters and disturbance intensities in nonlinear stochastic differential equation (SDE) models. This algorithm uses a Laplace approximation and B-spline basis functions for approximating the likelihood function of the parameters given the measurements. The resulting Laplace approximation maximum likelihood estimation (LAMLE) algorithm is tested using a nonlinear continuous stirred tank reactor (CSTR) model. Estimation results for four model parameters, two process disturbance intensities and two measurement noise variances are obtained using LAMLE and are compared with results from two other maximum-likelihood-based methods, the continuous-time stochastic method (CTSM) of Kristensen and Madsen (2003) and the Fully Laplace Approximation Estimation Method (FLAEM) (Karimi and McAuley, 2014). Parameter estimations using 100 simulated data sets reveal that the LAMLE estimation results tend to be more precise and less biased than corresponding estimates obtained using CTSM and FLAEM.",
     "keywords": ["Maximum likelihood", "Laplace approximation", "Stochastic differential equation", "Process disturbance", "Measurement noise variance", "Parameter estimation"]},
    {"article name": "Control of milk pasteurization process using model predictive approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.01.018",
     "publication date": "07-2014",
     "abstract": "A milk pasteurization process, a nonlinear process and multivariable interacting system, is difficult to control by the conventional on\u2013off controllers since the on\u2013off controller can handled the temperature profiles for milk and water oscillating over the plant requirements. The multi-variable control approach with model predictive control (MPC) is proposed in this study. The proposed algorithm was tested for control of a milk pasteurization process in four cases of simulation such as set point tracking, model mismatch, difference control and prediction horizons, and time sample. The results for the proposed algorithm show the well performance in keeping both the milk and water temperatures at the desired set points without any oscillation and overshoot and giving less drastic control action compared to the cascade generic model control (GMC) strategy.",
     "keywords": ["Model predictive control", "Generic model control", "Milk pasteurization process"]},
    {"article name": "Black tea cream effect on polyphenols optimization using statistical analysis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.02.016",
     "publication date": "07-2014",
     "abstract": "Black tea cream formation is an inhibitor for the polyphenols separation since it decreases the amount of available polyphenols. Four factors that are considered to have an impact in the amount of tea cream and polyphenols availability are studied: temperature, amount of solids, pH and amount of EDTA.By using a design of experiments instead of a one-factor-at-a-time, additional information such as interaction effects can be obtained. The objective is to determine the optimum combination range for the factors that minimize the cream formation, while maximizing the amount of polyphenols in the clear phase.Statistical analysis is used to determine which factors significantly influence the responses and to generate polynomial models. This is a very effective tool and it indicates that EDTA is the only non-relevant factor. The optimization results in a 37% increase in the yield of theaflavins and a 20% increase in the yield of catechins.",
     "keywords": ["Polyphenols", "Tea cream", "Statistical analysis", "Design of experiments", "Optimization"]},
    {"article name": "Modeling and simulation of a small-scale trickle bed reactor for sugar hydrogenation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.02.025",
     "publication date": "07-2014",
     "abstract": "Laboratory-scale trickle bed reactor was modeled and simulated, taking into account axial dispersion, gas\u2013liquid, liquid\u2013solid and internal mass transfer as well as catalyst deactivation under isothermal conditions. For catalyst particles dynamic and steady state models were developed, including both mass and heat balances. Catalyst deactivation was included in the model by using the final activity concept for the catalyst particles. A well-working numerical algorithm (method of lines) was applied for solving the reactor model with Matlab 7.1 and the results followed experimental trends very well. The steady-state reactor model was based on simultaneous solution of mass balances. The aim was to illustrate how these parabolic partial differential equations could be solved with a step-by-step calculation for a selected geometry. The final model verification was done against experimental data from the hydrogenation of arabinose to arabitol on a ruthenium catalyst.",
     "keywords": ["Fixed bed modeling", "Kinetics", "Heat and mass transfer", "Axial dispersion", "Catalyst effectiveness", "Numerical solution"]},
    {"article name": "Biomass-to-bioenergy and biofuel supply chain optimization: Overview, key issues and challenges",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.11.016",
     "publication date": "07-2014",
     "abstract": "This article describes the key challenges and opportunities in modeling and optimization of biomass-to-bioenergy supply chains. It reviews the major energy pathways from terrestrial and aquatic biomass to bioenergy/biofuel products as well as power and heat with an emphasis on \u201cdrop-in\u201d liquid hydrocarbon fuels. Key components of the bioenergy supply chains are then presented, along with a comprehensive overview and classification of the existing contributions on biofuel/bioenergy supply chain optimization. This paper identifies fertile avenues for future research that focuses on multi-scale modeling and optimization, which allows the integration across spatial scales from unit operations to biorefinery processes and to biofuel value chains, as well as across temporal scales from operational level to strategic level. Perspectives on future biofuel supply chains that integrate with petroleum refinery supply chains and/or carbon capture and sequestration systems are presented. Issues on modeling of sustainability and the treatment of uncertainties in bioenergy supply chain optimization are also discussed.",
     "keywords": ["Supply chain modeling", "Biofuels", "Bioenergy", "Mathematical programming", "Multi-scale modeling"]},
    {"article name": "Multi-period synthesis of optimally integrated biomass and bioenergy supply network",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.02.020",
     "publication date": "07-2014",
     "abstract": "This contribution addresses the multi-period synthesis of an optimally integrated regional biomass and bioenergy supply network through a mixed-integer linear programing (MILP) approach. The production processes from different sources of biomass include first, second, and third generations of biofuels like bioethanol, biodiesel, hydrogen, Fischer Tropsch diesel, and green gasoline. The aim is to maximize the sustainably viable utilization of resources by accounting for the competition between fuels and food production. An MILP model for efficient bioenergy network optimization based on four layers is extended to include several features, such as seasonality and availability of resources, enabling recycles of products and total site heat integration in order to address real-world applications with a systematic decision-making approach. The multi-period optimization of a heat-integrated biorefinery's supply network is performed through maximization of the economic performance. Economically efficient solutions are obtained with optimal selection of raw materials, technologies, intermediate and final product flows, and reduced greenhouse-gas emissions.",
     "keywords": ["DDGS dried distilled grains with solubles", "dried distilled grains with solubles", "EU European Union", "European Union", "FT Fischer Tropsch", "Fischer Tropsch", "GAMS General Algebraic Modeling System", "General Algebraic Modeling System", "GHG greenhouse gas", "greenhouse gas", "GIS geographical information system", "geographical information system", "LCA Life Cycle Assessment or Life Cycle Analysis", "Life Cycle Assessment or Life Cycle Analysis", "MILP mixed-integer linear programming", "mixed-integer linear programming", "MINLP mixed-integer non-linear programming", "mixed-integer non-linear programming", "MOO multi-objective optimization", "multi-objective optimization", "NPV net present value", "net present value", "SOO single-objective optimization", "single-objective optimization", "Biomass supply network", "Biorefinery", "Bioenergy generation", "Mathematical programing", "Multi-period synthesis"]},
    {"article name": "Development of novel control strategies for single-stage autotrophic nitrogen removal: A process oriented approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.01.017",
     "publication date": "07-2014",
     "abstract": "The autotrophic nitrogen removing granular sludge process is a novel and intensified process. However, its stable operation and control remain a challenging issue. In this contribution, a process oriented approach was used to develop, evaluate and benchmark novel control strategies to ensure stable operation and rejection of disturbances. Three novel control strategies were developed, evaluated, and benchmarked against each other: a feedforward control (control structure 1 \u2013 CS#1), a rule-based feedback control (CS#2), and a feedforward\u2013feedback controller, in which the feedback loop updates the set point of the feedforward loop (CS#3). The CS#1 gave the best performance against disturbances in the ammonium concentration, whereas the CS#2 provided the best performance against disturbances in the organic carbon concentration and dynamic influent conditions. The CS#3 rejected both disturbances satisfactorily. Thus, the appropriate design will depend on the specific disturbances in the influent generated in the upstream units of the wastewater treatment plant.",
     "keywords": ["Autotrophic nitrogen removal", "Control", "Process intensification", "Bioprocess modeling", "Anammox"]},
    {"article name": "Multi-objectives, multi-period optimization of district energy systems: III. Distribution networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.02.018",
     "publication date": "07-2014",
     "abstract": "A systematic procedure including process design and integration techniques for designing and operating energy distribution networks, and for transportation of resources is presented in this paper. In the developed model a simultaneous multi-objectives and multi-period optimization is principally investigated. In addition to optimize the transportation of resources/products, the proposed method helps decision makers to decide; which type and size of poly-generation technologies, centralized or decentralized, are best suited for the district, where in the district shall the equipment be located (geographically), how the services should be distributed, and what are the optimal flow, supply and return temperatures of the distribution networks. The design and the extension of distribution networks and transportation of resources, based on the geographical information system (GIS), are the novelties of the present work.",
     "keywords": ["Distribution networks", "Typical operating periods", "District energy systems", "Multi-objectives optimization", "Geographical information system (GIS)", "CO2 mitigation"]},
    {"article name": "Optimizing the tactical planning in the Fast Moving Consumer Goods industry considering shelf-life restrictions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.01.020",
     "publication date": "07-2014",
     "abstract": "This paper addresses the optimization of the tactical planning for the Fast Moving Consumer Goods industry using an MILP model. To prevent unnecessary waste and missed sales, shelf-life restrictions are introduced using three methods. The direct method tracks the age of products directly. While it provides optimal solutions, it is computationally inefficient. The indirect method forces products to leave inventory before the end of their shelf-life. It obtains solutions within a few percent of optimality. Moreover, compared to the direct method, the computational time was on average reduced by a factor 32. The hybrid method models the shelf-life directly in the first and indirectly in the second storage stage. It obtains near optimal solutions and, on average, reduces the required computational time by a factor 5 compared to the direct method. Cases containing up to 25, 100, and 1000 SKUs were optimized using the direct, hybrid and indirect method respectively.",
     "keywords": ["Shelf-life", "Enterprise-Wide Optimization", "Tactical planning", "MILP", "Fast Moving Consumer Goods"]},
    {"article name": "Tactical management for coordinated supply chains",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.02.006",
     "publication date": "07-2014",
     "abstract": "Current supply chain (SC) optimization models deal with material and information flows along few echelons of the SC (\u201cown SC\u201d), minimizing the role of the complex behavior of third parties (raw materials and utilities suppliers, clients, waste and recovery systems, etc.) in the decision-making process of this SC of interest. Third parties are just represented by simplified parameters (capacity, cost, etc.) usually considered constant, but the decisions based on this picture are not adequate when the third parties\u2019 behavior is significantly affected by these decisions or other circumstances, especially when global coordination is attained. In this work, the role of these third parties, which might face different objectives, has been integrated and a solution based on the full SC management problem is proposed. This results on a generic model which may be used to optimize the planning decisions of the multi-product multi-site SC of interest (production/distribution echelons), taking into account the production vs. demand coherence among this SC and the third parties. The features of the proposed model are illustrated using a case study which considers the coordination of a series of resource (energy) generation SCs linked to a production/distribution SC (\u201cSC of interest\u201d). The results show how the behavior of the considered SCs determines the best planning decisions of each organization, which will depend on the way used to coordinated them (e.g. toward less total or individual costs), adding to the PSE science a new point of view which allows all involved organizations to share responsibilities in the system.",
     "keywords": ["Supply chain", "Tactical management", "Supply chains coordination", "SCM"]},
    {"article name": "A MILP (Mixed Integer Linear Programming) decomposition solution to the scheduling of heavy oil derivatives in a real-world pipeline",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.01.004",
     "publication date": "07-2014",
     "abstract": "This paper presents a novel approach to aid the operational decision-making of scheduling activities in a real-world pipeline, transporting heavy oil derivatives, which are products of less aggregate value, such as fuel oils, e.g. marine fuel. These products present special characteristics that influence their transport as the impossibility of being transferred at room temperature, due to their viscosity, or the use of shared tanks for different products. Thus, during the transport of such products, the entire pipeline network (and the tanks) must be maintained heated during all the pumping process. Such characteristics imply that a specific model oriented to this type of problem must be developed. The approach proposed in this work develops a decomposition procedure that uses a sequence of mathematic programming models and heuristics to solve the problem in hand. The proposed approach is tested using a real-world scenario, composed of a pipeline tree system.",
     "keywords": ["Planning", "Scheduling", "Pipeline network", "MILP", "Heavy oil derivates"]},
    {"article name": "Using mathematical knowledge management to support integrated decision-making in the enterprise",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.02.026",
     "publication date": "07-2014",
     "abstract": "The basis of decision-making in the enterprise consists in formally representing the system and its subsystems in models which adequately capture those features which are necessary to reach consistent decisions. This work represents the elements of the enterprise which are included in mathematical models (i.e. decisions, parameters, constraints, performance indicators) in an ontology which captures the knowledge of the mathematical domain. Thus, this ontology relates the mathematical elements of the models to their corresponding semantic representation within the enterprise ontology. As a result, the mathematical symbolic abstractions of a given enterprise element in different models are directly linked to their actual unique meaning, and the integration of decisions in the enterprise is transparent and improved. The purpose of this work is illustrated in a case study related to capacity planning in the supply chain and scheduling problems.",
     "keywords": ["Mathematical modeling", "Decision support systems", "Enterprise integration", "Knowledge management"]},
    {"article name": "Multi-period design and planning of closed-loop supply chains with uncertain supply and demand",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.02.027",
     "publication date": "07-2014",
     "abstract": "A design and planning approach is proposed for addressing general multi-period, multi-product closed-loop supply chains (CLSCs), structured as a 10-layer network (5 forward plus 5 reverse flows), with uncertain levels in the amount of raw material supplies and customer demands. The consideration of a multi-period setting leads to a multi-stage stochastic programming problem, which is handled by a mixed-integer linear programming (MILP) formulation. The effects of uncertain demand and supply on the network are considered by means of multiple scenarios, whose occurrence probabilities are assumed to be known. Several realistic supply chain requirements are taken into account, such as those related to the operational and environmental costs of different transportation modes, as well as capacity limits on production, distribution and storage. Moreover, multiple products are considered, which are grouped according to their recovery grade. The objective function minimizes the expected cost (that includes facilities, purchasing, storage, transport and emissions costs) minus the expected revenue due to the amount of products returned, from repairing and decomposition centers to the forward network. Finally, computational results are discussed and analyzed in order to demonstrate the effectiveness of the proposed approach. Due to the large size of the addressed optimization problem containing all possible scenarios for the two uncertain parameters, scenario reduction algorithms are applied to generate a representative, albeit smaller, subset of scenarios.",
     "keywords": ["Closed-loop supply chains", "Mathematical modeling", "Multi-stage stochastic approach"]},
    {"article name": "Robust optimization and stochastic programming approaches for medium-term production scheduling of a large-scale steelmaking continuous casting process under demand uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.02.028",
     "publication date": "07-2014",
     "abstract": "Scheduling of steelmaking-continuous casting (SCC) processes is of major importance in iron and steel operations since it is often a bottleneck in iron and steel production. In practice, uncertainties are unavoidable and include demand fluctuations, processing time uncertainty, and equipment malfunction. In the presence of these uncertainties, an optimal schedule generated using nominal parameter values may often be suboptimal or even become infeasible. In this paper, we introduce robust optimization and stochastic programming approaches for addressing demand uncertainty in steelmaking continuous casting operations. In the robust optimization framework, a deterministic robust counterpart optimization model is introduced to guarantee that the production schedule remains feasible for the varying demands. Also, a two-stage scenario based stochastic programming framework is investigated for the scheduling of steelmaking and continuous operations under demand uncertainty. To make the resulting stochastic programming problem computationally tractable, a scenario reduction method has been applied to reduce the number of scenarios to a small set of representative realizations. Results from both the robust optimization and stochastic programming methods demonstrate robustness under demand uncertainty and that the robust optimization-based solution is of comparable quality to the two-stage stochastic programming based solution.",
     "keywords": ["Scheduling", "Steelmaking", "Continuous casting", "Robust optimization", "Two-stage stochastic programming", "Demand uncertainty"]},
    {"article name": "A systematic framework for onsite design and implementation of a control system in a continuous tablet manufacturing process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.02.029",
     "publication date": "07-2014",
     "abstract": "A novel manufacturing strategy based on continuous processing integrated with online/inline monitoring tools coupled with an advanced control system is highly desired for efficient Quality by Design (QbD)-based pharmaceutical manufacturing. A control system ensures the predefined end product quality, satisfies the high regulatory constraints, facilitates real time release of the product, and optimizes the resources. In this work, a systematic framework for the onsite design and implementation of the control system in continuous tablet manufacturing process has been developed. The framework includes a generic methodology and supporting tools through which the control system can be designed at the manufacturing site and can be implemented for closed-loop operation. The control framework has different novel features such as the option to run the plant in closed-loop (MPC/PID), open-loop and simulation mode. NIR sensor, an online prediction tool, a PAT data management tool, and a control platform have been used to close the control loop.",
     "keywords": ["Systematic framework", "Control system implementation", "Model predictive control", "Pharmaceutical", "Continuous processing"]},
    {"article name": "A systematic methodology for design of tailor-made blended products",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.12.011",
     "publication date": "07-2014",
     "abstract": "A systematic methodology for design of tailor-made blended products has been developed. In tailor-made blended products, one identifies the product needs and matches them by blending different chemicals. The systematic methodology has four main tasks. First, the design problem is defined: the product needs are identified, translated into target properties and the bounds for each target property are defined. Secondly, target property models are retrieved from a property model library. Thirdly, a mixture/blend design algorithm is applied to obtain the mixtures/blends that match the design targets. The result is a set of blends that match the constraints, the composition of the chemicals present in the blend, and the values of the target properties. Finally, the mixture target property values are verified by means of rigorous models for the properties and the mixtures. In this paper, the methodology is highlighted through two case studies involving gasoline blends and lubricant base oils.",
     "keywords": ["2BE 2-butanone", "2-butanone", "2MT 2-methyltricosane", "2-methyltricosane", "3ET 3-ethyltetracosane", "3-ethyltetracosane", "3ME 3-methyleicosane", "3-methyleicosane", "9ODA cis-9-octadecenoic acid", "cis-9-octadecenoic acid", "ACE acetone", "acetone", "DFE 1H-dibenzo[a,i]fluorene,eicosahydro-", "1H-dibenzo[a,i]fluorene,eicosahydro-", "ETOH ethanol", "ethanol", "G gasoline", "gasoline", "GLY propane-1,2,3-triol", "propane-1,2,3-triol", "MeTHF furan,tetrahydro-2-methyl-", "furan,tetrahydro-2-methyl-", "MI main ingredient", "main ingredient", "MoT modeling tool", "modeling tool", "PE polyethylene", "polyethylene", "THF tetrahydrofuran", "tetrahydrofuran", "WCO waste cooking oil", "waste cooking oil", "Product design", "Blended product", "Gasoline blend", "Lubricant base oil"]},
    {"article name": "Prediction of the full molecular weight distribution in RAFT polymerization using probability generating functions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.02.017",
     "publication date": "07-2014",
     "abstract": "In this work, a model for the RAFT polymerization following the slow fragmentation approach was developed in order to obtain the full molecular weight distribution (MWD) using probability generating functions (pgf). A combination of univariate and bivariate pgf is applied to deal with the univariate chain length distributions of macroradical, dormant and dead polymer chains, and the bivariate distribution of the two arms intermediate adduct. This allows rigorous modeling of the polymerization system without simplifying assumptions. For comparison purposes, the population balances were solved by direct integration of the resulting equations. Our results show that the pgf technique allows obtaining an accurate solution efficiently in terms of computational time. What is more, the model provides a detailed characterization of the polymer that could be of great help for grasp the process fundamentals.",
     "keywords": ["Modeling", "Molecular weight distribution", "RAFT polymerization", "Probability generating function", "Slow fragmentation"]},
    {"article name": "The use of global sensitivity analysis for improving processes: Applications to mineral processing",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.01.008",
     "publication date": "07-2014",
     "abstract": "This paper analyzes the application of global sensitivity analysis (GSA) to the improvement of processes using various case studies. First, a brief description of the methods applied is given, and several case studies are examined to show how GSA can be applied to the study to improve the processes. The case studies include the identification of processes; comparisons of the Sobol, E-FAST and Morris GSA methods; a comparison of GSA with local sensitivity analysis; an examination of the effect of uncertainty levels and the type of distribution function on the input factors; and the application of GSA to the improvement of a copper flotation circuit. We conclude that GSA can be a useful tool in the analysis, comparison, design and characterization of separation circuits. In addition, we conclude that using the stage's recoveries of each species as input factors is a suitable choice for the GSA of a flotation plant.",
     "keywords": ["Mineral processing", "Flotation", "Process analysis", "Global sensitivity analysis", "Retrofit"]},
    {"article name": "Analysis of optimal operation of a fed-batch emulsion copolymerization reactor used for production of particles with core\u2013shell morphology",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.02.021",
     "publication date": "07-2014",
     "abstract": "In this paper dynamic optimization of a lab-scale semi-batch emulsion copolymerization reactor for styrene and butyl acrylate in the presence of a chain transfer agent (CTA) is studied. The mathematical model of the process, previously developed and experimentally validated, is used to predict the glass transition temperature of produced polymer, the number and weight average molecular weights, the monomers global conversion, the particle size distribution, and the amount of residual monomers. The model is implemented within gPROMS environment for modeling and optimization. It is desired to compute feed rate profiles of pre-emulsioned monomers, inhibitor and CTA that will allow the production of polymer particles with prescribed core\u2013shell morphology with high productivity. The results obtained for different operating conditions and various additional product specifications are presented. The resulting feeding profiles are analyzed from the perspective of the nature of emulsion polymerization process and some interesting conclusions are drawn.",
     "keywords": ["Emulsion copolymerization", "Dynamic optimization", "Chain transfer agent", "Fedbatch process", "Control vector parameterization"]},
    {"article name": "Model-based optimization of sulfur recovery units",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.01.019",
     "publication date": "07-2014",
     "abstract": "Multi-scale process modeling is very appealing methodology for process optimization since it highlights certain issues that remain unexplored with conventional methodologies and debottlenecks certain potentialities that remain unexploited in chemical plants. In this work, a kinetic model with 2400 reactions and 140 species is implemented in a proper reactor network to characterize the thermal furnace and the waste heat boiler of sulfur recovery units; the network with detailed kinetics is the kernel of a Claus process simulation that includes all the unit operations and the catalytic train. By doing so, reliable estimation of acid gas conversion, elemental sulfur recovery, and steam generation is achieved with the possibility to carry out an integrated process-energy optimization at the total plant scale.",
     "keywords": ["Acid gas", "Detailed kinetics", "Claus process", "Total plant optimization"]},
    {"article name": "Leak identification in a water distribution network using sparse flow measurements",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.11.017",
     "publication date": "07-2014",
     "abstract": "A linear programming technique is proposed to balance the flows in a network, matching flow measurements where available. Where necessary, a \u201cleak-out\u201d flow is invoked on a pipe section in order to achieve the balance. Usually, multiple solutions are possible, and these are sounded out by progressively increasing an integrity weight for each pipe section. A feature of the method is that it overlays \u201csnapshots\u201d of the network at a series of points in time, in order to progressively narrow down the part of the network which can commonly account for all observations.",
     "keywords": ["Reconciliation", "Clustering", "Fault detection", "Linear programming"]},
    {"article name": "Semantic input/output matching for waste processing in industrial symbiosis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.02.010",
     "publication date": "07-2014",
     "abstract": "Industrial symbiosis (IS) is a subdiscipline of Industrial Ecology that aims to bring together companies from different sectors to share resources, namely energy, materials, and water. The main goal of IS is to improve resource (materials, waste, energy) efficiency and lead to mutual environmental, financial and social benefits to participants.In this paper we present a semantic approach for IS input/output matching. This approach is based on knowledge modelling and ontologies.Ontologies are used to model all resources \u2013 waste, water, energy \u2013 along with details about their composition, characteristics (chemical and physical) and tacit knowledge about their flow.The input/output matching algorithm presented enables the valorisation of resources through industrial symbiosis networks.",
     "keywords": ["Semantics", "Ontology", "Industrial symbiosis", "Waste processing"]},
    {"article name": "Assessment of control techniques for the dynamic optimization of (semi-)batch reactors",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.02.022",
     "publication date": "07-2014",
     "abstract": "This work investigates how batch reactors can be optimized to increase the yield of a desired product coupling two appealing techniques for process control and optimization: the nonlinear model predictive control (NMPC) and the dynamic real-time optimization (D-RTO). The overall optimization problem is formulated and applied to calculate the optimal operating parameters of a selected case study and the numerical results are compared to the traditional control/optimization techniques. It has been demonstrated in our previous work (Pahija et al. (2013). Selecting the best control methodology to improve the efficiency of discontinuous reactors. Computer Aided Chemical Engineering, 32, 805\u2013810) that the control strategy can significantly affect optimization results and that the appropriate selection of the control methodology is crucial to obtain the real operational optimum (with some percent of improved yield). In this context, coupling NMPC and D-RTO seems to be the ideal way to improve the process yield. The results presented in this work have been obtained by using gPROMS\u00ae and MS C++ with algorithms of BzzMath library.",
     "keywords": ["Dynamic optimization", "Batch reactors", "Control techniques", "NMPC"]},
    {"article name": "Asynchronous optimisation with the use of a cascade search algorithm",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.02.009",
     "publication date": "07-2014",
     "abstract": "This paper introduces the development of an asynchronous approach coupled with a cascade optimisation algorithm. The approach incorporates concepts of asynchronous Markov processes and introduces a search process that is benefiting from distributed computing infrastructures. The algorithm uses concepts of partitions and pools to store intermediate solutions and corresponding objectives. Population inflections are performed periodically to ensure that Markov processes, still independent and asynchronous, make arbitrary use of intermediate solutions. Tested against complex optimisation problems and in comparison with commonly used Tabu Search, the asynchronous cascade algorithm demonstrates a significant potential in distributed operations with favourable comparisons drawn against synchronous and quasi-asynchronous versions of conventional algorithms.",
     "keywords": ["Markov processes", "Asynchronous optimisation", "Parallel and distributed computing"]},
    {"article name": "Heterogeneous parallel method for mixed integer nonlinear programming",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.11.009",
     "publication date": "07-2014",
     "abstract": "In a heterogeneous parallel structure, two types of algorithms, Quesada Grossmann's (QG) algorithm and Tabu search (TS), are used to solve mixed integer nonlinear programming (MINLP) simultaneously. Communication is well designed between two threads running the two algorithms individually by exchanging three kinds of information during iterations. First, the best feasible solution in TS can become a valid upper bound for QG. Second, new linearization which can further tighten the lower bound of QG can be generated at the node provided by the TS. Third, additional integer variables can be fixed in QG, thus reducing the search space of TS. Numerical results show that good performance can be achieved by using the proposed method. Further analysis reveals that the heterogeneous method has the potential for superlinear speedup, which may surpass that of the traditional homogeneous parallel method for solving MINLPs.",
     "keywords": ["Mixed integer nonlinear programming", "Parallel computing", "Heterogeneous method"]},
    {"article name": "Comparison of techniques for reconstruction of a distribution from moments in the context of a pharmaceutical drying process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.02.008",
     "publication date": "06-2014",
     "abstract": "The use of moment-based methods to solve a Population Balance Model (PBM) induces the need to reconstruct a distribution from the moments for system analysis. Several reconstruction methods are investigated (i.e. parameter fitting methods and the method of splines), compared with each other as well as with the result of a non-moment-based solution method for the PBM. The finetuning of the parameters for the method of splines was very important for the final result as well as for the computational time. An additional parameter, i.e. a different value for the first and the last interval for tolred, was introduced to improve the result and speed up the calculation. None of the parameter fitting methods was able to correctly predict several peaks in the final distribution. In contrast, the method of splines was able to reconstruct the distribution even without prior knowledge. However, prior knowledge about the distribution does facilitate the finetuning.",
     "keywords": ["PBM", "Reconstruction methods", "Mathematical modelling", "Solution methodology", "Pharmaceuticals"]},
    {"article name": "Recursive constrained state estimation using modified extended Kalman filter",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.02.013",
     "publication date": "06-2014",
     "abstract": "The extended Kalman filter (EKF) remains the most preferred state estimator for solving both unconstrained and constrained state estimation problems in the field of Chemical Engineering. Given, the wide spread use of EKF, we have proposed a novel optimization free recursive formulation of the EKF, to handle elegantly bounds on the estimated state variables of a stochastic non-linear dynamic system. It is well known that in the EKF, the prior and posterior distributions are approximated to be a multivariate normal distribution. In the presence of bounds imposed on the state variables, the accuracy of the first two moments of the initial state distribution and prior distribution namely the means and covariance matrices, plays a significant role in the extended Kalman filter performance. Hence, in this paper, we propose two novel schemes to modify the prior and posterior distributions of the EKF in order to satisfy the bound constraints. In addition, the initial state distribution is also suitably modified in order to satisfy the bound constraints. The efficacy of the proposed state estimation schemes using the EKF is validated on two benchmark problems reported in the literature namely a simulated gas-phase reactor and an isothermal batch reactor involving constraints on estimated state variables. Extensive simulation studies show the effectiveness of the proposed optimization free recursive constrained state estimation schemes using extended Kalman filter.",
     "keywords": ["Recursive Bayesian state estimation", "Constrained state estimation", "Nonlinear state estimators", "Extended Kalman filter", "Non-linear dynamic data reconciliation and truncated distributions"]},
    {"article name": "Novel fluid grid and voidage calculation techniques for a discrete element model of a 3D cylindrical fluidized bed",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.02.019",
     "publication date": "06-2014",
     "abstract": "A discrete element model (DEM) combined with computational fluid dynamics (CFD) was developed to model particle and fluid behaviour in 3D cylindrical fluidized beds. Novel techniques were developed to (1) keep fluid cells, defined in cylindrical coordinates, at a constant volume in order to ensure the conditions for validity of the volume-averaged fluid equations were satisfied and (2) smoothly and accurately measure voidage in arbitrarily shaped fluid cells. The new technique for calculating voidage was more stable than traditional techniques, also examined in the paper, whilst remaining computationally-effective. The model was validated by quantitative comparison with experimental results from the magnetic resonance imaging of a fluidised bed analysed to give time-averaged particle velocities. Comparisons were also made between theoretical determinations of slug rise velocity in a tall bed. It was concluded that the DEM-CFD model is able to investigate aspects of the underlying physics of fluidisation not readily investigated by experiment.",
     "keywords": ["Discrete element model", "Computation fluid mechanics", "Fluidization", "Voidage", "Granular material"]},
    {"article name": "Event-based predictive control of pH in tubular photobioreactors",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.03.001",
     "publication date": "06-2014",
     "abstract": "This work presents the application of an event-based model predictive control algorithm to regulate the pH in a microalgae production process. The control aim is to maintain the pH within specific limits and to minimize CO2 losses. The control scheme is based on a Generalized Predictive Control (GPC) algorithm with sensor deadband approach. In this algorithm, the controller execution frequency is adapted to the process dynamics. The event-based scheme works with low sampling frequency if controlled variable, pH in this case, is inside an established band. Otherwise, when the pH value is outside the band, the controller actuation frequency is increased to try to drive it quickly near the setpoint based on the selected tolerance. In such a way, the event-based control algorithm allows to establish a tradeoff between control performance and number of process update actions. This fact can be directly related with reduction of CO2 injection times, what is also reflected in CO2 losses. The control structure is first evaluated through simulations using a nonlinear model for microalgal production in tubular photobioreactors. Afterwards, real experiments are presented on an industrial photobioreactor in order to verify the results obtained through simulations. Additionally, the real tests on the industrial plant are used to verify the event-based control scheme in the case of plant-model mismatch, presence of measurement noise and disturbances. Moreover, the control results are compared with classical time-based solutions using well-known control performance indexes.",
     "keywords": ["Event-based control", "Model Predictive Control", "Generalized Predictive Control", "pH control", "Microalgae"]},
    {"article name": "A knowledge-based ingredient formulation system for chemical product development in the personal care industry",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.03.004",
     "publication date": "06-2014",
     "abstract": "The formulation of personal care products involves a trial-and-error approach to testing different combinations of chemicals. Specific knowledge plays an important role in creating the desired product properties. Without knowledge support tools, the formulation process becomes iterative. Furthermore, personal care products cannot be designed without analyzing market needs, and their development thus involves the collaboration of the formulators and the marketing teams. Miscommunication can reduce the efficiency and effectiveness of the process. This paper presents a knowledge-based ingredient formulation system for supporting chemical product development in the personal care industry. Case-based reasoning is used to solve ingredient formulation problems with reference to how similar past problems have been solved. The system also acts as a collaborative platform for sharing knowledge among the various stakeholders. A case study confirms the viability of the system, and the results show that the system provides formulators with key knowledge, enabling effective product formulation.",
     "keywords": ["Knowledge management", "New product development", "Ingredient formulation", "Chemical product", "Personal care industry", "Case-based reasoning"]},
    {"article name": "Multi-objectives, multi-period optimization of district energy systems: I. Selection of typical operating periods",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.03.005",
     "publication date": "06-2014",
     "abstract": "The long term optimization of a district energy system is a computationally demanding task due to the large number of data points representing the energy demand profiles.In order to reduce the number of data points and therefore the computational load of the optimization model, this paper presents a systematic procedure to reduce a complete data set of the energy demand profiles into a limited number of typical periods, which adequately preserve significant characteristics of the yearly profiles. The proposed method is based on the use of a k-means clustering algorithm assisted by an \u03f5-constraints optimization technique. The proposed typical periods allow us to achieve the accurate representation of the yearly consumption profiles, while significantly reducing the number of data points.The work goes one step further by breaking up each representative period into a smaller number of segments. This has the advantage of further reducing the complexity of the problem while respecting peak demands in order to properly size the system.Two case studies are discussed to demonstrate the proposed method. The results illustrate that a limited number of typical periods is sufficient to accurately represent an entire equipments\u2019 lifetime.",
     "keywords": ["Typical periods", "District energy systems", "Mixed integer linear programming", "Evolutionary algorithm", "Multi-objective optimization", "Cluster analysis", "k-means algorithm"]},
    {"article name": "Optimal planning of oil and gas development projects considering long-term production and transmission",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.03.002",
     "publication date": "06-2014",
     "abstract": "This paper proposes an integrated model for making a group of strategic decisions about oil and gas development projects simultaneously over a long-term planning horizon. These decisions involve: selection of field and pipeline development projects, scheduling of selected projects, production planning, and upstream transmission planning. The proposed model is formulated as a linear mixed-integer-programming model. It is implemented in a case study to demonstrate its usefulness and applicability in practice. Finally, a number of sensitivity analyses are carried out to analyze the impact of most influential uncertainties on the solutions and the corresponding results are discussed.",
     "keywords": ["Oil and gas development", "Production planning", "Transmission planning", "Mathematical modeling", "Optimization"]},
    {"article name": "Optimization of heat integration with variable stream data and non-linear process constraints",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.03.010",
     "publication date": "06-2014",
     "abstract": "Two novel formulations for the optimization of heat integration of chemical processes with variable stream data and non-linear process constraints are proposed. An NLP formulation utilizes a concept of pseudo stream temperatures and the Plus/Minus Principles simplifies the formulation with tight constraints. The NLP model is efficiency but the use of bi-linear constraints might sometimes deteriorate the solution quality comparing to the conventional Big-M disjunctive model. To overcome this, a Multi-M model is proposed that applies the same concepts as in the NLP model together with a set of Multi-M constraints. The Multi-M model significantly reduces the number of binary variables and minimizes the size of Ms for the use in the Multi-M constraints. Results show that the NLP model spends least time in solution but sometimes converges too soon at near or local optimum. The Multi-M model is the most robust and still maintains a high efficiency in solution quality and speed. For problem with non-linear process constraints, both NLP and Multi-M models perform much better than the traditional Big-M model.",
     "keywords": ["Heat integration", "Simultaneous optimization", "Big-M", "Multi-M"]},
    {"article name": "Optimal multi-scale capacity planning for power-intensive continuous processes under time-sensitive electricity prices and demand uncertainty. Part I: Modeling",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.01.016",
     "publication date": "06-2014",
     "abstract": "Time-sensitive electricity prices (as part of so-called demand-side management in the smart grid) offer economical incentives for large industrial customers. In part I of this paper, we propose an MILP formulation that integrates the operational and strategic decision-making for continuous power-intensive processes under time-sensitive electricity prices. We demonstrate the trade-off between capital and operating expenditures with an industrial case study for an air separation plant. Furthermore, we compare the insights obtained from a model that assumes deterministic demand with those obtained from a stochastic demand model. The value of the stochastic solution (VSS) is discussed, which can be significant in cases with an unclear setup, such as medium baseline product demand and growth rate, large variance or skewed demand distributions. While the resulting optimization models are large-scale, they can be solved within three days of computational time. A decomposition algorithm for speeding-up the solution time is described in part II.",
     "keywords": ["Multi-scale optimization", "Capacity planning", "Power-intensive process", "Demand-side management", "Smart grid", "Stochastic programming"]},
    {"article name": "Optimal multi-scale capacity planning for power-intensive continuous processes under time-sensitive electricity prices and demand uncertainty. Part II: Enhanced hybrid bi-level decomposition",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.02.012",
     "publication date": "06-2014",
     "abstract": "We describe a hybrid bi-level decomposition scheme that addresses the challenge of solving a large-scale two-stage stochastic programming problem with mixed-integer recourse, which results from a multi-scale capacity planning problem as described in Part I of this paper series. The decomposition scheme combines bi-level decomposition with Benders decomposition, and relies on additional strengthening cuts from a Lagrangean-type relaxation and subset-type cuts from structure in the linking constraints between investment and operational variables. The application of the scheme with a parallel implementation to an industrial case study reduces the computational time by two orders of magnitude when compared with the time required for the solution of the full-space model.",
     "keywords": ["Stochastic programming", "Integer recourse", "Decomposition", "Demand-side management", "Smart grid"]},
    {"article name": "Discrete element simulation of particle mixing and segregation in a tetrapodal blender",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.12.009",
     "publication date": "05-2014",
     "abstract": "One aspect that must be addressed when designing tumbling blenders is poor axial mixing, which can lead to non-homogeneous mixtures, especially when the particle physical and flow properties are different. To overcome these limitations, we recently undertook an interest in a tetrapodal mixing device patented in 1964. It can be described as two V-shaped pairs of arms connected at their bottoms, one of which is twisted by 90\u00b0. In this work, particle mixing and segregation are investigated using the discrete element method in both the V-blender and this tetrapodal blender. Results of mixing time and uniformity are compared for different loading profiles, fill levels and rotational speeds. Compared to the V-blender, this geometry is shown to provide better axial and radial mixing efficiency. Good behavior was also observed for size segregating granules, yet more investigation would be needed for worse cases involving granules with large size ratios and different densities.",
     "keywords": ["Tetrapodal blender", "V-blender", "Solids mixing", "Segregation", "Discrete element method"]},
    {"article name": "Multivariate video analysis and Gaussian process regression model based soft sensor for online estimation and prediction of nickel pellet size distributions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.01.010",
     "publication date": "05-2014",
     "abstract": "Accurate measurement and prediction of pellet size distributions are critically important for material processing because they are essential for model predictive control, real-time optimization, planning, scheduling and decision support of material production. Mechanical sieving is one of the traditional methods for pellet size measurement in industrial practice but cannot be applied in real-time fashion. Alternately, multivariate image analysis based pellet sizing methods may acquire the size information non-intrusively and thus can be implemented for on-line measurement in industrial applications. Nevertheless, the conventional multivariate image analysis based pellet sizing methods cannot effectively deal with the pellet overlapping effects in the still images, which may lead to inaccurate and unreliable measurements of size distributions. In our study, two novel video analysis based pellet sizing methods are proposed for measuring the pellet size distributions without any off-line and intrusive tests. The videos of free-falling pellets are first taken and then the free-falling tracks of pellets in video frames are analyzed through the two video analysis based pellet sizing approaches. In the first video analysis method, the Sobel edge detection strategy is adopted to identify and isolate the free-falling tracks in order to estimate the diameters of the corresponding pellets. For the second video analysis approach, the filtered gray-scale video frames are scanned row by row and then the particle diameters are estimated and predicted through the built Gaussian process regression (GPR) models and a fine designed counting rule so as to eliminate the overlapping effects of nickel pellets along the horizontal and vertical directions. The utility of these two video analysis based pellet sizing methods is demonstrated through the measurement and estimation of free-falling nickel pellets in two test videos.",
     "keywords": ["Pellet size distribution", "Multivariate image processing", "Video analysis", "Edge detection", "Soft sensor", "Gaussian process regression"]},
    {"article name": "Performance comparison of parameter estimation techniques for unidentifiable models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.01.009",
     "publication date": "05-2014",
     "abstract": "Four different estimation approaches exploiting sensitivities, eigenvalue analysis (rotational discrimination and automatic parameter selection and estimation), reparameterization via differential geometry and the classical nonlinear least squares are assessed in terms of predictivity, robustness and speed. A Monte Carlo methodology is adopted to evaluate the statistical information required to quantify the inherent uncertainty of each approach. The results show that the rotational discrimination method presents the best characteristics among the evaluated methods, since it requires less a priori information than the reparameterization via differential geometry, uses simpler stop criteria than the automatic selection, reduces the overfitting caused by the nonlinear least squares solution and because it estimates parameters with the best predictivity among the methods tested. Additionally, results suggest that assessing the goodness of the estimated parameters solely in the calibration set can be misleading, and that the statistical information obtained from a validation set is more valuable.",
     "keywords": ["Parameter estimation", "Identifiability", "Biased estimators", "Chemical kinetics"]},
    {"article name": "Adjoint-based estimation and optimization for column liquid chromatography models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.01.013",
     "publication date": "05-2014",
     "abstract": "Simulation and optimization of chromatographic processes are continuously gaining practical importance, as they allow for faster and cheaper process development. Although a lot of effort has been put into developing numerical schemes for simulation, fast optimization and estimation algorithms also are of importance. To determine parameters for an a priori defined model, a suited approach is the inverse method that fits the measurement data to the model response.This paper presents an adjoint method to compute model parameter derivatives for a wide range of differentiable liquid chromatography models and provides practical information for the implementation in a generic simulation framework by the example of ion-exchange chromatography.The example shows that the approach is effective for parameter estimation of model proteins and superior to forward sensitivities in terms of computational effort. An optimization of peak separation in salt step elution demonstrates that the method is not restricted to inverse parameter estimation.",
     "keywords": ["Column chromatography", "Inverse modeling", "Optimization", "Sensitivities", "Adjoint method"]},
    {"article name": "Automatic qualitative trend simulation method for diagnosing faults in industrial processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.01.007",
     "publication date": "05-2014",
     "abstract": "This work proposes an automatic method of qualitative simulation for industrial processes to predict the steady-state measurement patterns arising from different faults. Due to their characteristics, qualitative simulations tend to generate multiple spurious solutions. The proposed method limits the number of such spurious solutions by automatically generating new qualitative equations from the generic quantitative model (i.e. without the need of knowing the value of its parameters) and using simple qualitative known relations or other readily available information. An algorithm to obtain such solutions from the set of qualitative equations is also presented.",
     "keywords": ["Qualitative simulation", "Fault diagnosis", "Confluences"]},
    {"article name": "Modeling and global optimization of DNA separation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.01.012",
     "publication date": "05-2014",
     "abstract": "We develop a non-convex non-linear programming problem that determines the minimum run time to resolve different lengths of DNA using a gel-free micelle end-labeled free solution electrophoresis separation method. Our optimization framework allows for an efficient determination of the utility of different DNA separation platforms and enables the identification of the optimal operating conditions for these DNA separation devices. The non-linear programming problem requires a model for signal spacing and signal width, which is known for many DNA separation methods. As a case study, we show how our approach is used to determine the optimal run conditions for micelle end-labeled free-solution electrophoresis and examine the trade-offs between a single capillary system and a parallel capillary system. Parallel capillaries are shown to only be beneficial for DNA lengths above 230 bases using a polydisperse micelle end-label otherwise single capillaries produce faster separations.",
     "keywords": ["Global optimization", "Nonlinear programming", "DNA electrophoresis", "DNA separation", "End-labeled free-solution electrophoresis", "ELFSE"]},
    {"article name": "Economic model predictive control for inventory management in supply chains",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.01.003",
     "publication date": "05-2014",
     "abstract": "In this paper, we propose economic model predictive control with guaranteed closed-loop properties for supply chain optimization. We propose a new multiobjective stage cost that captures economics as well as risk at a node, using a weighted sum of an economic cost and a tracking stage cost. We also demonstrate integration of scheduling with control using a supply chain example. We integrate a scheduling model for a multiproduct batch plant with a control model for inventory control in a supply chain. We show recursive feasibility of such integrated control problems by developing simple terminal conditions.",
     "keywords": ["Economic model predictive control", "Supply chain optimization", "Inventory control", "Scheduling"]},
    {"article name": "Optimal design of reactive distillation systems: Application to the production of ethyl tert-butyl ether (ETBE)",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.01.014",
     "publication date": "05-2014",
     "abstract": "This work addresses the design of reactive distillation columns to produce ETBE, based on a detailed first-principles model that considers equilibrium and kinetic information, rigorous physical property data, and catalyst deactivation. An evolutionary algorithm is used to generate a sequence of feasible designs with improved characteristics in a sequential solution/optimisation strategy, by specifying the design variables (both integer and continuous) that characterise a particular column configuration. Two classes of optimisation algorithms are compared: genetic algorithms and particle swarm optimisation. The objective function considered is the gross annual profit.The results demonstrate that both algorithms are adequate to solve this design problem. The effect of catalyst deactivation included in the design stage played a determinant role in the optimal column specification. A post-design sensitivity analysis is developed to assess the quality of the solutions obtained, together with the individual effects of each design variable in the optimal configuration identified.",
     "keywords": ["Reactive distillation", "Catalyst deactivation", "Dynamic optimisation", "Evolutionary algorithms", "ETBE production"]},
    {"article name": "EVHE \u2013 A new method for the synthesis of HEN",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.01.015",
     "publication date": "05-2014",
     "abstract": "In this paper a new algorithm for the synthesis of cost optimal heat exchanger networks (HEN), called enhanced vertical heat exchange (EVHE), is introduced. It adopts aspects of the conventional vertical heat exchange but differs where, instead of using composite curves (CC), a new form of graphic depiction of the problem's data in a temperature\u2013enthalpy diagram is implemented. Another deviation from the conventional heuristics is to allow parallel and crossed connections of streams in the same diagram. The resulting HENs have in common that they are the solutions with a minimum number of heat exchangers (HE) for a default amount of integrated heat. Furthermore, the new method allows the option to take into account varying heat transfer coefficients by means of a substitution of the streams\u2019 data with virtual temperatures. The EVHE algorithm proved to produce better results on problems taken from the literature.",
     "keywords": ["Heat exchanger network synthesis", "Vertical heat exchange", "Composite curves", "Heat recovery"]},
    {"article name": "Stochastic optimal control model for natural gas networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.02.002",
     "publication date": "05-2014",
     "abstract": "We present a stochastic optimal control model to optimize gas network inventories in the face of system uncertainties. The model captures detailed network dynamics and operational constraints and uses a weighted risk-mean objective. We perform a degrees-of-freedom analysis to assess operational flexibility and to determine conditions for model consistency. We compare the control policies obtained with the stochastic model against those of deterministic and robust counterparts. We demonstrate that the use of risk metrics can help operators to systematically mitigate system volatility. Moreover, we discuss computational scalability issues and effects of discretization resolution on economic performance.",
     "keywords": ["Stochastic optimization", "Natural gas", "Real-time", "Optimal control"]},
    {"article name": "Adaptive gain sliding mode observer for state of charge estimation based on combined battery equivalent circuit model",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.02.015",
     "publication date": "05-2014",
     "abstract": "An adaptive gain sliding mode observer (AGSMO) for battery state of charge (SOC) estimation based on a combined battery equivalent circuit model (CBECM) is presented. The error convergence of the AGSMO for the SOC estimation is proved by Lyapunov stability theory. Comparing with conventional sliding mode observers for the SOC estimation, the AGSMO can minimise chattering levels and improve the accuracy by adaptively adjusting switching gains to compensate modelling errors. To design the AGSMO for the SOC estimation, the state equations of the CBECM are derived to capture dynamics of a battery. A lithium-polymer battery (LiPB) is used to conduct experiments for extracting parameters of the CBECM and verifying the effectiveness of the proposed AGSMO for the SOC estimation.",
     "keywords": ["Adaptive gain sliding mode observer", "Battery management system", "Combined battery equivalent circuit model", "Electric vehicle", "Lithium-polymer battery", "State of charge"]},
    {"article name": "Language-oriented rule-based reaction network generation and analysis: Algorithms of RING",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.02.007",
     "publication date": "05-2014",
     "abstract": "The underlying algorithms of the language interface and post-generation analysis modules in RING, a network generation and analysis tool, are discussed. The front-end is a domain-specific reaction language developed with Silver, a meta-language based on attribute grammars. The language compiler translates user inputs written as a program into internal instructions, catches syntactic and semantic errors, and performs domain-specific optimization to speed up execution. In addition to generating reaction networks, RING allows post-processing analysis options to: (a) obtain reaction pathways and overall mechanisms from initial reactants to desired products using graph traversal algorithms, (b) group together isomers to reduce the size of the network through a novel molecule hashing technique, (c) calculate thermochemical quantities through semi-empirical methods such as group additivity, and (d) formulate and solve kinetic models of the entire or lumped complex network based on a rule-based kinetics specification scheme.",
     "keywords": ["Automated reaction network generation", "Isomer lumping", "Reaction network analysis", "Kinetic modeling", "Domain-specific languages", "Extensible languages"]},
    {"article name": "Identification-based optimization of dynamical systems under uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.02.001",
     "publication date": "05-2014",
     "abstract": "The operation of chemical processes is inherently subject to uncertainty. Traditionally, uncertainties have been accounted for in system design by discretizing the uncertainty space and considering the resulting ensemble of scenarios in solving the design optimization problem. Scenario-based approaches are computationally demanding and can rapidly become intractable. We propose identification-based optimization (IBO) as a novel framework for the optimal design of dynamical systems under uncertainty. Our method originates in nonlinear system identification theory, and is predicated on representing uncertain variables as pseudo-random multi-level signals (PRMSs), which are imposed on the system model during each time integration step of a dynamic optimization. The uncertainty space is thus efficiently sampled without using computationally expensive scenario sets. We establish a procedure for generating PRMSs for uncertain variables based on their probability density functions. The computational benefits of IBO are illustrated through comparative case studies.",
     "keywords": ["Dynamical systems", "Optimization", "Uncertainty", "Random sequences", "System identification"]},
    {"article name": "Simulation of the compressible flow with mass transfer of semi-continuous mixtures using the direct quadrature method of moments",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.02.011",
     "publication date": "05-2014",
     "abstract": "We developed a method for the solution of the compressible flow with mass transfer of semi-continuous mixtures and it is based on the quadrature method of moments (QMoM) for continuous thermodynamics. The method extends the adaptive characterization of the continuous mixture to field problems and solves the mass transport equation for the continuous component. The method is referred as direct QMoM, or DQMoM, for continuous thermodynamics. It was implemented in the OpenFOAM\u00ae\u00a0as a compressible ideal gas flow solver. The DQMoM was applied to the mixing flow of two gas streams with different compositions of a mixture with 57 hydrocarbons diluted in nitrogen. We showed that 4 adaptive components reproduced the mixture properties within 3% accuracy. Furthermore, the DQMoM CFD solution was approximately two times faster than the solution using 58 discrete components.",
     "keywords": ["Continuous thermodynamics", "Mass transfer", "Compressible flow", "Computational fluid dynamics", "DQMOM", "OpenFOAM"]},
    {"article name": "Root cause analysis in multivariate statistical process monitoring: Integrating reconstruction-based multivariate contribution analysis with fuzzy-signed directed graphs",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.02.014",
     "publication date": "05-2014",
     "abstract": "Root cause analysis is an important method for fault diagnosis when used with multivariate statistical process monitoring (MSPM). Conventional contribution analysis in MSPM can only isolate the effects of the fault by pinpointing inconsistent variables, but not the underlying cause. By integrating reconstruction-based multivariate contribution analysis (RBMCA) with fuzzy-signed directed graph (SDG), this paper developed a hybrid fault diagnosis method to identify the root cause of the detected fault. First, a RBMCA-based fuzzy logic was proposed to represent the signs of the process variables. Then, the fuzzy logic was extended to examine the potential relationship from causes to effects in the form of the degree of truth (DoT). An efficient branch and bound algorithm was developed to search for the maximal DoT that explains the effect, and the corresponding causes can be identified. Except for the need to construct an SDG for the process, this method does not require historical data of known faults. The usefulness of the proposed method was demonstrated through a case study on the Tennessee Eastman benchmark problem.",
     "keywords": ["Fault diagnosis", "Fuzzy logic", "Multivariate statistical process monitoring", "Reconstruction-based multivariate contributions", "Signed directed graph"]},
    {"article name": "Optimal liquefaction process cycle considering simplicity and efficiency for LNG FPSO at FEED stage",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.12.007",
     "publication date": "04-2014",
     "abstract": "In this paper, the offshore selection criteria for the optimal liquefaction process system are studied to contribute to the future FEED engineering for the liquefied natural gas (LNG) floating, production, storage, and offloading (LNG FPSO) liquefaction process system.From the foregoing, it is clear that offshore liquefaction plants have process requirements different from those of the traditional onshore liquefaction plants. While thermodynamic efficiency is the key technical process selection criterion for large onshore liquefaction plants, the high-efficiency pre-cooled mixed refrigerant and optimized cascade plants that dominate the onshore LNG installations are unlikely to meet the diverse technical and safety needs of offshore liquefaction facilities. Offshore liquefaction technology developers are rightly focusing on process simplicity, low weight, small footprint, and other criteria. The key criteria that influence process selection and plant optimization for the offshore liquefaction cycle lead to some trade-offs and compromises between efficiency and simplicity. In addition, other criteria for offshore liquefaction cycles should also be considered, such as flexibility, safety, vessel motion, refrigerant storage hazard, proven technology, simplicity of operation, ease of start-up/shutdown, and capital cost.First of all, this paper proposes a generic mixed refrigerant (MR) liquefaction cycle based on four configuration strategies. The 27 feasible MR liquefaction cycles from such generic MR liquefaction cycle are configured for optimal synthesis. From the 27 MR liquefaction cycles, the top 10 are selected based on the minimum amount of power required for the compressors. Then, one MR liquefaction cycle is selected based on simplicity among the 10 MR process cycles, and this is called a \u201cpotential MR liquefaction cycle.\u201dSecond, three additional offshore liquefaction cycles \u2013 DMR for SHELL LNG FPSO, C3MR for onshore projects, and the dual N2 expander for FLEX LNG FPSO \u2013 are considered for comparison with the potential MR liquefaction cycle for the selection of the optimal offshore liquefaction cycle.Such four cycles are compared based on simplicity, efficiency, and other criteria. Therefore, the optimal operating conditions for each cycle with four LNG capacities (4.0, 3.0, 2.0, and 1.0 MTPA) are calculated with the minimum amount of power required for the compressors. Then the preliminary equipment module layout for the four cycles are designed as multi-deck instead of single-deck, and this equipment module layout should be optimized to reduce the area occupied by the topside equipment at the FEED stage. In this paper, the connectivity cost, the construction cost proportional to the deck area, and the distance of the main cryogenic heat exchanger (MCHE) and separators from the centerline of the hull are considered objective functions to be minimized. Moreover, the constraints are proposed to ensure the safety and considering the deck penetration of the long equipment across several decks. Considering the above, mathematical models were formulated for them. For example, the potential MR liquefaction cycle has a mathematical model consisting of 257 unknowns, 193 equality constraints, and 330 inequality constraints. The preliminary optimal equipment module layouts with four LNG capacities (4.0, 3.0, 2.0, and 1.0 MTPA) are then obtained using mixed-integer nonlinear programming (MINLP).Based on the above optimal operating conditions and equipment module layouts for the four potential offshore liquefaction cycles, trade-offs between simplicity and efficiency are performed for actual offshore application, and finally, the potential MR liquefaction cycle is selected for the optimal liquefaction cycle for LNG FPSO.",
     "keywords": ["Optimal offshore liquefaction cycle", "Offshore application", "Efficiency", "Simplicity", "Equipment module layout", "LNG FPSO"]},
    {"article name": "Framework for margins-based planning: Forest biorefinery case study",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.12.006",
     "publication date": "04-2014",
     "abstract": "The biorefinery concept offers a promising solution to transform the struggling forestry industry. Not only will the implementation of new products and processes help to diversify revenues, it will also offer an opportunity to change the manufacturing culture by better managing the flexibility of assets to react to volatile market conditions. In this paper, an integrated supply-chain planning framework is presented. It is based on optimizing a superstructure to help decision makers identify different supply-chain policies to adapt to different market conditions. It integrates revenue management concepts, activity-based cost accounting principles, manufacturing flexibility and supply-chain flexibility in a tactical model to maximize profit in a price-volatile environment. A case study of a newsprint mill implementing a parallel biomass fractionation line producing several biochemicals is used to illustrate this approach. Results and benefits are presented for the traditional pulp and paper business and for the transformed biorefinery in different market scenarios.",
     "keywords": ["Supply-chain management", "Manufacturing flexibility", "Forest biorefinery", "Tactical planning"]},
    {"article name": "Fully compositional and thermal reservoir simulation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.12.008",
     "publication date": "04-2014",
     "abstract": "Fully compositional and thermal reservoir simulation capabilities are important in oil exploration and production. There are significant resources in existing wells and in heavy oil, oil sands, and deep-water reservoirs. This article has two main goals: (1) to clearly identify chemical engineering sub-problems within reservoir simulation that the PSE community can potentially make contributions to and (2) to describe a new computational framework for fully compositional and thermal reservoir simulation based on a combination of the Automatic Differentiation-General Purpose Research Simulator (AD-GPRS) and the multiphase equilibrium flash library (GFLASH). Numerical results for several chemical engineering sub-problems and reservoir simulations for two EOR applications are presented. Reservoir simulation results clearly show that the Solvent Thermal Resources Innovation Process (STRIP) outperforms conventional steam injection using two important metrics \u2013 sweep efficiency and oil recovery.",
     "keywords": ["Process systems engineering", "Enhanced oil recovery", "Steam injection", "STRIP", "Fully compositional", "Thermal reservoir simulation"]},
    {"article name": "Simultaneous design and MPC-based control for dynamic systems under uncertainty: A stochastic approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.01.002",
     "publication date": "04-2014",
     "abstract": "A stochastic-based simultaneous design and control methodology for chemical processes under uncertainty is presented. An optimization framework is proposed with the aim of achieving a feasible and stable optimal process design in the presence of stochastic disturbances while using advanced model-based control schemes such as Model Predictive Control (MPC). The key idea is to determine the dynamic variability of the system that will be accounted for in the process design using a stochastic-based worst-case variability index. This index is computed from the probability distribution of the worst-case variability of the process variables that determine the dynamic feasibility or the dynamic performance of the system under random realizations in the disturbances. A case study of an actual wastewater treatment industrial plant is presented and used to test the proposed methodology and compare its performance against the sequential design approach and a simultaneous design and control method using conventional PI-based control schemes.",
     "keywords": ["Simultaneous design and control", "Model Predictive Control", "Process design", "Uncertainty", "Flexibility", "Feasibility"]},
    {"article name": "A qualitative comparison between population balances and stochastic models for non-isothermal antisolvent crystallization processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.01.001",
     "publication date": "04-2014",
     "abstract": "The goal of the present work is to model the crystal growth processes mediated by both antisolvent feed and temperature variations through the time evolution of the Particle Size Distribution (PSD). The study is carried out by exploiting two different approaches. In the first approach a population balance equation (PBE) model is devised, where crystal nucleation and growth phenomena are developed taking into account rigorous first principle assumptions. The second approach is based on a phenomenological stochastic formulation leading to a global Fokker\u2013Planck Equation (FPE) governed by a limited number of parameters, describing the time evolution of the probability density function representing the crystal PSD. Validations against experimental data are presented for the NaCl\u2013water\u2013ethanol ternary system. The pros and cons of both approaches are discussed.",
     "keywords": ["Crystallization", "Stochastic modeling", "Population balance", "Antisolvent and temperature effects"]},
    {"article name": "Numerical investigation of the crystallization and orientation behavior in polymer processing with a two-phase model",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.12.010",
     "publication date": "04-2014",
     "abstract": "The crystallization and orientation behavior of a polymeric material can significantly influence the performance of products in practical processing. In this study, the variations in morphology that occur during solidification in polymer processing are mathematically modeled using a two-phase model. The amorphous phase is approximated as a finite extensible nonlinear elastic dumbbell with a Peterlin closure approximation (FENE-P) fluid, and the semi-crystalline phase is modeled as rigid rods oriented within the flow field. The crystallization and orientation behavior are numerically investigated using the penalty finite element\u2013finite difference method with a decoupled algorithm. The evolution of the crystallization process is described by Schneider's equation, which differentiates between the effects of thermal and flow states. The hybrid closure approximation is adopted for the calculation of the three-dimensional orientation tensor. The discrete elastic viscous split stress (DEVSS) algorithm, which incorporates the streamline upwind scheme, is introduced to improve calculation stability. The variations in morphology during polymer processing are successfully predicted using the proposed mathematical model and numerical method. The influence of processing conditions on the crystallization and orientation behavior is further discussed.",
     "keywords": ["Polymer", "Crystallization", "Orientation", "Simulation", "Two-phase"]},
    {"article name": "PGS-COM: A hybrid method for constrained non-smooth black-box optimization problems: Brief review, novel algorithm and comparative evaluation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.12.014",
     "publication date": "04-2014",
     "abstract": "In the areas of chemical processes and energy systems, the relevance of black-box optimization problems is growing because they arise not only in the optimization of processes with modular/sequential simulation codes but also when decomposing complex optimization problems into bilevel programs. The objective function is typically discontinuous, non-differentiable, not defined in some points, noisy, and subject to linear and nonlinear relaxable and unrelaxable constraints. In this work, after briefly reviewing the main available direct-search methods applicable to this class of problems, we propose a new hybrid algorithm, referred to as PGS-COM, which combines the positive features of Constrained Particle Swarm, Generating Set Search, and Complex. The remarkable performance and reliability of PGS-COM are assessed and compared with those of eleven main alternative methods on twenty five test problems as well as two challenging process engineering applications related to the optimization of a heat recovery steam cycle and a styrene production process.",
     "keywords": ["Black-box optimization", "Process optimization", "Non-smooth optimization", "Direct-search methods", "Evolutionary algorithms"]},
    {"article name": "An MFA optimization approach for pollution trading considering the sustainability of the surrounded watersheds",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.01.005",
     "publication date": "04-2014",
     "abstract": "This paper proposes a mathematical programming model for the pollution trading among different pollution sources which considers the sustainability of the surrounding watershed. The formulation involves the minimization of the costs associated to the implementation of the required technology to satisfy the environmental constraints in order to achieve optimal water quality conditions. The model uses a material flow analysis technique to represent changes on the behavior of the watershed due to the polluted discharges. The material flow analysis considers all discharges and extractions (i.e., industrial and residential discharges, pluvial precipitation, evaporation, etc.) as well as the chemical and biochemical reactions taking place in the watershed. In the context of pollution trading, the implementation of the proposed formulation determines if an industrial source must buy credits to compensate the violation of environmental constraints, or if it requires the installation of treatment technologies to sell credits to another source. The formulation was applied to a case study involving the drainage system of the Bahr El-Baqar region in Egypt; the results show the advantages of the proposed approach in terms of cost and sustainability.",
     "keywords": ["Pollution trading", "Material flow analysis", "Optimization", "Watershed", "Sustainable design"]},
    {"article name": "Parameter estimation in stochastic chemical kinetic models using derivative free optimization and bootstrapping",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.01.006",
     "publication date": "04-2014",
     "abstract": "Recent years have seen increasing popularity of stochastic chemical kinetic models due to their ability to explain and model several critical biological phenomena. Several developments in high resolution fluorescence microscopy have enabled researchers to obtain protein and mRNA data on the single cell level. The availability of these data along with the knowledge that the system is governed by a stochastic chemical kinetic model leads to the problem of parameter estimation. This paper develops a new method of parameter estimation for stochastic chemical kinetic models. There are three components of the new method. First, we propose a new expression for likelihood of the experimental data. Second, we use sample path optimization along with UOBYQA-Fit, a variant of Powell's unconstrained optimization by quadratic approximation, for optimization. Third, we use a variant of Efron's percentile bootstrapping method to estimate the confidence regions for the parameter estimates. We apply the parameter estimation method in an RNA dynamics model of Escherichia coli. We test the parameter estimates obtained and the confidence regions in this model. The testing of the parameter estimation method demonstrates the efficiency, reliability, and accuracy of the new method.",
     "keywords": ["Stochastic chemical kinetic model", "Parameter estimation", "UOBYQA -Fit", "Derivative free optimization", "Efron 's percentile bootstrapping", "Confidence level estimation"]},
    {"article name": "Fast moving horizon estimation for a two-dimensional distributed parameter system",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.12.005",
     "publication date": "04-2014",
     "abstract": "Partial differential equations (PDEs) pose a challenge for control engineers, both in terms of theory and computational requirements. PDEs are usually approximated by ordinary differential equations or difference equations via the finite difference method, resulting in a high-dimensional state-space system. The obtained system matrix is oftentimes symmetric, which allows this high-dimensional system to be decomposed into a set of single-dimensional systems using its singular value decomposition. Any linear constraints in the original problem can also be simplified by replacing it with an ellipsoidal constraint. Based on this, speedup of the moving horizon estimation is achieved by employing an analytical solution obtained by augmenting the ellipsoidal constraint into the objective function as a penalty weighted by a decreasing scaling parameter. The approximated penalty method algorithm allows for efficient parallel computation for sub-problems. The proposed algorithm is demonstrated for a two-dimensional diffusion problem where the concentration field is estimated using distributed sensors.",
     "keywords": ["Moving horizon estimation", "Kalman filter", "Ellipsoid constraint", "Singular value decomposition", "Penalty method", "Partial differential equation"]},
    {"article name": "A methodology for the conceptual design of concentration circuits: Group contribution method",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.12.015",
     "publication date": "04-2014",
     "abstract": "This paper presents a new methodology for the conceptual design of concentration circuits based on the group contribution method. The methodology includes three decision levels: (1) definition and analysis of the problem, (2) synthesis and screening of alternatives, and (3) final design. In this manuscript, the emphasis is on the description of the methodology, justification of the assumptions, and group contribution method. The group contribution models were developed to estimate the global recovery in concentration circuits. The procedure is general and can be applied to any circuit consisting of stages that generate two product streams: concentrate and tail. The developed models can be applied to estimate the recoveries in concentration circuits with a maximum of six stages. The models were fitted using mass balance data from 46 circuits, generating 35 process groups. Case studies were used to illustrate the methodology.",
     "keywords": ["Process design", "Group contribution", "Concentration circuit", "Flotation"]},
    {"article name": "A monolithic approach to vehicle routing and operations scheduling of a cross-dock system with multiple dock doors",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.12.012",
     "publication date": "04-2014",
     "abstract": "Cross-docking is a logistic strategy for moving goods from suppliers to customers via a cross-dock terminal with no permanent storage. The operational planning of a cross-dock facility involves different issues such as vehicle routing, dock door assignment and truck scheduling. The vehicle routing problem seeks the optimal routes for a homogeneous fleet of vehicles that sequentially collects goods at pickup points and delivers them to their destinations. The truck scheduling problem deals with the timing of unloading and reloading operations at the cross-dock. This work introduces a mixed-integer linear programming formulation for the scheduling of single cross-dock systems that, in addition to selecting the pickup/delivery routes, simultaneously decides on the dock door assignment and the truck scheduling at the cross-dock. The proposed monolithic formulation is able to provide near-optimal solutions to medium-size problems involving up to 70 transportation orders, 16 vehicles and 7 strip/stack dock doors at acceptable CPU times.",
     "keywords": ["Cross-docking", "Vehicle routing", "Truck scheduling", "MILP mathematical model", "Sweeping-based approach"]},
    {"article name": "Steady state identification for on-line data reconciliation based on wavelet transform and filtering",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.02.003",
     "publication date": "04-2014",
     "abstract": "In order to derive higher value operational knowledge from raw process measurements, advanced techniques and methodologies need to be exploited. In this paper a methodology for online steady-state detection in continuous processes is presented. It is based on a wavelet multiscale decomposition of the temporal signal of a measured process variable, which simultaneously allows for two important pre-processing tasks: filtering-out the high frequency noise via soft-thresholding and correcting abnormalities by analyzing the maximums of wavelet transform modulus. Wavelet features involved in the pre-processing task are simultaneously exploited in analyzing a process trend of measured variable. The near steady state starting and ending points are identified by using the first and the second order of wavelet transform. Simultaneously a low filter with a probability density function is employed to approximate the duration of a near stationary condition. The method provides an improvement in the quality of steady-state data sets, which will directly improve the outcomes of data reconciliation and manufacturing costs. A comparison with other steady-state detection methods on an example of case study indicates that the proposed methodology is efficient in detecting steady-state and suitable for online implementation.",
     "keywords": ["Signal processing", "Steady-state detection", "Wavelet analysis", "Plant-wide application", "Data cleansing"]},
    {"article name": "A comparison between two methods of stochastic optimization for a dynamic hydrogen consuming plant",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.02.004",
     "publication date": "04-2014",
     "abstract": "The following work shows the application of two methods of stochastic economic optimization in a hydrogen consuming plant: two-stage programming and chance constrained optimization. The system presents two main sources of uncertainty described with a binormal probability distribution function (PDF). Both methods are formulated in the continuous domain. For calculating the probabilistic constraints the inverse mapping method was written as a nested parameter estimation problem. On the other hand, to solve the two stage optimization, a discretization of the PDF in scenarios was applied with a scenario aggregation formulation to take into account the nonanticipativity constraints. Finally, a framework generalizing this solution based on interpolation was proposed. Both optimization methods, two-stage programming and chance constrained optimization, were tested using Monte Carlo simulation in terms of feasibility and optimality for the application considered. The main problem appears to be the large computation times associated.",
     "keywords": ["Two-stage stochastic optimization", "Chance constrained optimization", "Monte Carlo simulation", "Economic dynamic optimization", "Hydrodesulphurisation process"]},
    {"article name": "Lagrangian relaxation based decomposition for well scheduling in shale-gas systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2014.02.005",
     "publication date": "04-2014",
     "abstract": "Suppressing the effects of liquid loading is a key issue for efficient utilization of mid and late-life wells in shale-gas systems. This state of the wells can be prevented by performing short shut-ins when the gas rate falls below the minimum rate needed to avoid liquid loading. In this paper, we present a Lagrangian relaxation scheme for shut-in scheduling of distributed shale multi-well systems. The scheme optimizes shut-in times and a reference rate for each multi-well pad, such that the total produced rate tracks a given short-term gas demand for the field. By using simple, frequency-tuned well proxy models, we obtain a compact mixed-integer formulation which by Lagrangian relaxation renders a decomposable structure. A set of computational tests demonstrates the merits of the proposed scheme. This study indicates that the method is capable of solving large field-wide scheduling problems by producing good solutions in reasonable computation times.",
     "keywords": ["Lagrangian relaxation", "Mixed-integer linear programming", "Shale-gas production", "Parameter estimation"]},
    {"article name": "The importance of the sequential synthesis methodology in the optimal distillation sequences design",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.11.010",
     "publication date": "03-2014",
     "abstract": "The sequential design method is presented as a complementary tool of the systematical synthesis procedure that allows to define a clear connection among the different types of distillation column sequences. In particular, the connection with the simple column subspace is considered, since this subspace represents the comparison reference for all the alternatives considered. The sequential design procedure, based on the correspondence between the functionality of the column's section among the simple columns and the derived sequences, is compared with a mathematical based optimization algorithm. The separations of a four-component near ideal mixture and the azeotropic ethanol\u2013water mixture are considered as case studies and the designs obtained applying both methods have been compared. The results confirmed that the sequential design method is a fast and reliable tool in the optimal design of the column sequence.",
     "keywords": ["Distillation", "Design", "Optimization", "Simulation", "Process synthesis", "Process modeling"]},
    {"article name": "Bifurcation analysis of dynamic process models using Aspen Dynamics\u00ae and Aspen Custom Modeler\u00ae",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.11.013",
     "publication date": "03-2014",
     "abstract": "Dynamic simulation and modeling has become a necessity to process and control engineers, therefore dynamic simulation software is being used constantly for the assessment of control and optimization technologies. This work implements a bifurcation and analysis framework using the recursive projection method to perform bifurcation and eigenvalue analysis over dynamical models implemented in Aspen Dynamics and Aspen Custom Modeler, enabling commercial simulation software users to perform quick and reliable dynamic model bifurcation analysis, without recurring to non-chemical engineering specialized software.",
     "keywords": ["Recursive projection method", "Aspen Dynamics", "Aspen Custom Modeler", "Bifurcation analysis", "Dynamical modeling"]},
    {"article name": "Multi-objective optimization of industrial waste management in chemical sites coupled with heat integration issues",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.11.012",
     "publication date": "03-2014",
     "abstract": "This work presents a multi-period waste management multi-objective optimization, considering economic and environmental issues. The behavior of waste treatment units is included in the optimization problem as black-box models based on industrial practice. A multi-objective mathematical strategy based on the normalized constrained method is applied. An industrial based case study is analyzed. The proposed rigorous multi-objective optimization leads to reduced computation effort and better solutions in terms of solution quality, since waste stream scheduling has been included in decision-making. In addition, a sequential approach is followed to further estimate the minimum heat requirements for the different solutions obtained in the Pareto front using a MILP formulation of the heat exchange problem. Hot and cold sink requirements can be reduced by 80% and 99% respectively.",
     "keywords": ["Industrial waste management", "Process scheduling", "Multi-objective optimization", "Heat integration"]},
    {"article name": "Robust decision making for hybrid process supply chain systems via model predictive control",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.10.019",
     "publication date": "03-2014",
     "abstract": "Model predictive control (MPC) is a promising solution for the effective control of process supply chains. This paper presents an optimization-based decision support tool for supply chain management, by means of a robust MPC strategy. The proposed formulation: (i) captures uncertainty in model parameters and demand by stochastic programming, (ii) accommodates hybrid process systems with decisions governed by logical conditions/rulesets, and (iii) addresses multiple supply chain performance metrics including customer service and economics, within an integrated optimization framework. Two mechanisms for uncertainty propagation are presented \u2013 an open-loop approach, and an approximate closed-loop strategy. The performance of the robust MPC framework is analyzed through its application to two process supply chain case studies. The proposed approach is shown to provide a substantial reduction in the occurrence of back orders when compared to a nominal MPC implementation.",
     "keywords": ["Robust model predictive control", "Supply chain optimization", "Multi-objective optimization", "Stochastic optimization", "Supply chain management"]},
    {"article name": "A systematic study of solvent effect on the crystal habit of dirithromycin solvates by computer simulation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.11.005",
     "publication date": "03-2014",
     "abstract": "The crystal habits of five dirithromycin solvates were employed to unravel the solvent effect by using the modified attachment energy (AE) model. Solvents with different polarity and hydrogen bond donor/acceptor ability (acetone, 1-propanol, acetonitrile, water and cyclohexane) were studied. The good consistency between experimental results and predictions confirmed the applicability of modified AE model. Simulation results showed that all solvates underwent a change in morphological importance (MI) of crystal faces except for the cyclohexane trisolvate. A detailed analysis of the results indicated that the polar solvent had a stronger interaction with crystal face than the non-polar solvent due to the formation of hydrogen bond. Furthermore, crystals with similar structure in different solvents exhibited different aspect ratios. The computer-aided study approach in this work could be helpful to control the morphology of crystal by tailor-made solvents or additives.",
     "keywords": ["Dirithromycin", "Crystal habit", "Solvent effect", "Molecular dynamics (MD)", "Modified attachment energy (AE)"]},
    {"article name": "A new decomposition algorithm for multistage stochastic programs with endogenous uncertainties",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.11.011",
     "publication date": "03-2014",
     "abstract": "In this paper, we present a new decomposition algorithm for solving large-scale multistage stochastic programs (MSSPs) with endogenous uncertainties. Instead of dualizing all the initial non-anticipativity constraints (NACs) and removing all the conditional NACs to decompose the problem into scenario subproblems, the basic idea relies on keeping a subset of NACs as explicit constraints in the scenario group subproblems while dualizing or relaxing the rest of the NACs. It is proved that the algorithm provides a dual bound that is at least as tight as the standard approach. Numerical results for process network examples and oilfield development planning problem are presented to illustrate that the proposed decomposition approach yields significant improvement in the dual bound at the root node and reduction in the total computational expense for closing the gap.",
     "keywords": ["Multistage stochastic programming", "Endogenous uncertainties", "Non-anticipativity constraints", "Lagrangean decomposition", "Process networks", "Oil and gas exploration"]},
    {"article name": "An SKU decomposition algorithm for the tactical planning in the FMCG industry",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.11.008",
     "publication date": "03-2014",
     "abstract": "In this paper we propose an MILP model to address the optimization of the tactical planning for the Fast Moving Consumer Goods (FMCG) industry. This model is demonstrated for a case containing 10 Stock-Keeping Units (SKUs), but is intractable for realistically sized problems. Therefore, we propose a decomposition based on single-SKU submodels. To account for the interaction between SKUs, slack variables are introduced into the capacity constraints. In an iterative procedure the cost of violating the capacity is slowly increased, and eventually a feasible solution is obtained. Even for the relatively small 10-SKU case, the required CPU time could be reduced from 1144\u00a0s to 175\u00a0s using the algorithm. Moreover, the algorithm was used to optimize cases of up to 1000 SKUs, whereas the full model is intractable for cases of 25 or more SKUs. The solutions obtained with the algorithm are typically within a few percent of the global optimum.",
     "keywords": ["Tactical planning", "Optimization", "MILP", "Decomposition algorithm", "Fast Moving Consumer Goods", "Enterprise-Wide Optimization"]},
    {"article name": "A consistent momentum interpolation method for steady and unsteady multiphase flows",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.12.002",
     "publication date": "03-2014",
     "abstract": "A consistent momentum interpolation approach is proposed for the discretization on collocated grids of the Eulerian\u2013Eulerian equations governing unsteady multiphase flows. The procedure, which includes a correction for interphase forces (drag), fulfills additional requirements that a multiphase method has to meet with respect to single-phase algorithms. Results reveal that the application to multiphase flows of the simple extension of the often-used formulation for single-phase flows (Choi, 1999; Shen et al., 2001) may notably result in spurious spatial oscillations in the fields, and steady solutions of transient flows that depend significantly on the time-step size. The proposed new approach, instead, is free from these deficiencies.",
     "keywords": ["Multiphase flows", "Eulerian\u2013Eulerian models", "Finite volume method", "Momentum interpolation consistency", "Collocated grid"]},
    {"article name": "Plantwide control structure selection methodology for the benchmark vinyl acetate monomer plant",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.12.003",
     "publication date": "03-2014",
     "abstract": "In this paper, the regulatory control structure selection (RCSS) methodology of Psaltis, Kookos, and Kravaris (2013, Computers & Chemical Engineering, 52, 240\u2013248) is applied on the benchmark case study of the vinyl acetate (VAc) monomer plant. The VAc monomer plant consists of 10 unit operations with high level of interactions. The mathematical model of the process involves 246 state variables, 26 potential manipulated variables and 46 measurements. It is the first time that the proposed RCSS methodology is applied to a plantwide case study and this was made possible due to the improvements in the classical \u201cback-off\u201d methodology proposed by Psaltis et al. (2013). The control structure obtained for VAc plant is implemented on the nonlinear plant model using decentralized PI controllers and the simulation results illustrate its satisfactory performance.",
     "keywords": ["VAc plant", "Plantwide control", "Mathematical Programming", "Back-off methodology"]},
    {"article name": "Representation of a small-scale nozzle for gas\u2013liquid injections into a fluidized bed on a large-scale grid",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.12.004",
     "publication date": "03-2014",
     "abstract": "A novel method for efficient computations of a fluidized bed reactor with liquid injections is developed. It allows economical simulations for a reactor that contains multiple nozzles using a relatively coarse grid, while still accounting for the influence of particular features of individual nozzles. The method relies on patching variables in the near nozzle area obtained on the fine nozzle-scale grid onto the coarse reactor-scale grid followed by the solution of the flow equations elsewhere in the coarse grid domain. The procedure is tested for a small fluidized bed that permits both fine and coarse grid solutions. It was found that the developed procedure represents the flow adequately and allows for the distinction of different nozzle geometries.",
     "keywords": ["Multiscale modelling", "Numerical simulations", "Fluidized bed", "Gas\u2013liquid injection", "Nozzle"]},
    {"article name": "Computer algebra systems coming of age: Dynamic simulation and optimization of DAE systems in Mathematica\u2122",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.11.004",
     "publication date": "03-2014",
     "abstract": "In this article, DAEparser and SQPsolver, new Computer Algebra System packages specialized in Differential-Algebraic Equations for Mathematica\u2122 are presented. These packages joint capabilities for dynamical system analysis, simulation and dynamic optimization through the direct sequential approach are presented with examples and case studies highlighting applications of practical interest to chemical engineers. An overview of the relevant theoretical topics to each of the features of the packages are presented as well as implementation insights. This work paves the way for innovative R&D platforms both capable of solving practical problems of interest as well as offer seamless computational workflow.",
     "keywords": ["Differential algebraic equations", "Simulation", "Parametric sensitivity", "Dynamic optimization"]},
    {"article name": "From discretization to regularization of composite discontinuous functions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.11.019",
     "publication date": "03-2014",
     "abstract": "Discontinuities between distinct regions, described by different equation sets, cause difficulties for PDE/ODE solvers. We present a new algorithm that eliminates integrator discontinuities through regularizing discontinuities. First, the algorithm determines the optimum switch point between two functions spanning adjacent or overlapping domains. The optimum switch point is determined by searching for a \u201cjump point\u201d that minimizes a discontinuity between adjacent/overlapping functions. Then, discontinuity is resolved using an interpolating polynomial that joins the two discontinuous functions.This approach eliminates the need for conventional integrators to either discretize and then link discontinuities through generating interpolating polynomials based on state variables or to reinitialize state variables when discontinuities are detected in an ODE/DAE system. In contrast to conventional approaches that handle discontinuities at the state variable level only, the new approach tackles discontinuity at both state variable and the constitutive equations level. Thus, this approach eliminates errors associated with interpolating polynomials generated at a state variable level for discontinuities occurring in the constitutive equations.Computer memory space requirements for this approach exponentially increase with the dimension of the discontinuous function hence there will be limitations for functions with relatively high dimensions. Memory availability continues to increase with price decreasing so this is not expected to be a major limitation.",
     "keywords": ["Discontinuity detection", "Discontinuity resolution", "Solving differential equations."]},
    {"article name": "Scope for industrial applications of production scheduling models and solution methods",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.12.001",
     "publication date": "03-2014",
     "abstract": "This paper gives a review on existing scheduling methodologies developed for process industries. Above all, the aim of the paper is to focus on the industrial aspects of scheduling and discuss the main characteristics, including strengths and weaknesses of the presented approaches. It is claimed that optimization tools of today can effectively support the plant level production. However there is still clear potential for improvements, especially in transferring academic results into industry. For instance, usability, interfacing and integration are some aspects discussed in the paper. After the introduction and problem classification, the paper discusses some lessons learned from industry, provides an overview of models and methods and concludes with general guidelines and examples on the modeling and solution of industrial problems.",
     "keywords": ["Scheduling", "Industrial applications", "Best practices", "Integration", "Challenges"]},
    {"article name": "Optimal supply chain design and management over a multi-period horizon under demand uncertainty. Part I: MINLP and MILP models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.10.007",
     "publication date": "03-2014",
     "abstract": "An optimization model is proposed to redesign the supply chain of spare part delivery under demand uncertainty from strategic and tactical perspectives in a planning horizon consisting of multiple periods. Long term decisions involve new installations, expansions and elimination of warehouses and factories handling multiple products. It is also considered which warehouses should be used as repair work-shops in order to store, repair and deliver used units to customers. Tactical planning includes deciding inventory levels (safety stock and expected inventory) for each type of spare part in distribution centers and customer plants, as well as the connection links between the supply chain nodes. Capacity constraints are also taken into account when planning inventory levels. At the tactical level it is determined how demand of failing units is satisfied, and whether to use new or used parts. The uncertain demand is addressed by defining the optimal amount of safety stock that guarantees certain service level at a customer plant. In addition, the risk-pooling effect is taken into account when defining inventory levels in distribution centers and customer zones. Due to the nonlinear nature of the original formulation, a piece-wise linearization approach is applied to obtain a tight lower bound of the optimal solution. The formulation can be adapted to several industry-critical units and the supply chain of electric motors is provided here as an example.",
     "keywords": ["Supply chain", "Demand uncertainty", "Inventory management", "Mixed integer non-linear programming"]},
    {"article name": "Optimal supply chain design and management over a multi-period horizon under demand uncertainty. Part II: A Lagrangean decomposition algorithm",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.11.014",
     "publication date": "03-2014",
     "abstract": "In Part I (Rodriguez, Vecchietti, Harjunkoski, & Grossmann, 2013), we proposed an optimization model to redesign the supply chain of spare parts industry under demand uncertainty from strategic and tactical perspectives in a planning horizon consisting of multiple time periods. To address large scale industrial problems, a Lagrangean scheme is proposed to decompose the MINLP of Part I according to the warehouses. The subproblems are first approximated by an adaptive piece-wise linearization scheme that provides lower bounds, and the MILP is further relaxed to an LP to improve solution efficiency while providing a valid lower bound. An initialization scheme is designed to obtain good initial Lagrange multipliers, which are scaled to accelerate the convergence. To obtain feasible solutions, an adaptive linearization scheme is also introduced. The results from an illustrative problem and two real world industrial problems show that the method can obtain optimal or near optimal solutions in modest computational times.",
     "keywords": ["Supply chain", "Lagrangean decomposition", "Adaptive piecewise linearization"]},
    {"article name": "Thermodynamic analysis of non-isothermal mixing's influence on the energy target of water-using networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.10.008",
     "publication date": "02-2014",
     "abstract": "In this paper, a thermodynamic analysis of non-isothermal mixing's influence on the energy target of water-using network is presented. Firstly, water streams in the network are divided into two categories, and then based on the classification all the non-isothermal mixing patterns between two streams are defined. Through thermodynamic analysis from energy composite curve of hot and cold water streams, the influences of non-isothermal mixing on the energy target of water-using network are explored and some mixing rules are obtained, which can be used to simplify the heat exchanger network of a given water-using system and improve the system's energy performance through identifying the beneficial non-isothermal mixings. The applicability of these rules is illustrated by an example in the paper and the result is very encouraging.",
     "keywords": ["Energy", "Mixing", "Optimization", "Systems engineering", "Water-using network", "Thermodynamic analysis"]},
    {"article name": "Fault isolation using modified contribution plots",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.10.004",
     "publication date": "02-2014",
     "abstract": "Investigating the root causes of abnormal events is a crucial task for an industrial process. When process faults are detected, isolating the faulty variables provides additional information for investigating the root causes of the faults. Numerous data-driven approaches require the datasets of known faults, which may not exist for some industrial processes, to isolate the faulty variables. The contribution plot is a popular tool to isolate faulty variables without a priori knowledge. However, it is well known that this approach suffers from the smearing effect, which may mislead the faulty variables of the detected faults. In the presented work, a contribution plot without the smearing effect to non-faulty variables was derived. A continuous stirred tank reactor (CSTR) example and the industrial application were provided to demonstrate that the proposed approach is not only capable of locating different faulty variables when the fault was propagated by the controllers, but also capable of identifying the variables responsible for the multiple sensor faults.",
     "keywords": ["Fault isolation", "Principal component analysis", "Contribution plots", "Missing data analysis"]},
    {"article name": "An effective procedure for sensor variable selection and utilization in plasma etching for semiconductor manufacturing",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.09.016",
     "publication date": "02-2014",
     "abstract": "Plasma etching processes have a potentially large number of sensor variables to be utilized, and the number of the sensor variables is growing due to advances in real-time sensors. In addition, the sensor variables from plasma sensors require additional knowledge about plasmas, which becomes a big burden for engineers to utilize them in this filed. Thus an effective procedure for sensor variable selection with minimum plasma knowledge is needed to develop in plasma etching. The integrated squared response (ISR) based sensor variable selection method which facilitates collecting and analyzing sensor data at one time with regard to manipulated variables (MVs) is suggested in this paper. The reference sensor library as well as sensor ranking tables constructed on the basis of ISR can give insight into plasma sensors. The ISR based sensor variable selection method is incorporated with relative gain array (RGA) or non-square relative gain array (NRGA) for effective variable selection in building a virtual metrology (VM) system to predict critical dimension (CD) in plasma etching. The application of the technique introduced in this paper is shown to be effective in the CD prediction in plasma etching for a dynamic random access memory (DRAM) manufacturing. The procedure for sensor variable selection introduced in this paper can be a starting point for various sensor-related applications in semiconductor manufacturing.",
     "keywords": ["Variable selection", "Integrated squared response", "Plasma etching", "Multivariable control", "Virtual metrology", "Relative gain array"]},
    {"article name": "Numerical study on hydrodynamic characteristics of plate-fin heat exchanger using porous media approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.10.010",
     "publication date": "02-2014",
     "abstract": "A numerical plate-fin heat exchanger (PFHE) model was proposed to investigate the hydrodynamic characteristics of a full-size PFHE by using the porous media approach. Based on the model, effects of the fluid dynamic viscosity and perforated fins on flow distribution and pressure drop of the PFHE were studied. The results showed that flow distribution of the PFHE was improved by increasing the fluid dynamic viscosity or adding perforated fins in each fin channel, but at the cost of an increased pressure drop. Therefore, the relationship between flow distribution and pressure drop was further analyzed under various Reynolds numbers. Based on the results, a correlation among flow distribution, pressure drop, and Reynolds number was derived. Finally, two strategies, the fin channel-based strategy and the header-based strategy were proposed and numerically verified to improve flow distribution of the PFHE. Our results indicate that the first strategy is better than the latter one.",
     "keywords": ["Computational fluid dynamics", "Flow distribution", "Perforated fins", "Plate-fin heat exchanger", "Porous media"]},
    {"article name": "Tuning of NMPC controllers via multi-objective optimisation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.10.003",
     "publication date": "02-2014",
     "abstract": "Nonlinear Model Predictive Control (NMPC) is a powerful technique that can be used to control many industrial processes. Different and often conflicting control objectives, e.g., reference tracking, disturbance rejection and minimum control effort, are typically present. Most often these objectives are translated into a single weighted sum (WS) objective function. This approach is widespread because it is easy to use and understand. However, selecting an appropriate set of weights for the objective function is often non-trivial and is mainly done by trial and error. The current study proposes a systematic procedure for tuning Nonlinear MPC based on multi-objective optimisation methods. Advanced methods allow an efficient solution of the multi-objective problem providing a systematic overview of the controller behaviour. Moreover, through analytic relations it is possible to link a solution obtained with these novel methods to a set of weights for a weighted sum objective function. Applying this set of weights causes the WS to generate the same solution as obtained with the advanced method. Hence, an appropriate controller can be selected based on the alternatives generated by the advanced method, while the corresponding weights for a WS can be derived for implementing the controller in practice. The procedure is successfully tested on two benchmark applications: the Van de Vusse reactor and the Tennessee Eastman plant.",
     "keywords": ["Multi-objective optimisation", "Nonlinear Model Predictive Control", "Nonlinear optimisation", "Tennessee Eastman", "Dynamic optimisation", "ACADO toolkit"]},
    {"article name": "Process design optimization strategy to develop energy and cost correlations of CO2 capture processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.10.011",
     "publication date": "02-2014",
     "abstract": "In the context of CO2 emissions reduction from power plants, CO2 removal from flue gas by chemical absorption with monoethanolamine is analyzed in detail. By applying process integration and multi-objective optimization techniques the influence of the operating conditions on the thermo-economic performance and on the optimal thermal integration within a power plant is studied. With the aim of performing optimization of complex integrated energy systems, simpler parameterized models of the CO2 capture process are developed. These models predict the optimized thermo-economic performances with regard to the capture rate, flue gas flowrate and CO2 concentration. When applied to overall process optimization, the optimization time is considerably reduced without penalizing the overall power plant model quality. This approach is promising for the preliminary design and evaluation of process options including a CO2 capture unit.",
     "keywords": ["CO2 capture", "Chemical absorption", "Blackbox model", "Multi-objective optimization", "Process design"]},
    {"article name": "Rigorous formulation for the scheduling of reversible-flow multiproduct pipelines",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.10.006",
     "publication date": "02-2014",
     "abstract": "Pipelines play a major role in the petroleum industry by providing a safe, reliable and economical transportation mode over land. Frequently, they connect a pair of refineries or harbors with the purpose of sharing oil products. As the construction of twin pipelines transporting products in opposite directions demands large capital investments, reversible-flow pipelines arise as a promising alternative. This paper introduces a novel continuous-time formulation for the short-term operational planning of reversible multiproduct pipelines. The proposed model allows to change the flow direction as many times as needed to meet terminal demands, determining precise time instants for flow reversals. It provides the input and output schedules in a single step, and the most convenient product used as filler to push current batches out of the line. Three examples are successfully solved with much less computational effort than previous approaches.",
     "keywords": ["Multiproduct pipelines", "Reversible-flow", "Operational scheduling", "Monolithic model", "Continuous approach"]},
    {"article name": "A new representation for modeling biomass to commodity chemicals development for chemical process industry",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.10.012",
     "publication date": "02-2014",
     "abstract": "A new representation combining network and stage-gate frameworks to study the impact of capital and research & development (R&D) decisions on the evolution of biomass to commodity chemicals system is presented. The network representation is used to universally express the interconnections between the processing technologies and to track the material flow among them. The stage-gate representation is used to express the discrete nature of technology maturity levels. The corresponding mixed-integer nonlinear program is developed and solved for a case study. In this case study, ethylene and propylene can be produced from naphtha and/or biomass. The results of the sensitivity analysis reveal that the raw material costs are the dominant factors that dictate the optimum investment decisions and production plan for this system.",
     "keywords": ["Network representation", "Stage-gate framework", "Technology development", "Investment and production planning"]},
    {"article name": "Globally convergent exact and inexact parametric algorithms for solving large-scale mixed-integer fractional programs and applications in process systems engineering",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.10.017",
     "publication date": "02-2014",
     "abstract": "This paper is concerned with the parametric algorithms for solving large-scale mixed-integer linear and nonlinear fractional programming problems, as well as their application in process systems engineering. By developing an equivalent parametric formulation of the general mixed-integer fractional program (MIFP), we propose four exact parametric algorithms based on the root-finding methods, including bisection method, Newton's method, secant method and false position method, respectively, for the global optimization of MIFPs. We also propose an inexact parametric algorithm that can potentially outperform the exact parametric algorithms for some types of MIFPs. Extensive computational studies are performed to demonstrate the efficiency of these parametric algorithms and to compare them with some general-purpose mixed-integer nonlinear programming methods. The applications of the proposed algorithms are illustrated through two case studies on process scheduling. Computational results show that the proposed exact and inexact parametric algorithms are more computationally efficient than several general-purpose solvers for solving MIFPs.",
     "keywords": ["Global optimization", "Parametric approach", "Inexact algorithms", "Process scheduling", "Mixed-integer fractional programming"]},
    {"article name": "A modeling framework for design of nonlinear renewable energy systems through integrated simulation modeling and metaheuristic optimization: Applications to biorefineries",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.10.005",
     "publication date": "02-2014",
     "abstract": "This study presents the development and implementation of a novel framework for optimal design of new and emerging renewable energy production systems by considering an iterative strategy which integrates the Net Present Value optimization along with detailed mechanistic modeling, simulation, and process optimization which yields optimal capacity plan, and operating conditions for the process. Due to the non-linear nature of process conversion mechanisms, metaheuristic algorithms are implemented in the framework to optimize operating conditions of process. Further, to apply complex kinetics in the process, we have made a linkage between process simulator (Aspen Plus) and Matlab. To demonstrate the effectiveness of the proposed methodology, a hypothetical case study of a lignocellulosic biorefinery is utilized. The proposed framework results reveal a deviation in optimal process yields and production capacities from initial literature estimates. These results indicate the importance of developing a multi-layered framework to optimally design a renewable energy production system.",
     "keywords": ["Process optimization", "Process simulation", "Renewables", "Lignocellulosic biorefinery", "Design framework", "Metaheuristic optimization"]},
    {"article name": "Optimal synthesis of thermally coupled distillation sequences using a novel MILP approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.10.015",
     "publication date": "02-2014",
     "abstract": "This paper introduces a novel MILP approach for the design of distillation columns sequences of zeotropic mixtures that explicitly include from conventional to fully thermally coupled sequences and divided wall columns with a single wall. The model is based on the use of two superstructure levels. In the upper level a superstructure that includes all the basic sequences of separation tasks is postulated. The lower level is an extended tree that explicitly includes different thermal states and compositions of the feed to a given separation task. In that way, it is possible to a priori optimize all the possible separation tasks involved in the superstructure. A set of logical relationships relates the feasible sequences with the optimized tasks in the extended tree resulting in a MILP to select the optimal sequence. The performance of the model in terms of robustness and computational time is illustrated with several examples.",
     "keywords": ["Distillation", "Thermally coupled distillation", "MILP", "Divided wall column", "Superstructure optimization", "GDP"]},
    {"article name": "Approximate solution of mp-MILP problems using piecewise affine relaxation of bilinear terms",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.10.009",
     "publication date": "02-2014",
     "abstract": "We propose an approximate solution strategy for multi-parametric mixed integer linear programming (mp-MILP) problems with parameter dependency at multiple locations in the model. A two-stage solution strategy, consisting of an approximation stage and a multi-parametric programming stage, is introduced. At the approximation stage, surrogate mp-MILP models are derived by overestimating bilinear terms in the constraints over an ab initio partitioning of the domain. We then incorporate piecewise affine relaxation based models using a linear partitioning scheme and a logarithmic partitioning scheme, respectively. The models are tuned by the number of partitions chosen. Problem sizes of the varied models, and computational requirements for the algorithmic procedure are compared. The conservatism of the suboptimal solution of the mp-MILP problem for the piecewise affine relaxation based two-stage method is discussed.",
     "keywords": ["Multi-parametric programming", "Mixed-integer programming", "Bilinear programming"]},
    {"article name": "Implementation of Galerkin and moments methods by Gaussian quadrature in advection\u2013diffusion problems with chemical reactions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.11.001",
     "publication date": "02-2014",
     "abstract": "This work presents a method to solve boundary value problems based on polynomial approximations and the application of the methods of moments and the Galerkin method. The weighted average residuals are evaluated by improved Gauss-Radau and Gauss-Lobatto quadratures, capable to exactly compute integrals of polynomials of degree 2n and 2n\u00a0+\u00a02 (where n is the number of internal quadrature points), respectively. The proposed methodology was successfully applied to solve stationary and transient problems of mass and heat diffusion in a catalyst particle and of a tubular pseudo-homogeneous chemical reactor with axial advective and diffusive transports. Through the improvement of the usual procedures of numerical quadratures, it was possible to establish a direct connection between the residuals on internal discrete points and the residuals on the boundaries, allowing the method to exactly reproduce the moments and Galerkin methods when applied to linear problems.",
     "keywords": ["Orthogonal collocation method", "Method of moments", "Galerkin method", "Gaussian quadrature"]},
    {"article name": "Direct numerical simulation of catalytic combustion in a multi-channel monolith reactor using personal computers with emerging architectures",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.11.002",
     "publication date": "02-2014",
     "abstract": "Computational Fluid Dynamic modeling of full-scale monolithic catalytic reactors has remained elusive due to the extreme computational requirements. While simulation of full-scale catalytic reactors would require domain decomposition based parallelism and use of multiple central processing units, significant performance enhancement can be achieved by fully utilizing the compute resources available within each node in emerging architectures. Here, a serial reacting flow solver was used as a starting point. Performance was enhanced using multi-threading for acceleration of surface chemistry, material properties calculations, and species equation solvers, and using graphical processing units for acceleration of the linear solvers and pre-conditioners. Of the two test cases presented here, the largest test case entails steady-state calculations for catalytic methane\u2013air combustion with 22 reaction steps and 19 species within a 13-channel catalytic monolith reactor discretized using 313,872 control volumes. For this particular test case, a speed-up factor of about 4.5 over serial calculations is noted.",
     "keywords": ["CFD", "Reacting flow", "Catalytic reactor", "GPU", "Multi-core", "High performance computing"]},
    {"article name": "On two-compartment population balance modeling of spray fluidized bed agglomeration",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.11.003",
     "publication date": "02-2014",
     "abstract": "The present work focuses on the modeling and analysis of a spray fluidized bed granulation (SFBG) process based upon the concept that particles are communicating between the two compartments at some steady state mass flow rate. A numerical technique for solving the proposed two-compartment model (2CM) is developed and validated against some newly derived analytical solutions. Moreover, the inverse technique for extracting the rate constant of one-compartment model (1CM) is extended to 2CM. A correlation of aggregation rate constant of 2CM with the rate constant of conventional 1CM under some restrictions is investigated and it is found that the 1CM cannot be used, in general, to predict results of 2CM. Furthermore, it is observed that the existence of two zones in SFBG is responsible to certain extent for time dependent behaviour of aggregation rate constant. Finally, influence of compartment sizes and particles residence times on particle size distribution is investigated.",
     "keywords": ["Modeling", "Aggregation", "Fluidized bed", "Two compartments", "Population balances"]},
    {"article name": "Oxidant control and air-oxy switching concepts for CFB furnace operation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.10.018",
     "publication date": "02-2014",
     "abstract": "Oxy combustion in circulating fluidized bed (CFB) boilers was investigated in this paper. Oxy combustion is a carbon capture and storage technology, which uses oxygen and recirculated flue gas (RFG) instead of air as an oxidant. Air and oxy combustion were compared through physical considerations and simulations, focusing on process dynamics, transients and control. The oxidant specific heat capacity and density are elevated in oxy combustion, which leads to slower temperature dynamics. Flue gas recirculation introduces internal feedback dynamics to the process. The possibility to adjust the RFG and oxygen flows separately gives an additional degree of freedom for control. In the simulations, \u201cdirect\u201d and \u201csequenced\u201d switches between air- and oxy-firing were compared. Fast \u201cdirect\u201d switches with simultaneous ramping of all inputs should be preferred due to the resulting smooth temperature responses. If these process input changes are unfeasible, the fuel should be altered after the gaseous flows (\u201csequenced\u201d method).",
     "keywords": ["Power plant", "Oxy combustion", "CCS technology", "Simulation", "System dynamics", "Process control"]},
    {"article name": "Macroscopic modelling of baker's yeast production in fed-batch cultures and its link with trehalose production",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.11.007",
     "publication date": "02-2014",
     "abstract": "A macroscopic model describing the influence of nitrogen on a fed-batch baker's yeast production process is proposed. First, on the basis of a set of biological reactions, inspired by the model of Sonnleitner and K\u00e4ppeli (1986), a model in which the nitrogen and glucose consumption are coordinated is proposed. Second, an attempt of estimating trehalose concentration in yeast cells through an extension of this model is presented. The model parameters are obtained via a non-linear least squares identification. It is validated with experimental data and successfully predicts the dynamics of growth, substrate consumption (nitrogen and carbon sources) and metabolite production (ethanol and trehalose). This model allows, on the one hand, quantitatively describing the link between nitrogen and glucose consumption in yeast cultures and, on the other hand, will be valuable for the determination of culture conditions aiming at maximizing yeast productivity while guaranteeing the accumulation of a required amount of trehalose.",
     "keywords": ["Mathematical modelling", "Baker's yeast", "Saccharomyces cerevisiae", "Nitrogen uptake", "Carbon uptake", "Trehalose production"]},
    {"article name": "A computationally efficient technique for the solution of multi-dimensional PBMs of granulation via tensor decomposition",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.10.020",
     "publication date": "02-2014",
     "abstract": "Multi-dimensional population balance equations (PBEs) are commonly used to describe the dynamics of particulate processes such as granulation. Such a class of equations are numerically complex and computationally intensive to solve due to the multiple internal coordinates involved. A computationally efficient model reduction technique would overcome the computational overheads associated with the solution of multi-dimensional PBEs. Moreover, this enables the process model to be used efficiently in process control and optimization. This study is concerned with the development of a novel reduced order model for a three-dimensional population balance model (PBM) for granulation, using a tensor decomposition technique in combination with separation of variables and singular value decomposition. These techniques were used to decompose the complex aggregation and breakage integrals. The developed model is faster by two orders of magnitude, requires less memory allocation for the storage of variables and results in negligible error when compared with the full model.",
     "keywords": ["Granulation", "Multi-dimensional population balance model", "Aggregation", "Breakage", "Model reduction", "Tensor decomposition"]},
    {"article name": "Dynamic modelling and optimisation of flexible operation in post-combustion CO2 capture plants\u2014A review",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.11.015",
     "publication date": "02-2014",
     "abstract": "The drive for efficiency improvements in post-combustion CO2 capture (PCC) technologies continues to grow, with recent attention being directed towards flexible operation of PCC plants. However, there is a lack of research into the effect of process disturbances when operating flexibly, justifying a need for validated dynamic models of the PCC process. This review critically examines the dynamic PCC process models developed to date and analyses the different approaches used, as well as the model complexity and their limitations. Dynamic process models coupled with economic analysis will play a crucial role in process control and optimisation. Also discussed are key areas that need to be addressed in future dynamic models, including the lack of reliable dynamic experimental data for their validation, development of feasible flexible operation and process control strategies, as well as process optimisation by integrating accurate process models with established economic analysis tools.",
     "keywords": ["Dynamic modelling", "Flexible operation", "Post-combustion carbon capture", "Coal-fired power stations"]},
    {"article name": "CFD\u2013DEM modeling of gas\u2013solid flow and catalytic MTO reaction in a fluidized bed reactor",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.08.007",
     "publication date": "01-2014",
     "abstract": "The methanol-to-olefins (MTO) process is currently being implemented successfully in fluidized bed reactors (FBRs) in China. Characterizing the gas\u2013solid flow is crucial in operating MTO FBRs effectively. In this work, a combined discrete element method (DEM) and computational fluid dynamics (CFD) model is developed to describe the gas\u2013solid flow behavior in an MTO FBR. In this model, the particles are modeled using DEM, and the gas is modeled using Navier\u2013Stokes equations. The combined model incorporates the lumped kinetics in the gas phase to achieve the MTO process. Moreover, the combined model can characterize the heat transfer between particles as well as that between the gas and the particles. The distinct advantage of the combined model is that real-time particle activity can be calculated by tracking the motion history of the catalyst particle with respect to heat transfer. The simulation results effectively capture the major features of the MTO process in FBR. Moreover, the simulation results are in good agreement with the classical calculation and experimental data. The particle motion pattern and distributions of a number of key flow-field parameters in the reactor are analyzed based on the validated model. The effects of operating conditions on FBR performance are also investigated. The simulation results show that the particle motion exhibits a typical annulus\u2013core structure, which promotes excellent transfer efficiency. The results also demonstrated that the feed temperature, inlet gas velocity, and feed ratio of water to methanol significantly affect reaction efficiency.",
     "keywords": ["Fluidization", "Granulation", "Multiphase reactors", "Mathematical modeling", "DEM", "MTO process"]},
    {"article name": "Multi-scale optimization for process systems engineering",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.07.009",
     "publication date": "01-2014",
     "abstract": "Efficient nonlinear programming (NLP) algorithms and modeling platforms have led to powerful process optimization strategies. Nevertheless, these algorithms are challenged by recent evolution and deployment of multi-scale models (such as molecular dynamics and complex fluid flow) that apply over broad time and length scales. Integrated optimization of these models requires accurate and efficient reduced models (RMs). This study develops a rigorous multi-scale optimization framework that substitutes RMs for complex original detailed models (ODMs) and guarantees convergence to the original optimization problem. Based on trust region concepts this framework leads to three related NLP algorithms for RM-based optimization. The first follows the classical gradient-based trust-region method, the second avoids gradient calculations from the ODM, and the third avoids frequent recourse to ODM evaluations, using the concept of \u03f5-exact RMs. We illustrate these algorithms with small examples and discuss RM-based optimization case studies that demonstrate their performance and effectiveness.",
     "keywords": ["Nonlinear programming", "Model reduction", "Trust region", "Process optimization"]},
    {"article name": "Parametric optimization with uncertainty on the left hand side of linear programs",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.08.005",
     "publication date": "01-2014",
     "abstract": "Although parametric optimization with uncertainties on the objective function (OF) or on the so-called \u201cright-hand-side\u201d (RHS) of the constraints has been addressed successfully in recent papers, very little work exists on the same with uncertainties on the left-hand-side (LHS) of the constraints or in the coefficients of the constraint matrix. The goal of this work has been to develop a systematic method to solve such parametric optimization problems. This is a very complex problem and we have begun with the simplest of optimization problems, namely the linear programming problem with a single parameter on the LHS. This study reviews the available work on parametric optimization, describes the challenges and issues specific to LHS parametric linear programming (LHS-pLP), and presents a solution algorithm using some classic results from matrix algebra.",
     "keywords": ["Parametric programming", "Left-hand-side", "Uncertainty", "Linear program", "LP"]},
    {"article name": "Comparison of methods for multivariate moment inversion\u2014Introducing the independent component analysis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.08.006",
     "publication date": "01-2014",
     "abstract": "Despite the advantages of the moment based methods in solving multivariate population balances models, these methods still suffer with the so-called multivariate moment inversion problem. Although univariate moment inversion is achieved without major problems this is not true for multivariate cases, for which there is no well established methodology. This work presents a comparative analysis of the existing methods regarding their accuracy and robustness. A new moment inversion method based on the independent component analysis was proposed and analyzed. Improvements in accuracy and robustness were achieved by combination of different moment inversion methods.",
     "keywords": ["Population balance", "Numerical analysis", "Optimization", "Mathematical modeling", "Multivariate moment inversion", "Independent component analysis"]},
    {"article name": "Application of a multi objective multi-leader particle swarm optimization algorithm on NLP and MINLP problems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.08.004",
     "publication date": "01-2014",
     "abstract": "This paper presents a modified particle swarm optimization algorithm for handling a variety of single and multi objective mixed integer nonlinear optimization problems that have equality and inequality constraints. An efficient multi-objective multi leader particle swarm optimization algorithm is used to handle the extra objective imposed by a novel constraint handling method. In addition, a modified method of handling binary variables is used and the algorithm is adapted to update discrete variables independent from continuous variables using these methods. The algorithm was applied on several well known test problems in the field of chemical engineering including the William Otto process. The results proved the applicability and the efficiency of this method for handling single and multi objective optimization problems in mixed integer and nonlinear decision spaces arising in the field of chemical engineering.",
     "keywords": ["Mixed integer nonlinear programming (MINLP)", "Particle swarm optimization", "Nonconvex", "Constrained", "Multi-objective"]},
    {"article name": "CATalytic \u2013 Post Processor (CAT-PP): A new methodology for the CFD-based simulation of highly diluted reactive heterogeneous systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.08.011",
     "publication date": "01-2014",
     "abstract": "Scaling-up and -down of conventional ideal lab-scale catalytic reactors often introduces strong non-idealities in the flow field which may drastically affect the expected reactor performances and effectiveness. Computational fluid-dynamic (CFD) analysis is required to assess the extent of such non-idealities. In the case of reactive systems, and in particular in the case of heterogeneous catalytic processes, however, computational efforts are huge and, due to the wide range of space and time scales, may become hardly sustainable when detailed chemical kinetics have to be implemented and solved for transient processes. A specific case, encountered in most of the applications of environmental catalysis, is represented by highly diluted process streams, where reactants and products have concentrations of the order of hundreds of part per million and the reaction duty is extremely low. In this case, the reactor fluid dynamic is not affected by the occurrence of chemical reactions, while the activity of the system can be significantly affected by the reactor hydrodynamics. A new simulation approach, named CATalytic \u2013 Post-Processor (CAT-PP), is proposed in this paper to treat those systems. Such approach results in a numerical tool which acquires the flow-field data from non-reactive CFD simulations, performed by a commercial code (ANSYS Fluent) and post-processes them by solving the species transport equations with a detailed kinetic scheme. The potential of the proposed approach is demonstrated through its application to the dynamic analysis of a FTIR-operando reactor-cell loaded with a lean-NOx-trap catalyst. It is shown that CAT-PP allows to effectively simulate a transient diluted catalytic process taking care of the actual reactor hydrodynamics even implementing detailed surface (or gas/solid) kinetic schemes.",
     "keywords": ["Reactive heterogeneous systems", "Reactive CFD simulations", "Catalysis", "Large-scale ODE solvers", "Non-ideal reactors", "Lean-NOx-Trap"]},
    {"article name": "Hybrid semi-parametric modeling in process systems engineering: Past, present and future",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.08.008",
     "publication date": "01-2014",
     "abstract": "Hybrid semi-parametric models consist of model structures that combine parametric and nonparametric submodels based on different knowledge sources. The development of a hybrid semi-parametric model can offer several advantages over traditional mechanistic or data-driven modeling, as reviewed in this paper. These advantages, such as broader knowledge base, transparency of the modeling approach and cost-effective model development, have been widely recognized, not only in academia but also in the industry.In this paper, the most common hybrid semi-parametric modeling and parameter identification techniques are revisited. Applications in the areas of (bio)chemical engineering for process monitoring, control, optimization, scale-up and model-reduction are reviewed. It is outlined that the application of hybrid semi-parametric techniques does not automatically lead into better results but that rational knowledge integration has potential to significantly improve model-based process operation and design.",
     "keywords": ["Hybrid modeling", "Hybrid neural modeling", "Semi-mechanistic modeling", "Hybrid grey-box modeling", "Hybrid semi-parametric modeling", "Process operation/design"]},
    {"article name": "Chemical supply chain modeling for analysis of homeland security events",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.07.014",
     "publication date": "01-2014",
     "abstract": "The potential impacts of man-made and natural disasters on chemical plants, complexes, and supply chains are of great importance to homeland security. To be able to estimate these impacts, we developed an agent-based chemical supply chain model that includes: chemical plants with enterprise operations such as purchasing, production scheduling, and inventories; merchant chemical markets, and multi-modal chemical shipments. Large-scale simulations of chemical-plant activities and supply chain interactions, running on desktop computers, are used to estimate the scope and duration of disruptive-event impacts, and overall system resilience, based on the extent to which individual chemical plants can adjust their internal operations (e.g., production mixes and levels) versus their external interactions (market sales and purchases, and transportation routes and modes). To illustrate how the model estimates the impacts of a hurricane disruption, a simple example model centered on 1,4-butanediol is presented.",
     "keywords": ["Agent-based modeling", "Supply chains", "Linear programming", "Economic markets", "Transportation networks", "Supply chain resilience"]},
    {"article name": "Investigation of the accuracy of the extrapolation method for the lattice Boltzmann simulation of viscous fluid flow in a Maxblend impeller system",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.08.013",
     "publication date": "01-2014",
     "abstract": "This paper offers a thorough assessment on the performance of the extrapolation method for the lattice Boltzmann simulation of viscous mixing flows. This method appears to be well-suited for the treatment of the complex boundary conditions found in various mixing systems. Here, the ability to simulate accurate power consumption and pumping capacity is evaluated on several configurations of the Maxblend mixing system, which has proven efficient in a wide range of applications. First, the impact of the boundary conditions on the spatial convergence of the lattice Boltzmann method (LBM) is determined on the 3D Couette flow, clearly showing that small modifications of the boundary conditions may reduce the accuracy of the predicted shear rate and power. Second, a parallel LBM scheme was used to simulate fluid flow within a Maxblend mixing system. For the unbaffled configuration, the simulated power consumption and the pumping capacity are observed to be in good agreement with experimental data and finite element simulation results. The effect of the bottom clearance is also successfully evaluated, suggesting that the standard bottom clearance is not optimum in the transitional regime. Lastly, results for the most geometrically complex case (baffled configuration) indicate that the power consumption is affected by numerical perturbations appearing around the moving impeller. Overall, these results show that, when combined with the extrapolation method for the treatment of boundary conditions, the LBM is an efficient tool for the investigation of viscous flow in mixers of industrial relevance.",
     "keywords": ["Lattice Boltzmann method", "Extrapolation method", "Viscous mixing flow", "Maxblend impeller", "Power consumption"]},
    {"article name": "IDEAS based synthesis of minimum volume reactor networks featuring residence time density/distribution models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.07.005",
     "publication date": "01-2014",
     "abstract": "This work addresses for the first time, the synthesis of globally minimum volume reactor networks, featuring segregated flow reactors (SFR) and/or maximum mixedness reactors (MMR), with the same normalized residence time density (NRTd) function. Global optimality is ascertained by demonstrating that the input\u2013output information maps of SFR and MMR with general RTd/RTD models satisfy all properties required for the application of the infinite dimensional state-space (IDEAS) approach to the RTd/RTD reactor network synthesis problem. The resulting IDEAS formulation is shown to possess a number of novel properties, which can be used to facilitate its solution. The power of the proposed methodology is demonstrated on three case studies featuring segregated laminar flow reactors (SLFR) in which the Trambouze reaction scheme is carried out. In one of the case studies, the identified reactor network is shown to have volume that is as low as half the volume of a single reactor.",
     "keywords": ["CSTR continuous stirred tank reactor", "continuous stirred tank reactor", "DN distribution network", "distribution network", "IDEAS infinite dimensional state-space", "infinite dimensional state-space", "ILP infinite-dimensional linear program", "infinite-dimensional linear program", "MINLP mixed-integer nonlinear program", "mixed-integer nonlinear program", "MMR maximum mixedness reactor", "maximum mixedness reactor", "NRTd normalized residence time density function", "normalized residence time density function", "OP operator network", "operator network", "PFR plug flow reactor", "plug flow reactor", "RTd residence time density", "residence time density", "RTD residence time distribution", "residence time distribution", "SFR segregated flow reactor", "segregated flow reactor", "SLFR segregated laminar flow reactors", "segregated laminar flow reactors", "SR segregated reactor", "segregated reactor", "Reactor", "Network", "RTD", "Volume", "Global", "Optimum"]},
    {"article name": "A robust and efficient triangulation-based optimization algorithm for stochastic black-box systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.09.003",
     "publication date": "01-2014",
     "abstract": "Optimization of process variables is an important, yet difficult, task for systems-level analysis and design of complex stochastic systems. Here, we introduce the Simplex-Triangulation Optimization (STO) algorithm to optimize stochastic black-box systems efficiently in fewer iterations than other comparable algorithms without requiring gradient information or detailed initial guesses. The STO algorithm is shown to converge linearly. Several test functions are utilized to compare the STO algorithm to the Particle Swarm Optimization (PSO) and Finite Difference Stochastic Approximation (FDSA) algorithms, which are often used for parameter optimization in stochastic systems.",
     "keywords": ["Stochastic optimization", "Equation-freesystems", "Smulation-based optimization", "Kinetic Monte Carlo"]},
    {"article name": "Computational methods for the simultaneous strategic planning of supply chains and batch chemical manufacturing sites",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.09.001",
     "publication date": "01-2014",
     "abstract": "In this work we present efficient solution strategies for the task of designing supply chains with the explicit consideration of the detailed plant performance of the embedded facilities. Taking as a basis a mixed-integer linear programming (MILP) model introduced in a previous work, we propose three solution strategies that exploit the underlying mathematical structure: A bi-level algorithm, a Lagrangean decomposition method, and a hybrid approach that combines features from both of these two methods. Numerical results show that the bi-level method outperforms the others, leading to significant CPU savings when compared to the full space MILP.",
     "keywords": ["Supply chain", "Design and planning", "Mixed-integer linear programming", "Large-scale optimization", "Decomposition methods"]},
    {"article name": "A hybrid stochastic-deterministic algorithm for lattice-gas models of catalytic reactions and the computation of TPD spectra",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.08.010",
     "publication date": "01-2014",
     "abstract": "We present a hybrid numerical approach for modeling surface reactions in the framework of a lattice-gas model with lateral interactions between adsorbed particles. A hybrid multiscale algorithm, which we refer to as Quasi-Equilibrium Kinetic Monte Carlo (QE-KMC), comprises traditional Metropolis Monte Carlo (MMC) simulations of equilibrium systems and standard numerical methods for deterministic ordinary differential equations (ODEs). The functional dependence of these ODEs on the macroscopic state variables (adsorbate coverages) is not explicitly known, but their right-hand sides can be evaluated \u201con the fly\u201d with prescribed accuracy by means of the MMC simulations. At the time scale of these ODEs it is assumed that an equilibrium statistical distribution of adsorbed particles on an infinite lattice is attained at every moment in time due to infinitely fast surface diffusion. QE-KMC and conventional KMC simulations are used to study the temperature-programmed desorption (TPD) spectra of adsorbed particles. We critically discuss results of previous studies that applied Monte Carlo simulations to describe the TPD spectra in the case of fast adsorbate diffusion and strong lateral interactions. We show that the quasi-equilibrium TPD spectra can be quickly and accurately estimated by the QE-KMC algorithm, while the KMC simulations require much more extensive computational resources to obtain the same results.",
     "keywords": ["Catalysis", "Kinetics", "Desorption", "Diffusion", "Simulation", "Multiscale modeling"]},
    {"article name": "Implementation of multi agents based system for process supervision in large-scale chemical plants",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.08.012",
     "publication date": "01-2014",
     "abstract": "Modern chemical plants have evolved into extremely large and complex operations. Operators rely on the plant automation system, particularly the DCS, for managing the plant operations which themselves have become open, and involve multiple third-party technologies, instruments, and software. The structural-, scale- and dynamic-complexity makes it challenging for operators to infer the conditions in the plant quickly and make timely decisions, especially during abnormal situations. A process supervision system that assists the operators by providing holistic decision support is therefore essential. Here, we propose an multi agents based architecture for supervision of large-scale chemical plants. The key insight in the proposed architecture is that the process descriptors used for developing the supervision models themselves are not static and change routinely. The proposed architecture uses an ontology to represent all the process descriptors formally, so that any changes can be captured and their effects propagated seamlessly. This architecture has been implemented as a multi agent system called ENCORE. The detailed implementation of ENCORE is presented and its benefits are illustrated through an offshore oil and gas production case study.",
     "keywords": ["Process retrofits", "Transitions", "OntoSafe", "Monitoring", "Fault diagnosis"]},
    {"article name": "A fully coupled, parallel approach for the post-processing of CFD data through reactor network analysis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.09.002",
     "publication date": "01-2014",
     "abstract": "In this paper we describe and apply a parallel code, named KPPSMOKE, for the prediction of pollutant emissions from combustion devices operating in turbulent conditions. The approach is based on the kinetic post-processing of CFD simulations, which are transformed into equivalent networks of perfectly stirred reactors and solved using a detailed kinetic mechanism (hundreds of species). The numerical algorithm is based on a fully-coupled technique, in which the highly non-linear mass balance equations are solved together, by alternating different resolution methods in order to ensure high accuracy and fast convergence. As a result of KPPSMOKE parallel structure, large reactor networks characterizing industrial devices (105\u2013106 reactors) can be solved in reasonable times (\u223chours).The accuracy and the reliability of the algorithm was demonstrated on a lab-scale burner and on a full-scale industrial device, i.e. a combustor for aircrafts. The numerical performance was also assessed in terms of parallel efficiency and speedup.",
     "keywords": ["Turbulent combustion", "NOx", "Non-linear systems", "Kinetic post-processing", "MPI", "Detailed kinetics"]},
    {"article name": "An efficient multi criteria process optimization framework: Sustainable improvement of the Dimethyl Ether Process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.09.011",
     "publication date": "01-2014",
     "abstract": "No longer is it appropriate to evaluate processes for economic feasibility alone; social benefits and environmental impacts must also be considered. This work develops a multi criteria systematic framework for optimum process design with sustainability considerations and addresses a major challenge in process design as engineers will now be able to design for multiple criteria, specifically sustainability concerns. A novel multi objective optimization algorithm capable of handling a multitude of objectives in mixed integer nonlinear search space and a novel decision making methodology are integrated with a sequential modular process simulator to optimize processes with a multitude of objectives rigorously and efficiently. The application of this research extends well beyond sustainability considerations to the very vast area of optimum design with multi criteria. The application of the proposed framework is demonstrated through a case study for process retrofit of Dimethyl Ether production plant.",
     "keywords": ["Sustainable process design", "Multi objective optimization", "Particle swarm", "Nonconvex", "Fuzzy decision making", "Superstructure optimization"]},
    {"article name": "A phenomenological model of the mechanisms of lignocellulosic biomass pyrolysis processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.09.008",
     "publication date": "01-2014",
     "abstract": "A comprehensive particle scale model for pyrolysis of biomass has been developed by coupling the reaction mechanisms and transport phenomena. The model, which also accounts for the combined effect of various parameters such as particle shrinkage and drying, was validated using available experimental data from the literature. The validated model was then used to study the effect of operating temperature and biomass particle size, both of which strongly influenced the rate of biomass conversion. For example, for particle sizes less than 1\u00a0mm, a uniform temperature throughout the particle was predicted, thus leading to higher conversion rates in comparison to those in the larger particles. On the other hand, any increase in moisture content led to considerable decrease in the rate of biomass conversion. For the operating conditions considered in this study, the volumetric particle shrinkage also increased the decomposition of biomass to end products.",
     "keywords": ["Biomass", "Biochar", "Kinetics", "Modelling", "Pyrolysis", "Tar"]},
    {"article name": "Dynamic optimization using adaptive direct multiple shooting",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.09.017",
     "publication date": "01-2014",
     "abstract": "We present a wavelet-based grid refinement approach for direct multiple shooting applied to dynamic optimization problems. The algorithm, named adaptive multiple shooting, automatically generates a problem-dependent parameterization of the control profiles: Starting from an initially coarse parameterization, the control grid is refined iteratively using a wavelet analysis of the previously obtained optimal solution. Additional grid points are only inserted where required and redundant grid points are eliminated. Hence, the algorithm minimizes the number of grid points required to obtain accurate optimal control trajectories.First, we demonstrate the superiority of adaptive grid refinement over an equidistant discretization for the Williams\u2013Otto semi-batch reactor employing multiple and single shooting. Here, the accuracy is checked using an optimal solution obtained by an indirect optimization approach. Second, we successfully demonstrate the efficiency of adaptive grid refinement compared to an equidistant discretization employing multiple shooting to a dynamically unstable HIPS (high impact polystyrene) polymerization reactor.",
     "keywords": ["Dynamic optimization", "Optimal control", "Direct multiple shooting", "Adaptive grid refinement", "Semi-batch reactor", "HIPS polymerization"]},
    {"article name": "Optimal variable selection for effective statistical process monitoring",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.09.014",
     "publication date": "01-2014",
     "abstract": "In a typical large-scale chemical process, hundreds of variables are measured. Since statistical process monitoring techniques typically involve dimensionality reduction, all measured variables are often provided as input without weeding out variables. Here, we demonstrate that incorporating measured variables that do not provide any additional information about faults degrades monitoring performance. We propose a stochastic optimization-based method to identify an optimal subset of measured variables for process monitoring. The benefits of the reduced monitoring model in terms of improved false alarm rate, missed detection rate, and detection delay is demonstrated through PCA based monitoring of the benchmark Tennessee Eastman Challenge problem.",
     "keywords": ["Fault detection", "Optimization", "Process control", "Safety", "Systems engineering", "Tennessee Eastman Process"]},
    {"article name": "Hybrid method integrating agent-based modeling and heuristic tree search for scheduling of complex batch processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.09.004",
     "publication date": "01-2014",
     "abstract": "We propose a hybrid method integrating agent-based modeling and heuristic tree search to solve complex batch scheduling problems. Agent-based modeling describes the batch process and constructs a feasible schedule under various constraints. To overcome myopic decisions of agents, the agent-based simulation is embedded into a heuristic search algorithm. The heuristic algorithm partially explores the solution space generated by the agent-based simulation. Because global information of the objective function value is used in the search algorithm, the schedule performance is improved. The proposed method shares the advantages from both agent-based modeling and mixed integer programing, achieving a better balance between the solution efficiency and the schedule performance. As a polynomial-time algorithm, the hybrid method is applicable to large-scale complex industrial scheduling problems. Its performance is demonstrated by comparing with agent-based modeling and mixed integer programing in two case studies, including a complex one from The Dow Chemical Company.",
     "keywords": ["Complex batch scheduling", "Hybrid method", "Agent-based modeling", "Heuristic search", "Mixed integer programing"]},
    {"article name": "Adaptive mesh refinement of gas\u2013liquid flow on an inclined plane",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.09.007",
     "publication date": "01-2014",
     "abstract": "Carbon Capture & Storage (CCS) is one of the various methods that can be used to reduce the carbon footprint of the energy sector. The efficiency with which CO2 is absorbed from flue gas using packed columns is highly dependent on the structure of the liquid films that form on the packing materials. This work examines the hydrodynamics of these liquid films using the CFD solver, OpenFOAM to solve two-phase, isothermal, non-reacting flow using the volume-of-fluid (VOF) method. Local adaptive mesh refinement (AMR) is used to ensure improved resolution of the geometrical grids at the gas\u2013liquid interface. Comparisons are made between the solutions obtained using AMR and those obtained using highly refined static meshes. It was observed that local AMR produced results with much better correlation to experimental data.",
     "keywords": ["CCS", "AMR", "Liquid film flow", "Packed columns", "Wetted area"]},
    {"article name": "Optimization of reagents injection in a stirred batch reactor by numerical simulation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.09.005",
     "publication date": "01-2014",
     "abstract": "Computational fluids dynamic was used to analyze the mixing operation within a stirred batch reactor to distribute rapid and homogeneously reagents, used in the refining process of liquid lead. The flow pattern and distribution of the reagents inside the reactor were analyzed through tracer response curves obtained by numerical simulation.The predominant mechanism of momentum and mass transfer for macro-mixing is convection for the mean and eddy flows. Based on the assumption that the tracer is distributed in the vessel by convection and diffusion, the dynamic distribution of the tracer concentration inside the stirred batch reactor was calculated by solving the Reynolds-averaged conservation equations and the Realizable \u03ba\u2013\u025b turbulence model. The mean and tracer flow was considered as incompressible, isothermal and single phase under turbulent conditions.To optimize the injection point of reagents in the stirrer batch reactor, several simulated tracer concentration curves were obtained from monitoring points located at different radial and axial positions.",
     "keywords": ["Tracer", "Concentration", "Evolution", "Curve", "Batch reactor", "CFD"]},
    {"article name": "Robust nonlinear state estimation of bioreactors based on H\u221e hybrid observers",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.09.013",
     "publication date": "01-2014",
     "abstract": "This paper proposes a robust nonlinear observer for bioreactors combining the classical asymptotic observer and a nonlinear Luenberger-like observer. The resulting hybrid observer considers a new definition of the hybridization parameter which reflects the kinetic model confidence. The nonlinear observer is tuned on the basis of robust H-infinity approach and the differential-algebraic representation (DAR) of nonlinear systems. A simulated case study concerning fed-batch animal cell cultures is presented to demonstrate the potentials and advantages of the proposed approach for state estimation of bioreactors.",
     "keywords": ["State estimation", "Nonlinear observer", "Hybrid observer", "Robust H\u221e observer", "DAR approach", "Bioreactor"]},
    {"article name": "Column generation heuristics for ship routing and scheduling problems in crude oil transportation with split deliveries",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.09.019",
     "publication date": "01-2014",
     "abstract": "We propose a column generation based heuristic algorithm to solve a ship routing and scheduling problem for crude oil transportation with split deliveries. The problem is to find an optimal assignment and sequence and loading volume of demand simultaneously in order to minimize the total distance satisfying the capacity of tankers. The problem can be considered as a multi-product heterogeneous fleet split pickup ship routing problem with finite capacity and loading constraints. An efficient heuristic algorithm based on the column generation method is developed to generate a feasible solution taking into account of practical constraints. The performance of the proposed method is compared with the branch and bound algorithm and that of human operators. Computational results demonstrate the effectiveness of the proposed algorithm for a real case.",
     "keywords": ["Crude oil transportation", "Split delivery vehicle routing problem", "Column generation", "Heuristics"]},
    {"article name": "Bayesian and Expectation Maximization methods for multivariate change point detection",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.09.012",
     "publication date": "01-2014",
     "abstract": "Process data are the most important information in all aspects of plant monitoring and control applications. These data, stemming from instruments, carry the necessary information that assists plant operations. One of the common problems of process instrument readings is their deviation from true values due to instrument bias or systematic error. Detection of change points in process data is the first step for a more insightful analysis of hidden factors affecting the process. In this paper, both Bayesian and Expectation and Maximization (EM) methods are considered for change point detection problem of multivariate data with both single and multiple changes. The performance of EM is compared with the Bayesian approach. Simulation results show superiority of EM in the case of improper selection of priors while the Bayesian approach has less computation demand. The proposed algorithms are evaluated through several examples, two from simulated random data and one from a CSTR problem. It is also verified through an experimental study of a hybrid tank system.",
     "keywords": ["Change point detection", "Bayesian inference", "Expectation Maximization"]},
    {"article name": "Multi-criteria optimization in chemical process design and decision support by navigation on Pareto sets",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.09.015",
     "publication date": "01-2014",
     "abstract": "Designing chemical processes is a multi-criteria optimization problem with conflicting objectives. It can efficiently be solved using Pareto sets. These sets contain all solutions for which an improvement in any objective can only be achieved by accepting a decline in at least one other objective. This work integrates a novel algorithm to determine Pareto sets in a state-of-the-art steady-state flow sheet simulator. An approximation of predefined accuracy of the Pareto set, which can be convex or non-convex, is calculated. The decision maker can then navigate interactively on the Pareto set and explore the different optimal solutions. His decision is, hence, embedded in the knowledge of the entire Pareto set. The application of the method is illustrated by an example in which a distillation process for the separation of an azeotropic mixture (acetone\u00a0+\u00a0chloroform) is designed. Two process variants are compared: a pressure-swing and an entrainer distillation.",
     "keywords": ["Multi-objective optimization", "Non-convex Pareto frontier", "Interactive navigation", "Computer-aided process design", "Algorithm", "Software tool"]},
    {"article name": "An active specification switching strategy that aids in solving nonlinear sets and improves a VNS/TA hybrid optimization methodology",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.10.002",
     "publication date": "01-2014",
     "abstract": "A method to aid in convergence of nonlinear equations by relocating the solver is introduced. Deactivating known variables and activating intelligently chosen unknown variables at predicted values efficiently formulates initial conditions. This repair strategy is integrated into an optimization procedure. The repair algorithm has the ability to (1) relocate the solver for non-converged models caused by poor initial conditions and (2) utilize points formulated in relocating non-converged models caused by infeasible sets of decision variables as a perturbation phase of a stochastic optimization algorithm. To show the effectiveness of the proposed repair strategy, a hybrid metaheuristic of a Variable Neighborhood Search (VNS) and Threshold Accepting (TA) is tested on the optimization of a large-scale industrial process, the primary units of a crude oil refinery. The performance of the hybrid VNS/TA Metaheuristic with repair is compared to the algorithm without the repair technique.",
     "keywords": ["Nonlinear set solutions", "Hybrid metaheuristics", "Distillation optimization", "Stochastic optimization"]},
    {"article name": "Alternate approximation of concave cost functions for process design and supply chain optimization problems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.10.001",
     "publication date": "01-2014",
     "abstract": "This short note presents an alternate approximation of concave cost functions used to reflect economies of scale in process design and supply chain optimization problems. To approximate the original concave function, we propose a logarithmic function that is exact and has bounded gradients at zero values in contrast to other approximation schemes. We illustrate the application and advantages of the proposed approximation.",
     "keywords": ["Concave cost functions", "Nonlinear optimization", "Process synthesis"]},
    {"article name": "Multivariate monitoring of a carbon dioxide removal process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.09.010",
     "publication date": "01-2014",
     "abstract": "Process hardware improvements have significantly increased the amount of information collected in industrial facilities, which allows the use of tools such as multivariable statistical analysis for process monitoring. Nevertheless, such statistical models tend to be static and extremely general when implemented in process facilities with several modes of operation, as is the case of carbon dioxide removal processes. This work demonstrates the use of multivariable statistical analysis for process transitions between different modes of operation. Continuous process analytics are used to define key variables, named \u201cstate variables\u201d, to determine the current mode of operation. This work also makes use of parallel coordinates to illustrate the simultaneous visualization of several transition paths and statistical tests. Such tests apply confidence limits that are appropriate to the current mode of operation. This methodology is successfully tested in the CO2 capture plant at the University of Texas in Austin. The results show the effectiveness of using this type of application to detect abnormal operating conditions at different levels of operations.",
     "keywords": ["Process monitoring", "Multivariable statistical control", "Carbon dioxide removal", "Parallel coordinates", "Fault detection", "Multimode operations"]},
    {"article name": "A computer aided optimal inventory selection system for continuous quality improvement in drug product manufacture",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.09.020",
     "publication date": "01-2014",
     "abstract": "The multivariate interaction of the raw materials\u2019 physical properties can be critical to the quality of the final drug product. Although an elegant solution to this problem is the establishment of multivariate specifications this becomes difficult (if not impossible) to implement when the interactions take place across materials that are sourced by different vendors. As an alternate solution, this work presents a feed-forward corollary approach to model predictive control (MPC) to improve the product quality from a lot-driven-operation; where there are no available manipulated variables (MV) in the process. In these special cases the only degree of freedom available to be used as a MV for control is the lot-to-lot variability in the raw materials. This work presents an extension to our earlier work (Ind. Eng. Chem. Res. 2013, 52 (17), pp. 5934\u20135942) to consider a horizon of n lots to be manufactured. By considering this horizon of future lots (rather than just the next one) our method allows the discretionary use of all materials to ensure that the quality of all the future n lots is within specification. This paper presents a detailed discussion of the objective function used and also reports the results of implementing this method to the manufacture of a pharmaceutical drug product in a commercial manufacturing setting.",
     "keywords": ["Quality by design", "Latent variable regression", "Pharmaceutical drug product", "Optimization", "PLS", "MINLP"]},
    {"article name": "Computational Fluid Dynamics modeling of micromixing performance in presence of microparticles in a tubular sonoreactor",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.09.006",
     "publication date": "01-2014",
     "abstract": "This paper reports the results of CFD modeling for evaluating micromixing efficiency in presence of polymeric microparticles in a continuous tubular sonoreactor. The studied tubular sonoreactor was equipped with four 1.7\u00a0MHz ultrasound transducers and micromixing efficiency was analyzed using Villermaux/Dushman reaction. The main objective of this study is to illustrate the simultaneous effects of 1.7\u00a0MHz ultrasound waves and polymeric microparticles on micromixing performance from the fluid dynamics point of view. In order to model the presence of these microparticles, the Eulerian multiphase model was applied based on kinetic theory of granular flow. The dynamic mesh method was used to model the vibration of 1.7\u00a0MHz piezoelectric transducers. CFD modeling results indicate the positive effects of the presence of microparticles on micromixing efficiency and more efficient velocity distribution inside the sonoreactor. This was interpreted as the ability of high frequency ultrasound waves (1.7\u00a0MHz) to move and disperse the microparticles.",
     "keywords": ["Sonoreactor", "CFD modeling", "Micromixing", "Microparticle", "Segregation index", "Ultrasound"]},
    {"article name": "Design of multi-functional microfluidic ladder networks to passively control droplet spacing using genetic algorithms",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.09.009",
     "publication date": "01-2014",
     "abstract": "Accelerated progress in the use of droplet-based microfluidics for high throughput screening and biochemical analysis will require development of devices that are robust to experimental uncertainties and which offer multiple functionalities. Achieving precise functionalities in microfluidic devices is challenging because droplets exhibit complex dynamic behavior in these devices due to hydrodynamic interactions and discontinuities that are a result of discrete decision-making at junctions. For example, even a simple loop device can show transitions from periodic to aperiodic/chaotic behavior based on input conditions. Hence, rational design frameworks that handle this complexity are required to move this field from labs to industrial practice. Two main challenges that need to be confronted in the realization of such a rational design framework are: (i) computational science related to rapid simulation of very large networks; development of predictive models that will form the basis for characterizing droplet motion through interconnected and intricate large-scale networks, and (ii) conceptualization of a design approach that is generic in nature and not very narrowly defined limiting its application potential. In this paper, we develop a GA approach for the design of ladder networks that are used to control the relative droplet distance at the exit. Through several case studies, the potential of the proposed GA approach in designing exquisite ladder structures for multiple functions is demonstrated. A recently proposed network model is used as the basis for all the computational studies reported in this paper.",
     "keywords": ["Droplet-based microfluidics", "Genetic algorithms", "Ladder networks", "Synchronization"]},
    {"article name": "Improved initialization of players in leapfrogging optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.08.009",
     "publication date": "01-2014",
     "abstract": "A method to determine the initial number of players (particles) in Leapfrogging, a multi-player optimization algorithm, to best balance the probability of finding the global minimum with fewest objective function evaluations is developed and demonstrated.",
     "keywords": ["Optimization", "Player initialization", "Leapfrogging"]},
    {"article name": "A systematic methodology for the environomic design and synthesis of energy systems combining process integration, Life Cycle Assessment and industrial ecology",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.05.025",
     "publication date": "12-2013",
     "abstract": "This paper presents a systematic methodology for sustainable process systems design, combining the principles of industrial ecology, process design and process integration, Life Cycle Assessment (LCA) and multi-objective optimization (MOO). The superstructure considers an extended decision perimeter and embeds models based either on flowsheeting software or average market technologies, for which energy and material flows are extracted from the Life Cycle Inventory (LCI) database. Therefore, the overall supply chain can be synthesized within a given action system and the systematic recyclings identified. The methodology can be used to design eco-industrial parks or urban systems, to identify the best conversion pathways of resources or waste, or to fix the optimal value of environmental taxes. It is illustrated by an application to the environomic design of an urban energy system. This case study considers multiple energy services to be supplied and waste to be treated, with their seasonal variations, indigenous and imported resources, as well as different candidate conversion technologies. Results demonstrate that integrating an environmental objective in the design procedure leads to consider different system configurations than if only economic aspects are considered. The problematic of the optimal value of a CO2 tax is as well addressed.",
     "keywords": ["Process systems design", "Process integration", "Optimization", "Energy conversion systems", "Life Cycle Assessment", "Industrial ecology", "Urban systems"]},
    {"article name": "An application of a multi-agent auction-based protocol to the tactical planning of oil product transport in the Brazilian multimodal network",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.06.007",
     "publication date": "12-2013",
     "abstract": "The transportation planning of oil products in Brazil's multimodal network is a complex problem that involves various negotiations between decentralized entities in order to agree on which products to transfer, their amounts, and the allocation of shared resources. Due to such complexity, there is a clear need for a decision support tool. This paper presents a model for solving this problem based on a new multi-agent auction protocol. The developed model is used to solve a real scenario of the Brazilian oil supply chain in a multi-level topological approach.",
     "keywords": ["Multi-agent", "Auction", "Planning", "Oil derivatives", "Transport"]},
    {"article name": "An ontological approach towards enabling processing technologies participation in industrial symbiosis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.03.022",
     "publication date": "12-2013",
     "abstract": "A new ontological framework which supports processing technologies participation in Industrial Symbiosis (IS) is proposed. The framework uses semantic web service formalism to describe technology based on tacit knowledge embedded in the domain ontology and explicit knowledge acquired from the users. To enhance technology discovery, IS relevant processing technology classification and characterization is proposed. Technology participation in IS is addressed. Partial semantic input\u2013output matching is used to propose complex and innovative networks assessed and ranked by their technological, economic and environmental benefits. The proposed framework is implemented as a web service and operation demonstrated using a case study.",
     "keywords": ["Ontology", "Processing technology", "Industrial symbiosis"]},
    {"article name": "A systematic framework for enterprise-wide optimization: Synthesis and design of processing networks under uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.03.018",
     "publication date": "12-2013",
     "abstract": "In this paper, a systematic framework for synthesis and design of processing networks under uncertainty is presented. Through the framework, an enterprise-wide optimization problem is formulated and solved under uncertain conditions, to identify the network (composed of raw materials, process technologies and product portfolio) which is feasible and have optimal performances over the entire uncertainty domain. Through the integration of different methods, tools, algorithms and databases, the framework guides the user in dealing with the mathematical complexity of the problems, allowing efficient formulation and solution of large and complex enterprise-wide optimization problem. Tools for the analysis of the uncertainty, of its consequences on the decision-making process and for the identification of strategies to mitigate its impact on network performances are integrated in the framework. A decomposition-based approach is employed to deal with the added complexity of the optimization under uncertainty. A network benchmarking problem is proposed as a benchmark for further development of methods, tools and solution approaches. To highlight the features of the framework, a large industrial case study dealing with soybean processing is formulated and solved.",
     "keywords": ["Enterprise-wide optimization", "Integrated business and engineering", "Decision-making under uncertainty", "Mixed-integer non-linear program (MINLP)", "Vegetable oil", "Process synthesis", "Product portfolio management"]},
    {"article name": "Model-based conceptual design and optimization tool support for the early stage development of chemical processes under uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.06.017",
     "publication date": "12-2013",
     "abstract": "Despite many efforts to formalize the early phase of conceptual process design, no tool-supported systematic method for the conceptual design of chemical processes has been developed to date. This contribution presents a methodology and a software tool for the early conceptual design phase, which is characterized by limited and uncertain information on the available process units. The proposed methodology supports the graphical modeling of hierarchical superstructures which are subsequently optimized by a hybrid evolutionary algorithm considering uncertainties in model parameters.",
     "keywords": ["Chemical processes", "Design", "Mathematical modelling", "Optimisation"]},
    {"article name": "Optimal design of chemical processes with chance constraints",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.05.029",
     "publication date": "12-2013",
     "abstract": "Generally, chemical processes (CP) are designed with the use of inaccurate mathematical models. Therefore, it is important to create a chemical process that guarantees satisfaction of all design specifications either exactly or with some probability. The paper considers the issue of chemical process optimization when at the operation stage the design specification should be met with some probability and the control variables can be changed. We have developed a common approach for solving the broad class of optimization problems with normally distributed uncertain parameters. This class includes the one-stage and two-stage optimization problems with chance constraints. This approach is based on approximate transformation of chance constraints into deterministic ones.",
     "keywords": ["Systems engineering", "Chemical processes", "Optimization", "Uncertainty", "Normal distribution"]},
    {"article name": "A new dual modifier-adaptation approach for iterative process optimization with inaccurate models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.03.019",
     "publication date": "12-2013",
     "abstract": "In order to deal with plant-model mismatch, iterative process optimization schemes use some adaptation strategy based on measurements. The modifier-adaptation approach consists in performing first-order corrections of the cost and constraint functions in the model-based optimization problem. The approach has the ability to converge to the true process optimum but the first-order corrections require the experimental estimation of the process gradients. Dual modifier-adaptation algorithms estimate the gradients by finite difference approximation based on the measurements obtained at the current and past operating points. In order to guarantee the accuracy of the estimated gradients a constraint is added to the optimization problem in order to position the next operating points with respect to the previous ones. This paper presents an alternative first-order correction, which provides an improved approximation of the cost and constraint functions, together with a new gradient error constraint for use in dual modifier adaptation. By means of the Williams\u2013Otto reactor case study, the new dual modifier-adaptation approach is compared in simulation with a previous approach found in the literature showing faster convergence to a neighborhood of the plant optimum.",
     "keywords": ["Iterative process optimization", "Real-time optimization", "Modifier adaptation"]},
    {"article name": "Optimal operation of RO system with daily variation of freshwater demand and seawater temperature",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.03.020",
     "publication date": "12-2013",
     "abstract": "The optimal operation policy of flexible RO systems is studied in this work. The design and operation of RO process is optimized and controlled considering variations in water demands and changing seawater temperature throughout the day. A storage tank is added to the system layout to provide additional operational flexibility and to ensure the availability of freshwater to customer at all times. A steady state model for the RO process is developed and linked with a dynamic model for the storage tank. The membrane modules are divided into a number of groups to add flexibility in operation to RO network. The total operating cost of the RO process is minimized in order to find the optimal layout and operating variables at discreet time intervals for three design scenarios.",
     "keywords": ["RO", "Dynamic modeling", "Optimization", "Variable water demand", "Seawater temperature"]},
    {"article name": "Iterative learning control for the systematic design of supersaturation controlled batch cooling crystallisation processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.05.027",
     "publication date": "12-2013",
     "abstract": "The paper presents an approach to improve the product quality from batch-to-batch by exploiting the repetitive nature of batch processes to update the operating trajectories using process knowledge obtained from previous runs. The data based methodology is focused on using the linear time varying (LTV) perturbation model in an iterative learning control (ILC) framework to provide a convergent batch-to-batch improvement of the process performance indicator. The major contribution of this work is the development of a novel hierarchical ILC (HILC) scheme for systematic design of the supersaturation controller (SSC) of seeded batch cooling crystallizers. The HILC is used to determine the required supersaturation setpoint for the SSC and the corresponding temperature trajectory required to produce crystals with desired end-point property. The performance and robustness of these approaches are evaluated through simulation case studies. These results demonstrate the potential of the ILC approaches for controlling batch processes without rigorous process models.",
     "keywords": ["Batch processes", "Iterative learning control", "LTV perturbation model", "Operating-data based control", "Systematic supersaturation control", "Hierarchical ILC"]},
    {"article name": "A method to coordinate decentralized NMPC controllers in oxygen distribution networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.05.023",
     "publication date": "12-2013",
     "abstract": "This paper deals with the optimal operation of large scale systems composed by local processes liked by shared resources. A decentralized architecture plus a coordinator, which guarantees the satisfaction of the global constraints of the process, is presented. The decomposition of the control problem into smaller ones is based on Lagrangean decomposition and on price coordination methods to update the prices. A coordination method that allows formulating the price assignment as a control problem is presented besides a formulation based on market behaviour. Both approaches are driven by the difference between the total shared resources available and demanded by the local NMPC controllers. One advantage of this approach is that in the low layer only requires adding an extra term in the cost function of the existing NMPC controllers. Moreover, there is no communication between local controllers, only between each local controller and the coordinator.",
     "keywords": ["Predictive control", "Nonlinear models", "Price coordination", "Large-scale systems", "Distributed control"]},
    {"article name": "Improving dryer energy efficiency and controllability simultaneously by process modification",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.06.006",
     "publication date": "12-2013",
     "abstract": "This work establishes a relationship between dryer energy performance and controllability using energy balances and process resiliency analysis. It is shown that using the process gain matrix, the dryer energy efficiency can be reliably calculated with conditions for simultaneous controllability improvement established. By incorporating a drying rate modifying system such as a desiccant dehumidifier as an add-on, these conditions are shown to be achievable due to the extra dehumidification which can be manipulated using the additional degrees of freedom introduced by the sorption system. Due to the adsorbent regulation properties which are enhanced by high-temperature regeneration, the resilience of energy performance to disturbances is significantly improved compared to conventional dryers. Also, a desiccant system performance indicator, the \u201cadsorber\u2013regenerator net energy efficiency ARNEE\u201d is introduced and it is shown that energy efficiency improvement is possible only if the ARNEE is greater than the energy efficiency of the stand-alone dryer.",
     "keywords": ["Dryer energy efficiency", "Dryer controllability", "Desiccant dehumidification", "Desiccant performance indicators", "Design and control"]},
    {"article name": "Oversizing analysis in plant-wide control design for industrial processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.03.021",
     "publication date": "12-2013",
     "abstract": "In this work, an alternative plant-wide control design approach based on oversizing analysis is presented. The overall strategy can be divided in two main sequential tasks: 1 \u2013 defining the optimal decentralized control structure, and 2 \u2013 setting the controller interaction degree and its implementation. Both problems represent combinatorial optimizations based on multi-objective functional costs and were solved efficiently by genetic algorithms. The first task defines the optimal selection of controlled and manipulated variables simultaneously, the input\u2013output pairing, and the overall controller dimension in a sum of square deviations context. The second task analyzes the potential improvements by defining the controller interaction degree via the net load evaluation approach. In addition, some insights are given about the feasibility (implementation load) of these control structures for a decentralized or centralized framework. The well-known Tennessee Eastman (TE) process is selected here for sake of comparison with other multivariable control designs.",
     "keywords": ["Plant-wide control", "Oversizing analysis", "Servo-regulator trade-off", "Sparse controller", "Controller interaction degree"]},
    {"article name": "Quality assessment support system and its use in pharmaceutical plant operations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.06.010",
     "publication date": "12-2013",
     "abstract": "On the basis of the quality by design approach, aggregation of knowledge in process chemistry and manufacturing operations is essential for the continuous development of pharmaceutical products and processes. We develop a model-based framework to describe process chemistry oriented phenomenal sequences leading to quality hazards (process deviation scenario), which should be independent of any specific plant structural information, and manufacturing procedural error oriented event propagations (procedural control scenario), which concern a specific plant structure. We propose a system, Quality-HAZOP, to analyze the scenarios in which manufacturing errors can affect product quality via complex propagation pathways. This system supports bidirectional improvement proposal between recipe scientists and process engineers. This proposal triggers risk information exchange that supports the continuous improvement of the process in life-cycle pharmaceutical development.",
     "keywords": ["Quality by design", "Risk information exchange", "Pharmaceutical process development", "Hazard and operability studies", "Quality-HAZOP"]},
    {"article name": "A knowledge-driven approach for process supervision in chemical plants",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.06.009",
     "publication date": "12-2013",
     "abstract": "In this work, an ontology-based framework for process supervision in chemical plants is presented. A conceptualization of equipment, control systems and hazards has been developed. This conceptual model includes the semantic of each modeled term in order to obtain a heavyweight ontology. The ontology has been formalized using Description Logic (DL) (Kr\u00f6tzsch et al., 2012). A knowledge-driven approach has been adopted in order to demonstrate how DL reasoning could be used to support process supervision, detecting and diagnosing faults, without the help of external agents. In the proposed approach, a DL reasoner adds implicit facts to the ontology through forward chaining reasoning, from the current measurements to the characterization of hazards. Additionally, the system is able to check knowledge consistency and formally explain the obtained results. The system functionality has been illustrated in the Tennessee Eastman process.benchmark.",
     "keywords": ["Process supervision", "Description Logic", "Ontology", "Alarm management", "Tennessee Eastman process."]},
    {"article name": "Operational optimization of crude oil distillation systems using artificial neural networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.05.030",
     "publication date": "12-2013",
     "abstract": "A new methodology for optimizing heat-integrated crude oil distillation systems is proposed in this work. The new procedure considers an artificial neural networks (ANN) model for representing the distillation column. Models of the distillation column and the associated existing heat exchanger network are incorporated in an optimization framework to systematically determine the operating conditions that improve the overall process economics. Of particular interest is the problem of optimizing the net value of the products obtained from the column by increasing the yield of higher-value products at the expense of less valuable products, while taking into account feasibility of the distillation specifications, heat recovery, energy and equipment constraints. A two-stage procedure is applied to first optimize the column operating conditions based on minimum utility requirements. In the second stage the heat exchanger network is designed.",
     "keywords": ["Heat integration", "Heat exchanger networks", "Product yields"]},
    {"article name": "Profit optimization for chemical process plant based on a probabilistic approach by incorporating material flow uncertainties",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.05.026",
     "publication date": "12-2013",
     "abstract": "This paper reports how the economic performance of a chemical process plant is affected by material flow uncertainties from the plant inlet and outlet. Two chance-constrained optimization models were proposed. The models were tested using case studies of an existing gas processing plant. Profit optimization for the case studies was made with respect to the reliability of holding the process constraints at a certain confidence level [0.5, 1]. The optimal profit change for uncertainty from the plant inlet within the confidence interval [0.96, 1] was 86%. On the other hand, the optimal profit change for uncertainty from the plant outlet was only 2% for the same confidence level interval considered. This suggests that the uncertainty from the plant inlet has a major impact on the overall economic performance of the plant. Sensitivity analysis showed how uncertain parameters from both plant sides can affect the overall profit significantly.",
     "keywords": ["HYSYS", "GAMS", "Optimization", "Profit", "Uncertainty"]},
    {"article name": "Improvements in surrogate models for process synthesis. Application to water network system design",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.05.024",
     "publication date": "12-2013",
     "abstract": "High accuracy models can be obtained by using different types of surrogate models that accurately approximate equipment phenomenological models and can be used in synthesis problems, leading to faster and more precise solutions. Two types of surrogate models are used to approximate equipment phenomenological models: polynomial and neural network-based. In some cases, these surrogate models are not able to represent more complex equipment. An original methodology to reformulate these models using equations from shortcut equipment design is proposed. A medium-size case study involving fifteen units is presented. The synthesis problem is solved in a short computational time, leading many local solutions. Since several local optima objective function values are very close to each other, the choice of the best configuration among those found should be done qualitatively, because the differences among the objective function values are not significant if compared to the accuracy of equipment cost correlations in the literature.",
     "keywords": ["Process synthesis", "Surrogate model", "Wastewater network", "Refinery", "Superstructure"]},
    {"article name": "Modelling and simulation of suspension polymerization of vinyl chloride via population balance model",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.06.008",
     "publication date": "12-2013",
     "abstract": "A detailed population balance model is presented for suspension polymerization of vinyl chloride in an isothermal batch reactor perfectly mixed on macrolevel. Coalescence and breakage of monomer droplets, as well as mass exchange of species between the droplets induced by collisions, termed micromixing, are also included into the model forming a complex three-scale system. The resulted population balance equation is solved by coupling the deterministic continuous time computation of polymerization reactions inside the droplets with the random coalescence and breakage events of droplets using Monte Carlo simulation. The results obtained by simulation revealed that aggregation, breakage and micro-mixing of species induced by droplet collisions affect the process significantly.",
     "keywords": ["Suspension polymerization", "Vinyl chloride", "Population balance model", "Monte Carlo solution", "Simulation"]},
    {"article name": "Multi-scale modeling of Claus thermal furnace and waste heat boiler using detailed kinetics",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.05.028",
     "publication date": "12-2013",
     "abstract": "The modeling of thermal reaction furnaces of Sulfur Recovery Units (SRUs) is a rather complex problem since it involves different modeling scales such as the kinetic/molecular scale, the reactor scale, and the chemical process scale. This work introduces the multi-scale modeling approach to characterize the kinetic and reaction engineering scales for the thermal section of SRUs, involving the reactor furnace and the waste heat boiler. Specifically, also the waste heat boiler is modeled using detailed kinetics to characterize the recombination effects, which cannot be neglected any longer since they significantly influence the outlet compositions. The proposed models are validated on experimental and literature data for the kinetic scale. The reactor scale is validated on the industrial data coming from SRUs operating in Nanjing and Mumbai plants.",
     "keywords": ["Sulfur recovery", "Waste heat boiler", "Thermal furnace", "Partial oxidation", "Recombination effects", "Claus process"]},
    {"article name": "Multiscale analysis of simultaneous uptake of two reactive gases in the human lungs and its application to methemoglobin anemia",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.03.023",
     "publication date": "12-2013",
     "abstract": "We present a novel multiscale modeling and simulation methodology for quantifying the simultaneous uptake of two reactive gases in the human lungs, and apply it to predict pulmonary hypoxemia in patients suffering from methemoglobin anemia (resulting from excess inhaled nitric oxide (NO)). We start with the convection\u2013diffusion\u2013reaction equations at each scale of the lung and apply a spatial averaging technique (based on Liapunov\u2013Schmidt method of the classical bifurcation theory) to obtain low-dimensional multiscale models. Our simulations for methemoglobin anemia show that while breathing in room air, the O2 saturation in the patient's hemoglobin falls to below 94% at 50\u00a0ppm NO, and above 203\u00a0ppm, NO causes severe hypoxemia by reducing the O2 saturation to below its critical value of 88%. We predict that patients respond to O2 therapy up to inhaled NO levels of 271\u00a0ppm, above which they are candidates for methylene blue therapy.",
     "keywords": ["Methemoglobin anemia", "Multiscale modeling and simulation", "Pulmonary gas uptake", "Oxygen", "Nitric oxide", "Hypoxemia"]},
    {"article name": "Extended adaptive predictive controller with robust filter to enhance blood glucose regulation in type I diabetic subjects",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.06.012",
     "publication date": "12-2013",
     "abstract": "In this paper, an improved adaptive predictive control with robust filter is developed to be applied in an artificial pancreas. Several problems inherent to endocrine systems for diabetic persons have to be tackled such as nonlinearities, long time delays or daily variations of parameters. Three Finite Impulse Response models for insulin input and the same for meal intake (perturbations) corresponding to normal, hyper-hypoglycaemia levels to implement three zones control are taken into account. The glycaemia reference trajectory is shaped from a healthy person response. A variable weighting factor in the cost function is included to prevent dangerous glycaemia excursions out of the allowed limits. Additionally, a noisy blood glucose subcutaneous sensor model is used. This control strategy is tested on 30 virtual subjects from the UVa \u2013 Padova Simulator. Simultaneous meals and physiological disturbances are taken into account and the main conclusions are drawn from Control Variability Grid Analysis.",
     "keywords": ["Diabetes mellitus", "Artificial pancreas", "Adaptive predictive controller", "Meal announcement", "Zone control"]},
    {"article name": "Procurement planning in oil refining industries considering blending operations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.05.006",
     "publication date": "11-2013",
     "abstract": "This paper addresses procurement planning in oil refining, which has until now only had limited attention in the literature. We introduce a mixed integer nonlinear programming (MINLP) model and develop a novel two-stage solution approach, which aims at computational efficiency while addressing the problems due to discrepancies between a non-linear and a linearized formulation. The proposed model covers realistic settings by allowing the blending of crude oil in storage tanks, by modeling storage tanks and relevant processing units individually, and by handling more crude oil types and quality parameters than in previous literature. The developed approach is tested using historical data from Statoil A/S as well as through a comprehensive numerical analysis. The approach generates a feasible procurement plan within acceptable computation time, is able to quickly adjust an existing plan to take advantage of individual procurement opportunities, and can be used within a rolling time horizon scheme.",
     "keywords": ["Procurement planning", "Oil refining industry", "Mixed integer non-linear programming", "Solution approach", "Crude oil scheduling", "Decision support"]},
    {"article name": "Multi-objective multi-drug scheduling schemes for cell cycle specific cancer treatment",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.05.021",
     "publication date": "11-2013",
     "abstract": "This paper presents an investigation into the development of an optimal chemotherapy drug(s) scheduling scheme to control the drug doses to be infused to the patient's body. The current standard of practice of treatment is based on empirical evidence gathered from preclinical and clinical trials carried out during the drug development process. In general, most chemotherapy drugs used in cancer treatments are toxic agents and usually have narrow therapeutic indices; dose levels at which these drugs significantly kill the cancerous cells are close to those levels at which harmful toxic side effects occur. Therefore, an effective chemotherapy treatment protocol requires advanced automation and treatment design tools for use in clinical practice and the challenges inherent to complex biomedical systems and clinical deployment of technology (Parker, 2009). An optimum but effective drug scheduling requires suitable balancing between the beneficial and toxic side effects. Conventional clinical methods very often fail to find right drug doses that balance between these two constraints due to their inherent conflicting nature. A Multi-objective Genetic Algorithm Optimization (MOGA) process is employed to find the desired drug concentration at tumour sites that trade-off between the conflicting objectives. A close-loop control method, namely Integral-Proportional-Derivative (I-PD) is designed to control the drug to be infused to the patient's body and MOGA is used to find suitable/acceptable drug concentration at tumour site and parameters of the controller. Cell cycle specific cancer tumour models have been used in this work to show the effects of drug(s) on different cell populations, drug concentrations and toxic side effects. Results show that the applied multi-objective optimization approach can produce a wide range of solutions that trade-off between cell killing and toxic side effects and satisfy associated goals of chemotherapy treatment. Depending on the physiological state of the patient and state of the cancer, the oncologist can pick the right solution suitable for the patient. The chemotherapy drug schedules obtained by the proposed treatment protocols appears to be continuous on the time (day) scale, i.e., specific amount of drugs to be administered to the patient on daily basis which can be termed as Metronomics in nature. The dose duration and the interval period between dose applications can be adjusted in the proposed scheme either by setting the sampling time of closed-loop I-PD controller to any value depending on the state of the patient and disease (model parameters) or by using genetic optimization process aiming to minimize/maximize treatment objectives and satisfying treatment constraints. Regarding the total duration of the treatment, clinical knowledge can be utilized giving emphasis on physiological state of the patient, state of the tumour and disease. Moreover, the total duration of the treatment can also be found/determined for specific values of model parameters describing physiological state of the patient, state of the tumour and disease through multi-objective optimization process. It is noted that the proposed scheme offered the best treatment performance as compared to the reported work available so far. Moreover, robustness analysis shows that the control scheme is highly stable and robust despite the model uncertainties; from small to wide range, and the percentage of proliferating cell reduction is almost same as it is found with optimum model parameters without having any uncertainty.",
     "keywords": ["Cancer cell cycle", "Multi-objective optimization", "Scheduling", "Chemotherapy", "Integral-Derivative-Proportional"]},
    {"article name": "A new method for solving initial value problems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.06.001",
     "publication date": "11-2013",
     "abstract": "A more accurate method (comparing to the Euler, Runge\u2013Kutta, and implicit Runge\u2013Kutta methods) for the numerical solutions of ordinary differential equations (ODEs) is presented in this paper. The coefficients in the approximate solution for the ODE using the proposed method are divided into two groups: the fixed coefficients and the free coefficients. The fixed coefficients are determined by using the same way as in the traditional Taylor series method. The free coefficients are obtained optimally by minimizing the error of the approximate solution in each time interval. Examples are presented to compare the numerical solutions of the Rahmanzadeh, Cai, and White's method (RCW) to those of other popular ODEs methods.",
     "keywords": ["Euler", "Runge\u2013Kutta", "ODE"]},
    {"article name": "A non-Gaussian pattern matching based dynamic process monitoring approach and its application to cryogenic air separation process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.05.019",
     "publication date": "11-2013",
     "abstract": "Principal component analysis (PCA) based pattern matching methods have been applied to process monitoring and fault detection. However, the conventional pattern matching approaches do not specifically take into account the non-Gaussian dynamic features in chemical processes. Furthermore, those techniques are more focused on fault detection instead of fault diagnosis. In this study, a non-Gaussian pattern matching based fault detection and diagnosis method is developed and applied to monitor cryogenic air separation process. First, independent component analysis (ICA) models are built on the normal benchmark and monitored data sets along sliding windows. The IC subspaces from the benchmark and monitored data are then extracted to evaluate the non-Gaussian patterns and detect process faults through a mutual information based dissimilarity index. Further, a difference subspace between the two IC subspaces is computed to characterize the divergence of the dynamic and non-Gaussian patterns between the benchmark and monitored data. Subsequently, the mutual information between the IC difference subspace and each process variable direction is defined as a new non-Gaussian contribution index for fault identification and diagnosis. The presented approach is applied to a simulated cryogenic air separation plant and the monitoring results are compared against those of PCA based pattern matching techniques and ICA based monitoring method. The application study demonstrates that the developed non-Gaussian pattern matching approach can effectively monitor the complex air separation process with superior fault detection and diagnosis capability.",
     "keywords": ["Dynamic process monitoring", "Non-Gaussian pattern matching", "Mutual information", "Independent component analysis", "Fault diagnosis", "Cryogenic air separation process"]},
    {"article name": "Shut-in based production optimization of shale-gas systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.05.022",
     "publication date": "11-2013",
     "abstract": "This paper presents a novel operational scheme for enhanced utilization of late-life shale multi-well systems. These systems are characterized by a large number of geographically spread wells and pads, where a substantial number of the wells are producing at low erratic rates due to reservoir pressure depletion and well liquid loading. By applying a cyclic shut-in and production strategy, the scheme avoids well liquid loading and optimizes the production from a set of late-life wells at a shared production pad. The scheduling of well shut-ins is formulated as a generalized disjunctive program (GDP), using a novel shale-gas well and reservoir proxy model. The GDP formulation lends itself both to a complete MILP reformulation and reduced size MINLP reformulations; a computational study indicates in favor of the MILP formulation. We include numerical examples to demonstrate the potential benefit of applying the proposed cyclic scheme compared to a non-optimized approach.",
     "keywords": ["Shale-gas production", "Production planning", "Mixed integer programming", "Partial differential equations", "Reduced-order modeling"]},
    {"article name": "Predictions of CO and NOx emissions from steam cracking furnaces using GRI2.11 detailed reaction mechanism \u2013 A CFD investigation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.06.011",
     "publication date": "11-2013",
     "abstract": "This investigation develops a three-dimensional Computational Fluid Dynamics (CFD) model to simulate the turbulent diffusion flame on the fire-side of the radiation section of a thermal cracking test furnace coupled with a non-premixed low NOx floor burner. When this type of burners which uses the internal Flue Gas Recirculation (FGR) technique is coupled with large scale furnaces, both the turbulent mixing and chemical reaction rates are comparable and hence this should be considered in the model. Different combustion models are used to simulate the turbulence\u2013chemistry interactions for this flame. The CFD model, based on the Eddy Dissipation Concept (EDC) combustion model coupled with the detailed GRI2.11 reaction mechanism, gives the most reasonable predictions compared with the available experimental data or empirical correlations for the diffusion flame in the thermal cracking test furnace, especially for the flame length and the CO and NOx emissions.",
     "keywords": ["Natural gas combustion", "Thermal cracking furnaces", "EDC model", "GRI2.11", "CO", "NOx"]},
    {"article name": "Adaptive soft sensor for online prediction and process monitoring based on a mixture of Gaussian process models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.06.014",
     "publication date": "11-2013",
     "abstract": "Linear models can be inappropriate when dealing with nonlinear and multimode processes, leading to a soft sensor with poor performance. Due to time-varying process behaviour it is necessary to derive and implement some kind of adaptation mechanism in order to keep the soft sensor performance at a desired level. Therefore, an adaptation mechanism for a soft sensor based on a mixture of Gaussian process regression models is proposed in this paper. A procedure for input variable selection based on mutual information is also presented. This procedure selects the most important input variables for output variable prediction, thus simplifying model development and adaptation. Apart from online prediction of the difficult-to-measure variable, this soft sensor can be used for adaptive process monitoring. The efficiency of the proposed method is benchmarked with the commonly applied recursive PLS and recursive PCA method on the Tennessee Eastman process and two real industrial examples.",
     "keywords": ["Process modelling", "Online prediction", "Adaptive soft sensor", "Gaussian process regression", "Process monitoring", "Mutual information"]},
    {"article name": "Structured cell simulator coupled with a fluidized bed bioreactor model to predict the adaptive mercury uptake by E. coli cells",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.06.004",
     "publication date": "11-2013",
     "abstract": "A bi-level model is proposed by coupling a three-phase fluidized bioreactor (TPFB), used for mercury uptake from wastewaters by immobilized Escherichia coli cells, with a cellular simulator of the genetic regulatory circuit (GRC) controlling the mercuric ion reduction in the cytosol. While keeping a reasonable agreement with the experimental data from literature (free cell cultures with/without cell membrane permeabilization), the structured model advantages are coming from the prediction detailing degree [simulated 26\u00a0+\u00a03 (cell\u00a0+\u00a0bulk) vs. 3 (bulk) variable dynamics] covering a wide range of input Hg2+ loads (0\u2013100\u00a0mg\u00a0L\u22121), and cloned E. coli cells with various amounts of mer-plasmids (3\u2013140\u00a0nM). The model offers the possibility to predict the inner cell mercury reduction rate (different from the apparent rate observed in the bioreactor), the bacteria metabolism adaptation to environmental changes over several cell cycles, and the effect of cloning cells to modify their behaviour under stationary or perturbed conditions.",
     "keywords": ["Mercury uptake by E. coli", "Genetic regulatory circuit", "Structured dynamic models", "Fluidized-bed biological reactor"]},
    {"article name": "Generalized shape constrained spline fitting for qualitative analysis of trends",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.06.005",
     "publication date": "11-2013",
     "abstract": "In this work, we present a generalized method for analysis of data series based on shape constraint spline fitting which constitutes the first step toward a statistically optimal method for qualitative analysis of trends. The presented method is based on a branch-and-bound (B&B) algorithm which is applied for globally optimal fitting of a spline function subject to shape constraints. More specifically, the B&B algorithm searches for optimal argument values in which the sign of the fitted function and/or one or more of its derivatives change. We derive upper and lower bounding procedures for the B&B algorithm to efficiently converge to the global optimum. These bounds are based on existing solutions for shape constraint spline estimation via Second Order Cone Programs (SOCPs). The presented method is demonstrated with three different examples which are indicative of both the strengths and weaknesses of this method.",
     "keywords": ["Data mining", "Fault diagnosis", "Qualitative trend analysis", "Spline functions", "Global optimization", "Second order cone programming"]},
    {"article name": "Computing multi-species chemical equilibrium with an algorithm based on the reaction extents",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.06.013",
     "publication date": "11-2013",
     "abstract": "A mathematical model for the solution of a set of chemical equilibrium equations in a multi-species and multiphase chemical system is described. The computer-aid solution of model is achieved by means of a Newton\u2013Raphson method enhanced with a line-search scheme, which deals with the non-negative constrains. The residual function, representing the distance to the equilibrium, is defined from the chemical potential (or Gibbs energy) of the chemical system. Local minimums are potentially avoided by the prioritization of the aqueous reactions with respect to the heterogeneous reactions. The formation and release of gas bubbles is taken into account in the model, limiting the concentration of volatile aqueous species to a maximum value, given by the gas solubility constant.The reaction extents are used as state variables for the numerical method. As a result, the accepted solution satisfies the charge and mass balance equations and the model is fully compatible with general reactive transport models.",
     "keywords": ["Chemical equilibrium", "Speciation", "Reaction extent", "Reactive transport"]},
    {"article name": "Product sampling during transient continuous countercurrent hydrolysis of canola oil and development of a kinetic model",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.06.003",
     "publication date": "11-2013",
     "abstract": "A chemical kinetic model has been developed for the transient stage of the continuous countercurrent hydrolysis of triglycerides to free fatty acids and glycerol. Departure functions and group contribution methods were applied to determine the equilibrium constants of the four reversible reactions in the kinetic model. Continuous countercurrent hydrolysis of canola oil in subcritical water was conducted experimentally in a lab-scale reactor over a range of temperatures and the concentrations of all neutral components were quantified. Several of the rate constants in the model were obtained by modeling this experimental data, with the remaining determined from calculated equilibrium constants. Some reactions not included in the present, or previous, hydrolysis modeling efforts were identified from glycerolysis kinetic studies and may explain the slight discrepancy between model and experiment. The rate constants determined in this paper indicate that diglycerides in the feedstock accelerate the transition from \u201cemulsive hydrolysis\u201d to \u201crapid hydrolysis\u201d.",
     "keywords": ["Hydrolysis", "Triglycerides", "Diglycerides", "Free fatty acids"]},
    {"article name": "Thermodynamics based stability analysis and its use for nonlinear stabilization of the CSTR",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.06.016",
     "publication date": "11-2013",
     "abstract": "We show how the availability function as defined from the entropy function concavity can be used for the stability analysis and derivation of control strategies for non-isothermal Continuous Stirred Tank Reactors (CSTRs). We first propose an overview of the required thermodynamic concepts. Then, we show how the availability function restricted to the thermal domain can be used as a Lyapunov function. The derivation of the control law and the way the strict entropy concavity is insured are discussed. Numerical simulations illustrate the application of the theory to the open loop stability analysis and the closed loop control of liquid-phase non-isothermal CSTRs. The proposed approach is compared with the classical proportional control strategy. Two chemical reactions are studied: the acid-catalyzed hydration of 2-3-epoxy-1-propanol to glycerol subject to steady state multiplicity and the production of cyclopentenol from cyclopentadiene by acid-catalyzed electrophilic addition of water in dilute solution exhibiting a non-minimum phase behavior.",
     "keywords": ["Availability", "Entropy", "CSTR, Stability", "Nonlinear control", "Lyapunov function"]},
    {"article name": "Options analysis for long-term capacity design and operation of a lignocellulosic biomass refinery",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.07.004",
     "publication date": "11-2013",
     "abstract": "The growth of the lignocellulosic fuels has been hindered by technological and market uncertainty. This paper optimizes strategic investment decisions by prospective biobased fuel and chemical enterprises. A real options-based stochastic integer programming model is developed in this paper. We model a hypothetical, vertically integrated lignocellulosic enterprise that produces cellulosic ethanol and biosuccinic acid. Uncertainty is represented in bioproduct demands and prices. Strategic options including investment in research and development, investments in a flexible production platform and deferral of project investment are modeled. A hypothetical market model is also developed to correlate crude oil prices with the evolution of bioproduct markets. The discounted value of equity free cash flows is optimized. The optimal results include multiple capacity design plans based on the long term evolution of bioproduct markets. Monte Carlo simulations are also conducted to quantify the risk adjusted NPV's and returns on investment for the optimal capacity design trajectories.",
     "keywords": ["Real options", "Stochastic integer programming", "Multi-product biorefineries", "Uncertainty", "Lignocellulosic biomass"]},
    {"article name": "Solution of diffusion\u2013dispersion models using a computationally efficient technique of orthogonal collocation on finite elements with cubic Hermite as basis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.07.007",
     "publication date": "11-2013",
     "abstract": "In this paper, linear and non linear diffusion\u2013dispersion models involving fluid flow through porous cylindrical particles are solved using orthogonal collocation on finite elements with cubic Hermite as basis. The technique involves partitioning of axial domain into equal elements and then orthogonal collocation method with cubic Hermite as basis function is applied within each element. Exit concentration profiles are drawn for Peclet numbers ranging from 0 (perfect mixing) to \u221e (perfect displacement). Proposed technique is computationally efficient, stable and yields accurate solution, even for nonlinear stiff problem. Correlation with industrial parameters is also presented.",
     "keywords": ["Langmuir isotherm", "Axial dispersion model", "Diffusion", "Cubic Hermite basis", "Peclet number", "Exit solute concentration"]},
    {"article name": "On the optimal numerical time integration for DEM using Hertzian force models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.06.018",
     "publication date": "11-2013",
     "abstract": "The numerical accuracy of a selection of different time integration techniques used to solve particle motion is investigated using a normal collision employing the non-linear Hertzian contact force. The findings are compared against the linear force model where it has been found that the expected order of accuracy of higher-order integration schemes is not realised (Tuley et al., 2010). The proposed mechanism for this limitation has been cited as the errors in integration which occur across the force profile discontinuity. By investigating the characteristics of both the non-linear elastic and the non-linear damped Hertzian contact models, it has been found that higher orders of accuracy are recoverable and depends on the degree of the governing non-linear equation. The numerical errors of the linear and non-linear force models are however markedly different in character.",
     "keywords": ["Discrete element method", "Numerical integration", "Integration schemes"]},
    {"article name": "Evaluation of weighted residual methods for the solution of the pellet equations: The orthogonal collocation, Galerkin, tau and least-squares methods",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.07.002",
     "publication date": "11-2013",
     "abstract": "In recent years a number of publications have adopted the least-squares method for chemical reactor engineering problems such as the population balance equation, fixed bed reactors and pellet equations. Evaluation of the performance of the least-squares method compared to other weighted residual methods is therefore of interest. Thus, in the present study, numerical techniques in the family of weighted residual methods; the orthogonal collocation, Galerkin, tau, and least-squares methods, have been adopted to solve a non-linear comprehensive and highly coupled pellet problem. The methanol synthesis and the steam methane reforming process have been adopted for this study.Based on the residual of the governing equations, the orthogonal collocation method obtains the same accuracy as the Galerkin and tau methods. Moreover, the orthogonal collocation method is associated with the simplest algebra theory and thus holds the simplest implementation. Another benefit of the orthogonal collocation method is that the technique is more computational efficient than the other methods evaluated. The least-squares method does not obtain the same accuracy as the other weighted residual methods. In particular, the least-squares method is not suitable for strongly diffusion limited systems that give rise to steep gradients in the pellet. On the other side, considering the spectral\u2013element framework, the least-squares method is less sensitive to the placement of the element boundaries than observed for the orthogonal collocation, Galerkin and tau methods.The present paper outlines the algebra of the weighted residual methods and illustrate the numerical solution techniques through a simplified pellet problem.",
     "keywords": ["Weighted residual methods", "Pellet equation", "Multicomponent diffusion", "Methanol synthesis", "Steam methane reforming", "Numerical methods"]},
    {"article name": "Development and test of CFD\u2013DEM model for complex geometry: A coupling algorithm for Fluent and DEM",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.07.006",
     "publication date": "11-2013",
     "abstract": "CFD\u2013Discrete Element Method (DEM) model is an effective approach for studying dense gas\u2013solid flow in fluidized beds. In this study, a CFD\u2013DEM model for complex geometries is developed, where DEM code is coupled with ANSYS/Fluent software through its User Defined Function. The Fluent Eulerian multiphase model is employed to couple with DEM, whose secondary phase acts as a ghost phase but just an image copy of DEM field. The proposed procedure preserves phase conservation and ensures the Fluent phase-coupled SIMPLE solver work stable. The model is used to simulate four typical fluidization cases, respectively, a single pulsed jet fluidized bed, fluidized bed with an immersed tube, fluidization regime transition from bubbling to fast, and a simplified two-dimensional circulating fluidized bed loop. The simulation results are satisfactory. The present approach provides an easily implemented and reliable method for CFD\u2013DEM model on complex geometries.",
     "keywords": ["CFD\u2013DEM model", "Gas\u2013solid fluidized bed", "Complex geometry"]},
    {"article name": "Model selection and parameter estimation for chemical reactions using global model structure",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.07.013",
     "publication date": "11-2013",
     "abstract": "In complex reaction systems, such as those found in heterogeneous catalytic reactions, several alternative kinetic models are usually considered in an effort to describe reaction kinetics. The number of plausible mechanisms can be very large, even for systems with a small number of reactions and components. Usually, only a restricted number of models are investigated in detail, since the evaluation of a large number of complex models is extremely time-consuming. In this work, a methodology is described, which allows performing efficiently a global search within all plausible models and parameter sets using the Non-dominated Sorting Genetic Algorithm II (NSGA-II). The developed methodology is applied to the parameter estimation and model optimization of the partial oxidation of ethane reaction network. The present approach allows the reliable investigation of a considerable number of models mechanisms in an automatic manner and in a short computational time. It appears to be a very effective way to optimize complex reaction mechanisms.",
     "keywords": ["Parameter estimation", "Kinetic models", "Multi-objective optimization", "Genetic algorithm"]},
    {"article name": "Discontinuous Galerkin finite element methods with shock-capturing for nonlinear convection dominated models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.07.011",
     "publication date": "11-2013",
     "abstract": "In this paper, convection-diffusion-reaction models with nonlinear reaction mechanisms, which are typical problems of chemical systems, are studied by using the upwind symmetric interior penalty Galerkin (SIPG) method. The local spurious oscillations are minimized by adding an artificial viscosity diffusion term to the original equations. A discontinuity sensor is used to detect the layers where unphysical oscillations occur. Finally, the proposed method is tested on various single- and multi-component problems.",
     "keywords": ["Discontinuous Galerkin methods", "Shock-capturing", "Discontinuity sensor", "Convection dominated problems"]},
    {"article name": "Adaptive soft sensor model using online support vector regression with time variable and discussion of appropriate hyperparameter settings and window size",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.07.016",
     "publication date": "11-2013",
     "abstract": "Soft sensors have been widely used in chemical plants to estimate process variables that are difficult to measure online. One crucial difficulty of soft sensors is that predictive accuracy drops due to changes in state of chemical plants. The predictive accuracy of traditional soft sensor models decreases when sudden process changes occur. However, an online support vector regression (OSVR) model with the time variable can adapt to rapid changes among process variables. One crucial problem is finding appropriate hyperparameters and window size, which means the numbers of data for the model construction, and thus, we discussed three methods to select hyperparameters based on predictive accuracy and computation time. The window size of the proposed method was discussed through simulation data and real industrial data analyses and the proposed method achieved high predictive accuracy when time-varying changes in process characteristics occurred.",
     "keywords": ["Process control", "Soft sensor", "Degradation", "Online support vector machine", "Time variable", "Hyperparameter"]},
    {"article name": "Continuous prediction technique for fast determination of cyclic steady state in simulated moving bed process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.07.012",
     "publication date": "11-2013",
     "abstract": "In the simulation of cyclic processes, such as simulated moving bed (SMB), the system should be equilibrated to reach a cyclic steady state (CSS) before evaluating the process performance. However, the conventional method of successive substitution is quite time-consuming. In this work, a continuous predicting method (CPM) is developed for fast determination of CSS in SMB. In CPM, the direct prediction of state variable at CSS and solving model equation are conducted alternately until CSS is reached. In order to give a guideline for the selection of the acceleration factor, CPM is applied on SMB process for enantioseparation of 1,1\u2032-bi-2-naphtol racemate and with the optimized acceleration factor, 59% of computation time saved compared with successive substitution. In addition, this method is further successfully used in a sugar separation process. Given its efficiency and simplicity, this method could provide a useful tool for SMB simulations.",
     "keywords": ["Simulated moving bed", "Cyclic steady state", "Continuous predicting method", "Acceleration", "Adsorptive separation"]},
    {"article name": "Optimal processing pathway for the production of biodiesel from microalgal biomass: A superstructure based approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.08.002",
     "publication date": "11-2013",
     "abstract": "In this study, we propose a mixed integer nonlinear programming (MINLP) model for superstructure based optimization of biodiesel production from microalgal biomass. The proposed superstructure includes a number of major processing steps for the production of biodiesel from microalgal biomass, such as the harvesting of microalgal biomass, pretreatments including drying and cell disruption of harvested biomass, lipid extraction, transesterification, and post-transesterfication purification. The proposed model is used to find the optimal processing pathway among the large number of potential pathways that exist for the production of biodiesel from microalgae. The proposed methodology is tested by implementing on a specific case with different choices of objective functions. The MINLP model is implemented and solved in GAMS using a database built in Excel. The results from the optimization are analyzed and their significances are discussed.",
     "keywords": ["Superstructure optimization", "Biodiesel", "Microalgae", "Mixed integer nonlinear programming", "Biorefinery"]},
    {"article name": "Integration of production scheduling and dynamic optimization for multi-product CSTRs: Generalized Benders decomposition coupled with global mixed-integer fractional programming",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.08.003",
     "publication date": "11-2013",
     "abstract": "Integration of production scheduling and dynamic optimization can improve the overall performance of multi-product CSTRs. However, the integration leads to a mixed-integer dynamic optimization problem, which could be challenging to solve. We propose two efficient methods based on the generalized Bender decomposition framework that take advantage of the special structures of the integrated problem. The first method is applied to a time-slot formulation. The decomposed primal problem is a set of separable dynamic optimization problems and the master problem is a mixed-integer nonlinear fractional program. The master problem is then solved to global optimality by a fractional programming algorithm, ensuring valid Benders cuts. The second decomposition method is applied to a production sequence formulation. Similar to the first method, the second method uses a fractional programming algorithm to solve the master problem. Compared with the simultaneous method, the proposed decomposition methods can reduce the computational time by over two orders of magnitudes for a polymer production process in a CSTR.",
     "keywords": ["Production scheduling", "Dynamic optimization", "Generalized Benders decomposition", "Fractional programming", "Polymerization process"]},
    {"article name": "Optimizing process economics online using model predictive control",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.07.015",
     "publication date": "11-2013",
     "abstract": "Optimizing process economics in model predictive control traditionally has been done using a two-step approach in which the economic objectives are first converted to steady-state operating points, and then the dynamic regulation is designed to track these setpoints. Recent research has shown that process economics can be optimized directly in the dynamic control problem, which can take advantage of potential higher profit transients to give superior economic performance. However, in practice, solution of such nonlinear MPC dynamic control problems can be challenging due to the nonlinearity of the model and/or nonconvexity of the economic cost function. In this work we propose the use of direct methods to formulate the nonlinear control problem as a large-scale NLP, and then solve it using an interior point nonlinear solver in conjunction with automatic differentiation. Two case studies demonstrate the computational performance of this approach along with the economic performance of economic MPC formulation.",
     "keywords": ["Nonlinear control", "Economic MPC", "Direct methods", "Collocation", "Automatic differentiation"]},
    {"article name": "Systematic substrate adoption methodology (SAM) for future flexible, generic pharmaceutical production processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.07.010",
     "publication date": "11-2013",
     "abstract": "The discovery of an effective and safe pharmaceutical product is based on success in clinical trials. Often, several candidate compounds targeting the same disease area are tested in order to identify the most efficacious products. This involves the manufacture of small quantities of compounds (APIs) for early delivery campaigns. Of these candidates only a few will be successful such that further development is required to scale-up the process. Systematic computer-aided methods and tools are required for faster manufacturing of these API candidates. In this work, a substrate adoption methodology (SAM) for a series of substrates with similar molecular functionality has been developed. The objective is to achieve \u201cflexible, fast and future\u201d pharmaceutical production processes by adapting a generic modular process template. Application of the methodology is illustrated through a case study from the pharmaceutical industry. Use of computer-aided methods and tools as part of the methodology is also highlighted.",
     "keywords": ["Substrate adoption methodology", "Flexible process", "Fast development", "Nitro reduction", "Pharmaceutical", "Early delivery campaigns"]},
    {"article name": "Protein aggregation and lyophilization: Protein structural descriptors as predictors of aggregation propensity",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.07.008",
     "publication date": "11-2013",
     "abstract": "Lyophilization can induce aggregation in therapeutic proteins, but the relative importance of protein structure, formulation and processing conditions are poorly understood. To evaluate the contribution of protein structure to lyophilization-induced aggregation, fifteen proteins were co-lyophilized with each of five excipients. Extent of aggregation following lyophilization, measured using size-exclusion chromatography, was correlated with computational and biophysical protein structural descriptors via multiple linear regression. Descriptor selection was performed using exhaustive search and forward selection. The results demonstrate that, for a given excipient, extent of aggregation is highly correlated by eight to twelve structural descriptors. Leave-one-out cross validation showed that the correlations were able to successfully predict the aggregation for a protein \u201cleft out\u201d of the data set. Selected descriptors varied with excipient, indicating both protein structure and excipient type contribute to lyophilization-induced aggregation. The results show some descriptors used to predict protein aggregation in solution are useful in predicting lyophilized protein aggregation.",
     "keywords": ["Protein formulation", "Biologics", "Aggregation prediction", "Lyophilization", "Multiple linear regression", "Structural descriptors"]},
    {"article name": "A quasi-analytical solution of Frost\u2013Kalkwarf vapor pressure equation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.07.003",
     "publication date": "11-2013",
     "abstract": "For the solution of the Frost\u2013Kalkwarf vapor pressure equation (which is both implicit and transcendent), an explicit solution is established in terms of Lambert function. A simple approximation is provided for generating a good, simple and inexpensive starting value usable by any root finder. Using Newton methods, all calculations converge within one or two iterations.",
     "keywords": ["Frost\u2013Kalkwarf vapor pressure equation", "Numerical solution", "Lambert function approximation"]},
    {"article name": "Time optimal cyclic crystallization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.05.005",
     "publication date": "11-2013",
     "abstract": "A time optimal cyclic control scheme for shape manipulation of multivariate crystal populations involving sequences of subsequent growth and dissolution phases is proposed in this note. Such strategies employ the unequal growth and dissolution rates for attaining morphologies that do not result directly from a pure growth or dissolution phase only. We prove that minimum time trajectories can be constructed by means of convex programs resulting in globally optimal bimodal control policies with piecewise constant supersaturation.",
     "keywords": ["Crystal shape manipulation", "Optimal control", "Convex optimization", "Multivariate crystallization"]},
    {"article name": "Modeling the commodity fluctuations of OPEX terms",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.04.018",
     "publication date": "10-2013",
     "abstract": "A feasibility study of a new plant or even of a revamped one bases the forecast of incomes and outcomes on a discounting back approach. This means that both prices and costs of commodities (i.e. raw materials and products) are assumed constant for long time-horizons. Commodities together with utilities play a major role in the economic assessment of OPEXs (operative expenditures). The paper tackles the \u201cdiscounting back\u201d problem that sees a coming apart between the dynamics of real market prices/costs (subject to fluctuations, volatility, and the \u201csupply and demand\u201d law) and the constant prices/costs assumed in conventional feasibility studies. The manuscript presents and discusses a methodology to model the time evolution of prices and costs of commodities for the feasibility-study framework of dynamic conceptual design. It also provides an improved methodology respect to direct Monte Carlo sampling of quotations over historical ranges, which is effective for repeated design optimization.",
     "keywords": ["Price and cost forecast", "Commodities", "Crude oil quotation", "Feasibility study", "Process/plant design", "Market and demand fluctuations", "Dynamic conceptual design"]},
    {"article name": "Block-oriented modeling of superstructure optimization problems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.04.008",
     "publication date": "10-2013",
     "abstract": "We present a novel software framework for modeling large-scale engineered systems as mathematical optimization problems. A key motivating feature in such systems is their hierarchical, highly structured topology. Existing mathematical optimization modeling environments do not facilitate the natural expression and manipulation of hierarchically structured systems. Rather, the modeler is forced to \u201cflatten\u201d the system description, hiding structure that may be exploited by solvers, and obfuscating the system that the modeling environment is attempting to represent. To correct this deficiency, we propose a Python-based \u201cblock-oriented\u201d modeling approach for representing the discrete components within the system. Our approach is an extension of the Pyomo library for specifying mathematical optimization problems. Through the use of a modeling components library, the block-oriented approach facilitates a clean separation of system superstructure from the details of individual components. This approach also naturally lends itself to expressing design and operational decisions as disjunctive expressions over the component blocks. By expressing a mathematical optimization problem in a block-oriented manner, inherent structure (e.g., multiple scenarios) is preserved for potential exploitation by solvers. In particular, we show that block-structured mathematical optimization problems can be straightforwardly manipulated by decomposition-based multi-scenario algorithmic strategies, specifically in the context of the PySP stochastic programming library. We illustrate our block-oriented modeling approach using a case study drawn from the electricity grid operations domain: unit commitment with transmission switching and N\u00a0\u2212\u00a01 reliability constraints. Finally, we demonstrate that the overhead associated with block-oriented modeling only minimally increases model instantiation times, and need not adversely impact solver behavior.",
     "keywords": ["Superstructure optimization", "Generalized disjunctive programming", "Stochastic programming", "Algebraic modeling language"]},
    {"article name": "Barrier NLP methods with structured regularization for optimization of degenerate optimization problems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.04.023",
     "publication date": "10-2013",
     "abstract": "Barrier nonlinear programming (NLP) solvers exploit sparse Newton-based algorithms and are characterized by fast performance and global convergence properties. This makes them especially suitable for very large process optimization problems. On the other hand, they are frequently challenged by degenerate and indefinite problems, which lead to ill-conditioned Karush\u2013Kuhn\u2013Tucker (KKT) systems. Such problems arise when process optimization models contain linearly dependent constraints, or the reduced Hessian is not positive definite at the solution. This can lead to poor solver performance and may preclude finding successful NLP solutions. Moreover, such optimization models occur in blending problems and NLP subproblems generated by MINLP or global optimization strategies. To deal with these difficulties we present a structured regularization strategy for barrier methods that identifies and excludes dependent constraints in the KKT system while leaving independent constraints unchanged. As a result, more accurate Newton directions can be obtained and much faster convergence can be expected for the KKT system over the conventional regularization approach. Numerical experiments with examples derived from the CUTE and COPS test sets as well as two nonlinear blending problems demonstrate the effectiveness of the proposed method and significantly better performance of the NLP solver.",
     "keywords": ["Barrier NLP methods", "Dependent constraints", "KKT regularization", "Newton's method"]},
    {"article name": "SePTA\u2014A new numerical tool for simultaneous targeting and design of heat exchanger networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.05.008",
     "publication date": "10-2013",
     "abstract": "Pinch Analysis is an established insight-based methodology for design of energy-efficient processes. The Composite Curves (CCs) is a popular Pinch Analysis tool to target the minimum energy requirements. An alternative to the CCs is a numerical technique known as the Problem Table Algorithm (PTA). The PTA however, does not show individual hot and cold streams heat cascades and cannot be used for design of heat exchanger networks (HEN). This paper introduces the Segregated Problem Table Algorithm (SePTA) as a new numerical tool for simultaneous targeting and design of a HEN. SePTA shows profiles of heat cascade across temperature intervals for individual hot and cold streams, and can be used to simultaneously locate pinch points, calculate utility targets and perform SePTA Heat Allocation (SHA). The SHA can be represented on a new SePTA Network Diagram (SND) that graphically shows a heat exchanger network together with the amount of heat exchange on a temperature interval scale. This paper also shows that SePTA and SND can be a vital combination of numerical and graphical visualisation tools for targeting and design of complex HENs involving stream splitting, threshold problems and multiple pinches.",
     "keywords": ["Pinch analysis", "Segregated Problem Table Algorithm (SePTA)", "Heat exchanger network (HEN)", "Heat cascade", "Minimum utility targets", "Numerical approach"]},
    {"article name": "Design methodology for bio-based processing: Biodiesel and fatty alcohol production",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.01.018",
     "publication date": "10-2013",
     "abstract": "A systematic design methodology is developed for producing multiple main products plus side products starting with one or more bio-based renewable source. A superstructure that includes all possible reaction and separation operations is generated through thermodynamic insights and available data. The number of alternative processes is systematically reduced through a screening procedure until only feasible alternatives are obtained. As part of the methodology, process intensification involving reaction\u2013separation tasks is also considered to improve the design by shifting the equilibrium reactions. Economic analysis and net present value are determined to find the best economically and operationally feasible process. The application of the methodology is presented through a case study involving biodiesel and fatty alcohol productions.",
     "keywords": ["Design methodology", "Superstructure", "Process intensification", "Biodiesel production", "Fatty alcohol production"]},
    {"article name": "Design and control of a reactive-distillation process for esterification of an alcohol mixture containing ethanol and n-butanol",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.01.002",
     "publication date": "10-2013",
     "abstract": "This paper presents an economical design flowsheet and overall control strategy for simultaneous esterifications of ethanol and n-butanol mixtures with acetic acid. In this work, a difficult-to-design reactive-distillation (RD) process, classified as mixed Type-II/Type-III system, has been studied. After comparing three alternative design flowsheets, an indirect-sequence design containing a RD column, a top decanter, and a stripper gives significantly lower total annual cost and energy consumption than the other two designs. In the control strategy development, conventional inventory control with RD bottom level controlled by manipulating RD bottom flow shows that this process would exhibit multiple steady-states. An improved inventory control strategy with a rather unusual pairing by controlling this level using reboiler duty is proposed to alleviate this complex dynamic behavior. The dynamic results show that the proposed plant-wide control strategy is capable of holding product specifications despite disturbances from throughput and feed composition changes.",
     "keywords": ["Reactive distillation", "Esterification", "Ethyl acetate", "n-Butyl acetate", "Design and control"]},
    {"article name": "Toward integrated production and distribution management in multi-echelon supply chains",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.01.004",
     "publication date": "10-2013",
     "abstract": "The effective management of multi-site systems involves the proper coordination of activities performed in multiple factories, distribution centers (DCs), retailers and end-users located in many different cities, countries and/or continents. To optimally manage numerous production and transportation decisions, a novel monolithic continuous-time MILP-based framework is developed to determine the best short-term operational planning to meet all customer requests at minimum total cost. The formulation lies on the unit-specific general precedence concept for the production scheduling problem whereas the immediate precedence notion is used for transportation decisions. To illustrate the applicability and potential benefits of the model, a challenging example corresponding to a supply chain comprising several locations geographically spread in six European countries has been solved to optimality with modest CPU times. Several scenarios with different logistics features were addressed in order to remark the significant advantages of using the integrated approach.",
     "keywords": ["MILP-based approach", "Production and distribution scheduling", "Multi-site system", "Logistics"]},
    {"article name": "Multi-period scheduling of a multi-stage multi-product bio-pharmaceutical process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.03.009",
     "publication date": "10-2013",
     "abstract": "There have been several works in the literature for scheduling of multi-product continuous processes with significant attention laid on short-term scheduling. This work presents a continuous-time model for multi-period scheduling of a multi-stage multi-product process from bio-pharmaceutical industry. The overall model is a mixed-integer linear programming (MILP) formulation based on state-task-network (STN) representation of the process using unit-specific event-based continuous-time representation. The proposed model is an extension of model by Shaik and Floudas (2007, Industrial & Engineering Chemistry Research, 46, 1764) with several new constraints to deal with additional features such as unit and sequence dependent changeovers, multiple intermediate due dates, handling of shelf-life and waste disposal, and penalties on backlogs and late deliveries. Improved tightening and sequencing constraints have been presented. The validity of the proposed model has been illustrated through an example from the literature.",
     "keywords": ["Scheduling", "Multi-stage", "Multiproduct", "Optimization", "Bio-pharmaceutical process"]},
    {"article name": "Centralised utility system planning for a Total Site Heat Integration network",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.02.007",
     "publication date": "10-2013",
     "abstract": "Total Site Heat Integration (TSHI) is a technique of exchanging heat among multiple processes via a centralised utility system. An analysis of the integrated multiple processes, also known as the Total Site (TS) system sensitivity, is needed to characterise the effects of a plant maintenance shutdown, to determine the operational changes needed for the utility production and to plan mitigation actions. This paper presents an improved Total Site Sensitivity Table (TSST) to be used as a systematic tool for this purpose. The TSST can be used to consider various \u2018what if\u2019 scenarios. This tool can be used to determine the optimum size of a utility generation system, to design the backup generators and piping needed in the system and to assess the external utilities that might need to be bought and stored. The methodology is demonstrated by using an Illustrated Case Study consisting of three processes. During the TS normal operation, the Total Site Problem Table Algorithm (TS-PTA) shows that the system requires 1065\u00a0kW of High Pressure Steam and 645.5\u00a0kW of Medium Pressure Steam as the heating utility, while for the cooling utility, 553.5\u00a0kW of Low Pressure Steam and 3085\u00a0kW of cooling water are required. The results of the modified TSST proposed that a boiler and a cooling tower with the system design requiring a maximum capacity of 2.172\u00a0MW of steam and 4.1865\u00a0MW of cooling water are needed to ensure an operational flexibility between the three integrated processes.",
     "keywords": ["Total Site Heat Integration", "Pinch Analysis", "Total Site Sensitivity Table", "Sensitivity analysis", "Utility production planning"]},
    {"article name": "Generation of operating procedures for a mixing tank with a micro genetic algorithm",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.04.016",
     "publication date": "10-2013",
     "abstract": "This paper explores the use of a micro genetic algorithm that uses variable-length chromosomes and a seeding scheme based on tabu search. The problem is to find the sequence of actions that have to be executed in the shortest time possible, but also in a way that minimizes the possibility of situations that may endanger the plant personnel and plant facilities. The proposed approach was tested on the generation of the optimum sequences for startup and shutdown of a mixing vessel similar to the equipment used in the synthesis of acrylic acid. The results show that the proposed method outperforms the traditional GA algorithm both in terms of the quality of the solution and computational effort.",
     "keywords": ["Operating procedures", "Micro genetic algorithms", "Tabu search", "Mixing tank"]},
    {"article name": "Approximate dynamic programming based control of hyperbolic PDE systems using reduced-order models from method of characteristics",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.01.017",
     "publication date": "10-2013",
     "abstract": "Approximate dynamic programming (ADP) is a model based control technique suitable for nonlinear systems. Application of ADP to distributed parameter systems (DPS) which are described by partial differential equations is a computationally intensive task. This problem is addressed in literature by the use of reduced order models which capture the essential dynamics of the system. Order reduction of DPS described by hyperbolic PDEs is a difficult task as such systems exhibit modes of nearly equal energy. The focus of this contribution is ADP based control of systems described by hyperbolic PDEs using reduced order models. Method of characteristics (MOC) is used to obtain reduced order models. This reduced order model is then used in ADP based control for solving the set-point tracking problem. Two case studies involving single and double characteristics are studied. Open loop simulations demonstrate the effectiveness of MOC in reducing the order and the closed loop simulations with ADP based controller indicate the advantage of using these reduced order models.",
     "keywords": ["Method of characteristics", "Approximate dynamic programming", "Distributed parameter system", "Hyperbolic PDE", "Model based control"]},
    {"article name": "Optimization and control of crystal shape and size in protein crystallization process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.04.022",
     "publication date": "10-2013",
     "abstract": "Large molecule protein crystals have shown significant benefits in the delivery of biopharmaceuticals to achieve high stability, high concentration of active pharmaceutical ingredients (API), and controlled release of API. However, among the about 150 biopharmaceuticals on the market by 2004, only insulin has been marketed in crystalline form. A major technological challenge is that protein crystallization has a very complicated environment and is affected by many factors. There is currently a lack of knowledge on large scale production of protein crystals. In contrast to the majority of previous work on protein crystallization that was centered on single crystal scale, the current research is focused on computational study of protein crystallization at process scale, investigating the growth behavior of a population of crystals in a crystallizer. Using a newly developed morphological population balance model that can simulate the multidimensional size distributions of a population of crystals, known as shape distribution, an optimization technique is applied to optimize the growth of individual faces with the aim of obtaining desired crystal shape and size distributions. Using a target shape as the objective function, optimal temperature and supersaturation profiles leading to the desired crystal shape were derived. Genetic algorithm was investigated and found to be an effective optimization technique for the current application. Since tracking an optimum temperature or supersaturation trajectory can be easily implemented by manipulating the coolant flowrate in the reactor jacket, the methodology provides a feasible closed-loop mechanism for protein crystal shape tailoring and control.",
     "keywords": ["Morphological population balance model", "Multidimensional population balance model", "Crystal shape distribution", "Hen-egg-white lysozyme crystallization", "Protein crystallization optimization and control", "Genetic algorithm"]},
    {"article name": "Control strategies for thermal budget and temperature uniformity in spike rapid thermal processing systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.01.003",
     "publication date": "10-2013",
     "abstract": "Single wafer rapid thermal processing (RTP) is widely used in semiconductor manufacturing. A precisely applied thermal budget during RTP is crucial and relies on the temperature control of the wafer. However, temperature control in the RTP system with a spike-shaped temperature profile is a challenging task, and achieving perfect servo control is almost impossible because of the high temperature ramp-up/down rate and substantial nonlinearity of the process. This paper presents a novel method of control system design to provide a precise thermal budget in the spike RTP system. By tuning controller parameters and designing the set-point profile, the method targets thermal budget indices instead of temperature servo control. A nonlinear control strategy is proposed based on modeling the RTP system as a nonlinear Wiener model. Furthermore, a multivariable control structure is considered to maintain the temperature uniformity within the wafer. The simulation results show the effectiveness of the proposed control strategy and provide helpful guidelines for the design of a multivariable control configuration to achieve superior wafer temperature uniformity.",
     "keywords": ["Process control", "Rapid thermal processing", "Thermal budget control", "Nonlinear Wiener model", "Multivariable control"]},
    {"article name": "A mathematical programming formulation for temporal flexibility analysis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.04.001",
     "publication date": "10-2013",
     "abstract": "Realistic chemical processes are often operated in the presence of complex and uncertain dynamics. While an ill designed system may become inoperable due to variations in some process parameters at certain instances, the cumulative effects of temporary disturbances in finite time intervals can also result in serious consequences. The latter issue is studied in the present study on the basis of a novel concept \u2013 temporal flexibility. Specifically, the mathematical program used for evaluating the corresponding performance measure is built with a dynamic system model, which usually consists of a set of differential-algebraic equations (DAEs). The numerical technique of differential quadrature (DQ) is adopted to approximate these DAEs with equality constraints. As a result, any solution strategy for the conventional steady-state flexibility analysis is applicable. Two examples, a simple liquid storage tank and a solar thermal driven membrane distillation desalination process, are adopted to demonstrate the usefulness of temporal flexibility analysis. All results obtained in case studies show that the proposed approach is convenient and effective for assessing realistic issues in operating complex dynamic chemical processes.",
     "keywords": ["Dynamic flexibility", "Temporal flexibility", "Process design", "Differential quadrature"]},
    {"article name": "Parameter estimation in batch process using EM algorithm with particle filter",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.03.024",
     "publication date": "10-2013",
     "abstract": "This paper investigates a parameter estimation problem for batch processes through the maximum likelihood method. In batch processes, the initial state usually relates to the states of previous batches. The proposed algorithm takes batch-to-batch correlations into account by employing an initial state transition equation to model the dynamics along the batch dimension. By treating the unmeasured states and the parameters as hidden variables, the maximum likelihood estimation is accomplished through the expectation\u2013maximization (EM) algorithm, where the smoothing for the terminal state and the filtering for the initial state are specially considered. Due to the nonlinearity and non-Gaussianity in the state space model, particle filtering methods are employed for the implementation of filtering and smoothing. Through alternating between the expectation step and the maximization step, the unknown parameters along with states are estimated. Simulation examples demonstrate the proposed estimation approach.",
     "keywords": ["Batch process", "Parameter estimation", "Particle filter", "EM algorithm"]},
    {"article name": "Data-driven causal inference based on a modified transfer entropy",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.05.011",
     "publication date": "10-2013",
     "abstract": "Causality inference and root cause analysis are important for fault diagnosis in the chemical industry. Due to the increasing scale and complexity of chemical processes, data-driven methods become indispensable in causality inference. This paper proposes an approach based on the concept of transfer entropy which was presented by Schreiber in 2000 to generate a causal map. To get a better performance in estimating the time delay of causal relations, a modified form of the transfer entropy is presented in this paper. Case studies on two simulated chemical processes, including the benchmark Tennessee Eastman process are performed to illustrate the effectiveness of this approach.",
     "keywords": ["Causal inference", "Transfer entropy", "Process safety"]},
    {"article name": "Safety securing approach against cyber-attacks for process control system",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.04.019",
     "publication date": "10-2013",
     "abstract": "After the appearance of Stuxnet, the safety assurance against cyber-attacks has been a serious problem for process control. For safety assurance, not only information system securing approaches but also process control original measures are necessary. In this paper, a new protection approach is proposed. Application of an information system securing technique called \u201czones and conduits\u201d to process control is discussed. By dividing the control system network into plural zones, higher possibility of detecting cyber-attacks and preventing operational accidents can be achieved. By defining detectability and reachability matrices, zone division for cyber-attack detection can be designed.",
     "keywords": ["Cyber security", "Safety", "Plant control", "Design methodology"]},
    {"article name": "Design of optimal patient-specific chemotherapy protocols for the treatment of acute myeloid leukemia (AML)",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.02.003",
     "publication date": "10-2013",
     "abstract": "AML is a cancer of the blood and bone marrow which results from the combined effects of genetic mutations, aberrant interactions in the microenvironment and altered networks of complex chemical reactions at the molecular and cellular level, some of which can be targeted with anti-neoplastic drugs called chemotherapy. AML can be treated with chemotherapy, the types and doses of which are dependent on characteristics of the patient, the sub-type of the tumor and the use of other, often synergistic, anti-cancer drugs, and the doses for which are limited by toxic adverse effects of treatment. Current treatment protocols are designed based on pre-clinical animal experiments and on empirical clinical trials as well as the acquired experience of subspecialist physicians. Mathematical modeling can assist in improving chemotherapy effectiveness and limiting toxicity through a systematic approach in designing treatment protocols. Specifically, these mathematical models should enable a description of the normal and the leukemic cell populations as dependent on disease characteristics (cell cycle distribution into phases, proliferation rate, initial disease and normal population state) and on physiological characteristics of the patient such as age, sex, body surface area that control and define the drug kinetics (concentration profile in tumor site). Such a model can then lead to an optimal management of the available drug kinetics in order to effectively eradicate the maximum possible tumor volume while limiting toxicity of the normal cell population and that will be maintained within certain defined limits.Herein, a model is presented for the first cycle of chemotherapy induction treatment for AML using daunorubicin (DNR) and cytarabine (Ara-C) anti-leukemic agents, a standard intensive treatment protocol for AML. The proposed model combines critical targets of drug actions on the cell cycle, together with pharmacokinetic (PK) and pharmacodynamic (PD) aspects providing a complete description of drug diffusion and action after administration. Tumor-specific characteristics, such as tumor burden and cell cycle times, as well as patient-specific characteristics, such as gender, age, weight and height, are incorporated into the model in an attempt to gain insights into the personalized cell dynamics during treatment. Moreover, an optimal control problem is formulated and solved so as to obtain the chemotherapeutic schedule which would maximize leukemic cell kill (therapeutic efficacy) while minimizing death of the normal cell population, thereby reducing toxicities. Simulation results for a standard treatment protocol are obtained for a patient case study; an optimized treatment schedule is also obtained and the cell populations are analyzed and compared in detail for both the standard and the optimized treatment protocols.",
     "keywords": ["Mathematical modeling", "Chemotherapy optimization", "Cell cycle models", "Pharmacokinetics", "Pharmacodynamics"]},
    {"article name": "A mechanistic approach for modeling oral drug delivery",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.06.002",
     "publication date": "10-2013",
     "abstract": "A drug release mechanism is combined with a Compartmental Absorption and Transit (CAT) model within a pharmacokinetic framework for predicting orally administered drug release and transport. The developed model is used to evaluate pharmacokinetic metrics such as peak plasma concentration, area under the curve (AUC) and bioavailability. A comparative study has been performed on different cimetidine tablet formulations for which clinical drug profiles are available for model validation. The model predictions are in good agreement with the clinical results. Different scenarios based on induced changes in average patient body weight, fraction of drug in the tablet, tablet radius and their corresponding effects on the model predictions are discussed. The change in average patient body weight has a low effect on the plasma concentration profile while the fraction of drug in the tablet has a strong effect. In a scenario study within an optimization framework, the model has been able to determine the optimal dimensions of a tablet of fixed dosage in order to obtain maximum therapeutic effect from the tablet.",
     "keywords": ["CAT compartmental absorption and transit", "compartmental absorption and transit", "AUC area under the curve", "area under the curve", "GI gastrointestinal", "gastrointestinal", "ADME absorption, distribution, metabolism and excretion", "absorption, distribution, metabolism and excretion", "NSC normalized sensitivity coefficient", "normalized sensitivity coefficient", "ACAT advanced compartmental and transit", "advanced compartmental and transit", "Cmax peak plasma concentration", "peak plasma concentration", "fd fraction of drug in the tablet", "fraction of drug in the tablet", "Wp average patient body weight", "average patient body weight", "Oral drug delivery", "ACAT model", "Dissolution model", "Pharmacokinetic model", "Optimization"]},
    {"article name": "Extended method of moment for general population balance models including size dependent growth rate, aggregation and breakage kernels",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.04.017",
     "publication date": "09-2013",
     "abstract": "Among the various methods for the solution of population balances (PB) equations, the moments based methods are some of the most successful and popular. Moment based methods have demonstrated advantages such as reduced computational effort especially for applications that couple population balance (PB) simulation with computational fluid dynamics (CFD). The methods, however, suffers from two main drawbacks: the inability to yield the number density directly and limitation to system models that only consider size independent growth expression and certain breakage and aggregation kernels. This article presents an extension to the standard MOM (SMOM), the extended method of moment (EMOM), which not only allows the prediction of the number density directly in most cases, but also makes it applicable to systems with size dependent growth and any particle aggregation and breakage kernels. The method is illustrated and tested by reference to case studies that were well studied systems in the literature.",
     "keywords": ["AD-QMOM automatic differentiation-based QMOM", "automatic differentiation-based QMOM", "CFD computational fluid dynamics", "computational fluid dynamics", "EMOM extended method of moments", "extended method of moments", "PB population balance", "population balance", "PBE population balance equation", "population balance equation", "PSD particle size distribution", "particle size distribution", "PD product-difference", "product-difference", "QMOM Quadrature Method of Moments", "Quadrature Method of Moments", "DQMOM Direct Quadrature Method of Moments", "Direct Quadrature Method of Moments", "MOM method of moments", "method of moments", "SMOM standard method of moments", "standard method of moments", "SQMOM Sectional Quadrature Method of Moments", "Sectional Quadrature Method of Moments", "Population balance equation", "Size independent growth", "Breakage", "Aggregation", "Computational fluid dynamics (CFD)", "Extended method of moments (EMOM)"]},
    {"article name": "An alternative disjunctive optimization model for heat integration with variable temperatures",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.05.002",
     "publication date": "09-2013",
     "abstract": "This paper presents an alternative model to deal with the problem of optimal energy consumption minimization of non-isothermal systems with variable inlet and outlet temperatures. The model is based on an implicit temperature ordering and the \u201ctransshipment model\u201d proposed by Papoulias and Grossmann (1983). It is supplemented with a set of logical relationships related to the relative position of the inlet temperatures of process streams and the dynamic temperature intervals. In the extreme situation of fixed inlet and outlet temperatures, the model reduces to the \u201ctransshipment model\u201d. Several examples with fixed and variable temperatures are presented to illustrate the model's performance.",
     "keywords": ["Heat integration", "Disjunctive model", "MILP", "MINLP", "Logic disjunctions"]},
    {"article name": "Optimization of a simple LNG process using sequential quadratic programming",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.05.001",
     "publication date": "09-2013",
     "abstract": "The efficiency of using sequential quadratic programming (SQP) for the optimization of a PRICO process for the production of liquefied natural gas (LNG) is demonstrated. Most of the returned objective values have been better, and the execution times much lower, than in most previously published work on similar optimization cases. The optimization runs discussed in this paper require around 5\u00a0min of execution time.",
     "keywords": ["LNG", "PRICO", "Optimization", "Constraints", "Minimum temperature difference"]},
    {"article name": "Reduction of kinetic models using dynamic sensitivities",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.05.003",
     "publication date": "09-2013",
     "abstract": "The development of detailed chemical kinetic models is necessary for the design and optimization of complex chemical systems. However, it is also often desired to reduce the model size by excluding inconsequential chemical species and/or reactions for end-point applications, usually due to computational reasons. In this work, new model reduction methods based on dynamic sensitivities from the impulse parametric sensitivity analysis (iPSA) and the Green's function matrix (GFM) analysis have been developed. The iPSA and GFM were originally formulated to provide dynamical parameter-by-parameter and species-by-species information on how a system output behavior is achieved, respectively. The efficacies of the proposed reduction methods were compared with existing methods through applications to reduce detailed kinetic models of alkane pyrolysis and natural gas combustion (GRI Mech 3.0) and an ab initio kinetic model of industrial steam cracking of ethane.",
     "keywords": ["Model reduction", "Kinetic models", "Sensitivity analysis", "Pyrolysis", "GRI Mech 3.0"]},
    {"article name": "Fast and accurate parameter sensitivities for the general rate model of column liquid chromatography",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.04.021",
     "publication date": "09-2013",
     "abstract": "A fast and accurate solver for the general rate model is extended for computing sensitivities that describe the impact of small parameter changes on the simulated chromatograms. Parameter sensitivities are required by many optimization algorithms and are useful for understanding how chromatograms depend on specific system properties or operating conditions. They are efficiently computed with arbitrary precision by integrating a forward sensitivity DAE system that is derived from the original DAE system. The involved partial derivatives are either manually derived or computed by algorithmic differentiation. This approach is demonstrated to be more robust and faster for realistically sized problems, as compared to the traditional finite difference approach. Sensitivities are computed not only with respect to intrinsic model parameters, such as diffusion coefficients and isotherm parameters, but also with respect to parameters in the boundary concentrations, such as the slope of an elution salt gradient. The extended solver is part of the Chromatography Analysis and Design Toolkit (CADET).",
     "keywords": ["AD algorithmic differentiation", "algorithmic differentiation", "ADOL-C automatic differentiation by overloading in C++", "automatic differentiation by overloading in C++", "BDF backward differentiation formula", "backward differentiation formula", "CADET Chromatography Analysis and Design Toolkit", "Chromatography Analysis and Design Toolkit", "DAE differential-algebraic equation", "differential-algebraic equation", "FV finite volumes", "finite volumes", "FD finite differences", "finite differences", "GPL general public license", "general public license", "GRM general rate model", "general rate model", "IDA implicit differential-algebraic solver", "implicit differential-algebraic solver", "IDAS implicit differential-algebraic solver with sensitivity capabilities", "implicit differential-algebraic solver with sensitivity capabilities", "IVP initial value problem", "initial value problem", "JUROPA J\u00fclich Research on Petaflop Architectures", "J\u00fclich Research on Petaflop Architectures", "MCL multi-component Langmuir", "multi-component Langmuir", "MPM mobile phase modifier", "mobile phase modifier", "OpenMP open multi-processing", "open multi-processing", "PDAE partial differential-algebraic equation", "partial differential-algebraic equation", "SMA steric mass action", "steric mass action", "SUNDIALS suite of nonlinear and differential/algebraic equation solvers", "suite of nonlinear and differential/algebraic equation solvers", "WENO weighted essentially non-oscillatory", "weighted essentially non-oscillatory", "Column liquid chromatography", "General rate model", "Parameter sensitivities", "Staggered corrector", "Algorithmic differentiation"]},
    {"article name": "A heuristic batch sequencing for multiproduct pipelines",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.05.007",
     "publication date": "09-2013",
     "abstract": "Multiproduct pipelines are designed to transport refined petroleum products from major sources to distribution centers. Moreover, the existence of large, complex and world wide spread businesses leads to a complex system in which distribution can be seen as one of the key areas that needs to be efficiently and effectively managed. The system reported in this paper was composed of an oil refinery, a multiproduct pipeline connected to a storage tank farm that received large amounts of oil derivatives. In this work, a heuristic method was proposed for scheduling a real life system. The output was an operational program for running a pipeline efficiently in a given time horizon while considering almost all practical restrictions. The algorithm was able to achieve a near-optimal solution in a reasonable time. The approach was successfully applied to a number of pipelines including a real-life problem. The data and experimental results were reported.",
     "keywords": ["Pipeline scheduling", "Heuristic method", "Multi-product pipeline"]},
    {"article name": "Simultaneous study on energy consumption and emission generation for an ethylene plant under different start-up strategies",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.05.013",
     "publication date": "09-2013",
     "abstract": "Chemical plant start-ups are very critical dynamic operations, which normally consume huge amount of energy and also generate large quantities of off-spec products for flaring, causing significant and intensive air emissions. Many studies have been individually conducted on energy savings or emission reductions under plant normal conditions. However, quantitative studies on simultaneous energy consumption and emission generation for chemical plant start-ups are still lacking. In this study, plant-wide dynamic simulations are employed to investigate energy consumption and emission generation for an ethylene plant under different start-up strategies. Dynamic modeling and simulations for two start-up designs associated with three start-up operating procedures are performed. Based on plant-wide dynamic simulations, dynamic profiles of energy consumption and emission generation during the plant start-up are obtained and analyzed. Details of energy accounting on cooling and heating duties of key distillation towers, auxiliary heat exchanger duties, and power consumption for compressor system are provided for each start-up case. Through comprehensive analysis, the most desirable start-up solution is identified. This virtual study not only characterizes the emission generation during the plant start-up, supporting flare minimization activities that benefits environmental sustainability, but also enhances critical research on energy and raw material savings during the plant start-up that will benefit the industrial sustainability.",
     "keywords": ["Plant start-up", "Plant-wide dynamic simulation", "Emission reduction", "Energy saving"]},
    {"article name": "Selection of controlled variables: A novel perspective based on the singular energy of weighted graphs",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.05.014",
     "publication date": "09-2013",
     "abstract": "The optimal selection of controlled variables is a well-known plant-wide control subproblem. In this paper, a novel approach based on spectral graph theory is proposed. This strategy is useful from both graphical and mathematical point of views. It is shown here that if the closed-loop process is represented by a specific weighted graph, deviations in plant variables are bounded by the graph singular energy. Moreover, this graph-based methodology supports the fast interpretation of the magnitude and direction of influences between process variables at steady state. The suggested spectral approach is compared with the recently proposed minimum square deviation (MSD) methodology in detail. Indeed, both strategies have strong structural and behavioral resemblances, i.e. reducing specific deviations and improving the conditions of the subprocess to be controlled. The introduced graph representation is tested in the Shell oil fractionator process, giving a complete set of evaluations and results.",
     "keywords": ["Plant-wide control", "Weighted graphs energy", "Singular energy", "Selection of controlled variables"]},
    {"article name": "Suitability of parameters and models in the Discrete Element Method for simulation of mesoscale powder indentation experiments",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.05.015",
     "publication date": "09-2013",
     "abstract": "The Discrete Element Method (DEM) has been shown in the literature to be able to match the results of certain experimental powder process such as shear cell and angle of repose tests. In this work we try to discover which models and parameters are required by the DEM to simulate a mesoscale process. This work compares 2D and 3D simulations, linear and Hertzian stiffness models, friction and cohesion and several values of stiffness parameter. Using 3D models with friction and higher stiffness values produced the most similar behavior to the experiments. Spectral results show the scales of the particle rearrangements were reproduced by the DEM and independent of parameters and models considered in this work.",
     "keywords": ["2D", "3D", "Discrete Element Method (DEM)", "Probe indentation", "Validation"]},
    {"article name": "MILP based value backups in partially observed Markov decision processes (POMDPs) with very large or continuous action and observation spaces",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.04.020",
     "publication date": "09-2013",
     "abstract": "Partially observed Markov decision processes (POMDPs) serve as powerful tools to model stochastic systems with partial state information. Since the exact solution methods for POMDPs are limited to problems with very small sizes of state, action and observation spaces, approximate point-based solution methods like PERSEUS have gained popularity. In this work, a mixed integer linear program (MILP) is developed for calculation of exact value updates (in PERSEUS and similar algorithms), when the POMDP has very large or continuous action space. Since the solution time of the MILP is very sensitive to the size of the observation space, the concept of post-decision belief space is introduced to generate a more efficient and flexible model. An example involving a flow network is presented to illustrate the concepts and compare the results with those of the existing techniques.",
     "keywords": ["Markov decision processes", "Dynamic programming", "Mathematical programming", "Partial observation", "Network reliability"]},
    {"article name": "Classification and optimization of potentially runaway processes using topology tools",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.05.012",
     "publication date": "09-2013",
     "abstract": "Fast and strongly exothermic reactions can exhibit a well-known phenomenon called \u201cthermal runaway\u201d. To identify high productivity and safe conditions for these reactions, with particular reference to industrial semibatch processes carried out using both isothermal and isoperibolic temperature control mode, a dedicated classification/optimization software has been implemented. This software is able to both classify a set of operating parameters from the thermal stability viewpoint and identify the optimum reacting/synthesis conditions using a topological criterion that binds the so-called \u201cQFS region\u201d, where reactants accumulation is low and all the heat released is readily removed by the cooling equipment. During the search for system optimum a number of safety and quality constraints can be taken into account. The software reliability, in terms of both classification and optimization ability, has been tested using both laboratory and industrial scale experimental data. Obtained results confirm a good agreement between theoretical predictions and experimental evidence.",
     "keywords": ["Thermal runaway", "Loss of control", "Chemical reactors", "Safe optimization", "Topological criterion"]},
    {"article name": "Market-driven operational optimization of industrial gas supply chains",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.05.018",
     "publication date": "09-2013",
     "abstract": "This paper deals with the operational optimization of industrial gas supply chains. Industrial gas networks differ from other applications and require a rather special and shrewd approach to modeling and optimization issues. Currently in industrial gas plants: (i) the raw material is free; (ii) binding contracts and clauses place tight limits on supplies, production and distribution; (iii) electric energy cost fluctuations wield a greater influence over overall production than in other industrial areas; (iv) market demand is subject to multifaceted fluctuations and uncertainties; (v) long-term market demand is usually regulated by take-or-pay contracts; (vi) main users are on-site or connected via pipeline; and (vii) cryogenic storage constitutes a significant expense and encourages companies to veer toward just-in-time production.This paper also provides very general guidelines for dealing with the aforementioned peculiarities, and compares plant-wide and enterprise-wide optimizations of industrial gas networks. An existing network is adopted as industrial case study.",
     "keywords": ["Industrial gases", "Supply chain", "Mixed-integer programming", "Plant-wide optimization", "Enterprise-wide optimization", "Operational planning"]},
    {"article name": "Solving a separation-network synthesis problem by interval global optimization technique",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.05.009",
     "publication date": "09-2013",
     "abstract": "A novel method is presented here for solving certain classes of separation-network synthesis (SNS) problems, which are inherently non-linear, using branch-and-bound framework and linear programming. The proposed method determines effectively the structure and flowrates of the optimal separation network.The examined problem contains simple and sharp separators, mixers, and dividers; its aim is to produce three pure product streams from two three-component feed streams with minimal cost. The cost of the network is the sum of the costs of the separators and the cost of a separator is a concave function of its mass load. The mathematical programming model generated from the rigorous super-structure is non-linear.The goal of our work is to determine the optimum of the aforementioned model effectively. The splitting ratios of the dividers are handled as intervals. A B&B method operates on them. The cost functions of the separators are approximated with lower estimating functions.",
     "keywords": ["Separation synthesis", "Interval method", "Global optimization", "Branch-and-bound framework"]},
    {"article name": "An improved multi-objective differential evolution with a termination criterion for optimizing chemical processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.05.004",
     "publication date": "09-2013",
     "abstract": "Application problems have conflicting objectives and constraints, and maximum number of generations is the most common termination criterion in evolutionary algorithms used for solving these applications. This study develops a termination criterion using the non-dominated solutions obtained as the search progresses. For this, several performance metrics are modified, and their variation with generations has been assessed on many test functions. Based on this analysis, it is proposed to terminate the search if the improvement in variance of two selected performance metrics obtained in recent generations is statistically insignificant. Additionally, evaluation of objectives and constraints is computationally expensive in many applications. This study uses taboo list with multi-objective differential evolution to avoid re-visits and for better exploration of search space. Benefits of the termination criterion and taboo list are assessed on constrained benchmark problems. The developed approach is then evaluated on three chemical engineering applications, namely, alkylation, Williams-Otto and fermentation processes.",
     "keywords": ["Multi-objective optimization", "Differential evolution", "Termination criterion", "Alkylation process", "Williams-Otto process", "Fermentation process"]},
    {"article name": "Design of heat integrated distillation column by using H-xy and T-xy diagrams",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.05.020",
     "publication date": "09-2013",
     "abstract": "An interactive design methodology of HIDiC with finite internal heat exchange stages is proposed. When the number of heat integrated stages is limited, the number of possible combinations of heat integration increases. Thus, the optimal design problem usually becomes a complex mixed integer nonlinear programming problem. In this research, a novel design methodology using modified Ponchon\u2013Savarit diagram is proposed. The first modification of the diagram is the introduction of the reversible distillation curve, which shows the enthalpy profile at reversible condition. By using this curve, the target enthalpy profile in the column can be determined and the plausible stages for heat exchange can be determined by taking the predetermined theoretical stages into account. The second modification is the integration of the T-xy diagram with H-xy diagram. By doing so, the temperature difference of side exchangers can easily be identified, and it becomes possible to determine the pairing of heat integration stages between the rectifying section and the stripping section in consideration of the heat transfer area of side exchangers. The conditions obtained by the diagram methodology can be directly used in rigorous process simulation. Case studies demonstrated that the results obtained from the proposed methodology have good agreement with the conditions obtained through numerical optimization. The designer can modify the design condition interactively by checking the obtained diagram. Thus, the proposed methodology is especially beneficial at the preliminary design stage of HIDiC.",
     "keywords": ["HIDiC", "Distillation", "Ponchon\u2013Savarit diagram", "Energy saving"]},
    {"article name": "Supply-based superstructure synthesis of heat and mass exchange networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.05.010",
     "publication date": "09-2013",
     "abstract": "A new simultaneous mixed integer non-linear programing (MINLP) approach to heat exchange network synthesis (HENS) and mass exchange network synthesis (MENS) is presented. This supply-based superstructure (SBS) approach uses the supply temperatures/compositions of all the streams (including utilities) present in the synthesis problem to define heat/mass exchange superstructure intervals. The intermediate temperatures/compositions are variables used in the optimization of the network total annual cost (TAC). The ability of each stream to exchange heat/mass in any interval in the SBS is subject to thermodynamic/mass transfer feasibility. The paper presents the mathematical formulations for optimizing the TAC for HENS and MENS. The SBS synthesis technique has been applied to nine literature problems involving both HENS and MENS. The solutions obtained are in the same range as those in the literature, with one solution being the lowest of all.",
     "keywords": ["Mass exchange networks", "Synthesis", "Superstructure"]},
    {"article name": "MILP-based clustering method for multi-objective optimization: Application to environmental problems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.05.016",
     "publication date": "09-2013",
     "abstract": "Multi-objective optimization (MOO) has recently emerged as a useful technique in environmental engineering. One major limitation of this approach is that its computational burden grows rapidly with the number of environmental objectives, which causes difficulties regarding the computation and visualization of the Pareto solutions. In this work we present several theoretical and algorithmic developments for grouping environmental objectives into clusters on the basis of which the multi-objective optimization can be performed, thereby facilitating the computation and analysis of the Pareto solutions. Our method is based on a novel mixed-integer linear program (MILP) that identifies in a systematic manner groups of objectives that behave similarly. We test the capabilities of our approach using several examples, in which we compare it against other well-known clustering methods.",
     "keywords": ["Multi-objective optimization", "Clustering methods", "Pareto solutions", "Environmental objectives", "Life cycle assessment"]},
    {"article name": "A model to optimize facility layouts with toxic releases and mitigation systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.05.017",
     "publication date": "09-2013",
     "abstract": "The possibility of including mitigation systems in layout models is explored in this work. The model is based on a previous work by the authors to estimate toxic concentrations around each releasing facility surrounded by a mitigation system. The mitigation systems considered here includes water, steam, and air curtains and exponential decays are assumed for the concentrations shapes before and after the installed curtain. The selection of the mitigation system type to install is included as a variable to determine when solving the proposed MINLP model. Additional constraints include the conventional non-overlapping and risk estimations based on probit functions. The objective function includes occupied land costs, interconnection costs, risk damage costs, and mitigation costs. A software package called TROL has been developed to automatically interact with GAMS and ease the initial and final layout descriptions. Numerical results indicate that the proposed model produces more practical and optimal layouts.",
     "keywords": ["Facility layout", "Toxic releases", "Mitigation systems", "Water, steam and air curtains", "MINLP problem"]},
    {"article name": "A mixed-integer model predictive control formulation for linear systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.04.011",
     "publication date": "08-2013",
     "abstract": "Since their inception in the early 1980s industrial model predictive controllers (MPC) rely on continuous quadratic programming (QP) formulations to derive their optimal solutions. More recent advances in mixed-integer programming (MIP) algorithms show that MIP formulations have the potential of being advantageously applied to the MPC problem. In this paper, we present an MIP formulation that can overcome difficulties faced in the practical implementation of MPCs. In particular, it is possible to set explicit priorities for inputs and outputs, define minimum moves to overcome hysteresis, and deal with digital or integer inputs. The proposed formulation is applied to simulated process systems and the results compared with those achieved by a traditional continuous MPC. The solutions of the resulting mixed-integer quadratic programming (MIQP) problems are derived by a computer implementation of the Outer Approximation method (OA) also developed as part of this work.",
     "keywords": ["Mixed integer programming", "Predictive control", "Hybrid control"]},
    {"article name": "Data quality assessment of routine operating data for process identification",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.03.029",
     "publication date": "08-2013",
     "abstract": "In many chemical engineering plants, process identification is often performed de novo each time that it is needed. However, it is quite possible that sufficiently excited data regions, including routine operating regions, have already been collected and are available for identifying particular model structures. Therefore, there is a need to develop techniques for extracting these regions from the other uninformative regions. One potential approach to solving this problem is to consider the condition number of the Fisher information matrix for the desired model structure. The sensitivity of this approach to changes in sampling time, model structure, controller type, and number of data points is also examined. It is shown, through theoretical and simulation analysis that the proposed method determines data quality based on the situation. Practically, the proposed method can be used to determine the upper bound for the process model order that may be identified from the given data.",
     "keywords": ["Data quality", "Process identification", "Closed-loop", "Routine operating data"]},
    {"article name": "A branch-and-bound algorithm for the solution of chemical production scheduling MIP models using parallel computing",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.03.030",
     "publication date": "08-2013",
     "abstract": "Exploiting multiple cores in a computer or grid of computers can reduce the time required to solve a mixed-integer programming (MIP) model. Here, we develop a parallel branch-and-bound algorithm for a chemical production scheduling problem using a discrete-time model. The algorithm consists of initialization, submission, branching, collection, bounding, and pruning steps. We branch by adding a constraint to bound the total number of times each task runs. Each subproblem is solved as an MIP on a single core of a computer, so that many sub problems can be solved simultaneously. Also, we propose an algorithm, executed at each node of our branch-and-bound tree, to improve the bounds on the number of times a task is run based on the current objective value. We present computational results for several instances to show the parallel algorithm with the proposed branching strategy can solve more challenging problems than simply using the default parallel option.",
     "keywords": ["Chemical production scheduling", "Mixed-integer programming", "Parallel computing"]},
    {"article name": "Optimal control of polymer flooding based on simultaneous perturbation stochastic approximation method guided by finite difference gradient",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.04.009",
     "publication date": "08-2013",
     "abstract": "The paper established an optimal control model of polymer flooding, which involves the incremental cumulative net present value as objective function, the injection concentration and volume size in each slug of every injector as control variables, and the limitation of polymer concentration and injection amount as boundary constraints. An improved simultaneous perturbation stochastic approximation method guided by finite difference gradient was then proposed. It adjusted the ratio among perturbation steps of different control variables during iterations according to the finite difference gradient. The case study showed the improved algorithm needed much fewer simulations for convergence. Compared with uniform injection scheme, the allocation amount of polymer solution in well groups with strong vertical heterogeneity was increased and the incremental cumulative net present value increased by 11.64% after optimization. The paper finally investigated the effect of crude oil price on the optimum injection amount of polymer solution for a given reservoir.",
     "keywords": ["Polymer flooding", "Optimal control", "Finite difference", "Simultaneous perturbation"]},
    {"article name": "A new moment analysis method to estimate the characteristic parameters in chromatographic general rate model",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.04.015",
     "publication date": "08-2013",
     "abstract": "In conventional moment analysis, detailed information regarding the retention equilibrium and mass transfer kinetics is derived from the 1st normalized moment and standard deviation of the chromatograms respectively. The moment values are determined from their analytical expressions. In the moment analysis method of this work, the moment values are estimated accurately from the simulated chromatograms by the application of the weighed residual moment method. For parameters estimation, our moment analysis method can be implemented not only on the symmetric chromatograms with linear isotherms as in conventional moment analysis, but also to the analysis of asymmetric chromatograms with nonlinear and competitive isotherms. Also compared to the commonly used parameter estimation method (fitting with experimental concentration points), our moment analysis method approached faster to the optimized values and the final parameters were also better identified.",
     "keywords": ["Moment analysis", "Parameter optimization", "Chromatographic general rate model", "Numerical solutions"]},
    {"article name": "Solution of the population balance equation using parallel adaptive cubature on GPUs",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.04.012",
     "publication date": "08-2013",
     "abstract": "The Dual Quadrature Method of Generalized Moments (DuQMoGeM) is an accurate moment method for solving the population balance equation (PBE). The drawback of DuQMoGeM is the high computational cost associated with numerical integrations of the PBE integral terms in which each integrand can be integrated independently and, therefore, amenable to parallelization on GPUs. In this work, two parallel adaptive cubature algorithms were implemented on a hybrid architecture (CPU\u2013GPU) to accelerate the DuQMoGeM. The speedup and scalability of these parallel algorithms were studied with different types of Genz's test functions. Then, we applied these parallel numerical integration algorithms in the DuQMoGeM solution of the PBE for three bivariate cases, obtaining speedups between 11 and 15.",
     "keywords": ["Population balance modeling", "Quadrature Method of Generalized Moments", "Particulate systems", "Adaptive cubature", "GPU"]},
    {"article name": "Risk-based decision making in early chemical process development of pharmaceutical and fine chemical industries",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.03.032",
     "publication date": "08-2013",
     "abstract": "Decision making under uncertainty is vital in the early chemical process development. Competing objectives such as SHE and economics are considered in a data-lean environment. Screening of potential synthetic route options is done in a team environment with experts from different disciplines such as chemists and chemical engineers. Simple ranking methods are used and those hardly reveal the rationale behind choices.A systematic framework to support better decisions based on the SELECT (Safety, Environment, Legal, Economics, Control, Throughput) criteria has been developed and demonstrated using Cephalosporin process development. This approach links three components namely uncertainty, risk and capability analyses for multi-objective decision making. Each area of the decision criteria is scored using a hierarchical data structure for comparing different options. This approach improves the decision maker's understanding on the rationale behind choices and records them explicitly for improved learning. Further development and limitations of the approach are also discussed.",
     "keywords": ["Chemical route selection", "Early process development", "Risk analysis", "Multi-criteria decision analysis", "Knowledge management"]},
    {"article name": "A new metaheuristic based approach for the design of sensor networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.04.007",
     "publication date": "08-2013",
     "abstract": "The optimal design of sensor networks consists in selecting the type, number and location of sensors that provide the required quantity and quality of process information by optimizing an appropriate objective function. The problem is multimodal and involves many binary variables, therefore a huge combinatorial optimization problem results. In this work, the design is solved using a metaheuristic based approach. A strategy that combines the advantages of Tabu Search and Estimation of Distribution Algorithms is presented, which is able to solve high scale designs since it can be implemented to run in parallel. Application results of the methodology to the optimal selection of instruments for networks of incremental size are provided.",
     "keywords": ["Sensor network design", "Combinatorial optimization", "Estimation of Distribution Algorithms", "Tabu Search"]},
    {"article name": "Parallel nonconvex generalized Benders decomposition for natural gas production network planning under uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.04.006",
     "publication date": "08-2013",
     "abstract": "A scenario-based two-stage stochastic programming model for gas production network planning under uncertainty is usually a large-scale nonconvex mixed-integer nonlinear programme (MINLP), which can be efficiently solved to global optimality with nonconvex generalized Benders decomposition (NGBD). This paper is concerned with the parallelization of NGBD to exploit multiple available computing resources. Three parallelization strategies are proposed, namely, naive scenario parallelization, adaptive scenario parallelization, and adaptive scenario and bounding parallelization. Case study of two industrial natural gas production network planning problems shows that, while the NGBD without parallelization is already faster than a state-of-the-art global optimization solver by an order of magnitude, the parallelization can improve the efficiency by several times on computers with multicore processors. The adaptive scenario and bounding parallelization achieves the best overall performance among the three proposed parallelization strategies.",
     "keywords": ["Stochastic programming", "MINLP", "Parallel computing", "Benders decomposition", "Natural gas production network"]},
    {"article name": "Optimal producer well placement and production planning in an oil reservoir",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.04.002",
     "publication date": "08-2013",
     "abstract": "Most of the available literature on optimal well placement has employed numerical simulators in a black box manner linked to an external search engine. In this work, we formulate the contents of that box inside a mixed integer nonlinear programming model for optimal well placement. We provide a unified model that integrates the subsurface, wells, and surface levels of an upstream production project. It links the production plan with the aforementioned elements, and economics and market. This results in a complex spatiotemporal mixed integer nonlinear model, for whose solution we modify and augment an existing outer approximation algorithm. The model solution provides the optimal number of new producers, their locations, and optimal production plan over a given planning horizon. To our knowledge, this is the first contribution that uses mathematical programming in a real dynamic sense by honoring the constituent partial differential equations.",
     "keywords": ["Optimal well placement", "Oil production planning", "Mathematical programming", "Oil reservoir modeling", "Dynamic optimization"]},
    {"article name": "Dynamic optimization of the vulcanization of EPDM elastomers by short waves infrared technologies",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.04.004",
     "publication date": "08-2013",
     "abstract": "This article deals with the optimization of superficial vulcanization of elastomers by infrared heating. This mode of heating notably used in automotive industry is particularly attractive in the transformation processes of extruded elastomers. The use of infrared emitters working in the short waves range offers the possibility to apply to the surface of the product a great heat flux with a weak thermal inertia. Moreover, the energy directly transmitted is easily adjustable. Consequently, it is necessary to correctly evaluate the energy contribution.In this paper, the incident infrared irradiation and the process time enabling to reach a desired vulcanization profile in the product thickness is estimated off-line by an optimal command method. This estimation is based on the exploitation of a model including both the heat transfer and the vulcanization reaction. The tests are performed with EPDM based elastomer plane plates (8\u00a0mm thickness).",
     "keywords": ["Elastomer", "Numerical model", "Experimental test", "Off-line optimal control", "States of cure"]},
    {"article name": "Refinery scheduling of crude oil unloading with tank inventory management",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.04.003",
     "publication date": "08-2013",
     "abstract": "The aim of this study is to develop a methodology for short-term crude oil unloading, tank inventory management, and crude distillation unit (CDU) charging schedule using mixed integer linear programming (MILP) optimization model as an extension to a previous work reported by Lee et al. (1996). The authors attempt to improve the previous model by adding an interval\u2013interval variation constraint to avoid CDU charging rate fluctuation, a shutdown penalty within the scheduling cycle and a set up penalty for tank-tank transfer and introducing demand violation permit for a more flexible model against obtaining infeasible solution. Three different cases from the original paper were used to test the validity of the improved model. Comparison between Cases 1 and 2 shows the advantage of using smaller time interval as the operating cost of Case 2 is lower. Two scenarios were created from Case 3 to show the benefits of the improved model in deciding the best schedule to use. The improved model was implemented using the CPLEX solver in GAMS.",
     "keywords": ["Crude oil", "Refinery", "Scheduling", "Short-term operation", "Mixed-integer linear programming (MILP)"]},
    {"article name": "Effects of uncertainties in experimental conditions on the estimation of adsorption model parameters in preparative chromatography",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.04.013",
     "publication date": "08-2013",
     "abstract": "Model-based process design is increasingly popular when designing pharmaceutical purification processes. The effect of uncertainties in concentration measurements on the estimation of model parameters is analyzed for two cases of non-isocratic adsorption chromatography. A model, calibrated to experiments, is used to generate data by adding a Monte Carlo sampled error in the inlet concentrations. New model parameters are estimated by minimizing the deviation between the synthetic data and the model. The first case is a separation of rare earth elements by ion-exchange chromatography and the second case is a purification of insulin from a product-related impurity by reversed-phase chromatography. It is shown that normally distributed errors in the concentrations result in deviations in the UV-signal that are not normally distributed. With the applied method, known concentration distributions can be translated into probability distributions of the model parameters, which can be taken into account in the model-based process design.",
     "keywords": ["Modeling", "Ion-exchange chromatography", "Reversed-phase chromatography", "Model calibration", "Parameter estimation", "Parameter uncertainty"]},
    {"article name": "Simultaneous synthesis of heat exchanger networks with operability considerations: Flexibility and controllability",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.04.010",
     "publication date": "08-2013",
     "abstract": "In this work a computational framework is proposed for the synthesis of flexible and controllable heat exchanger networks. The synthesis is projected to operate over a specified range of expected variations in the inlet temperatures and flowrates of the process streams using a decentralized control system, such that the total annual cost involving the utility consumption and the investment are optimized simultaneously. The framework is based on a two-stage strategy. A design stage is performed prior to the operability analysis where the design variables are chosen. In the second stage, the control variables are adjusted during operation on the realizations of the uncertain parameters. The framework yields a HEN design, which is guaranteed to operate with the designed control system under varying conditions ensuring stream temperature targets and optimal energy integration. The application of the proposed framework and its computational efficiency are illustrated with some numerical examples.",
     "keywords": ["Heat integration", "Heat exchanger network synthesis", "Operability", "Flexibility", "Controllability"]},
    {"article name": "Accelerating the parameters identifiability procedure: Set by set selection",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.04.014",
     "publication date": "08-2013",
     "abstract": "In this paper, a numerical procedure based on the binary search is proposed for accelerating the parameters identifiability procedure. Basically, the parameters are selected set by set using a given criterion for ranking the parameters. Since parameters identifiability procedures are strongly dependent on the initial estimates of parameters values, simultaneous parameters re-estimation step has been proposed in this paper. Two examples were used to evaluate the performance of the proposed criterion. In both cases, a significant reduction of the computational time was observed, and the results regard to the model fit are similar to those criteria based on the selection of parameters one by one, as usually presented in the literature.",
     "keywords": ["Parameters identifiability", "Binary search", "Parameters estimation", "Scarce experimental data"]},
    {"article name": "Branch and bound method for regression-based controlled variable selection",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.03.006",
     "publication date": "07-2013",
     "abstract": "Self-optimizing control is a promising method for selection of controlled variables (CVs) from available measurements. Recently, Ye, Cao, Li, and Song (2012) have proposed a globally optimal method for selection of self-optimizing CVs by converting the CV selection problem into a regression problem. In this approach, the necessary conditions of optimality (NCO) are approximated by linear combinations of available measurements over the entire operation region. In practice, it is desired that a subset of available measurements be combined as CVs to obtain a good trade-off between the economic performance and the complexity of control system. The subset selection problem, however, is combinatorial in nature, which makes the application of the globally optimal CV selection method to large-scale processes difficult. In this work, an efficient branch and bound (BAB) algorithm is developed to handle the computational complexity associated with the selection of globally optimal CVs. The proposed BAB algorithm identifies the best measurement subset such that the regression error in approximating NCO is minimized and is also applicable to the general regression problem. Numerical tests using randomly generated matrices and a binary distillation column case study demonstrate the computational efficiency of the proposed BAB algorithm.",
     "keywords": ["Branch and bound", "Control structure design", "Controlled variables", "Combinatorial optimization", "Distillation", "Self-optimizing control"]},
    {"article name": "A systematic framework for design of process monitoring and control (PAT) systems for crystallization processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.03.003",
     "publication date": "07-2013",
     "abstract": "A generic computer-aided framework for systematic design of a process monitoring and control system for crystallization processes has been developed to study various aspects of crystallization operations. The systematic design framework contains a generic crystallizer modelling toolbox, a tool for generation of the supersaturation set-point for supersaturation control, as well as a tool for design of a process monitoring and control system (also called Process Analytical Technology (PAT) system). This systematic design allows one to generate the necessary problem-chemical system specific model, the necessary supersaturation set-point as well as a PAT system design including implementation of monitoring tools and control strategies in order to produce the desired target product properties notably crystal size distribution (CSD) and shape for a wide range of crystallization processes. Application of the framework is highlighted through a case study involving the design of a monitoring and control system for a potassium dihydrogen phosphate (KDP) crystallization process, where also the one-dimensional CSD and two-dimensional CSD modelling features are highlighted.",
     "keywords": ["Crystallization", "Generic modelling framework", "Analytical CSD estimator", "Response surface method", "PAT system"]},
    {"article name": "Simultaneous design of explicit/multi-parametric constrained moving horizon estimation and robust model predictive control",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.03.001",
     "publication date": "07-2013",
     "abstract": "In this work we present a rigorous methodology for the simultaneous design of moving horizon estimation (MHE) and robust model predictive control based on multi-parametric programming. First, an explicit/multi-parametric solution of the MHE is derived. Then, a novel method is presented that allows for the derivation of the estimation error dynamics, the bounding set of the estimation error, and the state estimate dynamic equations of constrained MHE. A framework is then presented for the design of robust explicit/multi-parametric model predictive control (MPC) controllers, based on tube-based MPC methods, which ensures that no constraints are violated due to the estimation error and the process noise in the system. This framework is first shown for the Kalman filter and unconstrained MHE and is then extended to the constrained MHE.",
     "keywords": ["Moving horizon estimation", "MHE", "Error dynamics", "Multi-parametric programming", "Robust MPC", "Explicit MPC"]},
    {"article name": "Simple mass balance controllers for continuous sedimentation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.03.014",
     "publication date": "07-2013",
     "abstract": "The rational use of water in the mineral processing industry has become an important issue due to the geographical location of many plants. The increase of capacity in many copper concentrators has lead to an increased effort for recovering the maximum amount of water in the solid\u2013liquid separation process. Thickeners work continuously to produce a concentrated underflow and a water overflow free from particulate matter. The behavior of many processes can be represented by a set of intensive and extensive variables. In this case, practice has shown that standard feedback control based on intensive variables has not been very easy to tune and effective in providing consistent operations. In many plants, thickeners operate with poor standards, with high dosages of flocculants, overflows with high fine particles contents and highly variable underflows. This work presents a novel nonlinear PI controller which is able to stabilize thickener operation using a simple control structure. An internationally accepted model and calibration using plant data is used to illustrate the design methodology and the level of performance attained by the controllers. The analysis of the results points out the improved performance by using extensive variables. In addition some guidelines concerning controllers tuning are also provided.",
     "keywords": ["Continuous sedimentation", "Clarifier\u2013thickener", "Steady state", "Control PI"]},
    {"article name": "Sustainable scheduling of batch processes under economic and environmental criteria with MINLP models and algorithms",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.03.013",
     "publication date": "07-2013",
     "abstract": "We address the bi-criterion optimization of batch scheduling problems with economic and environmental concerns. The economic objective is expressed in terms of productivity, which is the profit rate with respect to the makespan. The environmental objective is evaluated by means of environmental impact per functional unit based on the life cycle assessment methodology. The bi-criterion optimization model is solved with the \u03b5-constraint method. Each instance is formulated as a mixed-integer linear fractional program (MILFP), which is a special class of non-convex mixed-integer nonlinear programs. In order to globally optimize the resulting MILFPs effectively, we employ the tailored reformulation-linearization method and Dinkelbach's algorithm. The optimal solutions lead to a Pareto frontier that reveals the tradeoff between productivity and environmental impact per functional unit. To illustrate the application, we present two case studies on the short-term scheduling of multiproduct and multipurpose batch plants.",
     "keywords": ["Sustainability", "Process operations", "LCA", "Global optimization", "Multiobjective optimization", "Batch manufacturing"]},
    {"article name": "A Monte-Carlo based model approximation technique for linear model predictive control of nonlinear systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.03.004",
     "publication date": "07-2013",
     "abstract": "In this paper we present a model approximation technique based on N-step-ahead affine representations obtained via Monte-Carlo integrations. The approach enables simultaneous linearization and model order reduction of nonlinear systems in the original state space thus allowing the application of linear MPC algorithms to nonlinear systems. The methodology is detailed through its application to benchmark model examples.",
     "keywords": ["Model predictive control", "Model order reduction", "Model approximation"]},
    {"article name": "Scheduling of multiple chemical plant start-ups to minimize regional air quality impacts",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.03.027",
     "publication date": "07-2013",
     "abstract": "Chemical plant concentrated regions may suffer localized and transient air pollution events that violate national ambient air quality standards (NAAQS). Flaring emissions, especially intensive start-up flaring emissions from chemical plants, have potentially significant impacts on local air quality. Thus, when multiple plants in an industrial zone plan to start-up within a same time period, their start-up plans should be evaluated and optimally controlled so as to avoid unexpected air-quality violations in any air-quality concern regions (AQCRs). In this paper, a general systematic methodology for multi-plant start-up emission evaluation and control has been developed. The methodology starts with collecting regional meteorological information such as wind speed and temperature; geographical information of all of the involved chemical plants and AQCRs; as well as plant operation data such as the start-up time window, start-up duration, and estimated emission profile. Next, a regional air-quality evaluation based on Gaussian dispersion model will be conducted. If any air quality violation is predicted to an AQCR, a multi-objective scheduling problem will be generated and solved to optimize the start-up sequence and start-up beginning time for all chemical plants. The scheduling model minimizes the overall air quality impacts to all of the AQCRs as well as minimize the total start-up time mismatch of all plants, subject to the principles of atmospheric pollutant dispersion. This study may provide valuable quantitative decision supports for multiple stake holders, including government environmental agency, regional chemical plants, and local communities.",
     "keywords": ["Air quality control", "Chemical plant start-ups", "Modeling", "Optimization", "Scheduling"]},
    {"article name": "A neural network-based optimizing control system for a seawater-desalination solar-powered membrane distillation unit",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.03.015",
     "publication date": "07-2013",
     "abstract": "Several schemes have been proposed so far for coupling desalination processes with the use of renewable energy. One of their main drawbacks, however, is the nature of the energy source that requires a discontinuous and non-stationary operation, with some control and optimization problems. In the present work, a solar powered membrane distillation system has been used for developing an optimizing control strategy. A neural network (NN) model of the system has been trained and tested using experimental data purposely collected. Afterwards, the NN model has been used for the analysis of the process performance under various operating conditions, namely distillate production versus feed flow rate, solar radiation and cold feed temperature. On this basis, a control system that optimizes the distillate production under variable operating conditions has been developed, implemented and tested.",
     "keywords": ["Membrane distillation", "Solar energy", "Neural network", "Control", "Optimization"]},
    {"article name": "First principles pharmacokinetic modeling: A quantitative study on Cyclosporin",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.03.026",
     "publication date": "07-2013",
     "abstract": "Unknown dose regimes are typically assessed on animals prior to clinical trials. Recent advances in the evaluation of new leads\u2019 efficacy have been achieved by pharmacokinetic modeling. Further improvements, including determination of the drug's mechanism of action and organism biodistribution, require an effective methodology for solving parameter estimation challenges. This article solves the problem of rigorously estimating unknown biochemical reaction and transport parameters from in vivo datasets and identifying whole-body physiologically based pharmacokinetic (PBPK) models.A rat blood circulation model was combined with biotransport, biochemical reactions and metabolism of the immunosuppressant Cyclosporin. We demonstrate the proposed methodology on a case study in Sprague-Dawley rats by bolus iv injections of 1.2, 6 and 30\u00a0mg/kg. Key pharmacokinetic parameters were determined, including renal and hepatic clearances, elimination half-life, and mass transfer coefficients, to establish drug biodistribution dynamics in all organs and tissues. This multi-scale model satisfies first principles and conservation of mass, species and momentum.Prediction of organ drug bioaccumulation as a function of cardiac output, physiology, pathology or administration route may be possible with the proposed PBPK framework. Successful application of our model-based drug development method may lead to more efficient preclinical trials, accelerated knowledge gain from animal experiments, and shortened time-to-market of new drugs.",
     "keywords": ["Whole body physiologically based pharmacokinetics", "Parameter estimation", "Drug delivery", "Immunosuppressant", "Cyclosporin", "Kinetic rate determination"]},
    {"article name": "Simulation of magnetic suspensions for HGMS using CFD, FEM and DEM modeling",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.03.012",
     "publication date": "07-2013",
     "abstract": "Properties of magnetic suspensions depend on the fluid, the particles and the magnetic background field. The simulation is aimed at understanding the influence of magnetic properties in High Gradient Magnetic Separation processes. In HGMS magnetic particles are collected on magnetic wires for separation. External magnetic forces are calculated or simulated using the Finite Element Method and embedded first in a Computational Fluid Dynamics simulation. In the simulation, elliptic and rectangular wires aligned in field direction reach higher separation efficiencies than cylindrical wires. Magnetic forces from FEM with implemented dipole forces in a Discrete Element Method code show magnetically induced agglomeration and yield an acceptable agreement with experiments. Particle deposition on wires is investigated under the influence of different parameters. The porosity of the deposit is dependent on the magnetization of the wire and particles. A centrifugal force of 60\u00a0g has an important influence.",
     "keywords": ["CFD", "DEM", "FEM", "HGMS", "Particle process", "MEC magnetically enhanced centrifugation", "magnetically enhanced centrifugation", "HGMS high gradient magnetic separation", "high gradient magnetic separation", "DEM discrete element method", "discrete element method", "FEM finite element method", "finite element method", "CFD computational fluid dynamics", "computational fluid dynamics"]},
    {"article name": "Control of carbon dioxide solubility in aqueous piperazine",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.03.010",
     "publication date": "07-2013",
     "abstract": "Solubility correlations are of great importance in the design of industrial carbon dioxide capture processes. Nevertheless, the development of this correlations are rarely used in the design of industrial control systems. The main objective of this work is to provide the basis for using solubility correlations in fast dynamic systems for industrial model based control applications.",
     "keywords": ["Control design", "Model base control", "MISO system", "Aqueous piperazine", "Loading control", "CO2 capture"]},
    {"article name": "Metrics for evaluating the forest biorefinery supply chain performance",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.03.031",
     "publication date": "07-2013",
     "abstract": "For sustainable decision-making regarding biorefinery strategies, different criteria, i.e. economic, environmental, social, should be considered. However, the economic criteria typically do not consider market volatility, whereas today's market involves price and demand volatilities. Biorefinery strategies must be flexible to be robust to market volatility. Therefore, relevant metrics must be developed to quantify the system's performance against volatility. This paper presents metrics of flexibility and robustness which analyze the performance of the supply chain in a dynamic environment, providing additional information along with economic metrics. In this paper, the link between the two metrics, and how profitability and robustness change with flexibility are discussed. The results reveal that, although profitability does not always increase with more flexibility and there is an optimum level of flexibility, the system's robustness is improved by increasing flexibility. Moreover, a \u201cconditional value-at-risk\u201d parameter is introduced to show what patterns of sale lead to highest profit and robustnestness.",
     "keywords": ["Forest biorefinery", "Supply chain", "Flexibility", "Robustness", "Value-at-risk"]},
    {"article name": "An online method for detection and reduction of chattering alarms due to oscillation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.03.025",
     "publication date": "07-2013",
     "abstract": "Chattering alarms, which repeatedly and rapidly make transitions between alarm and normal states in a short time period, are the most common form of nuisance alarms that severely degrade the performance of alarm systems for industrial plants. One reason for chattering alarms is the presence of oscillation in process signals. The paper proposes an online method to promptly detect the chattering alarms due to oscillation and to effectively reduce the number of chattering alarms. In particular, a revised chattering index is proposed to quantify the level of chattering alarms; the discrete cosine transform-based method is used to detect the presence of oscillation; two mechanisms by adjusting the alarm trippoint and using a delay timer are exploited to reduce the number of chattering alarms. An industrial case study is provided to illustrate the effectiveness of the proposed method.",
     "keywords": ["Process safety", "Chattering alarms", "Oscillation", "Alarm systems"]},
    {"article name": "Brownian dynamics simulation of the aggregation of submicron particles in static gas",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.03.028",
     "publication date": "07-2013",
     "abstract": "A Brownian dynamics simulation was conducted to investigate the formation of aggregates that are composed of submicron particles such as soot. Three models were considered for aggregation: a diffusion-limited aggregation model, in which an aggregate grows around a fixed particle; a particle\u2013cluster aggregation model, in which a single aggregate grows by collisions between particles and the aggregate; and a cluster\u2013cluster aggregation (CCA) model, in which many particles and aggregates form multiple aggregates. A comparison of the three aggregation models showed that the CCA model resulted in a soot-like branching shape. The aggregation was investigated by employing the CCA model; it was determined that increase in gas temperature affected the shielding effect of the aggregate branch by changing the displacement and velocity of Brownian particles. Furthermore, these simulations demonstrated that the size and aspect ratio of the field and the particle density also affected aggregation shape.",
     "keywords": ["Brownian dynamics simulation", "Aggregation", "Submicron particle"]},
    {"article name": "A virtual prototyping environment for a robust design of an injection moulding process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.04.005",
     "publication date": "07-2013",
     "abstract": "This paper proposes a new approach that enables a robust optimisation of the injection moulding process, based on the integration of numerical simulations, Response Surface Methodology and stochastic simulations in a type of integrated environment known as a virtual prototyping environment (VPE). The principal aim of the proposed approach is to include in the numerical setup of injection moulding the effects of fluctuations of process parameters.To clarify the proposed methodology, the paper details its application to the injection moulding process for the production of an engine cover. The moulded part presents some critical tolerances on different dimensions because of sealing and assembly requirements and the application of the VPE makes it possible to perform a robust setup taking into account the process fluctuations. The numerical prediction was confirmed by real production measurements on small pre-production runs performed adopting the moulding window explored in the virtual setup.",
     "keywords": ["Injection moulding process", "Finite Element Method", "Response Surface Methodology", "Robust design", "Stochastic simulations"]},
    {"article name": "Perspectives on industrial reactor control 2: An update from CPC 3",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.03.017",
     "publication date": "07-2013",
     "abstract": "This paper will discuss the evolution of reactor control over the last 25 years within the DuPont Company and its subsidiaries. It will focus on high level trends in control philosophy, systems and approaches. These changes have been necessary in order to achieve higher rates, better yields, improved uptime and a more sustainable footprint. This paper is an update to an article that was presented twenty-five years ago at Chemical Process Control (CPC) 3. The main focus will be on the use of Model Predictive Control (MPC) for reactor processes.",
     "keywords": ["Industrial", "Reactor", "Model Predictive Control", "Advanced regulatory control", "Monitoring", "Modeling"]},
    {"article name": "Recursive subspace identification with prior information using the constrained least squares approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.03.016",
     "publication date": "07-2013",
     "abstract": "It is essential to develop high quality models for process control and other applications. The incorporation of prior information in subspace identification has been investigated to obtain improved model quality. One of the recent developments incorporates the prior information using the constrained least squares (CLS). In many online applications, the amount of process data for model identification grows with time, and it is therefore necessary to develop a recursive algorithm for online identification of process models and to address the time-varying characteristics of the systems. In this paper, a recursive subspace identification algorithm incorporating prior information is developed using the constrained recursive least squares (CRLS). It is shown via a simulation example that the state space model identified using the proposed algorithm possesses improved accuracy.",
     "keywords": ["Linear systems", "System identification", "Adaptive control"]},
    {"article name": "A systematic approach for synthesizing combined mass and heat exchange networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.02.005",
     "publication date": "06-2013",
     "abstract": "Mass and heat are very important resources in the process industry. Numerous approaches have been proposed for the optimization of either mass or heat exchange networks. Since the process usage of mass and heat is typically intertwined, it is important to account for such interactions. The objective of this paper is to introduce a systematic method for the simultaneous synthesis of combined mass- and heat-exchange networks (CMAHENs). The proposed method is based on a novel approach that incorporates the mass pinch technology (MPT) for mass exchange networks (MENs) synthesis and the pseudo-T-H diagram approach (PTHDA) for the heat exchange networks (HENs) synthesis. New bypass streams are included in the structural representation of the problem to expand the search space. A combined optimization approach is applied to minimize the total annualized cost of the CMAHEN. Finally, two cases are solved to illustrate the application of the proposed method.",
     "keywords": ["ACC annualized capital cost", "annualized capital cost", "AOC annualized operating cost", "annualized operating cost", "CMAHEN combined mass- and heat-exchange network", "combined mass- and heat-exchange network", "EC effctive crowding", "effctive crowding", "GA genetic algorithm", "genetic algorithm", "GA\u2013SA combined genetic algorithm\u2013simulated annealing algorithm", "combined genetic algorithm\u2013simulated annealing algorithm", "HEN heat exchange network", "heat exchange network", "HTTDC heat transfer temperature difference contribution", "heat transfer temperature difference contribution", "MEN mass exchange network", "mass exchange network", "MINLP mixed integer non-linear program", "mixed integer non-linear program", "MPT mass pinch technology", "mass pinch technology", "MSA mass separating agent", "mass separating agent", "OCX orthogonal crossover", "orthogonal crossover", "PTHDA pseudo-T-H diagram approach", "pseudo-T-H diagram approach", "SA simulated annealing algorithm", "simulated annealing algorithm", "TAC total annual cost", "total annual cost", "WAN water allocation network", "water allocation network", "Mass exchange network", "Heat exchange network", "Combined mass- and heat-exchange network", "Bypass stream", "Simultaneous synthesis"]},
    {"article name": "Bayesian method for state estimation of batch process with missing data",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.01.011",
     "publication date": "06-2013",
     "abstract": "Formulated under the state space framework, most previous methods for the state estimation typically treated batch processes in the same way as continuous ones, and only considered the state transition within a single batch. Considering that the initial state of the current batch is often related to that of the previous one, this paper incorporates the information of the previous batches into the estimation of the current state, where the filtering and smoothing for the previous batch is implemented and then the initial state of the current batch is estimated by treating the smoothed initial state of the previous batch as a \u201cmeasurement\u201d. To deal with the nonlinear and non-Gaussian property of batch processes, the particle filter method is employed as the key algorithm for filtering and smoothing. In addition, in order to make full use of various measurements, the case of missing data is considered during the implementation of the particle filter algorithm. The proposed method is illustrated and evaluated through the simulation on a penicillin fed-batch fermentation process.",
     "keywords": ["Batch process", "Bayesian estimation", "Particle filter"]},
    {"article name": "Cell population balance and hybrid modeling of population dynamics for a single gene with feedback",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.02.006",
     "publication date": "06-2013",
     "abstract": "Latest research on biological systems is steadily shifting from isolated single cells to entire cell populations. The latter are inherently heterogeneous, and their modeling requires approaches that explicitly account for this property. A comprehensive such approach is the cell population balance (CPB), which, however, is computationally expensive and becomes intractable for multi-variable models.In this work, we demonstrate the use of model-reduction to efficiently simulate cell population heterogeneity in a genetic network of a single gene with feedback. Starting from a 4-species model, we use singular perturbation analysis to derive a single equation for the intracellular protein concentration. We subsequently incorporate this equation to a hybrid model consisting of a CPB for the cell volume, and a continuum equation for the protein concentration. We finally compare the results obtained with the hybrid model with those of the full CPB, demonstrating the accuracy and computational efficiency of the hybrid methodology.",
     "keywords": ["Heterogeneity", "Cell population balance", "Hybrid model", "Kinetic Monte Carlo", "Singular perturbation", "Model reduction"]},
    {"article name": "Embedding structural information in simulation-based optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.02.004",
     "publication date": "06-2013",
     "abstract": "This paper proposes and explores an algorithm designed to find optimal settings for a process network. Emphasis is put on the system being divisible into components, as this underlying assumption motivates the algorithm in its entirety in that rather simple relations between the system components are modeled as explicit structural constraints, while the significantly more complex relations within each component are approximated based on the underlying simulator data. Although the approach taken in this paper is rather broadly applicable we are, in particular, interested in its application to production optimization problems in the oil and gas industry. We give limited numerical results for one such example that clearly indicates the advantages of our approach. We show the advantages of both decomposing the problem of interest and accounting for the structure from the point of view of exploiting, where ever possible, the explicitly analytic aspects of the problem. The advantage of doing the former is that the considered subproblems are significantly smaller than the overall problem. The advantage of the latter is that one can use derivatives for the analytic parts whereas they are unavailable for the simulators. The underlying approach is a trust-region one with a mixed integer nonlinear program formulation. There are some significant differences in the details of the algorithm from those generally available for such problems.",
     "keywords": ["Petroleum production optimization", "MINLP", "Interior point", "Derivative free", "Trust region"]},
    {"article name": "An improved framework for solving NLIPs with signomial terms in the objective or constraints to global optimality",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.01.015",
     "publication date": "06-2013",
     "abstract": "Real application problems are often formulated as nonlinear integer programming problems or as discrete global optimization problems with signomial terms in the objective or constraints. Although various approaches have been proposed to solve the problems, they either utilize numerous extra binary variables and constraints to reconstruct the problems for finding a global solution or are unable to obtain globally optimized solutions. This study proposes a novel linearization method that employs a logarithmic number of extra binary variables and constraints to reformulate a signomial term with discrete variables. The original nonlinear integer program is therefore converted into a mixed-integer linear program solvable to obtain a global optimum. Several numerical experiments are presented to demonstrate the computational efficiency of the proposed methods in solving nonlinear integer problems, especially for treating signomial functions with large-interval variables or multiple variables.",
     "keywords": ["Nonlinear programming", "Discrete global optimization", "Integer programming"]},
    {"article name": "Scheduling a multi-product pipeline network",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.01.019",
     "publication date": "06-2013",
     "abstract": "Daily some millions barrels of oil are moved around the world in imports and exports and domestically within countries. While ships are the main mode for intercontinental transport, pipelines are the chief form of transcontinental transport, while regional and local transports is performed by trains and trucks. Despite high installation costs, pipelines are considered highly efficient as a mode for transporting large amounts of oil and oil products over long distances, because they offer lower operation costs, higher reliability rates, lower product loss rates, less environmental impact, and less susceptibility to adverse weather conditions than other modes. This study deals with a multi-product pipeline system that transports a set of oil products (diesel, gasoline and kerosene, for example), which have to be moved from points (operating areas) where they are produced or stored (refineries, terminals) to points where they are needed (other refineries, distribution centers, terminals, ports, customers) through a pipeline or set of pipelines.The present study contributes primarily by offering an efficient tool for the problem of scheduling multi-product pipeline networks. The methodology proposed takes the approach of discretizing both pipelines and planning horizon and combines an efficient MILP model with a post-processing heuristic. When compared with previous models, we propose a more efficient one in which the set of volumetric constraints is modeled in the form of knapsack cascading constraints and constraints on products in pipeline sections, which made for significantly improved performance in the experiments that were conducted. The proposed methodology thus constitutes an advance in terms of modeling the problem, making it feasible to solve problems increasingly close to the realities confronting oil industry operators.",
     "keywords": ["Pipeline networks", "Scheduling", "Multi-product network", "Oil products"]},
    {"article name": "Multiple and nonuniform time grids in discrete-time MIP models for chemical production scheduling",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.01.014",
     "publication date": "06-2013",
     "abstract": "The modeling of time plays a key role in the formulation of mixed-integer programming (MIP) models for scheduling, production planning, and operational supply chain planning problems. It affects the size of the model, the computational requirements, and the quality of the solution. While the development of smaller continuous-time scheduling models, based on multiple time grids, has received considerable attention, no truly different modeling methods are available for discrete-time models. In this paper, we challenge the long-standing belief that employing a discrete modeling of time requires a common uniform grid. First, we show that multiple grids can actually be employed in discrete-time models. Second, we show that not only unit-specific but also task-specific and material-specific grids can be generated. Third, we present methods to systematically formulate discrete-time multi-grid models that allow different tasks, units, or materials to have their own time grid. We present two different algorithms to find the grid. The first algorithm determines the largest grid spacing that will not eliminate the optimal solution. The second algorithm allows the user to adjust the level of approximation; more approximate grids may have worse solutions, but many fewer binary variables. Importantly, we show that the proposed models have exactly the same types of constraints as models relying on a single uniform grid, which means that the proposed models are tight and that known solution methods can be employed. The proposed methods lead to substantial reductions in the size of the formulations and thus the computational requirements. In addition, they can yield better solutions than formulations that use approximations. We show how to select the different time grids, state the formulation, and present computational results.",
     "keywords": ["Chemical production scheduling", "Modeling of time", "Mixed-integer programming"]},
    {"article name": "Development of a generic process model for membrane adsorption",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.03.005",
     "publication date": "06-2013",
     "abstract": "In this work, a generic model describing the dynamic adsorption behaviour of proteins on membrane adsorbers over complete purification cycles under consideration of module geometry and of the interaction between multiple transport mechanisms is developed.A general rate model for membrane adsorption, in which the interaction between multiple phenomena, like mass transfer and adsorption kinetics are considered, is formulated. Hereby, the implemented isotherms describe the influence of eluting agents on the adsorption behaviour, so that complete purification cycle (loading, washing and elution operation) can be simulated.Using the developed model the theoretical influence of relevant transport phenomena, operating conditions and process scale on affinity and ion exchange membrane adsorption of proteins are investigated. An example on ion exchange membrane adsorption illustrates the possibility to predict scale up effects occurring in configurations of multiple membrane adsorber modules. The obtained simulation results are in accordance with experimental observations reported in literature.",
     "keywords": ["CSTR continuous stirred tank reactor", "continuous stirred tank reactor", "PFR plug flow reactor", "plug flow reactor", "SMA steric mass action", "steric mass action", "Membrane chromatography", "Model based scale up", "Affinity membrane adsorption", "Ion exchange membrane adsorption", "Process simulation"]},
    {"article name": "Margin design, online optimization, and control approach of a heat exchanger network with bypasses",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.02.002",
     "publication date": "06-2013",
     "abstract": "During the life cycle operation of a heat exchanger network (HEN), factors such as equipment aging, gradually decrease heat transfer performance and increase energy consumption. Industrial HEN design currently fails to effectively solve these problems. To resolve this problem, we present an online optimization and control approach method for an HEN with bypasses. The approach is based on the principles of sustainable energy conservation during the life cycle. The area margin of the heat exchanger is gradually released via bypass adjustment, thereby resulting in energy conservation. First, bypasses are set on the HEN to enhance HEN control and enable optimal manipulation of the equipment. Then, the total cumulative cost increment of the HEN, including the increment of utility costs and equipment investment costs, is regarded as the objective function. The effects of the heat transfer efficiency of the heat exchanger and the effects of bypass adjustment are also taken into account. We solve the optimal design margin of the HEN, thereby providing an operational space for optimal control. Finally, using the margin optimization design of the HEN with bypasses as basis, we treat the cumulative costs of the HEN in a certain cycle as the objective function to solve the optimal opening dynamically. While, we present an optimal control structure, which is combined with existing conventional control loops. The HEN of a given crude distillation unit in a refinery is chosen as the research object. And, results illustrate the effectiveness and application prospects of the proposed method.",
     "keywords": ["Heat exchanger network", "System engineering", "Design", "Optimization", "Process control"]},
    {"article name": "A discretization-based approach for the optimization of the multiperiod blend scheduling problem",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.01.016",
     "publication date": "06-2013",
     "abstract": "In this paper, we introduce a new generalized multiperiod scheduling version of the pooling problem to represent time varying blending systems. A general nonconvex MINLP formulation of the problem is presented. The primary difficulties in solving this optimization problem are the presence of bilinear terms, as well as binary decision variables required to impose operational constraints. An illustrative example is presented to provide unique insight into the difficulties faced by conventional MINLP approaches to this problem, specifically in finding feasible solutions. Based on recent work, a new radix-based discretization scheme is developed with which the problem can be reformulated approximately as an MILP, which is incorporated in a heuristic procedure and in two rigorous global optimization methods, and requires much less computational time than existing global optimization solvers. Detailed computational results of each approach are presented on a set of examples, including a comparison with other global optimization solvers.",
     "keywords": ["Pooling problem", "Mixed-integer nonlinear programming", "Bilinear programming", "Global optimization", "Petroleum operations"]},
    {"article name": "Efficient simulation of a separation column with axial diffusion and mass transfer resistance",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.03.008",
     "publication date": "06-2013",
     "abstract": "The optimization of simulated moving bed systems is a complex task, and one of the difficulties is the lack of simulation methods that are sufficiently accurate and fast to be incorporated in the optimization algorithms. This paper presents a simulation of an adsorption column with finite differences based on a Lagrangian approach. The results obtained with this integration method were compared to values reported in the literature; the comparison shows that the accuracy of the integration method is not lower than that obtained with published methods and that this integration method requires a much lower cost in computation time. Various simulations were compared with experimental data for injections of caffeine and sodium 2-naphthalenesulfonate and with published results for the separation of isomers of omeprazole. The effects of axial diffusion and resistance to mass transfer on the elution curves were studied, and the simulation results were compared with the known theoretical analytical solution for a linear isotherm.",
     "keywords": ["SMB", "Preparative chromatography", "Simulation", "Lagrangian finite differences"]},
    {"article name": "Moment based weighted residual method\u2014New numerical tool for a nonlinear multicomponent chromatographic general rate model",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.02.008",
     "publication date": "06-2013",
     "abstract": "A new numerical method is proposed to solve a nonlinear general rate model, frequently used to describe chromatographic multicomponent separations. The method is based on minimization of errors in chromatographic column profile moments, and it belongs to the family of weighted residual methods. Compared to most traditional weighted residual methods, the present formulation has some clear advantages. Firstly, it is inherently mass conserving. Secondly, the separation characteristic values of the effluent curve (retention time, physical dispersion and skewness) are predicted with good accuracy. Thirdly, the boundary conditions are treated naturally as source terms. The method is inherently of high order, so it gives high accuracy with a relatively low number of variables. This is a remarkable benefit especially for model parameter fitting or process optimization, when the model has to be solved repeatedly.",
     "keywords": ["Chromatographic general rate model", "High resolution numerical methods", "Moment transformation", "Polynomial approximation"]},
    {"article name": "Development and economic assessment of different WWTP control strategies for optimal simultaneous removal of carbon, nitrogen and phosphorus",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.03.007",
     "publication date": "06-2013",
     "abstract": "This paper presents the comparison of four control strategies for the A2/O WWTP configuration for simultaneous C, N and P removal. The control strategies: (i) external COD-P control; (ii) external recycle flow-P control; (iii) nitrate control in the last anoxic reactor; (iv) ammonia control in the last aerobic reactor, were combined with other common control loops to build different control structures and were simulated in Matlab/Simulink under different influent conditions. A systematic approach was conducted with all the strategies to assess their potential effectiveness, according to the following steps: theoretical design, setpoint optimization and, finally, a detailed comparison of the control results against a reference operation and an optimized reference scenario. The optimization of the reference operation presented a 7% reduction of the total operational cost. The simulation results showed that some control strategies further reduced 3\u20137.5% the WWTP operational costs while the effluent quality is greatly improved.",
     "keywords": ["A2/O", "EBPR", "Control", "Optimization", "WWTP", "MPC"]},
    {"article name": "Dynamic modelling of the absorber of a post-combustion CO2 capture plant: Modelling and simulations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.03.002",
     "publication date": "06-2013",
     "abstract": "Modelling work related to carbon dioxide (CO2) capture technologies is of great importance with respect to the design, control, and optimization of the capture process. Development of dynamic models as such is important since there is much information embedded with the dynamics of a plant which cannot be studied with steady state models. A model for the absorption column of a post-combustion CO2 capture plant is developed following the rate based approach to represent heat and mass transfer. The Kent\u2013Eisenberg model is used to compute the transfer and generation rates of the species. Sensitivity of the model for different physiochemical property correlations is analyzed. The predictions of the dynamic model for the capture plant start-up scenario and operation of the absorption column under varying operating conditions in the up-stream power plant and the down-stream stripping column are presented. Predictions of the transient behaviour of the developed absorber model appear realistic and comply with standard steady state models.",
     "keywords": ["CO2 capture", "Dynamic model", "Absorption", "MATLAB"]},
    {"article name": "Phase stability analysis using a modified affine arithmetic",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.03.011",
     "publication date": "06-2013",
     "abstract": "Phase stability analysis is a crucial step in the determination of multiphase equilibrium. This analysis by the tangent plane distance (TPD) minimization is a well-known technique, as well as the difficulties in providing guarantees that the global minimum has been found. On this regard, interval methods are powerful tools since they provide such guarantees. In this work, an interval Newton method plus generalized bisection, based on a modified affine arithmetic, is used to reliably find all possible stationary points of the TPD function. Additionally, an improved convergence test is suggested as well as a special treatment for mole fraction weighted averages. Several mixtures with up to 5 components, including LLE island type ternary systems, were studied. Both activity coefficient models and cubic equations of state were considered. For all the cases tested, the proposed modified affine arithmetic method was superior to other interval-based methods.",
     "keywords": ["Affine arithmetic", "Phase stability", "Global optimization", "Gibbs excess models"]},
    {"article name": "A complete 3D simulation of a crystallization process induced by supercritical CO2 to predict particle size",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.12.002",
     "publication date": "05-2013",
     "abstract": "Crystallization induced by compressed CO2 is a process that operates under several MPa of pressure. By rendering on-line measurements very difficult to perform, simulation appears as a suitable tool to better identify the important parameters of the process. A mathematical model is developed in the case of a spray-crystallization process in which a solution of minocycline-ethanol is injected into carbon dioxide as antisolvent. The model accounts for the main physical phenomena involved, i.e. hydrodynamics, mass transfer, phase equilibrium, crystallization kinetics. Simulations are performed in 3D with a special insight in turbulence modeling. Numerical results are compared with experimental data from literature. Although experimental and simulated PSD fit satisfactorily, results emphasize the major role of the crystal\u2013fluid interfacial tension on the accuracy. Numerical investigations are further performed to highlight the effects of injection velocity and solution concentration on the spatial distribution of the important variables in crystallization.",
     "keywords": ["CFD", "Crystallization", "Supercritical fluids", "Large Eddy Simulation"]},
    {"article name": "Computing fuzzy trajectories for nonlinear dynamic systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.11.008",
     "publication date": "05-2013",
     "abstract": "One approach for representing uncertainty is the use of fuzzy sets or fuzzy numbers. A new approach is described for the solution of nonlinear dynamic systems with parameters and/or initial states that are uncertain and represented by fuzzy sets or fuzzy numbers. Unlike current methods, which address this problem through the use of sampling techniques and do not account rigorously for the effect of the uncertain quantities, the new approach is not based on sampling and provides mathematically and computationally rigorous results. This is achieved through the use of explicit analytic representations (Taylor models) of state variable bounds in terms of the uncertain quantities. Examples are given that demonstrate the use of this new approach and its computational performance.",
     "keywords": ["Uncertainty", "Fuzzy sets", "Fuzzy numbers", "Nonlinear dynamic systems", "Interval analysis", "Dynamic simulation"]},
    {"article name": "Simulation and economic analysis of 5-hydroxymethylfurfural conversion to 2,5-furandicarboxylic acid",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.12.005",
     "publication date": "05-2013",
     "abstract": "Two processes for converting 5-hydroxymethylfurfural (HMF) to 2,5-furandicarboxylic acid (FDCA) were designed using literature data together with simplified process simulation models. The main reaction step is HMF catalytic oxidation using aqueous acetic acid as solvent, Pt/ZrO2 as catalyst and air as oxidant. The first process investigated involves a mixed-suspension, mixed-product-removal crystallizer and a filter for separating solid FDCA from the solvent. The calculated minimum sale price of FDCA is 3157\u00a0$/t, whereas, if pure oxygen is used as oxidant the FDCA price reduces to 2458\u00a0$/t. Due to the high melting point of FDCA, the second alternative considered introduces trioctylamine as solvent to facilitate separation of FDCA from the solvent using distillation. The estimated FDCA price using this process is 3885\u00a0$/t. Sensitivity analysis shows that selectivity and conversion have small impact on FDCA price, whereas plant capacity, catalyst and HMF costs have large effects on the price of FDCA.",
     "keywords": ["FDCA", "HMF", "Biorefinery", "Biomass", "Economy Analysis", "Process Simulation"]},
    {"article name": "Design and implementation of a next-generation software interface for on-the-fly quantum and force field calculations in automated reaction mechanism generation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.11.009",
     "publication date": "05-2013",
     "abstract": "A software interface for performing on-the-fly quantum and force field calculations has been developed and integrated into RMG, an open-source reaction mechanism generation software package, to provide needed estimates of thermodynamic parameters. These estimates based on three-dimensional molecular geometries bypasses the traditional group-additivity-based approach, which can suffer from lack of availability of necessary parameters; this issue is particularly evident for polycyclic species with fused rings, which would require ad hoc ring corrections in the group-additivity framework. In addition to making extensive use of open-source tools, the interface takes advantage of recent developments from several fields, including three-dimensional geometry embedding, force fields, and chemical structure representation, along with enhanced robustness of quantum chemistry codes. The effectiveness of the new approach is demonstrated for a computer-constructed model of combustion of the synthetic jet fuel JP-10. The interface also establishes a framework for future improvements in the chemical fidelity of computer-generated kinetic models.",
     "keywords": ["Automated reaction mechanism generation", "Parameter estimation", "Thermodynamic properties", "Quantum chemistry", "Molecular structure", "Cheminformatics"]},
    {"article name": "Micro-scale fluid model for drying of highly porous particle aggregates",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.12.003",
     "publication date": "05-2013",
     "abstract": "A discrete three-dimensional model for the fluid flow and phase transition at the microscopic scale during convective drying of highly porous particle aggregates has been developed. The phase distributions are described by time-dependent cell volume fractions on a stationary cubic mesh. The solid phase volume fractions are computed from an arbitrary collection of spherical primary particles generated by gravitational deposition using the discrete element method. The volume of fluid method is used to track the liquid\u2013gas interface over time. Local evaporation rates are computed from a finite difference solution of a vapor diffusion problem in the gas phase, and the liquid\u2013gas interface dynamics is described by volume-conserving mean curvature flow, with an additional equilibrium contact angle condition along the three-phase contact lines. The evolution of the liquid distribution over time for different wetting properties of the solid surface as well as binary liquid bridges between solid particles are presented.",
     "keywords": ["Gel drying", "Volume-of-fluid method", "Surface tension flow", "Capillary effects", "Numerical simulation"]},
    {"article name": "Sustainable design and synthesis of hydrocarbon biorefinery via gasification pathway: Integrated life cycle assessment and technoeconomic analysis with multiobjective superstructure optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.12.008",
     "publication date": "05-2013",
     "abstract": "This paper proposes a multiobjective, mixed-integer nonlinear programming (MINLP) model for the superstructure optimization of hydrocarbon biorefineries via gasification pathway under economic and environmental criteria. The proposed hydrocarbon biorefinery superstructure includes a number of major processing stages, such as drying of the cellulosic biomass feedstocks, air separation unit, gasification, syngas conditioning, Fischer\u2013Tropsch synthesis, hydroprocessing, power generation, and the diesel and gasoline production. The superstructure considers alternatives of technologies and equipment, such as gasification technologies, cooling options, hydrogen production sources, and Fischer\u2013Tropsch synthesis catalysts. The economic objective is measured by the net present value (NPV), and the environmental concern is measured using global warming potential (GWP) that follows the life cycle assessment procedures, which evaluates the gate-to-gate environmental impacts of hydrocarbon biofuels. The multiobjective MINLP model simultaneously determines the technology selection, operation conditions, flow rate of each stream, energy consumption of each unit, economic performance, environmental impacts, and equipment sizes. The multiobjective MINLP problem is solved with the \u025b-constraint method. The resulting Pareto-optimal curve reveals the trade-off between the economic and environmental performances. The optimal solution reveals that the high-temperature gasification, direct cooling, internal hydrogen production and cobalt catalysis have the best environmental and economic performances. At the breakeven point, where the optimal NPV is 0, the unit production cost of hydrocarbon biorefinery is $4.43 per gasoline-equivalent gallon (GEG) with unit GWP of 20.92\u00a0kg CO2 eqv./GEG. In the case of maximum NPV of $810 MM, the corresponding unit production cost is $3.17/GEG.",
     "keywords": ["Gasification", "Fischer\u2013Tropsch", "MINLP", "Multiobjective optimization", "LCA"]},
    {"article name": "A multi-period model for optimal planning of an integrated, resource-efficient rice mill",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.12.007",
     "publication date": "05-2013",
     "abstract": "Rice is one of the world's most important staple foods. Previous studies have focused on the yield improvement for an individual rice mill. There is a need to develop a framework to address the multitude of variables influencing the design of a rice mill complex, which include fluctuating thermal and electrical energy demands, diverse energy supply options, fluctuating product demands, resource availability and product degradation. The objective of this study is to develop a framework for the optimal design and planning of the product portfolio and processing route of an integrated, resource-efficient (IRE) rice mill complex. The objective function is to maximise the profitability of the rice mill by using the developed multi-period mathematical model. Sensitivity analysis was performed on the case study to evaluate the impact of fluctuating product demands, product prices and electricity cost on the production throughput, process configuration and profitability of the IRE rice mill complex.",
     "keywords": ["Rice mill", "Multi-period", "Optimisation", "Product portfolio", "Rice supply chain"]},
    {"article name": "Economic Nonlinear Model Predictive Control for periodic optimal operation of gas pipeline networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.11.011",
     "publication date": "05-2013",
     "abstract": "We study a Nonlinear Model Predictive Control (NMPC) formulation for optimizing the operational costs of gas pipeline networks. The major operating cost comes from running the compressors, which compensate for the frictional pressure loss as gas flows over long distances. We use an economic NMPC formulation, which directly considers the compressor operating cost as the controller objective. Due to diurnal gas demands, the optimal operation is a cyclic steady state. The controller objective and terminal constraints are suitably defined to ensure asymptotic convergence and closed-loop stability of the cyclic steady state. It is shown through simulations that the performance of the economic NMPC formulation is better than a tracking NMPC. The inherent robustness of the formulation also ensures convergence to a region around the cyclic steady state when demand forecasts are inaccurate. The large scale NLP is also solved within a reasonable CPU time making it practical for online application.",
     "keywords": ["Economic Nonlinear Model Predictive Control", "Gas pipeline networks", "Cyclic steady state", "Electricity pricing", "Nonlinear programming", "Dynamic optimization"]},
    {"article name": "Dynamic modeling and validation of 2-ethyl-hexenal hydrogenation process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.11.012",
     "publication date": "05-2013",
     "abstract": "The paper evaluates, by modeling and simulation, 2-ethyl-hexenal hydrogenation process in catalytic trickle bed three-phase reactors. The mathematical model consists of balance equations for gas and liquid phases. Reaction rate equations, transport models and mass balances were coupled to generalized heterogeneous models which were solved with respect to time and space with algorithms suitable for partial differential equations. The importance of mass transfer resistance inside the catalyst pellets as well as the dynamics of the different phases being present in the reactor is revealed. The dynamic mathematical model presented can be used to analyze and understand the interaction of various processes that take place inside the hydrogenation reactor and also to make preliminary calculation of experimental parameters. Another important use of the mathematical model is to determine the optimal operation conditions and to design the control system. The model is implemented in Matlab and tested in simulations achieving successful results.",
     "keywords": ["Hydrogenation process", "Dynamic mathematical model"]},
    {"article name": "Generation and verification of optimal dispatching policies for multi-product multi-tool semiconductor manufacturing processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.12.009",
     "publication date": "05-2013",
     "abstract": "Semiconductor manufacturing is one of the fastest-growing industries today. As the recent requirements for feature sizes and wafer sizes change rapidly, it becomes imperative to configure increasingly intricate control schemes to maintain product quality and tool utilization rate. For this purpose, it is assumed in this study that a semiconductor production environment can be viewed as multiple queues operated in parallel and, also, the EWMA controllers can be implemented independently to adjust the process recipes of different products in each queuing system. Based on these assumptions, a MINLP model is formulated to determine the optimal dispatching policies. Systematic numerical simulation procedure is also devised to confirm the validity of the dispatching model. Since accurate estimates of the model parameters may not always be available, the effects of model mismatch have been analyzed and the proper range of controller tuning parameter is recommended to achieve an acceptable level of process capability.",
     "keywords": ["EWMA", "Dispatching policy", "Queuing system", "Semiconductor manufacturing", "Numerical simulation"]},
    {"article name": "Parallel and high resolution numerical solution of concentration and temperature distributions in fluidized beds",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.12.004",
     "publication date": "05-2013",
     "abstract": "In this article higher order in time numerical schemes with efficient time stepping for the solution of concentration and temperature distributions in fluidized beds using parallel computers are presented. The mathematical model equations consist of strongly coupled and semi linear convection-diffusion-reaction equations. Invariant regions for the model are derived to check the solution bounds. The numerical discretization for the space using the finite element method is presented and the numerical treatment is enhanced by using adaptive and higher order linearly implicit Runge\u2013Kutta methods for the time discretization. For different time stepping methods and different spatial grid sizes numerical results are obtained and compared. The methods used show a clear improvement for the problem under consideration compared to previously presented results (Nagaiah, Warnecke, Heinrich, & Peglow, 2008). Additionally, the higher order time stepping methods yield a good parallel efficiency, paving the way for the efficient study of more complex phenomena.",
     "keywords": ["Fluidized beds", "Convection\u2013diffusion-reaction system", "Invariant regions", "FEM", "Rosenbrock type methods", "Parallelization"]},
    {"article name": "Numerical study of flow uniformity and pressure characteristics within a microchannel array with triangular manifolds",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.12.010",
     "publication date": "05-2013",
     "abstract": "Flow uniformity among individual channels within a microchannel array can be a significant factor affecting the performance of laminated structured micro-devices. In this study, numerical modeling is used to quantitatively investigate the impact of the geometry of the right triangular manifold and the dimensions of microchannels on desired uniformity and pressure drop. The CFD tool COMSOL is used to perform the simulations within the low-Reynolds number system (5\u00a0\u2264\u00a0Re\u00a0\u2264\u00a025). In our biomedical application, it is important to have low dead volume and residence time in the manifolds. A methodical approach is introduced to identify a design that balances low manifold volume and maintenance of flow uniformity. It has also been shown that including a short vertical spacing at the corner of manifolds is critical to achieving a high level of flow uniformity. Careful analysis and physical interpretation of trends herein enables a more intuitive approach to array design.",
     "keywords": ["Microchannel", "Triangular manifold", "Uniform flow distribution", "Computational fluid dynamics (CFD)"]},
    {"article name": "Hydrodynamic model of a microstructured plate reactor",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.12.006",
     "publication date": "05-2013",
     "abstract": "A numerical model of a two-phase flow in a plate microreactor was developed. The model is based on a computational fluid dynamics (CFD) approach and was constructed with the help of an open-source toolbox OpenFOAM, which utilizes a finite volume method to solve the equations of a fluid motion. A cold flow, i.e. a flow without a reaction and a mass transfer was considered in this study. The model was validated by comparing with experimentally measured hydrodynamic parameters: gas holdup and the interfacial area of gas bubbles inside the existing prototype. Validated model was used to test several proposed plate structures for the next prototype. The structure with the highest gas\u2013liquid interfacial area was was chosen for the construction.",
     "keywords": ["Microreactor", "CFD", "Multiphase flow", "Interfacial area"]},
    {"article name": "Reduced model for the planar solid oxide fuel cell",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.12.011",
     "publication date": "05-2013",
     "abstract": "3D modeling for fuel cells is generally computationally expensive, especially for stacks. In order to reduce computational cost, spatial smoothing over the parallel plain channels in flow fields is introduced and applied to a 3D steady-state isothermal planar solid oxide fuel cell model, which is validated with experiment from literature. The 3D model is reduced to 2D coupled with effective parameters and correlation factors, and then asymptotically reduced to parabolic PDEs and ODEs associated with space marching. The correlation factors, which are derived based on a full set of governing equations for electrokenitics over a cell cross section, can handle not only variations in diffusion pathways due to ribs but also the coupling effect between governing equations. The reduced models are verified with the 3D counterpart in view of global and local properties. Good agreement with a quantitative loss of information is achieved. The reduction in computational cost is investigated.",
     "keywords": ["Asymptotic", "Flow field", "Model reduction", "Solid oxide fuel cell", "Space marching", "Spatial smoothing"]},
    {"article name": "Thermal\u2013hydraulic simulation of a radiant steam boiler using Relap5 computer code",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.01.001",
     "publication date": "05-2013",
     "abstract": "Numerical simulations are nowadays increasingly used to assess the thermal\u2013hydraulic behavior of a steam boiler instead of carrying out expensive tests in experimental facilities. In the present work, the Relap5/Mod3.2 thermal\u2013hydraulic computer code, which is largely used in the nuclear engineering framework, is used to carry out a tube rupture transient of a high power industrial steam boiler. For this purpose a Relap5 model was developed and qualified at the steady-state conditions using the plant available data. The control and regulation systems are also modeled. The main outcome of this work is a successful application of a thermal\u2013hydraulic system code in predicting the main key parameters of an industrial radiant steam boiler under steady and transient operating conditions.",
     "keywords": ["Thermal\u2013hydraulic simulation", "Radiant steam boiler", "Natural circulation", "System code Relap5/Mod3.2", "Steady-state", "Accidental transient", "Relap5 reactor excursion leak analysis program 5eme version", "reactor excursion leak analysis program 5eme version", "HTS high temperature superheater", "high temperature superheater", "LTS low temperature superheater", "low temperature superheater", "ECO economizers", "economizers", "LWR light water reactor", "light water reactor", "INEL Idaho National Engineering Laboratory", "Idaho National Engineering Laboratory", "NRC Nuclear Regulatory Commission", "Nuclear Regulatory Commission", "TDV time-dependent-volume", "time-dependent-volume", "B BRANCH", "BRANCH", "V valve", "valve", "P pipe", "pipe", "PI proportional-integral", "proportional-integral"]},
    {"article name": "Comparing models for lot-sizing and scheduling of single-stage continuous processes: Operations research and process systems engineering approaches",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.01.006",
     "publication date": "05-2013",
     "abstract": "In the last years, several researchers from two different academic communities, the Operations Research and the Process Systems Engineering, have been developing mathematical formulations for the lot-sizing and scheduling of single-stage continuous processes with complex setup structures. This problem has been intensively studied due to its importance to a wide range of industries where a single-stage approach is suitable for production planning. This is the case of the glass container, beer, and dairy production. Recent works have been performed by both mentioned communities, however, no intense communication between these research efforts has been observed. This work attempts a systematic analysis on recent formulation developments of both communities. Based on the result of this comparison, a reformulation is proposed that outperforms in the majority of the cases the previous existent formulations for a set of systematically generated random instances.",
     "keywords": ["Production planning and control", "Mathematical programming"]},
    {"article name": "Evaluation of configuration alternatives for multi-product polyester synthesis by reactive distillation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.01.008",
     "publication date": "05-2013",
     "abstract": "Reactive distillation (RD) combines reaction and separation in an integrated setup that is able to reduce the energy use, improve productivity and selectivity, and increase the efficiency. In previous work, we showed the design, modeling and experimental validation of a RD process for synthesis of unsaturated polyesters. The results of our previous work clearly demonstrated that reactive distillation is indeed a very promising alternative for the polyesters synthesis.This study explores the best suitable internals and various feed configurations of a reactive distillation process for unsaturated polyester synthesis. Multi-product simulations were performed to find the operational parameters for producing different grades of polyester in the same equipment. The product transition time during product changeover is determined for various configurations and product grades. The selection criteria for the best configuration are the minimum requirements of volume and energy to produce 100\u00a0ktpy polyesters.The results of the rigorous simulations carried out in Aspen Custom Modeler shows that the best configuration has the reactive stripping section as a packed or trayed bubble column, and the reactive rectifying section as a packed column. With respect to the feed configuration, the feeding of monoesters to the RD column significantly intensifies the polyester process as compared to an anhydrous reactant fed to the column. Moreover, the product transition time in this configuration is also significantly reduced as compared to the other configurations.",
     "keywords": ["Reactive distillation", "Polyesters", "Multi-product transition", "Rate based modeling"]},
    {"article name": "Simulation study on biodiesel production by reactive distillation with methanol at high pressure and temperature: Impact on costs and pollutant emissions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.01.007",
     "publication date": "05-2013",
     "abstract": "Recently, a two-step biodiesel production process which uses short-chain alcohols at supercritical conditions has been proposed. In addition, literature reports suggest that the COSMO-SAC thermodynamic model is a suitable alternative for the prediction of VLE for supercritical methanol/methyl esters mixtures. Thus, in this work a simulation study of the two-step supercritical method for the production of biodiesel is performed by using the COSMO-SAC model. Further, alternative system configurations for biodiesel production based on reactive distillation are proposed and their total emissions are compared to those corresponding to the conventional catalytic method. The study demonstrates the benefits of using reactive distillation for the esterification step and discusses the environmental impact of the supercritical production process. It has been found that the intensified alternatives reduce the emissions considerably and, through the reuse of the excess methanol, the emissions level of the supercritical process can be compared to those of the catalytic method.",
     "keywords": ["Biodiesel production", "Reactive distillation", "Pollutant emissions"]},
    {"article name": "Modeling induced nucleation processes during batch cooling crystallization: A sequential parameter determination procedure",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.12.001",
     "publication date": "05-2013",
     "abstract": "An existing model is extended to simulate batch cooling crystallizations with induced nucleation processes like ultrasound or gassing. All important phenomena such as nucleation, growth, agglomeration and breakage are taken into account. A differentiation between ultrasound and gassing is necessary. Induced nucleation processes also require a modification of crystal growth mechanism.In general, the model parameters required for the kinetics are determined simultaneously by fitting them to experimental data. Mostly a correlation model results without physical basis. Here, the model parameters are sequentially determined by decoupling the mechanisms, saving effort and time, and make it possible to reduce the number of parameters also. The model and the model parameter determination procedure are validated using three different organic solute/solvent systems. The number of simulation runs for parameter fitting was reduced to less than 100 instead of simultaneous parameter determination, which requires several hundreds of thousands runs, resulting in physically reasonable solutions.",
     "keywords": ["Crystallization", "Induced nucleation", "Mathematical modeling", "Population balance", "Simulation"]},
    {"article name": "Quality assessment of a variance estimator for Partial Least Squares prediction of batch-end quality",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.01.012",
     "publication date": "05-2013",
     "abstract": "This paper studies batch-end quality prediction using Partial Least Squares (PLS). The applicability of the zeroth-order approximation of Faber and Kowalski (1997) for estimation of the PLS prediction variance is critically assessed. The estimator was originally developed for spectroscopy calibration and its derivation involves a local linearization under specific assumptions, followed by a further approximation. Although the assumptions do not hold for batch process monitoring in general, they are not violated for the selected case study. Based on extensive Monte Carlo simulations, the influence of noise variance, number of components and number of training batches on the bias and variability of the variance estimation is investigated. The results indicate that the zeroth-order approximation is too restrictive for batch process data. The development of a variance estimator based on a full local linearization is required to obtain more reliable variance estimations for the development of prediction intervals.",
     "keywords": ["Batch processes", "Partial Least Squares", "End-quality prediction", "Variance estimation"]},
    {"article name": "Plant-wide control structure selection methodology based on economics",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.01.005",
     "publication date": "05-2013",
     "abstract": "An important and challenging problem is the determination of appropriate control structures that minimize the loss of process performance under the effect of uncertainties. This can be achieved by selecting subsets of controlled and manipulated variables and designing their interconnection. In this paper, a systematic optimization methodology for the Control Structure Selection Problem (CSSP) is presented. The proposed formulation (a) improves the accuracy of calculations and (b) reduces computational time and effort necessary. Specifically, the error involved in the approximation of the nonlinear constraint that defines the magnitude of the back-off vector is reduced by the introduction of a more accurate linear approximation. In addition, the methodology is able to handle simultaneously occurring disturbances and estimate their worst impact on process economics. The computational time is significantly reduced by eliminating the state variables from the final formulation. Two case studies are presented to demonstrate the benefits of the proposed algorithm.",
     "keywords": ["Process control", "Control structure selection", "Mathematical programming"]},
    {"article name": "Comparison of global optimization algorithms for the design of water-using networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.01.013",
     "publication date": "05-2013",
     "abstract": "We address a special class of bilinear process network problems with global optimization algorithms iterating between a lower bound provided by a mixed-integer linear programming (MILP) formulation and an upper bound given by the solution of the original nonlinear problem (NLP) with a local solver. Two conceptually different relaxation approaches are tested, piecewise McCormick envelopes and multiparametric disaggregation, each considered in two variants according to the choice of variables to partition/parameterize. The four complete MILP formulations are derived from disjunctive programming models followed by convex hull reformulations. The results on a set of test problems from the literature show that the algorithm relying on multiparametric disaggregation with parameterization of the concentrations is the best performer, primarily due to a logarithmic as opposed to linear increase in problem size with the number of partitions. The algorithms are also compared to the commercial solvers BARON and GloMIQO through performance profiles.",
     "keywords": ["Mathematical modeling", "Nonlinear programming", "Integer programming", "Disjunctive programming", "Process design", "Water minimization"]},
    {"article name": "Steady-state design of thermally coupled reactive distillation process for the synthesis of diphenyl carbonate",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.02.001",
     "publication date": "05-2013",
     "abstract": "Diphenyl carbonate, a precursor in the production of polycarbonate, is traditionally synthesized by the transesterification reaction of dimethyl carbonate and phenol. In this study, phenyl acetate was used instead of phenol to react with dimethyl carbonate and yield diphenyl carbonate, due to its higher reaction conversion and the absence of side reactions and azeotropes. A plant-wide process with a reactive distillation (RD) column and a separation column was optimized by minimizing the total annual cost. The performance of the thermal coupling between these two columns was also investigated. RD with thermal coupling was demonstrated to provide better energy efficiency than conventional RD. The remixing phenomenon associated with thermodynamic inefficiency in conventional distillation sequences could be greatly reduced by implementing thermal coupling between columns. Reactant concentrations that were closer to stoichiometric balance in the reaction zone were given for the thermally coupled RD column.",
     "keywords": ["Reactive distillation", "Plant-wide design", "Diphenyl carbonate", "Thermal coupling"]},
    {"article name": "Decentralized plantwide control strategy for large-scale processes. Case study: Pulp mill benchmark problem",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.01.010",
     "publication date": "05-2013",
     "abstract": "The plantwide control (PWC) complexity increases for highly-integrated and large-scale chemical processes. This work presents a novel framework for decentralized PWC which includes: (i) the selection of the controlled variables (CVs), (ii) the pairing between the manipulated variables (MVs) and the CVs, and (iii) the determination of the controller algorithms as well as their tuning parameters for closed-loop operation. The proposal is to solve the steps (i) and (ii) simultaneously, driving the selection of the most effective PWC structure from a Pareto optimal set. Here, algorithms based only on steady-state information are considered to give a systematic procedure which tries to minimize the use of heuristic considerations. Genetic algorithms (GA) and the Hungarian algorithm (HA) are used here because they provide a good trade-off between computational effort and acceptable results. The proposed methodology is completely tested in a pulp mill benchmark and compared with a previous one.",
     "keywords": ["Plantwide control", "Decentralized control", "Systematic methodology", "Pulp mill process", "Large-scale process"]},
    {"article name": "An Eulerian\u2013Lagrangian approach for coupling fluid flow, heat transfer and liquid food product transformation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2013.01.020",
     "publication date": "05-2013",
     "abstract": "Numerical modeling of liquid food product transformation under thermo-mechanical treatment can often be achieved through computational fluid dynamics, but difficulties arise when the transformation cannot be reduced to a set of chemical reactions. Looking for a general strategy, a numerical approach is proposed by combining advantages of Eulerian and Lagrangian descriptions. Fluid flow and heat transfer are solved through the finite element method in the Eulerian frame, while the transformation is evaluated along representative Lagrangian trajectories. Any available model can be applied for assessing the transformation state, including population balance equations and stochastic models. The coupled problem is solved by iterating the Lagrangian and Eulerian steps. The approach is illustrated in studying the evolution of starch granules under hydrothermal treatment, where granule swelling is represented through a kinetic equation derived from experimental work. Results agree favorably with those obtained from a purely Eulerian representation of the coupled processes.",
     "keywords": ["Liquid food transformation", "Computational fluid dynamics", "Lagrangian trajectories", "Starch granule swelling", "Suspension viscosity"]},
    {"article name": "Integration of control theory and scheduling methods for supply chain management",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.012",
     "publication date": "04-2013",
     "abstract": "In this paper, we propose to use distributed model predictive control for supply chain optimization. In particular, we focus on inventory management in supply chains. We use cooperative model predictive control, in which each agent makes their local decisions by optimizing the overall supply chain objective. Motivated by recent results in Stewart, Wright, and Rawlings (2011), we develop a new cooperative MPC algorithm that is applicable to any stabilizable system, and in particular to supply chain models. We illustrate cooperative MPC for a two node supply chain example and compare its performance and properties with other classical distributed operating policies.",
     "keywords": ["Distributed control", "Cooperative control", "Model predictive control", "Supply chain optimization", "Inventorycontrol"]},
    {"article name": "Distributed model predictive control: A tutorial review and future research directions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.05.011",
     "publication date": "04-2013",
     "abstract": "In this paper, we provide a tutorial review of recent results in the design of distributed model predictive control systems. Our goal is to not only conceptually review the results in this area but also to provide enough algorithmic details so that the advantages and disadvantages of the various approaches can become quite clear. In this sense, our hope is that this paper would complement a series of recent review papers and catalyze future research in this rapidly evolving area. We conclude discussing our viewpoint on future research directions in this area.",
     "keywords": null},
    {"article name": "Dynamics and control of chemical process networks: Integrating physics, communication and computation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.05.016",
     "publication date": "04-2013",
     "abstract": "This paper provides the theoretical foundation for the modeling, analysis and control of integrated chemical process networks, or, in short, \u201cprocess networks.\u201d The dynamics of process networks is represented using state-space descriptions derived from classical irreversible thermodynamics and constrained by the second law so that dissipation is always non-negative. The state descriptions (models) derived from this point of view provide exact process representations. A unique, quadratic Lyapunov function for stability analysis and control design is derived directly from the entropy. The resulting process models are complex and simplifications may be needed in practical applications. Time-scale decomposition and singular perturbation theory provide the basis for exploring the network-level dynamic behavior that emerges as a result of tight inventory integration, and developing appropriate reduced-order models and a hierarchy of control systems for managing inventories and inventory flows. Model-based networked control and Lyapunov theory are leveraged to develop an integrated control and communication strategy that manages the information flows between the network components and explicitly accounts for communication constraints.",
     "keywords": ["Plantwide control", "Process control", "Decentralized control", "Networks", "Multi-scale systems", "Stability"]},
    {"article name": "Fast nonlinear model predictive control: Formulation and industrial process applications",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.011",
     "publication date": "04-2013",
     "abstract": "With the widespread availability of model predictive control (MPC), nonlinear MPC provides a natural extension to include nonlinear models for trajectory tracking and dynamic optimization. NMPC can include first principle models developed for off-line dynamic studies as well as nonlinear data-driven models, but requires the application of efficient large-scale optimization strategies to avoid computational delays and to ensure stability, robustness and superior performance. This study presents the application of the recently developed advanced step NMPC (asNMPC) strategy. This approach solves the detailed optimization problem in background and applies a sensitivity-based update on-line. Two large-scale process case studies are considered: detailed distillation control and multi-stage operation for steam generation in a power plant. In both cases, efficient and robust controller performance is achieved with nonlinear dynamic optimization.",
     "keywords": ["Nonlinear model predictive control (NMPC)", "Dynamic optimization", "Nonlinear programming (NLP)", "NLP sensitivity"]},
    {"article name": "Systems and control challenges in photovoltaic manufacturing processes: A modeling strategy for passivation and antireflection films",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.043",
     "publication date": "04-2013",
     "abstract": "A view of contemporary systems and control challenges in photovoltaic cell manufacturing is given in this paper, with emphasis on developing a modeling strategy for the optimization of silicon nitride SiNx:H films used for passivation and antireflection coatings in single and multicrystalline silicon solar cells. The overall framework integrates three modeling modules: a remote plasma-enhanced chemical vapor deposition reactor process model that predicts film composition and thickness based on process input parameters, a solar-optical module that relates antireflection film physical and chemical properties to the degree to which the spectral irradiance distribution is attenuated, and a solar cell device model that predicts cell power output and efficiency from the film properties and irradiance. Because the model couples process inputs to both photovoltaic cell performance and manufacturing process efficiency, the modeling approach can be used for the simultaneous optimization of process and product performance.",
     "keywords": ["Photovoltaic solar cell", "Process simulation", "Process optimization", "Chemical vapor deposition"]},
    {"article name": "Predictive control methods to improve energy efficiency and reduce demand in buildings",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.08.003",
     "publication date": "04-2013",
     "abstract": "This paper presents an overview of results and future challenges related to temperature control and cost optimization in building energy systems. Control and economic optimization issues are discussed and illustrated through simulation examples. The paper concludes with results from model predictive control solutions and identification of important directions for future work.",
     "keywords": null},
    {"article name": "Process systems opportunities in power generation, storage and distribution",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.027",
     "publication date": "04-2013",
     "abstract": "This paper presents an overview of the current process systems opportunities in power generation, storage and distribution. It puts in perspective how process systems engineering (PSE) has contributed to the area and explores the current technical problems that PSE can contribute to. Fuel cells, solar cells, wind turbines, flow batteries and rechargeable batteries as well as their interactions with the smart grid are considered. PSE has contributed and will contribute to the design as well as optimal integration and operation of power generators, storage systems and power grids, through mathematical modeling, control and optimization.",
     "keywords": ["Power generation", "Power storage", "Power distribution", "Fuel cells", "Wind turbines", "Rechargeable batteries", "Smart grids"]},
    {"article name": "Computer-aided process engineering in oil and gas production",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.08.014",
     "publication date": "04-2013",
     "abstract": "As oil and gas production moves to ever more challenging areas, increased use of technology becomes increasingly important. One technology that can contribute towards achieving safe and economic production from such areas is computer-aided process engineering. The use of such technology is outlined for two (of many) instances: Managed pressure drilling (MPD) for deep-offshore applications and natural gas production from shales. A broader picture of the importance of computer-aided process engineering for energy is briefly touched upon.",
     "keywords": ["Oil and gas", "Managed pressure drilling (MPD)", "Shale gas", "Natural gas", "Unconventional resources"]},
    {"article name": "State reduction in molecular simulations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.029",
     "publication date": "04-2013",
     "abstract": "Model reduction is an important systems task with a long history in traditional chemical engineering modeling. We discuss its interplay with modern data-mining tools (such as Local Feature Analysis and Diffusion Maps) through illustrative examples, and comment on important open issues regarding applications to large systems arising in molecular/atomistic simulations.",
     "keywords": ["Model reduction", "Data mining", "Principal Component Analysis", "Local Feature Analysis", "Diffusion Maps"]},
    {"article name": "Advances and selected recent developments in state and parameter estimation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.001",
     "publication date": "04-2013",
     "abstract": "This paper deals with two topics from state and parameter estimation. The first contribution of this work provides an overview of techniques used for determining which parameters of a model should be estimated. This is a question that commonly arises when fundamental models are used as these models often contain more parameters than can be reliably estimated from data. The decision of which parameters to estimate is independent of the observer/estimator design, however, it is directly affected by the structure of the model as well as the available data. The second contribution is an overview of recent developments regarding the design of nonlinear Luenberger observers, with special emphasis on exact error linearization techniques, but also discussing more general issues, including observer discretization, sampled data observers and the use of delayed measurements.",
     "keywords": ["State estimation", "Parameter estimation", "Observers", "Nonlinear system"]},
    {"article name": "Toward a low cost and high performance MPC: The role of system identification",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.07.005",
     "publication date": "04-2013",
     "abstract": "The high cost of model predictive control (MPC) technology has hampered its wide application in process industries beyond the refining/petrochemical industry. This work strives to increase the efficiency of MPC deployment. First, a semi-automatic MPC system is introduced. It consists of three modules: an MPC module, an online identification module and a control monitor module. The goal of the MPC technology is twofold: (1) to considerably reduce the cost of MPC commissioning and maintenance and (2) to increase control performance. System identification plays important roles in all the three parts of the MPC system. In the identification module, the so-called ASYM method of identification is used. It is demonstrated with an industrial application. In the control module, adaptive disturbance model identification is developed for improving control performance; in the monitor module, a method of model error detection method is developed. Industrial applications and simulations are used to demonstrate the ideas. Finally, we comment on some industrial needs on MPC research and development.",
     "keywords": ["Model predictive control (MPC)", "System identification", "Control performance monitoring", "Disturbance model", "Model error detection"]},
    {"article name": "The online use of first-principles models in process operations: Review, current status and future needs",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.07.008",
     "publication date": "04-2013",
     "abstract": "The online use of first-principles models (FPMs) to support process operations has been practised in the chemical and petroleum industry for over 40 years. FPMs can encapsulate a large amount of process knowledge and many companies have realized significant value from the use of these models in online model based applications (OMBAs). Such applications include real-time optimization, model predictive control, data reconciliation, virtual sensors, and process performance monitoring to name a few. The sophistication of both the FPM models and applications based on them has increased over time. At some points in the evolution certain applications were not successful due to issues related to sustainability, which includes model complexity, solvability, maintainability and tractability. Also, model development cost can be a factor in considering the type of model used in these applications. Hence many simplified and empirical model-based online applications became preferred in some domains, even though the overall prediction quality of the FPM may be superior. This paper will review the past experiences, current status and future challenges related to FPM based online modeling applications. There are many areas where the issues related to FPMs can be addressed through proper model management, better software tools and improved technical approaches and work processes. It is hoped that this paper can serve as a basis to promote an understanding of the issues for researchers, modeling software vendors, modeling engineers, and application engineers and help to stimulate improvements in this area leading to increased usage and value of FPMs in supporting process operations.",
     "keywords": ["Process operations", "Process modeling", "State estimation", "Model-based control", "First-principles models"]},
    {"article name": "Control of nano and microchemical systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.07.004",
     "publication date": "04-2013",
     "abstract": "Many advances in the development of nano and microchemical systems have occurred in the last decade. These systems have significant associated identification and control challenges, including high state dimensionality, limitations in real-time measurements and manipulated variables, and significant uncertainties described by non-Gaussian distributions. Some strategies for addressing these challenges are summarized, which include exploiting structure within the stochastic Master equations that describe molecular interactions, manipulating molecular bonds at system boundaries, and manipulating molecules and nanoscale objects through magnetic and electric fields. The strategies are illustrated in a variety of applications that include the estimation of nucleation kinetics of protein and pharmaceutical crystals within fluidic devices, the estimation of concentration fields using DNA-wrapped single-walled carbon nanotube-based sensor arrays, the simultaneous control of nanoscale geometry and electrical activation during thermal annealing in a semiconductor material, and the control of nanostructure formation on surfaces. Promising directions for research and technology development are identified for the next decade.",
     "keywords": ["Nanotechnology", "Microchemical systems", "Molecular nanotechnology"]},
    {"article name": "Process systems engineering tools in the pharmaceutical industry",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.014",
     "publication date": "04-2013",
     "abstract": "The purpose of this paper is to provide a summary of the current state of the application of process systems engineering tools in the pharmaceutical industry. In this paper, we present the compiled results of an industrial questionnaire submitted to pharmaceutical industry professionals. The topics covered in the questionnaire include process analytics, process monitoring, plant-wide information systems, unit operation modeling, quality control, and process optimization. A futuristic view of what process systems engineering tools will enable the pharmaceutical industry will be also be presented. While the industry is regularly using the traditional Design of Experiments approach to identify key parameters and to define control spaces, these approaches result in passive control strategies that do not attempt to compensate for disturbances. Special new approaches are needed for batch processes due to their essential dependence on time-varying conditions. Lastly, we briefly describe a novel data driven modeling approach, called Design of Dynamic Experiments that enables the optimization of batch processes with respect to time-varying conditions through an example of a simulated chemical reaction process. Many more approaches of this type are needed for the calculation of the design and control spaces of the process, and the effective design of feedback systems.",
     "keywords": ["Process systems engineering", "Process analytical technology", "Plant wide information technology systems", "Process monitoring", "Control", "Optimization"]},
    {"article name": "On the role of the necessary conditions of optimality in structuring dynamic real-time optimization schemes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.07.012",
     "publication date": "04-2013",
     "abstract": "In dynamic optimization problems, the optimal input profiles are typically obtained using models that predict the system behavior. In practice, however, process models are often inaccurate, and on-line model adaptation is required for appropriate prediction and re-optimization. In most dynamic real-time optimization schemes, the available measurements are used to update the plant model, with uncertainty being lumped into selected uncertain plant parameters; furthermore, a piecewise-constant parameterization is used for the input profiles. This paper argues that the knowledge of the necessary conditions of optimality (NCO) can help devise more efficient and more robust real-time optimization schemes. Ideally, the structuring decisions involve the NCO as follows: (i) one measures or estimates the plant NCO, (ii) a NCO-based input parameterization is used, and (iii) model adaptation is performed to meet the plant NCO. The benefit of using the NCO in dynamic real-time optimization is illustrated in simulation through the comparison of various schemes for solving a final-time optimal control problem in the presence of uncertainty.",
     "keywords": ["Dynamic real-time optimization", "Plant-model mismatch", "Modeling for optimization", "Input parameterization", "Plant model", "Solution model"]},
    {"article name": "Exploiting market fluctuations and price volatility through feedback control",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.022",
     "publication date": "04-2013",
     "abstract": "We explore how feedback control can make chemical producers responsive to market forces through the use of dynamic operating policies. Using a toy model of a marginal chemical producer operating in a dynamic market, we examine two different control strategies for dealing with stochastic price fluctuations. The key contribution of this work is the derivation of switching rules under risk-neutral and risk-sensitive control formulations for problems where the dynamics arise from the market. These results provide a basis for exploring more complex control problems that include the effects of market forces.",
     "keywords": ["Dynamic programming", "Process control", "Economics", "Capacity utilization"]},
    {"article name": "A model of neutrophil dynamics in response to inflammatory and cancer chemotherapy challenges",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.07.003",
     "publication date": "04-2013",
     "abstract": "A mathematical model of neutrophil and granulocyte colony stimulating factor (G-CSF) dynamics is developed to capture the response of circulating neutrophil levels to inflammatory and anticancer drug challenges. Severe infection or trauma induces inflammation, leading to: (i) the recruitment of neutrophils to the site of infection; (ii) misdirected neutrophil recruitment to healthy tissue, which may cause tissue damage; and (iii) an increase in neutrophil production through the G-CSF signaling cascade. The model is calibrated using fast (endotoxin challenge) and slow (docetaxel chemotherapy) response data and used to examine neutrophil dynamics in response to different chemotherapy schedules and G-CSF mitigation of, or rescue from, neutropenia. The explicit incorporation of biology in this model provides a superior structure for use in designing and evaluating treatments aimed at modulating neutrophil dynamics in chemotherapy and responses to severe infection or trauma.",
     "keywords": ["Nonlinear dynamic modeling", "Biomedical systems", "Inflammation", "Cancer chemotherapy", "Neutrophils", "LPS", "G-CSF"]},
    {"article name": "Comparison of extractive distillation and pressure-swing distillation for acetone/chloroform separation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.10.014",
     "publication date": "03-2013",
     "abstract": "The separation of acetone from chloroform is difficult because the highly nonideal vapor\u2013liquid equilibrium produces a maximum-boiling azeotrope. An earlier paper (Luyben, W. L. (2008). Control of the maximum-boiling acetone/chloroform azeotropic distillation system. Industrial & Engineering Chemistry Research, 47, 6140\u20136149) discussed the use of extractive distillation for making this separation. This paper studies the use of pressure-swing distillation for making the same separation.Results show that the extractive distillation process is much more attractive from the standpoint of both capital investment and energy consumption. But pressure-swing distillation avoids the potential problem of product contamination by the extractive solvent that must be added to the binary system.",
     "keywords": ["Extractive distillation", "Pressure-swing", "Azeotrope", "Distillation design", "Acetone/chloroform"]},
    {"article name": "SustainPro\u2014A tool for systematic process analysis, generation and evaluation of sustainable design alternatives",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.11.007",
     "publication date": "03-2013",
     "abstract": "Chemical processes are continuously facing challenges from the demands of the global market related to economics, environment and social issues. This paper presents the development of a software tool (SustainPro) and its application to chemical processes operating in batch or continuous modes. The software tool is based on the implementation of an extended systematic methodology for sustainable process design (Carvalho et al., 2008, Carvalho et al., 2009). Using process information/data such as the process flowsheet, the associated mass/energy balance data and the cost data, SustainPro guides the user through the necessary steps according to work-flow of the implemented methodology. At the end the design alternatives, are evaluated using environmental impact assessment tools and safety indices. The extended features of the methodology incorporate life cycle assessment analysis and economic analysis. The application and the main features of SustainPro are illustrated through a case study of \u03b2-galactosidase production.",
     "keywords": ["Process retrofitting", "Life cycle assessment", "Economic analysis", "Software"]},
    {"article name": "Production-ratio oriented optimization for multi-recipe material handling via simultaneous hoist scheduling and production line arrangement",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.10.016",
     "publication date": "03-2013",
     "abstract": "In multi-recipe and multi-stage material handling (M3H) processes, such as electroplating and polymeric coating, the productivity maximization under a customized production ratio of different types of jobs is an ultimate goal. Cyclic hoist scheduling (CHS) is certainly the most concerned aspect to improve the productivity. However, the production-ratio oriented productivity not only depends on the hoist scheduling, but also substantially relies on the production line arrangement (PLA), i.e., the spatial allocation of various processing units. This is because PLA determines the traveling time for the hoist to perform loaded and free moves among different processing units, which in turn inevitably affects the cyclic scheduling time and thus the total productivity. Therefore, CHS and PLA should be simultaneously optimized for the production-ratio oriented productivity maximization for M3H processes.In this paper, an integrated modeling methodology for productivity maximization of M3H processes has been developed with simultaneous consideration of CHS, PLA, and the customized production ratio. It introduces an MILP model that can successfully address all the major concerned issues for M3H processes, such as multiple recipes, multiple jobs, multi-capacity processing units, diverse processing time requirements, production line arrangement, and the customized production ratio in each cycle production. The efficacy of the proposed methodology is demonstrated by various case studies with in-depth analysis.",
     "keywords": ["Cyclic hoist scheduling", "Production line arrangement", "Optimization", "MILP"]},
    {"article name": "Development of a Population Balance Model of a pharmaceutical drying process and testing of solution methods",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.11.005",
     "publication date": "03-2013",
     "abstract": "Drying is frequently used in the production of pharmaceutical tablets. Simulation-based control strategy development for such a drying process requires a detailed model. First, the drying of wet granules is modelled using a Population Balance Model. A growth term based on a reduced model was used, which describes the decrease of the moisture content, to follow the moisture content distribution for a batch of granules. Secondly, different solution methods for solving the PBM are compared. The effect of grid size (discretization methods) is analyzed in terms of accuracy and calculation time. All tested methods are compared based on their ability to predict moment dynamics and the distribution, and their computational burden. The Method of Characteristics, a fast method, is able to calculate the distribution accurately with a coarse grid. The Quadrature Method of Moments requires even less calculation time, but results in a set of moments.",
     "keywords": ["Drying", "Pharmaceuticals", "Mathematical modelling", "PBM", "Solution methodology"]},
    {"article name": "Improvements on multiloop control design via net load evaluation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.11.004",
     "publication date": "03-2013",
     "abstract": "The plant-wide control problem is a very important topic in process control. A particular control structure design will define (restrict) the future operability degree for the plant under study. Classical control policies (decentralized or full) are not always the best solution. In this context a systematic and generalized strategy to solve the multivariable plant-wide control problem is proposed here. The methodology called minimum square deviation (MSD) considers several points such as the optimal controlled variables (CVs) selection based on the sum of square deviation (SSD) and controller structure design supported by net load evaluation (NLE) analysis. The overall problem is combinatorial and is solved by accounting several steady-state tools and new indexes minimizing the heuristic load. Four well-known case studies are presented and other approaches taken from the literature are accounted for the sake of comparison. A robust stability test, \u03bc-tools, is also performed for concluding about the control policies.",
     "keywords": ["Plant-wide control", "Disturbance rejection", "Controllability", "Sparse controller", "Control structure design"]},
    {"article name": "Life cycle optimization for sustainable design and operations of hydrocarbon biorefinery via fast pyrolysis, hydrotreating and hydrocracking",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.10.013",
     "publication date": "03-2013",
     "abstract": "This paper addresses the optimal design and operation of hydrocarbon biorefinery via fast pyrolysis, hydrotreating and hydrocracking of hybrid poplar feedstock under economic and environmental criteria. The hydrocarbon biorefinery encompasses fast pyrolysis for crude bio-oil production, upgrading of the bio-oil through hydrotreating, separation and hydrocracking of long chained hydrocarbons into gasoline and diesel range products, and steam reforming for hydrogen production. We propose a bi-criteria nonlinear programming (NLP) model that seeks to maximize the economic performance measured by the net present value (NPV) and to minimize the environmental impacts. The environmental objective is measured with the global warming potential (GWP) metric according to the life cycle assessment procedures, which covers gate-to-gate environmental impacts of the hydrocarbon biorefinery. The multiobjective NLP model simultaneously determines the production capacity, size of each process units, operational conditions, the flow rates of species and streams at each stage of the process, hydrocarbon biofuel yields, and consumption rate of feedstock, steam, electricity, and natural gas. The bi-criteria NLP model is solved with the \u025b-constraint method, and the resulting Pareto-optimal curve reveals the trade-off between the economic and environmental dimensions of the sustainable hydrocarbon biorefinery. The optimization results reveal that the unit production cost of the hydrocarbon biofuels is $2.31 per gallon of gasoline equivalent (GGE) for the maximum NPV solution and $3.67/GGE for the minimum GWP design. The corresponding greenhouse emission is 8.07\u00a0kgCO2-eq/GGE.",
     "keywords": ["NLP", "Fast pyrolysis", "Hydroprocessing", "Hydrocarbon biofuels", "LCA"]},
    {"article name": "One-step approach for heat exchanger network retrofitting using integrated differential evolution",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.10.018",
     "publication date": "03-2013",
     "abstract": "Heat exchanger network (HEN) retrofitting is more important and challenging than HEN synthesis since it involves modifying existing network for improved energy efficiency. Additional factors to be considered include spatial constraints, relocation and re-piping costs, reassignment and effective use of existing heat exchanger areas. The previous studies using stochastic global optimization algorithms are mainly focused on two-level approach: the first level uses a stochastic algorithm for optimizing structure, and the second level uses either a stochastic or a deterministic algorithm for optimizing continuous variables. In this study, we propose and test one-step approach where a stochastic global optimization method, namely, integrated differential evolution (IDE), handles both discrete and continuous variables together. Thus, HEN structure and retrofitting model parameters are simultaneously optimized by IDE, which avoids the algorithm trapping at a local optimum and also improves the computational efficiency. Results on HEN applications show that the proposed approach gives better solutions.",
     "keywords": ["Heat exchanger network retrofit", "MINLP", "Global optimization", "Differential evolution"]},
    {"article name": "Strategic value optimization and analysis of multi-product biomass refineries with multiple stakeholder considerations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.10.010",
     "publication date": "03-2013",
     "abstract": "The commercialization of cellulosic biofuels has been plagued by multiple sources of market and technology uncertainty, and the lack of capital. It is conjectured that government support for renewable fuels\u2019 has concealed the true market value of its producers, and its removal will lead to a slew of enterprise failures destroying significant private and tax-payer wealth. With such uncertainty, it is prudent to carefully evaluate capital investments in new facilities before actual investment is incurred. This paper presents a strategic optimization model for a multi-product biomass refinery that optimizes the long-term value of the facility to its stakeholders while yielding optimal decisions for input feedstock mix, product portfolio, conversion technology and process configuration, and capital structure for financing. The paper promotes the concept of stakeholder value, where actors at different supply chain nodes are included as a part of a co-operative enterprise, as a means to spread risks across the supply chain.",
     "keywords": ["Cellulosic biofuels", "Strategic optimization", "Stakeholder value", "Supply chain"]},
    {"article name": "A robust distributed model predictive control based on a dual-mode approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.11.002",
     "publication date": "03-2013",
     "abstract": "This paper proposes a new robust distributed model predictive control framework that uses a closed-loop dual-mode approach to reduce the demanding computations required to solve the on-line constrained optimization problem. The proposed algorithm requires solving N convex optimization problems in parallel based on exchange of information among the controllers. A relaxation technique is also developed to overcome the problem of feasibility for the initial iteration. Two simulation examples are used to illustrate the new method and for comparing the proposed algorithm with a previously developed technique in terms of performance and maximum CPU time per control interval. The simulation results showed that the new algorithm provides a significant reduction in online computations while resulting in comparative performance as compared to a previously reported algorithm.",
     "keywords": ["Robust distributed predictive control", "Robustness", "Linear matrix inequalities"]},
    {"article name": "A simple and unified algorithm to solve fluid phase equilibria using either the gamma\u2013phi or the phi\u2013phi approach for binary and ternary mixtures",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.11.006",
     "publication date": "03-2013",
     "abstract": "A new algorithm is proposed for calculating phase equilibria in binary systems at a fixed temperature and pressure. This algorithm is then extended to ternary systems (in which case, the mole fraction of one constituent in a given phase must be fixed in order to satisfy the Gibbs\u2019 phase rule). The algorithm has the advantage of being very simple to implement and insensitive to the procedure used to initialize the unknowns. Most significantly, the algorithm allows the same solution procedure to be used regardless of the thermodynamic approach considered (\u03b3\u2013\u03c6 or \u03c6\u2013\u03c6), the type of phase equilibrium (VLE, LLE, etc.) and the existence of singularities (azeotropy, criticality and so on).",
     "keywords": ["Phase-equilibrium calculation", "Algorithm", "Binary mixtures", "Ternary mixtures", "Phi\u2013phi", "Gamma\u2013phi"]},
    {"article name": "Finite element solution of coupled-partial differential and ordinary equations in multicomponent polymeric coatings",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.10.015",
     "publication date": "03-2013",
     "abstract": "Mass transport equations in multicomponent polymeric coatings are nonlinear coupled partial differential equations. These equations were solved using Galerkin's method of finite elements which converts them to ordinary differential equations. Residuals were made orthogonal by using quadratic basis functions. Non-uniform elements were used to capture steep concentration gradient near the top of the coating. Finite element formulation has been solved using ode15s of MATLAB. Results are in very good agreement with the earlier results using different solution techniques.",
     "keywords": ["Coupled partial differential equations", "MATLAB solution", "Multicomponent polymeric coatings", "Free volume theory", "Galerkin's method of finite elements"]},
    {"article name": "A Lagrangean decomposition approach for oil supply chain investment planning under uncertainty with risk considerations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.10.012",
     "publication date": "03-2013",
     "abstract": "We present a scenario decomposition framework based on Lagrangean decomposition for the multi-product, multi-period, supply investment planning problem considering network design and discrete capacity expansion under demand uncertainty. We also consider a risk measure that allows to reduce the probability of incurring in high costs while preserving the decomposable structure of the problem. To solve the resulting large-scale two-stage mixed-integer stochastic linear programming problem we propose a novel Lagrangean decomposition scheme, and compare different formulations for the non-anticipativity conditions. In addition, we present a new hybrid algorithm for updating the Lagrangean multiplier set based on the combination of cutting-plane, subgradient and trust-region strategies. Numerical results suggest that different formulations of the non-anticipativity conditions have a significant impact on the performance of the algorithm. Moreover, we observe that the proposed hybrid approach has superior performance in terms of faster computational times when compared with the traditional subgradient algorithm.",
     "keywords": ["Oil and gas", "Supply chain investment planning", "Stochastic integer programming", "Scenario decomposition", "Lagrangean decomposition", "Risk management"]},
    {"article name": "A multi-period optimization model for optimal planning of China's power sector with consideration of carbon mitigation\u2014The optimal pathway under uncertain parametric conditions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.10.005",
     "publication date": "03-2013",
     "abstract": "Optimal planning of power industry considering carbon mitigation for a long-term future is complex, involving many technical alternatives and infinite possible plants installations, retrofitting, and decommissions. Previously the authors built a multi-period superstructure optimization planning model of China's power sector, gaining the optimal pathway of China's power sector with fixed parametric input during 2010\u20132050. With that model, this paper attempted to optimize pathway of China's power sector under uncertainty, in which the most influential parameters were uncertain. A levelized optimal pathway of China's power sector was gained, reliability of which was verified by comparing it with optimal results for the stochastic samples. The levelized optimal pathway showed that in the presence of carbon tax, carbon emissions of the power sector were reduced significantly by developing low-carbon technologies including nuclear power, renewables, as well as carbon capture and sequestration (CCS), and CCS would be key to reduce carbon emissions from coal power.",
     "keywords": ["Power sector", "Levelizated optimization", "CCS"]},
    {"article name": "Dissipativity analysis for networks of process systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.11.010",
     "publication date": "03-2013",
     "abstract": "The paper presents stability and stabilizability analysis for plant-wide chemical processes. The process is modeled as a network of process systems with multiple recycle streams and energy integration. One of the key problems that we address in the paper is how to determine dissipativity properties of the process network from the dissipativity properties of the individual process systems and the topology of the network. The results in the paper extend the dissipativity analysis based on thermodynamics for individual processes to dissipativity-based conditions (storage functions and supply rates) suitable for process network analysis. The new storage and supply functions are based on balance equations represented in terms of temperatures and compositions rather than internal energy and chemical potentials. This makes the theory easier to apply to typical chemical process model systems. A simple example is provided to illustrate the application of the theory.",
     "keywords": ["Nonlinear process systems", "Plant-wide processes", "Dissipative systems", "Stability", "Stabilizability", "Process networks"]},
    {"article name": "Experimental and computational modeling of oscillatory flow within a baffled tube containing periodic-tri-orifice baffle geometries",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.09.015",
     "publication date": "02-2013",
     "abstract": "This paper describes numerical simulation and matching experimental results for oscillatory flow within a baffled tube containing tri-orifice baffles. The numerical simulation implemented a non-standard approach based on Implicit Large Eddy Simulation (ILES) to predict the flow in a situation where complex eddy formation occurs due to periodic separation for each oscillation of the flow. The set of performed experiments showed that within the cavity between each baffle, the flow structure was complex with elements of periodicity and a wide range of length scales. Conventional Large Eddy Simulation and comparison with experiments were used to validate the ILES results. It was concluded that the gross flow structure was captured by ILES, indicating that the numerical scheme is suitable for computation in the complex flow situation. In addition, ILES methodology was used to study the effect of increasing the oscillation frequency on the radial and axial velocities.",
     "keywords": ["Oscillatory flow mixing", "Simulation", "CFD", "Fluid mechanics", "Turbulence", "Visualization"]},
    {"article name": "Optimal patterning of heterogeneous surface charge for improved electrokinetic micromixing",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.09.004",
     "publication date": "02-2013",
     "abstract": "Micromixing is a key step in realizing fast analysis time in many bio-chemical, biological and detection applications of lab-on-a-chip (LOC) devices. The conventional T-mixer design requires longer channel lengths and times to achieve complete mixing owing to its dependence on transverse diffusion. As the surface properties of the microchannel govern the electro-osmotic flow characteristics, surface heterogeneity (non-uniform zeta potentials) can be exploited to generate vortices or specific flow structures to improve the mixing performance. Previous studies have shown that localized circulations or non-axial flow induced due to the presence of heterogeneity augment micromixing performance. However, the effect of heterogeneous charge patterns on mixing performance has not been studied systematically. In this computational study, a binary numerical optimization problem is formulated to achieve best mixing performance by identifying the optimal heterogeneous charge pattern. The resulting optimal design generates the most favorable transverse flow structure to provide optimal mixing performance. Various other configurations (staggered, herringbone, etc.) are examined over a range of operating conditions. The optimal design is found to be superior for all operating conditions with over 3-fold improvement in mixing performance with respect to homogeneous T-mixer.",
     "keywords": ["Micromixing", "Electrokinetics", "Binary optimization"]},
    {"article name": "Determination of the optimal operating conditions of the dual mixed refrigerant cycle for the LNG FPSO topside liquefaction process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.09.008",
     "publication date": "02-2013",
     "abstract": "With the increased demand for natural gas, there has been an increase in the research on and development of liquefied-natural-gas floating, production, storage, and offloading unit (LNG FPSO) technologies for LNG service in place of onshore LNG plants. The dual mixed refrigerant (DMR) cycle, which precools natural gas with the mixed refrigerants of ethane, propane, butane, and methane and then liquefies the natural gas with another set of mixed refrigerants (nitrogen, methane, ethane, and propane), is well known for having the highest efficiency among the liquefaction cycles, and is being examined for possible application to LNG FPSO. In this study, the optimal operating conditions for the DMR cycle are determined by considering the power efficiency. For this, a mathematical model of the DMR cycle was formulated in this study by referring to the results of a past study that formulated a mathematical model of the single mixed refrigerant (SMR) cycle. Finally, the optimal operating conditions from the formulated mathematical model were obtained using a hybrid optimization method that consists of the genetic algorithm (GA) and sequential quadratic programming (SQP). As a result, the required power at the determined optimal operating conditions was decreased by 34.5% compared with the patent (Roberts & Agrawal, 2001), and by 1.2% compared with the corresponding value from the past relevant study (Venkatarathnam, 2008).",
     "keywords": ["LNG FPSO", "Topside liquefaction process", "Pre-FEED", "Dual mixed refrigerant cycle", "Optimization"]},
    {"article name": "Dynamic optimization of bioreactors using probabilistic tendency models and Bayesian active learning",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.09.010",
     "publication date": "02-2013",
     "abstract": "Due to the complexity of metabolic regulation, first-principles models of bioreactor dynamics typically have built-in errors (structural and parametric uncertainty) which give rise to the need for obtaining relevant data through experimental design in modeling for optimization. A run-to-run optimization strategy which integrates imperfect models with Bayesian active learning is proposed. Parameter distributions in a probabilistic model of bioreactor performance are re-estimated using data from experiments designed for maximizing information and performance. The proposed Bayesian decision-theoretic approach resorts to probabilistic tendency models that explicitly characterize their levels of confidence. Bootstrapping of parameter distributions is used to represent parametric uncertainty as histograms. The Bajpai & Reuss bioreactor model for penicillin production validated with industrial data is used as a representative case study. Run-to-run convergence to an improved policy is fast despite significant modeling errors as long as data are used to revise iteratively posterior distributions of the most influencing model parameters.",
     "keywords": ["Bayesian inference", "Experimental design", "Fed-batch fermentation", "Modeling for optimization", "Run-to-run optimization", "Sensitivity analysis"]},
    {"article name": "Application of CFD in the optimal design of a SCR\u2013DeNOx system for a 300\u00a0MW coal-fired power plant",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.09.014",
     "publication date": "02-2013",
     "abstract": "Selective catalytic reduction (SCR) with ammonia or urea is regarded as one of the most important technologies to reduce the NOx emissions from coal-fired power plants. However, the design and development of SCR\u2013DeNOx systems are a complicated process involving the optimization of several parameters such as the ammonia/urea injection strategy, the installment of the gate leaf and the hybrid grid, as well as the thickness of straightener. These parameters determine the velocity and concentration distributions at the entrance of catalyst layers, which are key factors to affect the efficiency of flue gas denitrification and ammonia slip. In this work, CFD simulations are carried out to portray the performance of the SCR\u2013DeNOx facility in a 300\u00a0MW coal-fired power plant. The influences of the gate leaf, hybrid grid and straightener on the distributions of the velocity and concentration are investigated. And then the corresponding experiments are performed to qualitatively confirm the simulation results.",
     "keywords": ["Computational Fluid Dynamics (CFD)", "Selective catalytic reduction (SCR)", "Flue gas denitrification", "Gate leaf", "Hybrid grid", "Straightener"]},
    {"article name": "Numerical evaluation of the stability of stationary points of index-2 differential-algebraic equations: Applications to reactive flash and reactive distillation systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.09.021",
     "publication date": "02-2013",
     "abstract": "The dynamic behavior of many chemical processes can be represented by an index-2 system of differential-algebraic equations. This index can be reduced by differentiation, but unfortunately the index reduced systems are not guaranteed to possess the same stability characteristics as that of the original system. When the set of differential-algebraic equations can be written in Hessenberg form, the matrix pencil of the linearized system can be used to directly evaluate the stability of a steady state without the need for index reduction. Direct evaluations of stability of reactive flash and reactive distillation are presented. It is also shown that a commonly used index reduction will always result in null eigenvalues at steady state. Stabilization methods were successfully applied to this reduced system. An alternative index reduction method for a reactive flash is generalized and shown to be highly sensitive to minor changes in the jacobian.",
     "keywords": ["Index-2 DAE", "Stability", "DAE stabilization", "Reactive flash", "Reactive distillation"]},
    {"article name": "Using convex nonlinear relaxations in the global optimization of nonconvex generalized disjunctive programs",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.09.017",
     "publication date": "02-2013",
     "abstract": "In this paper we present a framework to generate tight convex relaxations for nonconvex generalized disjunctive programs. The proposed methodology builds on our recent work on bilinear and concave generalized disjunctive programs for which tight linear relaxations can be generated, and extends its application to nonlinear relaxations. This is particularly important for those cases in which the convex envelopes of the nonconvex functions arising in the formulations are nonlinear (e.g. linear fractional terms). This extension is now possible by using the latest developments in disjunctive convex programming. We test the performance of the method in three typical process systems engineering problems, namely, the optimization of process networks, reactor networks and heat exchanger networks.",
     "keywords": ["Generalized disjunctive programs", "Nonlinear relaxations", "Basic steps"]},
    {"article name": "A graph theoretical approach to the elucidation of reaction mechanisms: Analysis of the chlorine electrode reaction",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.10.007",
     "publication date": "02-2013",
     "abstract": "A methodology and algorithm are proposed in this work to conceive a limited set of dependent elementary reaction steps for a reaction mechanism by exploiting the reaction route (RR) graph of the mechanism. These steps are constrained by the conservation principles and must be subjected to thermodynamic feasibility studies. This method is expected to aid in the conception of new steps, which might otherwise be ignored, especially in the case of large mechanisms. In order to illustrate the method, it is applied to the chlorine electrode reaction mechanism. The result indicated that a reaction pathway consisting of one of the newly proposed steps is the dominant one. This example illustrated the potential of the proposed method to identify new steps to a mechanism and thereby help in mechanism elucidation. However, these new steps must be subjected to further critical study before they can be accepted.",
     "keywords": ["Reaction routes", "Reaction mechanism elucidation", "Graph theory", "Algorithmic approach", "Chlorine electrode reaction"]},
    {"article name": "A flexible and robust modelling framework for multi-stream heat exchangers",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.10.006",
     "publication date": "02-2013",
     "abstract": "Heat exchangers are important units in most industrial processes. They involve physical phenomena such as condensation and evaporation including several boiling regimes. Different types of heat exchangers constructed for different applications may differ much in geometrical design. This work explains and demonstrates a modelling framework which is capable of handling a multitude of geometries and relevant physical phenomena affecting the performance of the heat exchangers. The data structure and governing equations are explained, before the framework is demonstrated for a particular challenging test case with a heat exchanger operating similar to the main heat exchanger in a single mixed refrigerant cycle. In the test case, both evaporation and condensation may happen simultaneously along the length of the heat exchanger. 1000 cases with random changes within predefined intervals in inlet temperatures, mass flows and pressures were used to test the robustness of the model framework. The solution scheme converged in 98.7% of the cases, and in the non-converging cases, the operating conditions exceeded the physical limits of the heat exchanger. The framework demonstrated may thus be used to create flexible and robust heat exchanger models for use in process simulations, optimization, or as a stand-alone model.",
     "keywords": ["MRHP mixed refrigerant, high pressure", "mixed refrigerant, high pressure", "MRLP mixed refrigerant, low pressure", "mixed refrigerant, low pressure", "NG natural gas", "natural gas", "VPD vertical port distance (for a plate heat exchanger geometry)", "vertical port distance (for a plate heat exchanger geometry)", "HPD horizontal port distance (for a plate heat exchanger geometry)", "horizontal port distance (for a plate heat exchanger geometry)", "Heat exchanger", "Modelling", "Multi-stream"]},
    {"article name": "Fault diagnosis by Locality Preserving Discriminant Analysis and its kernel variation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.10.008",
     "publication date": "02-2013",
     "abstract": "Linear Discriminant Analysis (LDA) and its nonlinear kernel variation Generalized Discriminant Analysis (GDA) are the most popular supervised dimensionality reduction methods for fault diagnosis. However, we argue that they probably provide suboptimal results for fault diagnosis due to the Fisher's criterion they use. This paper proposes a new supervised dimensionality reduction method named Locality Preserving Discriminant Analysis (LPDA) and its kernel variation Kernel LPDA (KLPDA) for fault diagnosis. (K)LPDA maximizes a new criterion such that local discriminant structure and local geometric structure in data are optimally preserved simultaneously in each dimension of the reduced space. The criterion directly targets at minimizing local overlapping between different classes. Extensive simulations on the Tennessee Eastman (TE) benchmark simulation process and a waste water treatment plant (WWTP) clearly demonstrate the superiority of our methods in terms of misclassification rate and making use of extra training data.",
     "keywords": ["Fault diagnosis", "Multi-fault classification", "Feature extraction", "Local structure preserving", "Kernel methods"]},
    {"article name": "Large neighbourhood search applied to the efficient solution of spatially explicit strategic supply chain management problems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.09.006",
     "publication date": "02-2013",
     "abstract": "Supply chain management (SCM) has recently gained wider interest in both academia and industry given its potential to improve the benefits of a company through an integrated coordination of all its entities. Optimization problems in SCM are commonly cast as large scale mixed-integer linear programs (MILPs) that are hard to solve in short CPU times. This limitation is critical in spatially explicit SCM models since they require a large number of discrete variables to represent the geographical configuration of the network, which leads to complex MILPs. We present herein a novel solution method for this type of problems that combines the strengths of standard branch and cut techniques with the efficiency of large neighbourhood search (LNS). We illustrate the capabilities of this novel approach through its application to two case studies arising in energy applications: the design of supply chains (SCs) for bioethanol production and the strategic planning of hydrogen infrastructures for vehicle use.",
     "keywords": ["Supply chain", "Metaheuristics", "Hybrid metaheuristic", "Large neighbourhood search", "LNS"]},
    {"article name": "Simulation of vapor flows in short path distillation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.10.002",
     "publication date": "02-2013",
     "abstract": "Based on the direct simulation Monte Carlo (DSMC) method, one- and two-dimensional models of vapor flows in the short path distillation are established. To reflect the reality, molecular rotation is included in this study and a reasonable boundary condition is introduced. The simulations are tested by comparison with the previous experiment, which shows that the distillation rate and composition are closer to the experiment compared to the previous models, and the simulated temperature field is higher than that of the previous models. The agreements between experimental and simulated results show that the models represent well the phenomena that occur in the vapor space of the short path distillatory. Based on the revised model, macroscopic variables related to a particular position in the distillation gap are analyzed in detailed. Furthermore, dependence of the evaporation efficiency on the ratio of condensing area to evaporating area and influence of inert gas on the distillation process are investigated.",
     "keywords": ["Short path distillation", "Vapor phase", "DSMC"]},
    {"article name": "A systematic model-based analysis of a downer regenerator in fluid catalytic cracking processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.10.003",
     "publication date": "02-2013",
     "abstract": "Performance improvement of a catalyst regenerator for gasoline production in a fluid catalytic cracking (FCC) process is needed to achieve higher burning efficiency. This study performed a systematic model-based analysis of a downer-type regenerator to recover the activity of FCC catalyst by using a one-dimensional model of the regenerator coupled with its hydrodynamic characteristics and the kinetics of catalyst regeneration. The results of a sensitivity analysis showed that higher carbon content on spent catalyst causes a higher regeneration temperature. The ratio of the recycled-to-spent catalyst flow rate in range of 1.0\u20133.5 and temperatures of the spent catalyst in range of 703.15\u2013803.15\u00a0K have insignificant effects on the overall performance of the regenerator. The suitable superficial gas velocity and the spent catalyst flow rate are in range of 4\u20137\u00a0m\u00a0s\u22121 and 20\u201340\u00a0kg\u00a0m\u22122\u00a0s\u22121, respectively.",
     "keywords": ["Model-based analysis", "Downer regenerator", "Fluid catalytic cracking", "Performance analysis"]},
    {"article name": "Plant-wide utility disturbance management in the process industry",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.10.004",
     "publication date": "02-2013",
     "abstract": "Utilities, such as steam and cooling water, are often shared between production areas at large-scale sites. A disturbance in the supply of a utility is therefore likely to affect a large part of a site, and cause great loss of revenue. This study focuses on identifying disturbances in utilities and estimating the economical effects of such disturbances. A general method for reducing the loss of revenue due to utility disturbances, the utility disturbance management (UDM) method, is presented. Modeling of the effects of utility disturbances on production is needed to complete all steps of the method. In this paper, a simple on/off modeling approach is suggested to quickly obtain key performance indicators that may be used for decision support for proactive disturbance management. A matrix representation of a site and its utilities is introduced to simplify the computations. The UDM method is applied to an industrial case at Perstorp, Sweden.",
     "keywords": ["Process control", "Plant-wide disturbances", "Disturbance localization", "Availability", "Utilities", "Enterprise modeling"]},
    {"article name": "Optimal cut-times finding strategies for collecting a target component from overloaded elution chromatograms",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.09.009",
     "publication date": "02-2013",
     "abstract": "The determination of accurate cut-times plays an important role in the design and implementation of preparative chromatography. Modeling and optimization studies involving preparative isolation of target components from multi-component mixtures overwhelmingly depend on the target amount collected, defined by specific cut-times. The task of finding these times can be quite challenging for complex chromatograms of experimental or theoretical origin. In this study, two new alternate strategies to find optimal cut-times are introduced. Using simple linear and complex overloaded chromatograms, the performances of these new methods are compared with that an established technique based on evaluating local purities. To demonstrate the methods, concentration profiles were generated theoretically using empirical functions and the equilibrium dispersive model. The methods are compared in terms of their accuracy, speed, robustness and also their ability to find multiple fractionation intervals.",
     "keywords": ["Preparative", "Chromatography", "Optimization", "MINLP", "Cut-time", "Fractionation"]},
    {"article name": "Discrete element investigation of flow patterns and segregation in a spheronizer",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.09.023",
     "publication date": "02-2013",
     "abstract": "Powder flow in a spheronizer is characterized by a toroidal motion and the flow patterns depend on the disc velocity and the fill level, but also on the particle properties. This work investigates numerically, with a discrete element method (DEM), the impact of these parameters on segregation patterns and the flow dynamics for bidisperse particle size distributions. Characterization of the segregation, by means of a mixing index and the relation with the shear stress in the toroidal domain, is presented. Characteristics, such as mixing curves, concentration profiles and azimuthal velocity correlations, are discussed. A logarithmic expression has been developed to account for the shear stress on the evolution of the azimuthal velocity inside the particulate bed. The combination of the mixing indexes and the concentration profiles is used to quantify the changes observed on the segregation when the fill level and the rotational rate of the spheronizer disk are varied.",
     "keywords": ["Spheronizer", "Mixing", "Segregation", "Discrete element method", "Flow pattern", "Shear rate"]},
    {"article name": "Assessment of the potentials of implicit integration method in discrete element modelling of granular matter",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.10.009",
     "publication date": "02-2013",
     "abstract": "Discrete element method (DEM) is increasingly used to simulate the motion of granular matter in engineering devices. DEM relies on numerical integration to compute the positions and velocities of particles in the next time step. Typically, explicit integration methods are utilized in DEM. This paper presents a systematic assessment of the potentials of implicit integration in DEM. The results show that though the implicit integration enables larger time steps to be used compared to the common explicit methods, the overall speed up is overruled by higher computational costs of the implicit method.",
     "keywords": ["Implicit integration", "DEM", "Numerical analysis", "Granular material"]},
    {"article name": "Heat exchanger network cost optimization considering multiple utilities and different types of heat exchangers",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.10.017",
     "publication date": "02-2013",
     "abstract": "Supertargeting based on composite curves (CC) is widely used to determine the optimum approach temperature (\u0394Tmin) that yields the minimum total cost for heat exchange networks (HEN). Supertargeting using CC has two key limitations. Firstly, the HEN area calculations are drastically simplified through the assumption that CC segments may be considered as pseudo-single hot and cold streams exchanging heat via only one exchanger that is governed by a single cost correlation. Secondly, the current Supertargeting approach of considering only one hot and one cold utility level may lead to a crude estimation of the total HEN cost and the optimum \u0394Tmin. This work presents the stream temperature vs. enthalpy plot supertargeting (STEPS) method that overcomes these limitations. This paper proves that supertargeting based on CC can lead to up to 50% error in the total cost target and poor \u0394Tmin estimations.",
     "keywords": ["Pinch analysis", "Stream temperature versus enthalpy plot (STEP)", "Multiple utilities", "Composite curves", "Minimum temperature approach", "Supertargeting"]},
    {"article name": "Retrofit design of heat exchanger network of a fluid catalytic cracking plant and control based on MPC",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.11.001",
     "publication date": "02-2013",
     "abstract": "Nowadays, the optimal control of a heat-integrated plant becomes one of the most important research areas in the chemical industry. The heat transfer between the process and recycle flows may generate instability. Accordingly, the advanced control techniques have the challenging task of assuring the safety of the process operation and providing tight control for the heat integrated plants. Investigation of an entire fluid catalytic cracking (FCC) plant taking into account its complex dynamic behaviour for heat integration design and operation has not been studied yet. The reactor\u2013regenerator, the fractionation column and the heat exchanger network are included in the present study. Using the data from an industrial plant, a complex FCC process simulator was built. The structure of the optimal heat exchange network (HEN) was designed and implemented in the simulator. Comparison between PID and MPC control strategy was done and the incentives of the proposed MPC are revealed.",
     "keywords": ["CV controlled variable", "controlled variable", "FCC fluid catalytic cracking", "fluid catalytic cracking", "FCCU fluid catalytic cracking unit", "fluid catalytic cracking unit", "HCN heavy cat naphtha", "heavy cat naphtha", "HCO heavy cycle oil", "heavy cycle oil", "HEN heat exchanger network", "heat exchanger network", "IMC internal model control", "internal model control", "LCO light cycle oil", "light cycle oil", "MIMO multiple inputs-multiple outputs", "multiple inputs-multiple outputs", "MPC model predictive control", "model predictive control", "MV manipulated variable", "manipulated variable", "PI proportional integral", "proportional integral", "PID proportional integral derivative", "proportional integral derivative", "RGA relative gain array", "relative gain array", "RMSE root mean square error", "root mean square error", "VGO vacuum gas oil", "vacuum gas oil", "Fluid catalytic cracking", "Heat integration", "Dynamic behaviour", "Model predictive control"]},
    {"article name": "Applications of process synthesis: Moving from conventional chemical processes towards biorefinery processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.09.020",
     "publication date": "02-2013",
     "abstract": "Concerns about diminishing petroleum reserves, enhanced worldwide demand for fuels and fluctuations in the global oil market, together with climate change and national security have promoted many initiatives for exploring alternative, non-petroleum based processes. Among these initiatives, biorefinery processes for converting biomass derived carbohydrates into transportation fuels and chemicals are now gaining more and more attention from both academia and industry. Process synthesis, which has played a vital role for the development, design and operation of (petro) chemical processes, can be predicted to play a significant role in the design and commercialization of sustainable and cost-effective biorefinery processes. The main objective of this perspective paper is to elucidate the potential opportunities that biorenewables processing offers to optimal synthesis; challenges and future directions in this field are also concisely discussed. An attempt is made with this perspective to stimulate more and more efforts to optimally synthesize and design biorenewable conversion process to accelerate the commercialization of the biorefinery technology and further reduce the heavily reliance on petroleum-derive fuels.",
     "keywords": ["Chemical processes", "Systems engineering", "Biorenewable processing", "Process synthesis", "Design", "Optimization"]},
    {"article name": "Integral formulation of the population balance equation: Application to particulate systems with particle growth",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.08.001",
     "publication date": "01-2013",
     "abstract": "Numerical solution of the population balance equation (PBE) is widely used in many scientific and engineering applications. Available numerical methods, which are based on tracking population moments instead of the distribution, depend on quadrature methods that destroy the distribution itself. The reconstruction of the distribution from these moments is a well-known ill-posed problem and still unresolved question. The present integral formulation of the PBE comes to resolve this problem. As a closure rule, a Cumulative QMOM (CQMOM) is derived in terms of the monotone increasing cumulative moments of the number density function, which allows a complete distribution reconstruction. Numerical analysis of the method show two unique properties: first, the method can be considered as a mesh-free method. Second, the accuracy of the targeted low-order cumulative moments depends only on order of the CQMOM, but not on the discrete grid points used to sample the cumulative moments.",
     "keywords": ["Integral population balance", "Numerical solution", "Mathematical modeling", "CQMOM", "Particle growth"]},
    {"article name": "Estimating reaction model parameter uncertainty with Markov Chain Monte Carlo",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.07.011",
     "publication date": "01-2013",
     "abstract": "Predicting the performance of chemical reactions with a mechanistic model is desired during the development of pharmaceutical and other high value chemical syntheses. Model parameters usually must be regressed to experimental observations. However, experimental error may not follow conventional distributions and the validity of common statistical assumptions used for regression should be examined when fitting mechanistic models.This paper compares different techniques to estimate parameter confidence for reaction models encountered in pharmaceutical manufacturing, simulated with either normally distributed or experimentally measured noise. Confidence intervals were calculated following standard linear approaches and two Markov Chain Monte Carlo algorithms utilizing a Bayesian approach to parameter estimation: one assuming a normal error distribution, and a new non-parametric likelihood function. While standard frequentist approaches work well for simpler nonlinear models and normal distributions, only MCMC accurately estimates uncertainty when the system is highly nonlinear, and can account for any measurement bias via customized likelihood functions.",
     "keywords": ["Markov Chain Monte Carlo (MCMC)", "Non-parametric statistics", "Chemical kinetic modeling", "Nonlinear regression"]},
    {"article name": "Inspector interfaces to facilitate subjective reasoning about quantities in trends",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.07.010",
     "publication date": "01-2013",
     "abstract": "Nuclear safeguards inspectors view historical trends on a regular basis to reassure themselves that a plant is operating as declared. Other types of inspector are likely to perform similar activities. Nuclear safeguards are founded on materials accountancy, and hence nuclear safeguards inspectors often want to relate what they see to laws of mass conservation. The interfaces discussed in this paper facilitate a synergy between qualitative reasoning regarding trends with quantitative reasoning about simple models that are driven by forcing functions, which describe materials flows through a plant. The focus in the paper is on the assessment of data trends that pertain to a standard 3-tank arrangement in a nuclear fuel reprocessing plant, and on how inspectors might wish to interact with evaluations performed.",
     "keywords": ["Nuclear safeguards", "Inspector interfaces", "Monitoring", "Trend analysis", "Data analysis", "Diagnosis"]},
    {"article name": "Impacts of equipment off-design characteristics on the optimal design and operation of combined cooling, heating and power systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.08.007",
     "publication date": "01-2013",
     "abstract": "Optimal design and operation of combined cooling, heating and power (CCHP) systems are complicated due to the fluctuating energy demands. Many mathematical models for the design and/or operation of CCHP systems have been developed. Most of them adopt a constant efficiency assumption, whilst others take equipment off-design characteristics into account. In this paper, we present two mathematical models for the optimal design and operation of CCHP systems with the target of minimising the total annual cost. One model is formulated using the constant efficiency assumption. In the other model, off-design characteristics of all equipments are considered. Comparative studies using different models were performed to examine the impacts of equipment off-design characteristics on the accuracy of the optimal design of CCHP systems. Results show that introduction of thermal storage facilities, connection to power grid and well designed operation strategies can diminish the negative impacts of adopting the constant efficiency assumption.",
     "keywords": ["CCHP", "Off-design characteristics", "Internal combustion engine", "Absorption chiller"]},
    {"article name": "Finding all real solutions of nonlinear systems of equations with discontinuities by a modified affine arithmetic",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.08.002",
     "publication date": "01-2013",
     "abstract": "Chemical engineering is a rich area when comes to nonlinear systems of equations, possibly with multiple solutions, (unbounded) discontinuities, or functions which become undefined in terms of real values. In this work, a new approach is proposed for finding all real solutions of such systems within prescribed bounds. A modified affine arithmetic is used in an interval Newton method plus generalized bisection. A special constraint propagation is used to automatically remove regions where the functions are undefined for real numbers. Results for test problems have shown that the proposed implementation requires less computation effort than similar methods available in the literature for small continuous systems. Further, the method is able to find all real solutions of nonlinear systems of equations even when there are unbounded discontinuities or when functions become undefined within the given variable bounds.",
     "keywords": ["Affine arithmetic", "Interval arithmetic", "Root finding", "Discontinuous functions", "Equation-oriented simulator"]},
    {"article name": "Methodology for inferring kinetic parameters of diesel oil HDS reactions based on scarce experimental data",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.08.004",
     "publication date": "01-2013",
     "abstract": "Nowadays environmental regulations of fossil fuels emissions impose stricter limits for contaminants such as sulfur, nitrogen and aromatics from middle distillate petroleum fractions. The most important process used in oil refineries to reach the required specifications is catalytic hydrogenation. A key issue to optimize these units is the availability of reliable kinetic models for this complex, tri-phase reaction. A detailed, phenomenological model of the reactor would demand an exceeding experimental effort for consistently estimating all the necessary kinetic and transport parameters. Thus, a simplified approach is generally used for routine assessment of new catalysts and/or new streams to be processed. Due to the difficulty of characterization of these streams, which are very complex mixtures of numerous species, most models are based on pseudo-components. This approach, however, does not allow for model generalization with respect to feed composition. This paper presents and discusses a new methodology for dealing with this problem. Conventional neural network (NN) training algorithms are used for inducing NNs to predict kinetic parameters of simplified models for the catalytic hydrodesulfurization (HDS) reaction, using macro properties of the feed as input. As in practice there are rarely enough experimental data to subsidize empirical learning algorithms, the paper proposes and describes an ad hoc methodology for artificially enlarging the initial scarce experimental data. Results from inferring kinetic parameters of the catalytic removal of sulfur using NNs, based on macro-properties of oil middle distillates, are presented and discussed.",
     "keywords": ["Hydrodesulfurization (HDS) of diesel", "Kinetic parameters estimation", "Methodology for treating scarce data", "Neural networks"]},
    {"article name": "Modelling of homogeneously catalysed reactive distillation processes in packed columns: Experimental model validation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.07.015",
     "publication date": "01-2013",
     "abstract": "The design of reactive distillation processes requires reliable and accurate models to significantly decrease the expensive and time consuming experimental work. Different modelling approaches of varying complexity are available in the open literature. However, only few publications exist in which the question of the optimal modelling depth is discussed for homogeneously catalysed processes. Unlike these publications, we used experimental data in the present study to compare them with simulation results using different modelling depths for homogeneous reactive distillation processes. The nonequilibrium-stage model using the Maxwell\u2013Stefan equations, the nonequilibrium-stage model using effective diffusion coefficients, the equilibrium-stage model including reaction kinetics, and the equilibrium-stage model assuming chemical equilibrium were investigated. The homogeneously catalysed transesterification of dimethyl carbonate with ethanol, for which pilot-scale experimental data were available, was used as a test system.",
     "keywords": ["ACM Aspen Custom Modeler", "Aspen Custom Modeler", "CAS Chemical Abstracts Service", "Chemical Abstracts Service", "DEC diethyl carbonate", "diethyl carbonate", "DMC dimethyl carbonate", "dimethyl carbonate", "EMC ethyl methyl carbonate", "ethyl methyl carbonate", "EQ-EQ equilibrium-stage model assuming chemical equilibrium", "equilibrium-stage model assuming chemical equilibrium", "EQ-Kin equilibrium-stage model taking into account the reaction kinetics", "equilibrium-stage model taking into account the reaction kinetics", "EtOH ethanol", "ethanol", "Ha Hatta number", "Hatta number", "HETS height equivalent to a theoretical stage", "height equivalent to a theoretical stage", "MeOH methanol", "methanol", "MTBE methyl tert-butyl ether", "methyl tert-butyl ether", "NEQ-Eff nonequilibrium-stage model using effective diffusion coefficients", "nonequilibrium-stage model using effective diffusion coefficients", "NEQ-MS nonequilibrium-stage model using the Maxwell\u2013Stefan equations", "nonequilibrium-stage model using the Maxwell\u2013Stefan equations", "RD reactive distillation", "reactive distillation", "RTD residence time distribution", "residence time distribution", "UNIQUAC universal quasichemical", "universal quasichemical", "Nonequilibrium-stage model", "Equilibrium-stage model", "Homogeneous catalyst", "Diethyl carbonate", "Ethyl methyl carbonate", "Model validation"]},
    {"article name": "The mathematics of modelling the supercritical fluid extraction of essential oils from glandular trichomes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.08.006",
     "publication date": "01-2013",
     "abstract": "This article deals with mathematical tools used for solving equations of the improved mathematical model on the micro-scale for the process of supercritical fluid extraction of essential oils from glandular trichomes. Glandular trichomes are secretory structures of Lamiaceae plant family and as such represent the sites of essential oil synthesis and storage. It was previously noticed that during the extraction with carbon dioxide these secretory structures undergo cracking due to the solvent dissolving into the essential oil phase. In this study, the process of extraction is thoroughly analysed and mathematically presented on the fixed bed scale as well as on the single trichome scale. The finite differences method was applied for solving differential equations of the model. This included dividing the extractor vessel into twenty spatial, and extraction time into ten thousand time increments. Cracking time distribution of glandular trichomes in the form of Gamma distribution was incorporated in each of the twenty spatial increments. The model was applied to simulate experimental results of supercritical extraction from several species of the Lamiaceae family. The deviation of the model results from the experimental data was 9.6\u201335.7% lower for the improved model than for the model without the cracking time distribution function.",
     "keywords": ["Modelling", "Supercritical extraction", "Lamiaceae", "Essential oil"]},
    {"article name": "Process modelling of dimethyl ether production from Victorian brown coal\u2014Integrating coal drying, gasification and synthesis processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.08.008",
     "publication date": "01-2013",
     "abstract": "Power plants using Victorian brown coal operate at low efficiency. Being reactive and spontaneously combustible, dried brown coals cannot be exported either. Synthesis of dimethyl ether (DME) is one option for the production of liquid fuel, an exportable product for power generation and transportation. This paper presents a steady-state process model for DME production using brown coal including drying, gasification and DME synthesis. The yield of the DME was a maximum for H2 to CO molar ratio of 1.41 and 0.81 at the gasifier outlet and the DME reactor inlet respectively. A process efficiency of 32% and CO2 emission of 2.91\u00a0kg/kg of DME was obtained. Improved yield of DME is achieved when CO2 is removed from the fuel gas prior to feeding to the synthesis reactor. Integration of waste heat and design of appropriate catalyst for gasification and DME synthesis can result in further improvements in the process.",
     "keywords": ["ASU air separation unit", "air separation unit", "ATR auto thermal reforming", "auto thermal reforming", "CCS carbon capture and storage", "carbon capture and storage", "DME dimethyl ether", "dimethyl ether", "FTS Fischer\u2013Tropsch synthesis", "Fischer\u2013Tropsch synthesis", "GCR generalized comprehensive reactor", "generalized comprehensive reactor", "LHHW Lingmuir\u2013Hinshelwood\u2013Hougen\u2013Watson", "Lingmuir\u2013Hinshelwood\u2013Hougen\u2013Watson", "LPG liquefied petroleum gas", "liquefied petroleum gas", "MTBE methyl tert-butyl ether", "methyl tert-butyl ether", "MTG methanol-to-gasoline", "methanol-to-gasoline", "MTO methanol-to-olefins", "methanol-to-olefins", "POX partial oxidation", "partial oxidation", "PR Peng\u2013Robinson", "Peng\u2013Robinson", "RK Redlich\u2013Kwong", "Redlich\u2013Kwong", "SRK Soave\u2013Redlich\u2013Kwong", "Soave\u2013Redlich\u2013Kwong", "WGSR water gas shift reaction", "water gas shift reaction", "Victorian brown coal", "DME", "Drying", "Gasification"]},
    {"article name": "Sensor location for nonlinear dynamic systems via observability analysis and MAX-DET optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.07.014",
     "publication date": "01-2013",
     "abstract": "This paper presents an approach for determining the optimal placement of multiple sensors for processes described by a class of nonlinear dynamic systems. This approach is based upon maximizing a criterion, i.e., the determinant, applied to the empirical observability Gramian in order to optimize certain properties of the process state estimates. The determinant directly accounts for redundancy of information for placing multiple sensors via the covariance terms in the observability matrix. However, the resulting optimization problem is nontrivial to solve as it is a mixed integer nonlinear programming problem. In order to address this point, this paper also presents a decomposition of the optimization problem such that the formulated sensor placement problem can be solved quickly and accurately on a desktop PC. Properties of the presented technique are demonstrated and discussed in two case studies, one involving a binary distillation column and the other a packed bed reactor.",
     "keywords": ["Sensor network design", "State estimation", "Maximize determinant", "Observability"]},
    {"article name": "CFD modelling of two-phase stirred bioreaction systems by segregated solution of the Euler\u2013Euler model",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.08.005",
     "publication date": "01-2013",
     "abstract": "An advanced study of a bioreactor system involving a Navier\u2013Stokes based model has been accomplished. The model allows a more realistic impeller induced flow image to be combined with the Monod bioreaction kinetics reported previously. The time-course of gluconic acid production by Aspergillus niger strain is simulated at kinetic conditions proposed in the literature. The simulation is based on (1) a stepwise solution strategy resolving first the fluid flow field, further imposing oxygen mass transfer and bioreaction with subsequent analysis of flow interactions, and (2) a segregated solution of the model replacing the multiple iterations per grid cell with single iterations. The numerical results are compared with experimental data for the bioreaction dynamics and show satisfactory agreement. The model is used for assessment of the viscosity effect upon the bioreactor performance. A 10-fold viscosity rise results in 2-fold decrease of KLa and 25% decrease of the specific gluconic acid production rate. The model allows better understanding of the mechanism of the important bioprocess.",
     "keywords": ["CFD computational fluid dynamics", "computational fluid dynamics", "DO dissolved oxygen", "dissolved oxygen", "RANS Reynolds-averaged Navier Stokes", "Reynolds-averaged Navier Stokes", "STR stirred tank reactor", "stirred tank reactor", "UDF user-defined functions", "user-defined functions", "Modelling", "CFD", "Gluconic acid", "Stirred reactor", "Viscosity effects", "KLa"]},
    {"article name": "Integrated design and control using a dynamic inversely controlled process model",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.08.009",
     "publication date": "01-2013",
     "abstract": "The profitability of chemical processes depends on their design and control. If the process design is fixed, there is little room left to improve control performance. Many commentators suggest design and control should be integrated. Nevertheless, the integrated problem is highly complex and intractable. This article proposes an optimization framework using a dynamic inversely controlled process model. The combinatorial complexities associated with the controllers are disentangled from the formulation, but the process and its control structure are still designed simultaneously. The new framework utilizes a multi-objective function to explore the trade-off between process and control objectives. The proposed optimization framework is demonstrated on a case study from the literature. Two parallel solving strategies are applied, and their implementations are explained. They are dynamic optimization based on (i) sequential integration and (ii) full discretization. The proposed integrated design and control optimization framework successfully captured the trade-off between control and process objectives.",
     "keywords": ["Integrated design and control of chemical processes", "Stochastic mixed-integer dynamic programming", "Control structure selection", "Multi-objective optimization"]},
    {"article name": "Systematic mesh development for 3D CFD simulation of fixed beds: Contact points study",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.08.011",
     "publication date": "01-2013",
     "abstract": "In the development of meshes for computational fluid dynamics (CFD) simulations of transport in fixed beds of spheres, particle\u2013particle and wall\u2013particle contact points often present difficulties. We give results for drag coefficient (CD) and heat flow (Q) for flow past sphere\u2013sphere and wall\u2013sphere contact points, focusing on higher flow rates typical of industrial steam reformers (500\u00a0<\u00a0Re\u00a0<\u00a010,000). Global methods, in which all particles in a bed are either shrunk or enlarged uniformly, change bed voidage giving erroneous results for CD. Local methods, in which bridges are inserted or spherical caps are removed only at the points of contact, give much better results for CD. The bridges approach is preferable for heat transfer, as fluid gaps reduce heat transfer too much, and particle overlaps increase it. A set of graphs is presented to allow estimation of the error introduced by the various methods of dealing with contact points.",
     "keywords": ["Computational fluid dynamics", "Sphere", "Drag coefficient", "Heat transfer", "Contact points", "Packed bed"]},
    {"article name": "Another confirmation that counter-current logarithmic mean is upper bound and a note on approximations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.07.013",
     "publication date": "01-2013",
     "abstract": "The paper presents modifications on published demonstration that the counter-current logarithmic-mean-temperature-difference (\u0394TLM) is upper bound compared with the co-current \u0394TLM. In the published demonstration, an approach was proposed but it suffered from wrong assumption. In this paper, corrections to the wrong assumption and a property of a curve derived from the \u0394TLM curve have been used to propose another demonstration that counter-current \u0394TLM is upper bound. Optimization problems have been formulated to verify the proposed developments and demonstrate the results obtained.A class of \u0394TLM approximations including two accurate approximations proposed by the author are discussed. The selected approximations are designated as Underwood's class. This class generates accurate results over the problem temperature-difference-ratio range and has the advantage of direct use of the heat exchanger (HEX) terminal temperature differences.",
     "keywords": ["Co-current HEX", "Counter-current HEX", "Heat exchanger (HEX)", "Heat exchanger network (HEN)", "HEN analysis", "HEN synthesis", "HEN optimization", "Logarithmic-mean-temperature-difference (LMTD) method", "Logarithmic-mean-temperature-difference (\u0394TLM)", "Minimum rule (MR)", "Process integration", "Minimum temperature difference approach (\u0394Tmin)"]},
    {"article name": "Economic assessment of a distributed energy system in a new residential area with existing grid coverage in China",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.08.013",
     "publication date": "01-2013",
     "abstract": "A distributed energy system refers to an energy system where energy production is close to end use, typically relying on various small-scale energy generation, conversion and storage technologies. The Chinese government has recently expressed interest in promoting this type of energy system. The paper develops an optimization model to evaluate the economic feasibility of adopting a distributed energy system in a new residential community in Beijing, where grid coverage is already well developed and accessible. The economic implications of adopting different grid connection regimes are also assessed.Results show that compared to the more conventional approach of relying entirely on the grid for electricity supplies, a distributed energy system is cheaper when a connection to the power grid can still be used to draw some electricity during periods of peak demand. Additionally, the economic benefits of electricity buy-back provisions for the distributed energy system are found to be minimal.",
     "keywords": ["Energy systems engineering", "Distributed energy system", "Power grid", "Optimization"]},
    {"article name": "Parallel calculation methods for molecular weight distribution of batch free radical polymerization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.09.002",
     "publication date": "01-2013",
     "abstract": "The molecular weight distribution (MWD) is at the core of establishing key quality indices for free radical polymerization processes. Due to large-scale features of the rigorous model, consisting of a very large number of differential and algebraic equations, dynamic simulation of MWD is always challenging. A sequential variable decoupling method (SVD) has been proposed to calculate the MWD for any reasonably large chain-length number. In the current paper, parallel computing methods were developed to accelerate the MWD simulation. Both coarse-grained and fine-grained parallelism methods have been proposed. A theoretical analysis of the proposed methods was conducted to demonstrate its high efficiency in parallel computing. Both Intel multi-core-processor-based and NVIDIA graphics-processing-unit-based parallel computing platforms were implemented, achieving significant speedups in computation for MWD simulation.",
     "keywords": ["Molecular weight distribution", "Parallel computation", "Graphics processing unit", "Batch free radical polymerization", "Sequential variable decoupling"]},
    {"article name": "Qualitative Representation of Trends (QRT): Extended method for identification of consecutive inflection points",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.08.010",
     "publication date": "01-2013",
     "abstract": "In this paper a methodology for extraction of qualitative descriptions of data series is extended for proper inflection point recognition. The original method allows the extraction of qualitative information such as the sign of the first and second derivatives of the assumed signals underlying univariate noisy time series. However, it is not able to handle consecutive inflection points, i.e. inflection points which follow each other in time with no minimum or maximum in between them. To improve this method for proper assessment of inflection points, the original method is compared with three modifications of the original method. One of the alternatives based on repeated application of Witkin's stability criterion delivers better results for both identification of the qualitative descriptions and the locations in time of extrema and inflection points. Furthermore, the same modified method is shown to deliver the best fault diagnosis performance in a benchmark batch fermentation study. Furthermore, the same modified method is shown to deliver the best fault diagnosis performance in a benchmark batch fermentation study.",
     "keywords": ["Qualitative Trend Analysis (QTA)", "Qualitative Representation of Trends (QRT)", "Data mining", "Qualitative analysis"]},
    {"article name": "A powerful estimation scheme with the error-in-variables-model for nonlinear cases: Reactivity ratio estimation examples",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.08.015",
     "publication date": "01-2013",
     "abstract": "This paper gives an overview of the error-in-variables-model (EVM) procedure for parameter estimation with nonlinear models. It is shown that the nested-iterative EVM algorithm, used in this work, is efficient and powerful, since it provides both true values of the variables and the best estimates of the parameters. The step by step illustration along with evaluation techniques for results, are followed by further discussion about the importance and advantages of combining EVM with design of experiments strategies. With the focus on the performance of the EVM algorithm, an illustrative example of reactivity ratio estimation in copolymerization is included, with single-response (composition data) and multi-response (triad fraction data) scenarios.",
     "keywords": ["Error-in-variables-model (EVM)", "Parameter estimation in polymerization reactions", "Nonlinear copolymer composition models", "Single- and multi-response problems", "Monomer reactivity ratio estimation", "Copolymerization"]},
    {"article name": "Optimization based conceptual design of reactive distillation for selectivity engineering",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.09.018",
     "publication date": "01-2013",
     "abstract": "Reactive distillation can be effectively used to enhance the selectivity of the desired product in a multi-reaction system. Due to complex interaction of reactions and distillation, identification of an appropriate reactive distillation configuration for the known performance targets has been a challenge. The objective of this contribution is to present an MINLP optimization technique that would assist one to identify a suitable configuration for selectivity maximization at conceptual design level. An illustrative example of industrially important reaction of dimerization of isobutene for maximizing the selectivity toward di-isobutene is considered. The results of the optimization are found to be in agreement with those obtained by performing independent simulation using ASPEN PLUS simulator. Thus, the work highlights the potential of an optimization based tool for conceptual design of reactive distillation that involves complex reaction systems.",
     "keywords": ["Reactive distillation", "Selectivity", "MINLP optimization", "Dimerization"]},
    {"article name": "A comparative assessment of linearization methods for bilinear models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.09.011",
     "publication date": "01-2013",
     "abstract": "In this article, optimization problems with bilinear constraints involving one discrete variable are studied. Several industrial problems present bilinear non-convex constraints which are difficult to solve to global optimality. For this purpose models must be reformulated what in general terms increases the problem size. This article proposes two disjunctive transformation techniques which are compared to other approaches presented in the literature. An analysis is made comparing qualitative and quantitative characteristics of the methods employed. In order to implement proposed transformations, three industrial cases are studied: trim-loss in a paper mill, cutting stock in the production of carton board boxes and the purchase, inventory and delivery optimization problem. All of them are reformulated and solved using the strategies included in the paper. Several instances of each problem are evaluated and their results are analyzed comparing performance of the different methods.",
     "keywords": ["MINLP", "Linearization methods", "Bilinear terms", "Discrete decisions"]},
    {"article name": "Two dimensional numerical computation of a circulating fluidized bed biomass gasifier",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.09.012",
     "publication date": "01-2013",
     "abstract": "A two dimensional model for an atmospheric CFB biomass gasifier has been developed which uses the particle based approach and integrates and simultaneously predicts the hydrodynamic and gasification aspects. Tar conversion is taken into account in the model. The model calculates the axial and radial distribution of syngas mole fraction and temperature both for bottom and upper zones. The proposed model addresses both hydrodynamic parameters and reaction kinetic modeling. Results are compared with and validated against experimental data from a pilot scale air blown CFB gasifier which uses different types of biomass fuels given in the literature. Developed model efficiently simulates the radial and axial profiles of the bed temperature and H2, CO, CO2 and CH4 volumetric fractions and tar concentration versus gasifier temperature. The minimum error of comparisons is about 1% and the maximum error is less than 25%.",
     "keywords": ["Fluidized bed", "Simulation", "Biomass", "Gasification"]},
    {"article name": "New discounted cash flow method: Estimating plant profitability at the conceptual design level while compensating for business risk/uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.08.012",
     "publication date": "01-2013",
     "abstract": "Conceptual design of a new plant involves the evaluation of alternative plant configurations to determine physical feasibility (does each achieve desired production levels within the required quality limits?) and economic viability (is sufficient profit generated each year of a multiyear projected lifetime while requiring an acceptably low initial capital investment?). In testing alternatives, designers require both an absolute measure and a normalized measure in order to make a definitive evaluation. In recent years NPV (Net Present Value) has often been chosen as the absolute metric and IRR (Internal Rate of Return) as the normalized one. But these two measures provide insufficient information to develop an optimum design that can be guaranteed suitably profitable, i.e., with optimum design profits high enough to justify investment \u2026 more importantly, high enough to warrant taking on the risk/uncertainty characteristics of a particular product/plant with the business environment in which the constructed plant must operate.In this paper a relook at discounted cash flow procedures motivates a new metric based on a normalized and annualized value of NPV, designated NPV%. In line with traditional analysis, a required minimum value \u2013 N P V % r e q u i r e d (%/year) \u2013 once found via ad hoc and/or first principles methods, is assumed to represent suitably the Enterprise's profitability expectations plus the \u201cpremium\u201d needed to justify intrinsic business risk/uncertainty. The ability to achieve a value that justifies anticipated risk, however that property is calculated or characterized, is a major element in determining whether projected returns from a particular design are high enough to justify proceeding with the project.Surprisingly, a close analysis of spreadsheet methodology used for discounted cash flow calculations, such as in calculating NPV, reveals unexpected underlying linearities, e.g., N P V = a ( Profi t B T ) + b ( Fixed Capital ) when \u201cfactored estimates\u201d are used (usually the case at the conceptual design stage), as well as R O I B T = e ( N P V % ) + fThus maximizing either ROIBT or NPV% is equivalent to minimizing the time to achieve return of invested capital (exposure to risk). NPV measures the effect on a company's balance sheet that will result from a decision to design/construct/operate. Thus once an appropriate risk premium for a plant is agreed upon, a rigorous design procedure can utilize a constrained optimization procedure in which NPV (absolute profitability) and NPV% (inverse of time exposed to the risk of capital loss) are jointly optimized, subject to the constraint N P V % > N P V % r e q u i r e d . How much to weight the long-term profitability vs. speed of capital return is essentially a decision for the designers/investors.",
     "keywords": ["Plant design", "Profitability measures", "Discounted cash flow", "Net present value", "Risk and uncertainty in plant design", "Investment decisions"]},
    {"article name": "Managing the trade-offs between financial performance and credit solvency in the optimal design of supply chain networks under economic uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.09.019",
     "publication date": "01-2013",
     "abstract": "Inherent uncertainties and risks in the economic environment are diffused to all vital operations of a supply chain network (SCN). However, the great impact of this contagion is on the financial operation due to its interdependence with financial markets and business conditions. Economic uncertainty poses uncertainty in the financial status of SCNs and this in turns leads to sustainability and growth risks. Financial performance and credit solvency are two essential pillars of financial status capable of providing the necessary capitals to a SCN. As each of these pillars focuses on a different aspect of investment attractiveness, underlined trade-offs exist, under various economic conditions, and challenge further investigation. This paper aims to enrich the SCN design literature by introducing a mathematical model that integrates financial performance and credit solvency modelling with SCN design decisions under economic uncertainty. The proposed multi-objective mixed integer non linear programming (moMINLP) model enchases financial performance through economic value added (EVA\u2122) and credit solvency through a valid credit scoring model (Altman's Z-score). The applicability of the model is illustrated by using a real case study. The model could be used as an effective strategic decision tool by managers responsible for strategic SCN design.",
     "keywords": ["Supply chain network design", "Economic uncertainty", "Financial performance", "Credit solvency", "Economic value added (EVA\u2122)", "Altman's Z-score", "Weighted average cost of capital (WACC)"]},
    {"article name": "Necessary condition for applying experimental design criteria to global sensitivity analysis results",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.09.007",
     "publication date": "01-2013",
     "abstract": "Conventional techniques for optimal experimental design are based on local sensitivity analysis to quantify parameter effects on the output. However, one of the key challenges for experimental design is that the local sensitivity is dependent on the unknown parameter values for a nonlinear model. This problem can be addressed if the sensitivity matrix, used in experimental design, could be computed by global sensitivity analysis techniques rather than local sensitivity analysis methods. However, not all existing global sensitivity analysis measures can compute such a sensitivity matrix. This paper presents a necessary condition for integrating global sensitivity analysis with experimental design criteria, i.e., the design criterion of the global sensitivity matrix reduces to the one applied to the local sensitivity matrix if the parameter uncertainty range tends to zero. Four different sensitivity measures are analyzed using this condition and the results are illustrated in a detailed case study where a comparison with local design and Bayesian design is made.",
     "keywords": ["Local sensitivity analysis", "Global sensitivity analysis", "Optimal experimental design", "Nonlinear dynamic system", "Parameter estimation"]},
    {"article name": "A sweep-heuristic based formulation for the vehicle routing problem with cross-docking",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.09.016",
     "publication date": "01-2013",
     "abstract": "Cross-docking is a warehousing strategy in logistics used by process industries making products with high proportions of distribution costs. It is described as the process of moving goods from suppliers to customers through a cross-dock terminal without a long-term storage in this facility. The vehicle routing problem with cross-docking (VRPCD) consists of fulfilling a set of transportation requests using a fleet of homogeneous vehicles to sequentially accomplish the pickup and delivery tasks. Between those operations, there is a consolidation process of incoming shipments at the cross-dock. This work introduces a monolithic formulation for the VRPCD that determines pickup/delivery routes and schedules simultaneously with the truck scheduling at the terminal. To derive a more efficient formulation, a constraint set mimicking the widely known sweep algorithm was incorporated into the rigorous model. The resulting model based on the sweep heuristic can find near-optimal solutions to large problems at very acceptable CPU times.",
     "keywords": ["Cross-docking", "Supply chain management", "Vehicle routing", "Scheduling"]},
    {"article name": "MINLP formulations for solving strip packing problems in LCD mother glass production",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.09.003",
     "publication date": "01-2013",
     "abstract": "Mother glass production for liquid crystal displays involves the cutting of small rectangles from a large rectangle (strip) to minimize the length of the strip used. This is called the strip packing problem. The main concept for formulating this problem is to generate cutting patterns that can be used to produce items from the strip. This problem is a non-convex mixed integer nonlinear programming (MINLP) problem due to the bilinear terms in the demand and objective function. To obtain global optimal solutions, this problem should be transformed into several linear forms. Numerical examples based on the strip packing problem in LCD mother glass production are provided. Different objective functions for two problems are presented and compared.",
     "keywords": ["Mixed integer non-linear programming", "Trim loss problems", "LCD mother glass production optimization"]},
    {"article name": "Dynamic modeling of the methanol synthesis fixed-bed reactor",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.09.013",
     "publication date": "01-2013",
     "abstract": "A dynamic model for a fixed-bed reactor for methanol synthesis is presented. The model is compared with its steady state version. The analysis points out that the numerical stability of the dynamic model is improved by opportunely increasing the level of detail. It is appropriate to introduce the diffusion terms, to work with mass fractions, to select good discretization methods for each term of the model equations. Since these aspects are usually neglected in steady state analysis, this paper investigates step-by-step their implementation, emphasizing their importance (I) in the transformation of an original hyperbolic PDE system into a parabolic PDE system; (II) in removing non-physical oscillations generated by first-order systems that may lead to relevant model prediction errors; and (III) in the approximation of the convection terms using the forward formulation, which is more stable and provides more realistic solutions.",
     "keywords": ["Methanol synthesis", "Dynamic modeling", "Chemical reactors", "Chemical processes", "PDE systems"]},
    {"article name": "Operator training simulator process model implementation of a batch processing unit in a packaged simulation software",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.09.005",
     "publication date": "01-2013",
     "abstract": "In chemical industry, especially in the case of continuous processes, operator training simulators (OTS) are becoming widely used. With the help of these systems several operation and safety issues can be analysed, and the operating staff of the plant can be trained for handling different plant failures. The main part of the OTS is the process model that replaces the real technology. Hence, in control development the simulated process variables are required to be reasonably accurate. The paper presents the structure of the process model of a batch processing unit in UniSim Design, different model constructions of a jacketed batch reactor and the identification of the parameters affecting its hydrodynamic and thermal behaviour. Construction of the process model is the first step in developing the OTS of the pilot plant located in the authors\u2019 laboratory. It can be an effective tool in the development of model-based control systems.",
     "keywords": ["Batch", "Batch processing unit", "OTS", "UniSim Design", "Monofluid thermoblock", "Thermometer models"]},
    {"article name": "Advances in mathematical programming models for enterprise-wide optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.038",
     "publication date": "12-2012",
     "abstract": "Enterprise-wide Optimization (EWO) has become a major goal in the process industries due to the increasing pressures for remaining competitive in the global marketplace. EWO involves optimizing the supply, manufacturing and distribution activities of a company to reduce costs, inventories and environmental impact, and to maximize profits and responsiveness. Major operational items include planning, scheduling, real-time optimization and control. We provide an overview of EWO in terms of a mathematical programming framework. We first provide a brief overview of mathematical programming techniques (mixed-integer linear and nonlinear optimization methods), as well as decomposition methods, stochastic programming and modeling systems. We then address some of the major issues involved in the modeling and solution of these problems. Finally, based on the EWO program at the Center of Advanced Process Decision-making at Carnegie Mellon, we describe several applications to show the potential of this area.",
     "keywords": ["Planning", "Scheduling", "Supply chain optimization", "Mixed-integer programming", "Stochastic programming"]},
    {"article name": "Challenges and opportunities in enterprise-wide optimization in the pharmaceutical industry",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.07.002",
     "publication date": "12-2012",
     "abstract": "Enterprise-wide decision support applications have received increased attention in the chemical process industry in the last decade. In this paper applications, which have real or potential relevance to the pharmaceutical industry, are reviewed. Specific attention is given to the three key phases in the life cycle of an innovative drug product, namely, product development pipeline management, capacity planning and supply chain management. The status of published research in these domains is reviewed, some gaps in the literature are identified and opportunities for further research effort by the process systems engineering community suggested.",
     "keywords": ["New product development", "Capacity planning", "Supply chain management", "Pharmaceuticals"]},
    {"article name": "Process synthesis of hybrid coal, biomass, and natural gas to liquids via Fischer\u2013Tropsch synthesis, ZSM-5 catalytic conversion, methanol synthesis, methanol-to-gasoline, and methanol-to-olefins/distillate technologies",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.032",
     "publication date": "12-2012",
     "abstract": "Several technologies for synthesis gas (syngas) refining are introduced into a thermochemical based superstructure that will convert biomass, coal, and natural gas to liquid transportation fuels using Fischer\u2013Tropsch (FT) synthesis or methanol synthesis. The FT effluent can be (i) refined into gasoline, diesel, and kerosene or (ii) catalytically converted to gasoline and distillate over a ZSM-5 zeolite. Methanol can be converted using ZSM-5 (i) directly to gasoline or to (ii) distillate via olefin intermediates. A mixed-integer nonlinear optimization model that includes simultaneous heat, power, and water integration is solved to global optimality to determine the process topologies that will produce the liquid fuels at the lowest cost. Twenty-four case studies consisting of different (a) liquid fuel combinations, (b) refinery capacities, and (c) superstructure possibilities are analyzed to identify important process topological differences and their effect on the overall system cost, the process material/energy balances, and the well-to-wheel greenhouse gas emissions.",
     "keywords": ["Process synthesis with heat, power, and water integration", "Hybrid energy systems", "Mixed-integer nonlinear optimization", "Fischer\u2013Tropsch", "Methanol to gasoline", "Methanol to olefins and distillate"]},
    {"article name": "From multi-parametric programming theory to MPC-on-a-chip multi-scale systems applications",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.031",
     "publication date": "12-2012",
     "abstract": "An overview of multi-parametric programming and control is presented with emphasis on historical milestones, novel developments in the theory of multi-parametric programming and explicit MPC as well as their application to the design of advanced controller for complex multi-scale systems.",
     "keywords": ["Multi-parametric programming", "Explicit/multi-parametric MPC", "MPC-on-a-chip", "Fast MPC", "Multi-scale applications"]},
    {"article name": "Natural gas enhanced biomass to liquids: Project development and modeling",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.018",
     "publication date": "12-2012",
     "abstract": "The Renewable Fuels Standard program mandates the production of 15 billion gallons per year of liquid transportation fuels from cellulosic biomass by 2022. Praxair has developed a natural gas enhanced biomass to liquids process that increases liquid fuel yield by 2.5 times versus the base case of straight biomass to liquids. Based on this process, Praxair recently completed a comprehensive feasibility study for the conversion of 900 bone dry tons/day of wood chips to Fischer\u2013Tropsch liquids at a specific site in Southeastern US. However, the effort was time consuming and labor intensive. With a better modeling platform, the activity could have been executed more efficiently. Given the drive to reduce dependence on foreign oil, there will continue to be such project development activities. A concerted focus on developing a modeling tool for project development could result in valuable contributions from the process optimization community to the energy/renewable energy market.",
     "keywords": ["Biofuels", "Gasification", "Project development", "2nd generation biofuels", "Cellulosic biomass", "Fischer\u2013Tropsch liquids"]},
    {"article name": "Smart grid technologies and applications for the industrial sector",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.07.006",
     "publication date": "12-2012",
     "abstract": "Smart grids have become a topic of intensive research, development, and deployment across the world over the last few years. The engagement of consumer sectors\u2014residential, commercial, and industrial\u2014is widely acknowledged as crucial for the projected benefits of smart grids to be realized. Although the industrial sector has traditionally been involved in managing power use with what today would be considered smart grid technologies, these applications have mostly been one-of-a-kind, requiring substantial customization. Our objective in this article is to motivate greater interest in smart grid applications in industry. We provide an overview of smart grids and of electricity use in the industrial sector. Several smart grid technologies are outlined, and automated demand response is discussed in some detail. Case studies from aluminum processing, cement manufacturing, food processing, industrial cooling, and utility plants are reviewed. Future directions in interoperable standards, advances in automated demand response, energy use optimization, and more dynamic markets are discussed.",
     "keywords": ["Smart grids", "Electricity consumption", "Demand response", "Power systems", "Energy efficiency", "Smart Grid Interoperability Panel", "Electricity markets", "Ancillary services", "OpenADR"]},
    {"article name": "Assessment of catastrophe risk and potential losses in industry",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.033",
     "publication date": "12-2012",
     "abstract": "This paper describes the potential contribution of near-miss management systems to improving company profitability and reducing the frequency and severity of major industrial accidents. The near-miss concept has long been understood in different industries, as examples in this paper illustrate. However, what has been largely missing is the integration of near-miss management into the culture and day to day operations in a manner that underlines the critical connections between near-misses and behavior. Often, near-miss management has played an ex post forensic role in risk management rather than an alerting one, summarizing leading indicators and precursors of hazardous conditions. This paper describes several strands of recent research that aim to correct this and to make near-miss management an organic element of Enterprise Risk Management. In this respect, a new concept, \u201cpotential safety profit loss\u201d, is introduced to calculate the potential monetary losses due to unexpected shutdowns and accidents.",
     "keywords": ["Near-miss", "Enterprise Risk Management", "Safety pyramid", "Leading risk indicators", "Potential safety losses"]},
    {"article name": "A state-space model for chemical production scheduling",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.025",
     "publication date": "12-2012",
     "abstract": "We express a general mixed-integer programming (MIP) scheduling model in state-space form, and show how common scheduling disruptions, which lead to rescheduling, can be modeled as disturbances in the state-space model. We also discuss how a wide range of scheduling models, with different types of decisions and processing constraints, can be expressed in state-space form. The proposed framework offers a natural representation of dynamic systems, thereby enabling researchers in the chemical process control area to study scheduling problems. It also facilitates the application of known results for hybrid systems, as well as the development of new tools necessary to address scheduling applications. We hope that it will lead to the development of scheduling solution methods with desired closed-loop properties, a topic that has received no attention in the process operations literature.",
     "keywords": ["Chemical production scheduling", "Mixed-integer programming", "Closed-loop solution"]},
    {"article name": "Monitoring, fault diagnosis, fault-tolerant control and optimization: Data driven methods",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.017",
     "publication date": "12-2012",
     "abstract": "Historical data collected from processes are readily available. This paper looks at recent advances in the use of data-driven models built from such historical data for monitoring, fault diagnosis, optimization and control. Latent variable models are used because they provide reduced dimensional models for high dimensional processes. They also provide unique, interpretable and causal models, all of which are necessary for the diagnosis, control and optimization of any process. Multivariate latent variable monitoring and fault diagnosis methods are reviewed and contrasted with classical fault detection and diagnosis approaches. The integration of monitoring and diagnosis techniques by using an adaptive agent-based framework is outlined and its use for fault-tolerant control is compared with alternative fault-tolerant control frameworks. The concept of optimizing and controlling high dimensional systems by performing optimizations in the low dimensional latent variable spaces is presented and illustrated by means of several industrial examples.",
     "keywords": ["Multivariate statistical process monitoring", "Latent variable models", "Fault diagnosis", "Agent-based systems", "Fault-tolerant control", "Optimization", "Control", "Batch processes"]},
    {"article name": "Optimal operation: Scheduling, advanced control and their integration",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.039",
     "publication date": "12-2012",
     "abstract": "This paper discusses the integration of scheduling and advanced control. It gives a brief overview on the challenges for today's production systems, analyzes the functional hierarchy for plant operations and discusses similarities and differences between the two domains. Possible benefits of a closer integration are outlined and the realization of a tighter integration is discussed. This is followed by practical integration aspects and before the conclusions the main industrial requirements are highlighted.",
     "keywords": ["Advanced control", "Scheduling", "Online optimization", "Moving horizon techniques", "Integration", "Industrial challenges"]},
    {"article name": "Process energy systems: Control, economic, and sustainability objectives",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.019",
     "publication date": "12-2012",
     "abstract": "Economic, energy, and sustainability metrics are key performance indicators for process operations. The relative importance of these metrics varies from plant to plant, and often some metrics are in conflict with each other (sustainability vs. profitability). In this paper we discuss the current plant environment and how various metrics can be aligned by focusing on energy efficiency. Power-steam systems are the major energy drivers for most plants, and we discuss possible operational changes that might improve energy efficiency, as well as the role of process control. Managing the interplay of real-time optimization and regulatory control is a challenge for the future, as well as interfacing with the implementation of smart power grids by the utility industry. Combined heat and power along with energy storage presents interesting control and optimization opportunities to maximize energy efficiency.",
     "keywords": ["Sustainability", "Energy efficiency", "Cogeneration", "Carbon dioxide", "Process operations", "Process control", "Smart grids"]},
    {"article name": "Smart manufacturing, manufacturing intelligence and demand-dynamic performance",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.037",
     "publication date": "12-2012",
     "abstract": "Smart Manufacturing is the dramatically intensified and pervasive application of networked information-based technologies throughout the manufacturing and supply chain enterprise. The defining technical threads are time, synchronization, integrated performance metrics and cyber-physical\u2013workforce requirements. Smart Manufacturing responds and leads to a dramatic and fundamental business transformation to demand-dynamic economics keyed on customers, partners and the public; enterprise performance and variability management; real-time integrated computational materials engineering and rapid qualification, demand-driven supply chain services; and broad-based workforce involvement. IT-enabled Smart factories and supply networks can better respond to national interests and strategic imperatives and can revitalize the industrial sector by facilitating global competitiveness and exports, providing sustainable jobs, radically improving performance, and facilitating manufacturing innovation.",
     "keywords": ["Smart Manufacturing", "Advanced manufacturing", "Performance-based enterprises", "Supply chain performance", "Demand-dynamics", "Energy productivity", "Sustainability"]},
    {"article name": "Addressing the operational challenges in the development, manufacture, and supply of advanced materials and performance products",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.041",
     "publication date": "12-2012",
     "abstract": "The challenges of supplying advanced materials and performance products are very different from those associated with commodity chemicals. Yet these challenges are well addressed by enterprise-wide optimization techniques. This paper discusses the nature of advanced materials and some challenges and solutions related to product design, production scheduling, process reliability, and logistics. Ongoing research in the use of multi-agent systems for batch process management is also reviewed.",
     "keywords": ["Discrete event simulation", "Multi-agent systems", "Resource task network", "Crew scheduling"]},
    {"article name": "Optimal design and planning of biodiesel supply chain with land competition",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.044",
     "publication date": "12-2012",
     "abstract": "In this work we propose an MILP multiperiod formulation for the optimal design and planning of the Argentinean biodiesel supply chain, considering land competition and alternative raw materials. The country is divided into twenty three regions, each one including existing crops, oil and biodiesel plants and potential ones. The model includes intermediate and final products, i.e., seed, flour, pellets and expellers, oil, pure and blending biodiesel and glycerol. Crop fields, storage and production plants, as well as distribution centers for internal and external markets are also represented. We consider the possibility of sowing energetic crops, such as Jatropha curcas, in marginal areas. The time horizon is of seven years, divided into 84 periods. The mathematical model has been implemented in GAMS providing a powerful decision-making tool that can be applied to other regions or countries by adjusting specific data.",
     "keywords": ["Supply chain", "Biodiesel", "MILP", "Optimal planning", "Jatropha"]},
    {"article name": "Hybrid simulation based optimization approach for supply chain management",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.045",
     "publication date": "12-2012",
     "abstract": "In this work, we propose a hybrid simulation optimization approach that addresses the problem of supply chain management. We formulated the problem as a mathematical model which minimizes the summation of production cost, transportation cost, inventory holding and shortage costs, subject to capacity and inventory balance constraints and propose a hybrid approach combining mathematical programming and simulation model to solve this problem. The main objective of this approach is to overcome the computational complexity associated with solving the underlying large-scale mixed integer linear problem and to provide a better representation of supply chain reality. The simulation-based optimization strategy uses an agent-based system to model the supply chain network. Each entity in the supply chain is represented as an agent whose activity is described by a collection of behavioral rules. The overall system is coupled with an optimization algorithm that is designed to address planning and scheduling level decisions.",
     "keywords": ["Hybrid approach", "SCM", "Simulation", "Optimization"]},
    {"article name": "A stochastic programming approach for gas detector placement using CFD-based dispersion simulations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.05.010",
     "publication date": "12-2012",
     "abstract": "A stochastic programming formulation is developed for determining the optimal placement of gas detectors in petrochemical facilities. FLACS, a rigorous gas dispersion package, is used to generate hundreds of scenarios with different leak locations and weather conditions. Three problem formulations are investigated: minimization of expected detection time, minimization of expected detection time including a coverage constraint, and a placement based on coverage alone. The extensive forms of these optimization problems are written in Pyomo and solved using CPLEX. A sampling procedure is used to find confidence intervals on the optimality gap and quantify the effectiveness of detector placements on alternate subsamples of scenarios. Results show that the additional coverage constraint significantly improves performance on alternate subsamples. Furthermore, both optimization-based approaches dramatically outperform the coverage-only approach, making a strong case for the use of rigorous dispersion simulation coupled with stochastic programming to improve the effectiveness of these safety systems.",
     "keywords": ["Gas leak detection", "Process safety", "Sensor placement", "Stochastic programming", "Mixed-integer linear programming"]},
    {"article name": "SmartGantt \u2013 An interactive system for generating and updating rescheduling knowledge using relational abstractions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.021",
     "publication date": "12-2012",
     "abstract": "Generating and updating rescheduling knowledge that can be used in real time has become a key issue in reactive scheduling due to the dynamic and uncertain nature of industrial environments and the emergent trend towards cognitive systems in production planning and execution control. Disruptive events have a significant impact on the feasibility of plans and schedules. In this work, the automatic generation and update through learning of rescheduling knowledge using simulated transitions of abstract schedule states is proposed. An industrial example where a current schedule must be repaired in response to unplanned events such as the arrival of a rush order, raw material delay, or an equipment failure which gives rise to the need for rescheduling is discussed. A software prototype (SmartGantt) for interactive schedule repair in real-time is presented. Results demonstrate that responsiveness is dramatically improved by using relational reinforcement learning and relational abstractions to develop a repair policy.",
     "keywords": ["Batch plant management", "Cognitive production systems", "Manufacturing control", "Rescheduling", "Relational reinforcement learning", "Uncertainty"]},
    {"article name": "An improvement-based MILP optimization approach to complex AWS scheduling",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.036",
     "publication date": "12-2012",
     "abstract": "The automated wet-etch station (AWS) is one of the most critical stages of a modern semiconductor manufacturing system (SMS), which has to simultaneously deal with many complex constraints and limited resources. Due to its inherent complexity, industrial-sized automated wet-etch station scheduling problems are rarely solved through full rigorous mathematical formulations. Decomposition techniques based on heuristic, meta-heuristics and simulation-based methods have been traditionally reported in literature to provide feasible solutions with reasonable CPU times.This work introduces an improvement MILP-based decomposition strategy that combines the benefits of a rigorous continuous-time MILP (mixed integer linear programming) formulation with the flexibility of heuristic procedures. The schedule generated provides enhanced solutions over time to challenging real-world automated wet etch station scheduling problems with moderate computational cost. This methodology was able to provide more than a 7% of improvement in comparison with the best results reported in literature for the most complex problem instances analyzed.",
     "keywords": ["Hybrid decomposition approach", "MILP-based strategies", "Large-scale scheduling problems", "Semiconductor manufacturing system (SMS)", "Wafer fabrication", "Modeling and optimization"]},
    {"article name": "Mid-term planning optimization model with sales contracts under demand uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.040",
     "publication date": "12-2012",
     "abstract": "Uncertainty modeling is a challenging topic in supply chain and operation management. When planning material purchase and stock levels, demand uncertainty could have an important impact on the plan results and its feasibility. Additionally, uncertainty could greatly affect customer satisfaction, inventory costs and company profits. From a modeling perspective, problems considering uncertainty are difficult to tackle and lead to complex optimization approaches. This work proposes a mid-term planning model dealing with sales contracts to diminish the effect of uncertainty. Another interesting feature is given by the selection of different price levels. Price elasticity functions are introduced for each customer in order to jointly decide demand targets and prices. A linear generalized disjunctive programming model is developed. Short execution time shows that this model can be applied to analyze several real scenarios to decide material purchase plan, inventory levels, sales strategies, prices and demand levels in a medium term horizon planning.",
     "keywords": ["Demand uncertainty", "Planning", "Contracts", "Generalized disjunctive programming"]},
    {"article name": "Addressing the uncertain quality and quantity of returns in closed-loop supply chains",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.034",
     "publication date": "12-2012",
     "abstract": "In this work a two-stage scenario-based modeling approach is proposed in order to simultaneously deal with the design and planning decisions in supply chain networks, where both forward and reverse flows are considered (closed-loop supply chains) subject to uncertain conditions. A mixed integer linear programming (MILP) approach is developed with the underlying objective of profit maximization. Planning takes into account raw material acquisition and processing, storage and distribution of several products flowing through the network. Uncertainty is associated to the quantity and quality of the flow of products of the reverse network, which are directly affected by customers and sorting centers, respectively. The approach considers, by means of scenarios, the simultaneous integration of two important uncertainty sources, thus presenting an important modeling advantage, i.e. that of providing a better understanding of the CLSCs. The applicability of the formulation is tested with a real sized example of a Portuguese glass company.",
     "keywords": ["Closed-loop supply chain", "Design and planning", "Two-stage stochastic optimization", "Uncertainty"]},
    {"article name": "Integration of scheduling and control with online closed-loop implementation: Fast computational strategy and large-scale global optimization algorithm",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.035",
     "publication date": "12-2012",
     "abstract": "In this paper, we propose a novel integration method to solve the scheduling problem and the control problem simultaneously. The integrated problem is formulated as a mixed-integer dynamic optimization (MIDO) problem which contains discrete variables in the scheduling problem and constraints of differential equations from the control problem. Because online implementation is crucial to deal with uncertainties and disturbances in operation and control of the production system, we develop a fast computational strategy to solve the integration problem efficiently and allow its online applications. In the proposed integration framework, we first generate a set of controller candidates offline for each possible transition, and then reformulate the integration problem as a simultaneous scheduling and controller selection problem. This is a mixed-integer nonlinear fractional programming problem with a non-convex nonlinear objective function and linear constraints. To solve the resulting large-scale problem within sufficiently short computational time for online implementation, we propose a global optimization method based on the model properties and the Dinkelbach's algorithm. The advantage of the proposed method is demonstrated through four case studies on an MMA polymer manufacturing process. The results show that the proposed integration framework achieves a lower cost rate than the conventional sequential method, because the proposed framework provides a better tradeoff between the conflicting factors in scheduling and control problems. Compared with the simultaneous approach based on the full discretization and reformulation of the MIDO problem, the proposed integration framework is computationally much more efficient, especially for large-scale cases. The proposed method addresses the challenges in the online implementation of the integrated scheduling and control decisions by globally optimizing the integrated problem in an efficient way. The results also show that the online solution is crucial to deal with the various uncertainties and disturbances in the production system.",
     "keywords": ["Cyclic scheduling", "PI control", "MINLP", "Decomposition algorithm", "MMA polymerization process"]},
    {"article name": "Computational simulation applied to the investigation of industrial plants for bioethanol distillation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.004",
     "publication date": "11-2012",
     "abstract": "This work aimed to investigate a typical bioethanol distillation process considering an alcoholic wine with 19 components and to validate the simulation results against experimental data collected from a Brazilian sugar mill. The process was investigated in terms of bioethanol alcoholic graduation, ethanol recovery, energy consumption and ethanol loss. Two optimizing approaches were tested: the central composite design (CCD) and the sequential quadratic programming (SQP). Both approaches allowed the optimization of the equipment configuration used nowadays and provided similar optimal conditions. The results showed that the simulation approach was capable of correctly reproducing a real plant of bioethanol distillation and that the optimal conditions guaranteed the bioethanol production according to legislation, with low consumption of steam and high recovery of ethanol. On the other hand, substantial fluctuations in wine composition may require adjustments of operational conditions or the use of specific control loops to prevent an off-specification product.",
     "keywords": ["Bioethanol", "Aspen Plus", "Distillation", "Fusel oil", "Wine distillation", "Minor components"]},
    {"article name": "Planning the operation of a large real-world oil pipeline",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.010",
     "publication date": "11-2012",
     "abstract": "Pipelines are the most effective and reliable means for transporting oil derivatives from refineries to local depots. This paper considers the tactical distribution problem for a large pipeline network. Solutions to this problem consist of volume transfers satisfying local demands, correctly stocking production volumes, and observing a number of operational constraints. This work defines the tactical planning problem and proposes a novel and compact network flow model to address it. Also, a procedure is given to decompose the flow solution into pumping operations that can be used as input for short-term schedulers. Our experiments were performed over real instances provided by a large oil company. These instances consisted of 30 pipelines in an extension of 7000\u00a0km, 14 depots, 192 tanks, and more than 60 products. The model was tested with real-world instances and showed significant improvements over manually generated plans, which is the current practice.",
     "keywords": ["Network flow", "Planning", "Operation", "Oil pipeline"]},
    {"article name": "A disjunctive programming model and a rolling horizon algorithm for optimal multiperiod capacity expansion in a multiproduct batch plant",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.015",
     "publication date": "11-2012",
     "abstract": "This paper addresses a multiperiod mixed integer nonlinear programming problem for the capacity expansion of multiproduct batch plants. Given a certain batch plant with its current configuration, product recipes, and growing production targets, modular expansions are wanted so that new demand can be met. Unlike most work for the batch retrofit problem, a multiperiod disjunctive model is presented, such that long term investments and expansions can be planned out in advance. To solve the model we propose a rolling horizon algorithm that further exploits the advantages of a disjunctive programming model. Empirical work shows that the rolling horizon algorithm is very effective on finding near optimal solutions to large instances with a considerable number of time periods. Furthermore, the solution found by the proposed algorithm can be used as a starting solution for the direct method to the original problem delivering a global optimal solution.",
     "keywords": ["Batch retrofit", "Multiproduct batch plants", "Multiperiod MINLP model", "Disjunctive programming", "Rolling horizon algorithm"]},
    {"article name": "Graphical tools for routine assessment of industrial alarm systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.042",
     "publication date": "11-2012",
     "abstract": "Alarms are important for safe and reliable operation in process industries. Periodic alarm assessment is a crucial step in alarm management lifecycle that provides valuable feedback for fine tuning the alarm system. In this perspective tutorial, alarm data is represented using binary sequences and subsequently, two novel alarm data visualization tools are presented: (1) The High Density Alarm Plot (HDAP) charts top alarms over a given time period and (2) Alarm Similarity Color Map (ASCM) highlights related and redundant alarms in a convenient manner. The proposed graphical tools are instrumental in performance assessment of industrial alarm systems in terms of effectively identifying nuisance alarms such as chattering and related alarms based on routinely collected alarm event data. The special features and advantages of the proposed graphical tools are illustrated by successful application to two large scale industrial case studies, each involving over half a million observations for top fifty alarm tags.",
     "keywords": ["Alarm systems", "Performance monitoring", "Data visualization", "Pattern recognition", "Human\u2013machine interface"]},
    {"article name": "An optimal multiple-model strategy to design a controller for nonlinear processes: A boiler-turbine unit",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.005",
     "publication date": "11-2012",
     "abstract": "The paper documents an optimal multiple-model strategy to design a controller for the boiler-turbine unit. There are two important problems associated with multiple-model controllers: number of models and their locations in an operating space. A method based on Vinnicombe distance is used to find minimum number of local models. First the stability of the control scheme is verified in theory. Using the closed-loop stability condition the minimum number of models is found and the models are located in a space of an unknown parameter. The performance of the proposed controller is evaluated in simulation and compared with a PI controller designed using H \u221e approach and an MPC-based controller in the literature. The designed controller based on the linear local models can ensure satisfactory performance. The novel idea of this paper is the optimal strategy finding minimum number of local models in order to reduce the computational cost and processing time.",
     "keywords": ["Stable", "Multiple-model controller", "Boiler-turbine", "Number of models", "Vinnicombe metric", "Computation time"]},
    {"article name": "Modeling and simulation of unloading operations in petroleum product storage terminals",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.07.009",
     "publication date": "11-2012",
     "abstract": "Liquefied petroleum products such as propane, butane, LPG and LNG are transported by ships and stored in tanks in storage terminals. These products are conveyed from the jetty by above-ground insulated pipelines to storage terminals that are typically situated 12\u201320\u00a0km inland. Unloading of petroleum products is energy intensive and results in excessive BOG (boil off gas) generation and pressure build up in the storage tank. In this work, a dynamic model of the unloading operation is developed to predict temperature dynamics of pipeline, pressure and inventory changes in the storage tank. This model consists of a lumped dynamic model for the storage tank coupled with a distributed parameter model for the pipeline. The model is validated using limited data collected from an operating plant during unloading of propane. Dynamic simulation results of the model are presented to indicate the trade-off that exists between the pre-cooling and unloading operating strategies.",
     "keywords": ["Dynamic simulation", "Thermal analysis", "Storage terminals", "Pipeline technology", "Boil off gas"]},
    {"article name": "Nonlinear observer with adaptive grid allocation for a fixed-bed adsorption process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.07.001",
     "publication date": "11-2012",
     "abstract": "Real-time knowledge of concentration profiles inside the adsorption bed is essential for the feedback control of a pressure swing adsorption (PSA) process. A difficulty encountered when developing an observer for this purpose is that the fixed-bed adsorption model is very stiff; thus, either a large number of grid points or an adaptive grid allocation is required to solve the model accurately through discretization. The objective of this study is to present a nonlinear observer with an adaptive grid allocation for an adsorption bed to estimate a spatially distributed state. Experimental verification revealed that the proposed technique can accurately and rapidly predict the breakthrough curve in addition to other process variables despite uncertainties in the model parameters and in the feed composition.",
     "keywords": ["Nonlinear observer", "Moving finite element", "Adaptive grid allocation", "Pressure swing adsorption", "CO2 capture"]},
    {"article name": "Application of continuation method and bifurcation for the acetylcholine neurocycle considering partial dissociation of acetic acid",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.07.007",
     "publication date": "11-2012",
     "abstract": "In this paper, bifurcation and chaotic behavior of the two-enzyme\u2013two-compartment acetylcholine (ACh) neurocycle model developed earlier (Mustafa et al., 2009b, Mustafa et al., 2009a, Mustafa et al., 2012) are investigated considering the partial dissociation of acetic acid in both presynaptic and postsynaptic neurons as well. The two-parameter continuation technique is used to investigate static and dynamic solutions of the ACh cholinergic neurocycle system based on feed choline concentration as the main bifurcation parameter. A detailed bifurcation analysis is carried out in order to uncover some important features of the system, such as static bifurcation, dynamic bifurcation and chaotic behavior. These findings are related to the real phenomena occurring in the neurons, like periodic stimulation of neural cells and non-regular functioning of ACh receptors. It is found that pH exists in the range of 7.05\u20137.75 which is inside the physiological range of pH in the brain taking into consideration partial dissociation of the acetic acid. The disturbances and irregularities (chaotic attractors) occurring in the ACh cholinergic system may be good indications to cholinergic diseases such as the Alzheimer's and Parkinson's diseases.",
     "keywords": ["Acetylcholine", "Choline", "Acetic acid", "Hydrogen ions", "Period doubling", "Dynamic behavior", "Bifurcation", "Chaos"]},
    {"article name": "The optimization of the kind and parameters of kernel function in KPCA for process monitoring",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.023",
     "publication date": "11-2012",
     "abstract": "Kernel principal component analysis (KPCA) has been widely used in chemical processes monitoring due to its simple principle. However, how to select the kind and parameters of kernel function still limits the application of the method until now. In this paper, an optimization method based on genetic algorithm is developed to choose proper kind and parameters of kernel function. In this method, kernel kind and parameters are seen as decision variables of optimization, using correct monitoring rate, number of principal components, and statistical control limit of square prediction error (SPE) as multi-objective. For this specific problem, the fitness function, the algorithm of genetic selection, crossover and mutation are designed to ensure the diversity of kernel function and more selected chances of optimal individual in evolution process. A simple example and penicillin fermentation process are used to investigate the potential application of the proposed method; simulation results show that the proposed method is effective.",
     "keywords": ["Chemical processes", "KPCA", "Kernel function", "Optimization", "Genetic algorithm", "Fermentation"]},
    {"article name": "Process synthesis of biodiesel production plant using artificial neural networks as the surrogate models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.006",
     "publication date": "11-2012",
     "abstract": "Biodiesel is an attractive biofuel because it can be used directly with the traditional petro diesel engines, either as a substitute or as a blending component. There are several alternatives that can be used to produce biodiesel. In this work, we developed a superstructure optimization model to synthesize the optimum biodiesel production plant, i.e., the one that gives the minimum net present sink. To reduce the computational cost of solving the resulting disjunctive programming, the surrogate models utilizing artificial neural networks (ANNs), have been developed to replace the unit operation, thermodynamics and mixing models. The optimum solution with the alkali-catalyzed reactor (obtained in five CPU seconds) has a total net present sink of about $41 million, which differs less than one percent from the result obtained by modeling the solution in a process simulator. However, this level of accuracy required a large amount of data to train the ANNs.",
     "keywords": ["Biodiesel production", "Superstructure optimization", "Process synthesis", "Surrogate models artificial neural networks"]},
    {"article name": "An ontology for distributed process supervision of large-scale chemical plants",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.009",
     "publication date": "11-2012",
     "abstract": "As modern chemical processes evolve into extremely large and complex systems involving multiple third-party technologies, instruments, and software, the information necessary for process supervision is distributed across disparate sources. Supervision methods rely on models that are themselves based on these process descriptors. Therefore, any changes during operations have to be reflected to the supervision methods so that they can adapt suitably. In this paper, we propose an ontology, called OntoSafe, that provides the semantics for abnormal situation management. It enables the integration of all the information necessary for forming a cohesive judgment of the condition and state of the process. It also captures the underlying linkages so that changes in the process descriptors are propagated consistently. We illustrate the benefits of these using the ENCORE distributed intelligence system. The application to an offshore oil and gas production case study is also reported.",
     "keywords": ["Semantics", "OntoCAPE", "Fault detection and identification", "Process operations", "Multi agent system"]},
    {"article name": "Language-oriented rule-based reaction network generation and analysis: Applications of RING",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.003",
     "publication date": "11-2012",
     "abstract": "Applications of RING in the generation and analysis of complex thermochemical reaction networks are presented. Automated generation and topological network analysis features in RING allow for: (a) constructing reaction networks exhaustively in a rule-based manner, (b) identifying dominant pathways in networks using estimates of kinetic parameters, (c) hypothesis and testing of mechanisms by comparing pathway results from RING with experimental data, and (d) predicting atom-efficient synthetic routes to valuable chemicals from known chemistries and commonly available chemicals. Case studies involving three chemical systems are used to demonstrate these features in RING: (a) acid-catalyzed propane aromatization, (b) glycerol and acetone dehydration on acid catalysts, and (c) C4\u2013C9 mono-alcohols synthesis from C2 and C3 oxygenates on acid, base, and metal catalyzed chemistries. Through these case studies, we demonstrate that RING can be used to postulate mechanisms and predict likely products for a given system, thereby guiding experimentation and computational analysis.",
     "keywords": ["Rule-based network generation", "Reaction network analysis", "Pathway analysis", "Mechanism elucidation", "Mechanism hypothesis testing", "Synthesis routes prediction"]},
    {"article name": "Model-based optimization of economical grade changes for the Borealis Borstar\u00ae polyethylene plant",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.024",
     "publication date": "11-2012",
     "abstract": "Economical grade changes are considered for a Borealis Borstar\u00ae polyethylene plant model, incorporating two slurry-phase reactors, one gas-phase reactor and a recycle area with three distillation columns. The model is constructed in the Modelica language and the JModelica.org platform is used for optimization. The cost function expresses the economical profit during a grade change and is formulated using on-grade intervals for seven polymer quality variables such as melt index, density and reactor split factors. Additionally, incentives to produce polymer with quality variables on grade target values, not only inside grade intervals, are added. Twelve inflows and three purge flows are used as decision variables. Two grade changes are thoroughly reviewed, showing the effect of using a cost function that regards plant economy. Resulting trajectories can be divided into three phases with distinguishing features, and the synchronization of inflows and usage of recycle area off-gas flows are important in the grade changes.",
     "keywords": ["Optimization", "Grade change", "Polymer", "Economy"]},
    {"article name": "Flexible maintenance within a continuous-time state-task network framework",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.028",
     "publication date": "11-2012",
     "abstract": "Preventative maintenance is common in industry and aids in sustaining reliable process operations. If maintenance is executed within the production schedule, it imposes restrictions on the shared resources of the manufacturing facility. In multipurpose batch plants, these restrictions can affect many product pathways and have a significant impact on plant profitability. The combined optimization of process maintenance and production scheduling should therefore exploit the inherent flexibility of such plants and provide solutions with the highest profitability. This paper presents a mathematical model capable of addressing the combined scheduling of maintenance and production tasks within a continuous-time state-task network based framework. Multiple maintenance types are addressed within the framework. The proposed formulation is illustrated on an adaptation of a well cited literature problem, and the benefit of combined maintenance and production scheduling highlighted.",
     "keywords": ["Scheduling", "State-task network", "Flexible maintenance", "Continuous-time formulation"]},
    {"article name": "Integrated production optimization of oil fields with pressure and routing constraints: The Urucu field",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.016",
     "publication date": "11-2012",
     "abstract": "This paper develops a framework for integrated production optimization of complex oil fields such as Urucu, which has a gathering system with complex routing degree of freedom, limited processing capacity, pressure constraints, and wells with gas-coning behavior. The optimization model integrates simplified well deliverability models, vertical lift performance relations, and the flowing pressure behavior of the surface gathering system. The framework relies on analytical models history matched to field data and simulators tuned to reflect operating conditions. A mixed-integer linear programming (MILP) problem is obtained by approximating these models with piecewise-linear functions. Procedures were developed to obtain simplified piecewise-linear approximations that ensure a given accuracy with respect to complex and precise models. Computational experiments showed that the integrated production optimization problem can be solved sufficiently fast for real-time applications. Further, the operational conditions calculated with the simplified models during the optimization process match the precise models.",
     "keywords": ["Oil fields", "Integrated production optimization", "Pressure drop modeling", "Routing constraints", "Multidimensional piecewise-linear approximation"]},
    {"article name": "Numerical investigation of the thermally and flow induced crystallization behavior of semi-crystalline polymers by using finite element\u2013finite difference method",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.026",
     "publication date": "11-2012",
     "abstract": "The thermally and flow induced crystallization behavior of semi-crystalline polymer in processing can significantly influence the quality of final products. The investigation of its mechanism has both scientific and industrial interest. A mathematical model in three dimensions for thermally and flow induced crystallization of polymer melts obeying differential Phan-Thien and Tanner (PTT) constitutive model has been developed and solved by using the finite element\u2013finite difference method. A penalty method is introduced to solve the nonlinear governing equations with a decoupled algorithm. The corresponding finite element\u2013finite difference model is derived by using the discrete elastic viscous split stress algorithm incorporating the streamline upwind scheme. A modified Schneider's approach is employed to discriminate the relative roles of the thermal state and the flow state on the crystallization phenomenon. The thermally and flow induced crystallization characteristics of polypropylene is investigated based on the proposed mathematical model and numerical scheme. The half crystallization time of polypropylene in a cooled couette flow configuration obtained by simulation are compared with Koscher's experimental results, which show that they agree well with each other. Two reasons to cause crystallization of polypropylene in pipe extrusion process including the thermal state and the flow state are investigated. Both the crystalline distribution and crystalline size of polypropylene are obtained by using the finite element\u2013finite difference simulation of three-dimensional thermally and flow induced crystallization. The effects of processing conditions including the volume flow rate and the temperature boundary on the crystallization kinetics process are further discussed.",
     "keywords": ["Semi-crystalline polymers", "Thermally induced", "Flow induced", "Crystallization", "Viscoelastic", "Finite element\u2013finite difference simulation"]},
    {"article name": "Combined simulation\u2013optimization methodology for the design of environmental conscious absorption systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.030",
     "publication date": "11-2012",
     "abstract": "This work addresses the optimization of ammonia\u2013water absorption cycles for cooling and refrigeration applications with economic and environmental concerns. Our approach combines the capabilities of process simulation, multi-objective optimization (MOO), cost analysis and life cycle assessment (LCA). The optimization task is posed in mathematical terms as a multi-objective mixed-integer nonlinear program (moMINLP) that seeks to minimize the total annualized cost and environmental impact of the cycle. This moMINLP is solved by an outer-approximation strategy that iterates between primal nonlinear programming (NLP) subproblems with fixed binaries and a tailored mixed-integer linear programming (MILP) model. The capabilities of our approach are illustrated through its application to an ammonia\u2013water absorption cycle used in cooling and refrigeration applications.",
     "keywords": ["A absorber", "absorber", "AWRS ammonia\u2013water absorption refrigeration system", "ammonia\u2013water absorption refrigeration system", "C condenser", "condenser", "COP coefficient of performance", "coefficient of performance", "E evaporator", "evaporator", "ECO99 Eco-Indicator 99", "Eco-Indicator 99", "EI environmental Impact", "environmental Impact", "LCA life cycle assessment", "life cycle assessment", "LCI life cycle inventory", "life cycle inventory", "MILP mixed-integer linear programming", "mixed-integer linear programming", "MINLP mixed-integer non-linear programming", "mixed-integer non-linear programming", "moMINLP multi-objective mixed-integer non-linear programming", "multi-objective mixed-integer non-linear programming", "MOO multi-objective optimization", "multi-objective optimization", "NLP non-linear programming", "non-linear programming", "P pump", "pump", "RC rectification column", "rectification column", "SC refrigerant subcooler", "refrigerant subcooler", "SHX solution heat exchanger", "solution heat exchanger", "TAC total annualized cost", "total annualized cost", "VLV1 refrigerant expansion valve", "refrigerant expansion valve", "VLV2 solution expansion valve", "solution expansion valve", "Process simulation", "Multi-objective optimization", "Absorption cycle", "Cost analysis", "Life cycle assessment"]},
    {"article name": "A block-algebra approach to the solution of\u2211iai\u03bb\u2212ciequations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.05.015",
     "publication date": "11-2012",
     "abstract": "A bordered matrix is proposed, that with block-algebra manipulations rebuilds the form of the equation given in the title. It is shown that the determinant of the bordered matrix results in a Standard Eigenvalue Problem, where the roots are exactly the eigenvalues of the resulting problem. Even more, the block-algebra operations involved are extremely simple. Likewise, a specialized iterative procedure to find one eigenvalue for a Standard Eigenvalue Problem, based on multivariable Newton, is presented. Numerical experiments are presented to show the reliability of the proposed approach taken to find one or all the roots of the equation given in the title.",
     "keywords": ["Bordered matrix", "Standard Eigenvalue Problem", "Roots", "Flash", "Distillation short-cut design"]},
    {"article name": "A graphical CO2 emission treatment intensity assessment for energy and economic analyses of integrated decarbonised production systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.04.013",
     "publication date": "10-2012",
     "abstract": "Design of clean energy systems is highly complex due to the existence of a variety of CO2 abatement and integration options. In this study, an effective decision-making methodology has been developed for facilitating the selection of lowest energy or lowest cost intensity systems, from a portfolio of flowsheet configurations with different decarbonisation strategies. The fundamental aspect of the proposed methodology lies in thermodynamic feasibility assessment as well as quantification of CO2 emission treatment intensity using a graphical approach (CO2 emission balance diagram) for energy and economic performance analyses of integrated decarbonised systems. The relationship between the graphical representation and performances is established using blocks and boundaries on integrated systems. The effectiveness of the methodology has been demonstrated through a range of coal gasification based polygeneration and cogeneration systems, incorporating either of carbon capture and storage (CCS) or CO2 reuse options.",
     "keywords": ["Clean coal technology", "Coal to liquid fuel synthesis", "CO2 reuse", "Polygeneration", "Carbon capture and storage", "Process integration"]},
    {"article name": "Inferential monitoring and optimization of crude separation units via hybrid models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.05.012",
     "publication date": "10-2012",
     "abstract": "The hybrid model of a CDU introduced in this work can be used for both production planning and for RTO, since the model converges rapidly via SLP, it is small in size, its accuracy is comparable to rigorous models, and it can optimize the operating variables, including furnace duty and pumparound duties. This enables CDU part of the planning models to be as accurate as the RTO models, instead of using standard CDU modes of operation. The model includes mass and energy balances and statistical models relating crude distillation curve points, tower operating variables and tray temperatures to product distillation curves. Hybrid model of a sample CDU shows excellent agreement with property predictions from the rigorous model; RMSE of points on the product distillation curves is typically less than 0.5% of the rigorous model predictions. Optimization of the hybrid model via SLP converged to a better point than equation-oriented AspenPlus.",
     "keywords": ["Crude distillation", "Hybrid model", "Production planning", "Process optimization"]},
    {"article name": "Automated drop detection using image analysis for online particle size monitoring in multiphase systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.05.014",
     "publication date": "10-2012",
     "abstract": "Image analysis has become a powerful tool for the work with particulate systems, occurring in chemical engineering. A major challenge is still the excessive manual work load which comes with such applications. Additionally manual quantification also generates bias by different observers, as shown in this study. Therefore a full automation of those systems is desirable. A MATLAB\u00ae based image recognition algorithm has been implemented to automatically count and measure particles in multiphase systems.A given image series is pre-filtered to minimize misleading information. The subsequent particle recognition consists of three steps: pattern recognition by correlating the pre-filtered images with search patterns, pre-selection of plausible drops and the classification of these plausible drops by examining corresponding edges individually. The software employs a normalized cross correlation procedure algorithm. The program has reached hit rates of 95% with an error quotient under 1% and a detection rate of 250 particles per minute depending on the system.",
     "keywords": ["Particle size distribution", "Image analysis", "Online monitoring", "Sauter mean diameter", "Dispersion", "Automatic particle recognition"]},
    {"article name": "Optimal spatial sampling scheme for parameter estimation of nonlinear distributed parameter systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.04.014",
     "publication date": "10-2012",
     "abstract": "In this paper a methodology for the estimation of parameters of distributed parameter systems based on optimal spatial measurements is discussed. The concept of the covariance matrix for sampling design is exploited and D-optimality criteria concerning relevant metrics are linked with the computation of optimal measurement locations. These are obtained at the points where sensitivity functions reach their extrema values and coincide with the locations where system eigenmodes obtained by proper orthogonal decomposition (POD) are maximised or minimised as has been shown in a recent work (Ala\u00f1a & Theodoropoulos, 2011). A tubular reactor with recycle is used as illustrative example to demonstrate the sampling methodology, including cases where the system behaviour is unstable exhibiting sustained oscillations. The estimates with the highest deviation from the nominal parameter values are the ones with the lowest (absolute) sensitivity coefficients. Using a POD-based reduced model can significantly reduce computational costs and in addition improve the estimation procedure by using measurements at the locations where the POD modes extrema occur. The results are strongly influenced by experimental noise, and filtering techniques are needed to mitigate the related uncertainties.",
     "keywords": ["Covariance matrix", "Sensitivity coefficients", "Measurement points", "Proper orthogonal decomposition", "Empirical eigenfunctions", "Nonlinear distributed parameter system"]},
    {"article name": "Accounting robustly for instantaneous chemical equilibriums in reactive transport: A numerical method and its application to liquid\u2013liquid extraction modeling",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.05.017",
     "publication date": "10-2012",
     "abstract": "Reactive transport equations are used in numerous application fields: CO2 or nuclear waste storage monitoring, separation processes in chemical engineering. We present a general method to account robustly for instantaneous chemical equilibriums in reactive transport. This method is adapted to all kinds of hydraulic transport models including 1D to 3D convection\u2013diffusion equations. This leads to the resolution of a bound constrained system of Differential Algebraic Equations (DAEs). The algebraic constraints come from the adjunction of mass action laws related to the equilibriums, whereas the bounds account for the positivity of the computed quantities. In order to solve the numerical system associated with our method, we use an adaptation of the DASSL solver, CDASSL, that can handle the resolution of bound constrained DAE systems. We present an application of this method to liquid\u2013liquid extraction modeling. Numerical experiments demonstrate the interest of using the CDASSL solver to ensure the bound constraints are satisfied.",
     "keywords": ["Reactive transport", "Invariant method", "Liquid\u2013liquid extraction", "Mass transfer", "Instantaneous chemical equilibriums", "Method of lines", "DAE", "Bound constraints"]},
    {"article name": "Coupling multiple water-reuse network designs for agile manufacturing",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.002",
     "publication date": "10-2012",
     "abstract": "Water-reuse network design (WRND) is widely studied and employed in many industrial processes to minimize freshwater consumption and wastewater generation. Previous studies have been focusing almost exclusively on identifying one optimal WRND for a studied process system. Actually, the quest for agile manufacturing by some multi-purpose industrial processes needs a comprehensive WRND having the maximum operational flexibility to deal with various production requirements. Thus, the most desirable way for such agile manufacturing is to couple various WRNDs into one super design, and then through an effective control strategy to accomplish different production requests by switching among these WRNDs. In this paper, a novel methodology for the design of such a flexible super WRND has been developed. It simultaneously considers the integration of multiple WRNDs and network control switch based on different manufacturing purposes. The developed systematic methodology is general and thus applicable for many potential network integration problems. Case studies have demonstrated the efficacy of the development and its significance in the context of agile manufacturing.",
     "keywords": ["Water-reuse network design", "Agile manufacturing", "Design flexibility", "Optimization", "Network integration"]},
    {"article name": "A novel hybrid differential evolution approach to scheduling of large-scale zero-wait batch processes with setup times",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.05.008",
     "publication date": "10-2012",
     "abstract": "Due to their significance both in theory and industrial application, zero-wait scheduling problems (ZWSPs) of batch plants have received more and more attentions. However, how to tackle large-scale ZWSPs with setup times is still a challenging problem. This paper presents a novel hybrid permutation-based differential evolution (HPDE) for this purpose. More specifically, ZWSPs are formulated as asymmetrical traveling salesman problems (ATSP). To deal with ATSP model effectively, a permutation-based DE (PDE) algorithm and fast complex heuristic (FCH) local search scheme are proposed. Furthermore, HPDE, a hybrid approach of PDE and FCH local search, is proposed. The performances of HPDE are illustrated based on extensive experiments and comparisons with recently developed approaches in literature. The results demonstrate that HPDE reaches high-quality solutions in short computational time. Furthermore, it requires fewer user-defined parameters, rendering it applicable to real-life large-scale ZWSPs with setup times.",
     "keywords": ["Differential evolution", "Zero-wait scheduling problems", "Large-scale", "Setup times", "Batch plants"]},
    {"article name": "Dissipativity-based decentralized control of interconnected nonlinear chemical processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.05.013",
     "publication date": "10-2012",
     "abstract": "This paper presents an approach, based on dissipative systems theory, to the analysis and control design of interconnected nonlinear processes. The objective is to design distributed feedback controllers to achieve plant-wide stability. Extensions of classical results on the stability of large-scale interconnected systems lead to input\u2013output dissipativity constraints for each subsystems, encoded as supply rates from input to output interconnecting ports. For each subsystem, a parameterized nonlinear feedback controller is designed using nonlinear dissipative inequalities to ensure that the aforementioned dissipativity constraints are met in closed-loop. One focus of this paper is the design of domination-based nonlinear feedback controllers to meet the above interconnection constraints. This paper also presents new results on the construction of storage functions for control affine systems, as a generalization of physics-based approaches to dissipative systems theory. Control of interconnected chemical reactors with a recycle stream is presented throughout the paper to demonstrate the proposed construction.",
     "keywords": ["Process networks", "Distributed control", "Dissipative systems theory", "Nonlinear feedback control"]},
    {"article name": "Automating HAZOP studies using D-higraphs",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.007",
     "publication date": "10-2012",
     "abstract": "In this paper, we present the use of D-higraphs to perform HAZOP studies. D-higraphs is a formalism that includes in a single model the functional as well as the structural (ontological) components of any given system. A tool to perform a semi-automatic guided HAZOP study on a process plant is presented. The diagnostic system uses an expert system to predict the behavior modeled using D-higraphs. This work is applied to the study of an industrial case and its results are compared with other similar approaches proposed in previous studies. The analysis shows that the proposed methodology fits its purpose enabling causal reasoning that explains causes and consequences derived from deviations, it also fills some of the gaps and drawbacks existing in previous reported HAZOP assistant tools.",
     "keywords": ["HAZOP analysis", "Functional modeling", "Risk assessment"]},
    {"article name": "Language-oriented rule-based reaction network generation and analysis: Description of RING",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.008",
     "publication date": "10-2012",
     "abstract": "The input and output formats, and the structure of Rule Input Network Generator (RING), a computational tool for generation and analysis of complex reaction networks, are described with reference to the underlying algorithms from Cheminformatics and graph theory. RING consists of three modules: (a) a compiler that translates inputs written as a program in an English-like reaction language into internal representations and instructions, (b) a network generator that constructs an exhaustive reaction network from reaction rules and initial reactants specified, and (c) a post-processing module that can extract pathways, mechanisms, or lumps from the network based on user-specified instructions. RING can be used, in a rule-based manner, for constructing a large and complex reaction network from a set of elementary/overall reaction rules, and for elucidating transformations occurring in these networks through identifying pathways and mechanisms to specified products. RING is available open under GNU Lesser GPL.",
     "keywords": ["Rule-based network generation", "Reaction network analysis", "Pathway analysis", "Mechanism elucidation", "Domain specific language interface"]},
    {"article name": "CFD modeling of the mixing of water in oil emulsions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.013",
     "publication date": "10-2012",
     "abstract": "A computational fluid dynamics (CFD) model was developed for the mixing of water in oil emulsion in a lab-scale mixing tank equipped with a Rushton turbine impeller. Multiple reference frames (MRF) technique, k\u2013\u025b model, and Eulerian\u2013Eulerian approach were employed to model the impeller rotation, turbulence, and multiphase flow, respectively. The droplet size distribution within the mixing tank was estimated by means of the population balance approach, which employs the discrete method to describe coalescence and breakage of water droplets. To validate the CFD model, the cumulative probability size distribution computed using the model was compared with the experimentally determined values reported in the literature. This validated CFD model was then utilized to explore the effects of the impeller speed, oil type, and volume fraction of water on the cumulative probability droplet size distribution, number density, and distribution of local volume fraction of the dispersed phase.",
     "keywords": ["Computational fluid dynamics", "Emulsion", "Population balance", "Mixing", "Droplet size distribution"]},
    {"article name": "Eco-innovative design method for process engineering",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.06.020",
     "publication date": "10-2012",
     "abstract": "Due to the environmental issues, innovation is one way to challenge eco-friendly technologies, create new process options which are needed to meet the increasing demands for sustainable production. To accelerate and improve eco-innovative design, there is a need for the computer aided eco-innovation tools to support engineers in the preliminary design phase. Currently, several computer aided innovation tools with a clear focus on specific innovation tasks exist but very few of them deal with the eco-innovation issues. Therefore the purpose of this paper is to present the development of a computer aided model based preliminary design methodology focused on technological eco-innovation for chemical engineering. This methodology is based on modified tools of the structured TRIZ theory. The general systematic framework gives the same level of importance, to the technological and environmental requirements during the conceptual design phase. Integrating environment oriented design approach at the earliest, in the design phase, is essential for product effectiveness and future development. The methodology employs a decomposition based solution approach in hierarchical steps by analysing the problem faced, formulation of the problem and the generation of possible and feasible ideas. At each step, various methods and tools will be needed. In this paper some existing tools are adapted to chemical engineering and some tools of the structured TRIZ theory are modified and improved to build a specific methodology oriented towards the increasing technological complexity and environmental issues of current designs.Undoubtedly, the selection of materials and substances for a particular generated concept, mainly affects the structure, mechanical factors (processability and dimensions) and the environmental impact. In order to deal with these environmental criteria, the resources and their impacts are considered in the upstream phase of the design process and are introduced as constraints in our model.To highlight its capabilities, the methodology is illustrated through a case study dedicated to tars and ashes issues in biomass gasification.",
     "keywords": ["Eco-innovation design", "TRIZ", "CSP", "Multi contradiction", "Biomass"]},
    {"article name": "A Decision Support System for consumption optimization in a naphtha reforming plant",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.04.005",
     "publication date": "09-2012",
     "abstract": "In a naphtha distillation process, the natural objective is to perform an entire process maximization of the production rate while meeting required product qualities by searching for an optimal operating condition by manipulating the operating variables. The objective of this paper includes performing an energy process optimization. Not only is an adequate production rate met with the required product qualities but the operating cost is also minimized through a data mining approach. The study of the influence of all process attributes in the defined Energy Efficient Indicator (EEI) allows the construction of a multivariate linear model to aid human experts in the recovery of energy losses. A canonical discriminant function carried out the data prediction step. The quality of the Decision Support System framework is illustrated by a case study considering a real database. Also, a commercial software supported by this mining framework is presented.",
     "keywords": ["Energy systems engineering", "Plant operation", "Naphtha distillation", "Decision Support System", "Data mining cost optimization"]},
    {"article name": "Synthesis of mass integration networks: Integrated approach to optimization of stream matching for a metal pickling process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.04.012",
     "publication date": "09-2012",
     "abstract": "The purpose of this techno-economic feasibility study is to develop a procedure for the cost-effective synthesis and optimization of schemes to recover in-plant a valuable ingredient from waste streams and at the same time reduce the COD property. MILP and MINLP algorithms are used. The model minimizes the total annualized cost of the mass exchange network (MEN). The procedure involves a two-stage process: first, using the MILP, the pinch point location is identified along with the selection of an optimal set of mass separating agents (MSAs). At this stage the effect of each selected MSA on COD is scrutinized. In the second stage, the MINLP is applied to realize the optimal matching of waste and lean streams through the direct minimization of equipment capital cost. An example is presented to show the applicability and advantages of the proposed novel procedure. The project was found to be highly profitable.",
     "keywords": ["MILP", "MINLP", "Mass and property integration", "Pinch point", "Stream matching", "Techno-economic feasibility"]},
    {"article name": "Modeling spiral-wound membrane modules with applications for gas/vapor permeation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.05.001",
     "publication date": "09-2012",
     "abstract": "Gas/vapor permeation has become a common method for VOC (volatile organic compound) recovery and CO2 capture. The spiral-wound membrane module, which has high packing density, has been commonly used in gas/vapor permeation. In this study, the model that suits the spiral-wound PDMS (polydimethylsiloxane) membrane is obtained by using an integral transform from an N\u2013S equation (Navier\u2013Stokes) and a mass transfer differential equation. The predicted results indicate that the concentration polarization and spiral-wound membrane structure influence can be neglected. The mass transfer based on the MS\u2013UNIQUAC (MS\u2013U) model is also proposed and compared to that based on the Fick\u2013Henry (F\u2013H) model. The comparison results show that the mass transfer based on the F\u2013H model can only be used in a gas mixture without condensable gas; the MS\u2013U model should be used for a gas mixture that includes condensable gas. The influence of permeate flux and feed-side composition in the spiral-wound membrane at different operation conditions is also investigated using the MS\u2013U model.",
     "keywords": ["Spiral-wound membrane", "PDMS", "MS", "UNIQUAC"]},
    {"article name": "Study of the interfacial forces and turbulence models in a bubble column",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.04.007",
     "publication date": "09-2012",
     "abstract": "Three-dimensional Eulerian simulations of gas\u2013liquid flows in a cylindrical bubble column were performed in order to evaluate the flow pattern in the heterogeneous flow regime. Simulations were conducted numerically by the finite element method using a commercial CFX code (v.12.0) and an analysis of interfacial forces such as drag, lift and turbulent dispersion, as well as a comparison of k\u2013\u025b and Reynolds Stress (RSM) turbulence models were performed. Different bubble sizes and two boundary conditions for the column exit were also verified. Gas velocities and holdup profiles were compared with experimental data. Results have shown that at the sparger region and without any simplification on the boundary condition the RSM model better represents the flow. At the fully developed region k\u2013\u025b also showed good results.",
     "keywords": ["Bubble columns", "CFD", "Interfacial forces"]},
    {"article name": "From time representation in scheduling to the solution of strip packing problems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.05.002",
     "publication date": "09-2012",
     "abstract": "We propose two mixed-integer linear programming based approaches for the 2D orthogonal strip packing problem. Using knowledge from the alternative forms of time representation in scheduling formulations, we show how to efficiently combine three different concepts into the x- and y-dimensions. One model features a discrete representation on the x-axis (strip width) and a continuous representation with general precedence variables on the y-axis (strip height). The other features a full continuous-space representation with the same approach for the y-axis and a single non-uniform grid made up of slots for the x-axis. Through the solution of a set of twenty nine instances from the literature, we show that the former is a better approach, even when compared to three alternative MILP models ranging from a pure discrete-space to a pure continuous-space with precedence variables in both dimensions. All models are available in www.minlp.org.",
     "keywords": ["Optimization", "Integer programming", "2D strip packing", "Scheduling", "Search algorithm", "Event points"]},
    {"article name": "Optimal reconfiguration of multi-plant water networks into an eco-industrial park",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.05.004",
     "publication date": "09-2012",
     "abstract": "This paper presents an MINLP model to design an eco-industrial plant by retrofitting existing water networks from different industrial plants in the same industrial zone. The proposed model is based on a superstructure and takes into account both in-plant and inter-plant structural modifications, such as the placement/reassignment of existing treatment units, the required increase of capacity and/or efficiency of existing treatment units, the placement of new treatment units either within the participating plants or in a new shared wastewater treatment facility, and the stream re-piping associated with installing new treatment units and retrofitting existing treatment units. The model allows the tracking of changes in the process performance as a result of the stream re-routing and retrofitting activities. Two examples were solved, and the results show the economic and environmental benefits of the retrofitted networks within an eco-industrial park compared to the stand-alone retrofitted networks.",
     "keywords": ["Water networks", "Retrofit", "Eco-industrial parks", "Inter-plant integration", "Optimization", "Reconfiguration"]},
    {"article name": "A tighter cut generation strategy for acceleration of Benders decomposition",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.04.015",
     "publication date": "09-2012",
     "abstract": "This paper presents a novel strategy for speeding up the classical Benders decomposition for large-scale mixed integer linear programming problems. The proposed method is particularly useful when the optimality cut is difficult to obtain. A ratio of distances from a feasible point to an infeasible point and a feasibility cut is used as a metric to determine the tightest constraint for the region located by the feasible point, thus improving the convergence rate. Application of the proposed approach to a multi-product batch plant scheduling problem shows substantial improvement both in the computational time and the number of iterations.",
     "keywords": ["Mixed integer linear programming", "Feasibility cut", "Dinkelbach's method", "Scheduling", "Fractional programming", "CPLEX"]},
    {"article name": "Optimal design of sustainable chemical processes and supply chains: A review",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.05.006",
     "publication date": "09-2012",
     "abstract": "The importance of balancing social, environmental and economic objectives in companies\u2019 development has cultivated a growing awareness on the sustainable optimal design and planning of supply chains. In the past years, significant research effort has been devoted to extend current approaches to capture these objectives in order to guarantee long term sustainability. Among the various approaches developed, inventory management, product design, production planning and control for remanufacturing, product recovery, reverse logistics and closed-loop supply chains have gained more attention in the literature. In this paper, we review some of the relevant research on sustainable chemical processes and supply chain design focusing on three main areas: (i) sustainable supply chains with respect to energy efficiency and waste management, (ii) environmentally sustainable supply chains and (iii) sustainable water management. The emerging challenges in this area are summarized, and future opportunities are highlighted.",
     "keywords": ["Sustainability", "SCM", "Optimization", "Planning", "Design"]},
    {"article name": "Optimization strategy for long-term catalyst deactivation in a fixed-bed reactor for methanol synthesis process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.05.003",
     "publication date": "09-2012",
     "abstract": "In this study, the application of a repeated process estimation\u2013optimization strategy is investigated for a methanol synthesis reactor system in the presence of a slowly deactivating catalyst. The purpose of this strategy is to update the optimal transition of the control variable profile with feedback from the process measurements. A state and parameter estimation method is formulated where the objective is to determine the current reactor state and model parameter from available process measurements data. In this strategy, the corresponding optimization problems are formulated as DAE-constrained optimization problems that are solved by using full discretization and large-scale NLP solver. Simulation results indicate that the strategy is able to track the theoretical optimum profile of the selected control variable as the catalyst deactivates. Moreover, with the formulated strategy, the performance of the reactor system in the presence of a long-term catalyst deactivation and unexpected plant operational changes can be improved significantly.",
     "keywords": ["Dynamic optimization", "Process feedback", "Methanol synthesis", "Catalyst deactivation", "Simultaneous solution", "Interior point solver"]},
    {"article name": "A study on optimizing delivering scheduling for a multiproduct pipeline",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.05.007",
     "publication date": "09-2012",
     "abstract": "Multiproduct pipeline is one of the major ways to connect the large refineries and the consumer markets. The primary task of a pipeline operator is to make scheduling plans which can meet the demands of the delivery stations and maintain the normal operations of the pipeline. In this paper, the objective function is to minimize the total deviation of delivering operation time-windows from due time-windows at each station, with the customers\u2019 demand to be satisfied maximally within the time limits. A mathematical model of optimizing scheduling plan for each delivery station has been established and an optimal scheduling plan has been worked out through the heuristic rule. Furthermore, based on analyses, a scheduling simulation software (SSS) for a multiproduct pipeline has been developed and successfully applied to the operation and management of several multiproduct pipelines in China.",
     "keywords": ["Optimal delivering scheduling", "Multiproduct pipelines", "Time-windows"]},
    {"article name": "A comparative study of continuous-time models for scheduling of crude oil operations in inland refineries",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.05.009",
     "publication date": "09-2012",
     "abstract": "This work presents a comparative analysis of the state-of-the-art multiple time-grid representations and formulations for the crude oil scheduling problem in inland refineries. We compare the event-based model, the unit slot model and the multi-operations sequence (MOS) model. Pros and cons of different models are highlighted based on modeling and computational experiments. We also propose several extensions of the previous models. The MOS model solved the problem much faster comparing to the other two models, but failed to find the best solution of some problems.",
     "keywords": ["Crude oil scheduling", "Event-based model", "Unit slot model", "Multi-operations sequence model"]},
    {"article name": "Assessing the production of first and second generation bioethanol from sugarcane through the integration of global optimization and process detailed modeling",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.04.002",
     "publication date": "08-2012",
     "abstract": "There is a worldwide effort to make economically feasible the use of lignocellulosic biomass for production of biofuels. In sugarcane industry, cane juice (sucrose) is fermented for bioethanol production. Sugarcane bagasse is used as fuel in cogeneration systems, to produce steam and electric power to the plant, and the surplus of electric power may be delivered to the grid. The hydrolysis of bagasse to produce second generation ethanol poses a challenge: how much bagasse can be diverted, since the process must continue energetically self-sufficient. This work presents a computational tool developed within an equation-oriented process simulator that couples the simulation of first and second generation bioethanol production with a global optimization algorithm. The tool was robust, optimizing the steady state process in any economic scenario and for different process configurations. Four case studies are presented, and their implications on process internal demands and on the surplus electrical power are discussed.",
     "keywords": ["Sugarcane bioethanol", "Global optimization", "Equation based simulator", "Energy integration", "Biorefinery", "Lignocellulosic feedstock"]},
    {"article name": "An interactive multi-objective optimization framework for sustainable design of bioprocesses",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.04.011",
     "publication date": "08-2012",
     "abstract": "Interactive multi-objective optimization methods have considerable advantages over the other multi-objective optimization methods and are well suited for biochemical engineering problems.Interactive optimization is a very complex task which requires appropriate software to handle multiple objectives, displaying intermediary results and allowing the decision maker to specify preferences for each iteration.The proposed strategy combines Matlab and SuperPro Designer simulator. This way, the SuperPro Designer simulator benefits from the available toolboxes, computation, and visualization advanced features of Matlab. By linking Matlab and SuperPro Designer simulator, an optimization loop is created. This allows an automatically and repeatedly bidirectional exchange of variables between optimization algorithm from Matlab and SuperPro Designer simulator. In order to fully automate the optimization process, software based on Component Object Module technology, which contains three friendly graphical interfaces, was created.The presented strategy is implemented for a l-lysine feed supplement plant, both economic and environmental objectives being considered.",
     "keywords": ["Biochemical processes", "l-Lysine", "Simulation", "Component Object Model technology", "Interactive multi-objective optimization"]},
    {"article name": "The development of a maximum likelihood model for model-based applications",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.03.010",
     "publication date": "08-2012",
     "abstract": "Since model parameter uncertainties affect the accuracy of the model's outputs, this work describes the development of a maximum likelihood model based on robust parameter estimates to improve the model's results. A robust statistical theory framework is used to determine the robust parameter estimates. Next, it is proven that a process model parameterized by robust parameter estimates within their feasible ranges is a maximum likelihood model. A chemical reactor process is presented to demonstrate the development of the maximum likelihood model and its performance properties in a model-based predictive control framework.",
     "keywords": ["Robust state estimation", "Model predictive control", "Parameter uncertainty"]},
    {"article name": "Model order reduction of parametrized nonlinear reaction\u2013diffusion systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.03.013",
     "publication date": "08-2012",
     "abstract": "We present a model order reduction technique for parametrized nonlinear reaction\u2013diffusion systems. In our approach we combine the reduced basis method \u2013 a computational framework for rapid evaluation of functional outputs associated with the solution of parametrized partial differential equations \u2013 with the empirical interpolation method \u2013 a tool to construct \u201caffine\u201d coefficient-function approximations of nonlinear parameter dependent functions. We develop an efficient offline\u2013online computational procedure for the evaluation of the reduced basis approximation: in the offline stage, we generate the reduced basis space; in the online stage, given a new parameter value, we calculate the reduced basis output. The operation count for the online stage depends only on the dimension of the reduced order model and the parametric complexity of the problem. The method is thus ideally suited for the many-query or real-time contexts. We present numerical results for a non-isothermal reaction\u2013diffusion model to confirm and test our approach.",
     "keywords": ["Reaction\u2013diffusion equation", "Model order reduction", "Reduced basis approximation", "Empirical interpolation method", "Nonlinear systems"]},
    {"article name": "Quantification of numerical uncertainty in computational fluid dynamics modelling of hydrocyclones",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.04.009",
     "publication date": "08-2012",
     "abstract": "Large Eddy Simulations of the flow through a hydrocyclone are used to demonstrate that the Grid Convergence Index (GCI) is a practical method of accounting for numerical uncertainty. The small values of GCI (<7.2%) associated with the tangential velocity predictions suggest that numerical uncertainty due to discretization error does not greatly contribute to the disagreement between simulation and experiment in the tangential direction. The large values of GCI (<303.2%) associated with the axial velocity predictions imply that uncertainty due to discretization error is significant and further mesh refinement can yield better agreement in the axial direction. This was demonstrated through additional grid refinement which produced a reduction in the GCI of as much as 256.6% and a drop in the overall average difference between simulation and experimental of more than 36%. Overall, these results suggest the GCI is a useful tool for quantifying numerical uncertainty in CFD simulations.",
     "keywords": ["Hydrocyclone", "Computational fluid dynamics", "Numerical uncertainty", "Grid Convergence Index"]},
    {"article name": "Computing the correlation between catalyst composition and its performance in the catalysed process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.04.001",
     "publication date": "08-2012",
     "abstract": "The methodology for computing correlations between continuous descriptors of catalytic materials and their performance in the catalysed process is addressed. Continuous descriptors are typically molar fractions of individual components of the catalyst, whereas the performance is represented most frequently by yield or selectivity of reaction products or conversion of key feed components. Measures of various kinds of correlation are recalled, and their descriptor-wise application to catalytic data for computing correlations between the composition and performance of catalysts is presented. The paper also compares the application of correlation measures to catalytic data on the one hand with the analysis of variance, on the other hand with the application of regression trees. As a case study, the presented approaches are applied to data from high-temperature synthesis of hydrocyanic acid.",
     "keywords": ["Catalysed process", "Catalyst performance", "Correlation measures", "Estimating correlation value", "Analysis of variance", "Regression trees"]},
    {"article name": "A reduced-order model for heat transfer in multiphase flow and practical aspects of the proper orthogonal decomposition",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.04.003",
     "publication date": "08-2012",
     "abstract": "This paper discusses two practical aspects of reduced-order models (ROMs) based on proper orthogonal decomposition (POD) and presents the derivation and implementation of a ROM for non-isothermal multiphase flow. The POD method calculates basis functions for a reduced-order representation of two-phase flow by calculating the eigenvectors of an autocorrelation matrix composed of snapshots of the flow. The flow is divided into transient and quasi-steady regions and two methods are shown for clustering snapshots in the transient region. Both methods reduce error as compared to the constant sampling case. The ROM for non-isothermal flow was developed using numerical results from a full-order computational fluid dynamics model for a two-dimensional non-isothermal fluidized bed. Excellent agreement is shown between the reduced- and full-order models. The composition of the autocorrelation matrix is also considered for an isothermal case. An approach treating field variables separately is shown to produce less error than a coupled approach.",
     "keywords": ["Proper orthogonal decomposition", "Reduced-order modeling", "Multiphase flow", "Computational methods in fluid dynamics"]},
    {"article name": "Dynamic data reconciliation based on node imbalance autocovariance functions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.04.004",
     "publication date": "08-2012",
     "abstract": "To reduce impacts of measurement errors on plant variables, data reconciliation is widely applied in process industries. Reconciled measurements are used in applications such as performance monitoring, process control, or real-time optimization. However, precise estimation generally relies on accurate and detailed process models which could be difficult to build in practice. The trade-off between estimate precision and model complexity is a relevant challenge motivating the development of effective observers with limited modeling efforts. This paper proposes a data reconciliation method based on a simple mass and/or energy conservation sub-model that also considers the autocovariance function of plant node imbalances. The observer is applied to simulated benchmark plants and its performance is evaluated in terms of variance reduction and robustness against modeling errors. Results show a superior performance in comparison with classical sub-model based methods and reveal less performance degradation than the Kalman filter in presence of model uncertainties.",
     "keywords": ["Observer", "Data reconciliation", "Mass balance", "Node imbalance", "Autocovariance function"]},
    {"article name": "Optimal design and control of dynamic systems under uncertainty: A probabilistic approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.03.015",
     "publication date": "08-2012",
     "abstract": "This paper presents a new methodology for the simultaneous design and control of systems under random realizations in the disturbances. The key idea in this work is to perform a distribution analysis on the worst-case variability. Normal distribution functions, which approximate the actual distribution of the worst-case variability, are used to estimate the largest variability expected for the process variables at a user-defined probability limit. The resulting estimates in the worst-case variability are used to evaluate the process constraints, the system's dynamic performance and the process economics. The methodology was applied to simultaneously design and control a Continuous Stirred Tank Reactor (CSTR) process. A study on the computational demands required by the present method is presented and compared with a dynamic optimization-based methodology. The results show that the present methodology is a computationally efficient and practical tool that can be used to propose attractive (economical) process designs under uncertainty.",
     "keywords": ["Process design", "Process control", "Monte Carlo sampling", "Uncertainty"]},
    {"article name": "CFD simulation of transient stage of continuous countercurrent hydrolysis of canola oil",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.04.008",
     "publication date": "08-2012",
     "abstract": "Computational Fluid Dynamic (CFD) modeling of a continuous countercurrent hydrolysis process was performed using ANSYS-CFX. The liquid properties and flow behavior such as density, specific heats, dynamic viscosity, thermal conductivity, and thermal expansivity as well as water solubility of the hydrolysis components triglyceride, diglyceride, monoglyceride, free fatty acid, and glycerol were calculated. Chemical kinetics for the hydrolysis reactions were simulated in this model by applying Arrhenius parameters. The simulation was based on actual experimental reaction conditions including temperature and water-to-oil ratio. The results not only have good agreement with experimental data but also show instantaneous distributions of concentrations of every component in hydrolysis reaction. This model provided visible insight into the continuous countercurrent hydrolysis process.",
     "keywords": ["CFD", "Continuous hydrolysis", "Triglycerides", "Free fatty acid", "Biofuel production"]},
    {"article name": "pH control structure design for a periodically operated membrane separation process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.03.011",
     "publication date": "08-2012",
     "abstract": "A bioreactor integrated with an electrically driven membrane separation process (Reverse Electro-Enhanced Dialysis \u2013 REED) is under investigation as potential technology for intensifying lactic acid bioproduction. In this contribution the pH regulation issue in the periodically operated REED module is studied. A methodology for control structure design is proposed to handle the dynamic system. A sensitivity analysis is used for the conceptual design of the control structure. Dynamic simulations are employed to evaluate the sensitivity index. From the analysis a periodic input-resetting control structure is selected. The system controls pH using the imposed current density and resets the current density manipulating the hydroxide inlet concentration to the dialysate channel. The control structure is satisfactorily achieving a desired pH at the outlet of the feed channel in REED from period to period and resetting the current density. Thus suitable performance is achieved within a large part of the operating window.",
     "keywords": ["Reverse Electro-Enhanced Dialysis", "Control structure design", "Periodic system control", "Lactate recovery from fermentation broth"]},
    {"article name": "On a multivariate population balance model to describe the structure and composition of silica nanoparticles",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.04.010",
     "publication date": "08-2012",
     "abstract": "The aim of this work is to present the mathematical description of a detailed multivariate population balance model to describe the structure and composition of silica nanoparticles. A detailed numerical study of a stochastic particle algorithm for the solution of the multidimensional population balance model is presented. Each particle is described by its constituent primary particles and the connectivity between these primaries. Each primary in turn has internal variables that describe its chemical composition. The algorithms used to solve the population balance equations and to couple the population balance model to gas-phase chemistry are described. Numerical studies are then performed for a number of functionals calculated from the model to establish the convergence with respect to the numerical parameter that determines the number of computational particles in the system. The computational efficiency of the model is found to render it applicable to the simulation of industrial scale systems.",
     "keywords": ["Silica nanoparticle", "Population balance model", "Stochastic particle method", "Convergence"]},
    {"article name": "Improved three-index unit-specific event-based model for short-term scheduling of batch plants",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.03.014",
     "publication date": "08-2012",
     "abstract": "Short-term scheduling of batch operations has been an important research area in the past two decades. Recently, Shaik and Floudas (Industrial and Engineering Chemistry Research, 2009, 48, 2947) proposed a novel unified model for short-term scheduling using unit-specific event-based continuous-time representation employing three-index binary and continuous variables. In their comparative study it was shown that the unified model has superior performance among all the models compared in their work. In this work, we present an improved model by taking advantage of the three-index variables and effectively incorporating the concept of active task leading to fewer constraints and big-M terms. We propose improvements in allocation, duration and sequencing constraints, and investigate the effect of big-M terms in several constraints to enable solution of large-scale problems. The computational performance of the proposed formulation is compared with the original three-index model through various examples drawn from literature including Westenberger\u2013Kallrath challenging benchmark problem.",
     "keywords": ["Short-term scheduling", "Continuous-time formulation", "Active task", "Unlimited intermediate storage (UIS)", "Finite intermediate storage (FIS)"]},
    {"article name": "Optimization of extrusion production lines for EPDM rubber vulcanized with sulphur: A two-phase model based on Finite Elements and kinetic second order differential equation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.04.006",
     "publication date": "08-2012",
     "abstract": "A numerical two-phase approach, based on experimental curometer charts and aimed at predicting the optimal production line parameters (exposition time and cure temperature) for extruded thick rubber items cured with accelerated sulphur is presented.In the first phase, a simple kinetic model based on the actual reticulation reactions occurring during sulphur curing is utilized to fit experimental curometer data. The model is able to predict the degree of crosslinking at successive curing times and at different controlled temperatures and it requires the calibration of only three kinetic constants. The variation of such parameters with temperature is then evaluated by means of three experimental cure curves performed at three different temperatures. Both the case of indefinite increase of the torque and reversion can be handled.In the second phase, considering the same rubber compound of step one, kinetic reaction parameters are implemented in a Finite Element (FE) software, specifically developed to perform thermal analyses on complex 2D geometries. As an example, an extruded cylindrical thick EPDM item is considered and meshed through four-noded isoparametric plane elements. Several FE simulations are repeated by changing exposition time tc and external cure temperature Tn, to evaluate for each (tc,Tn) couple the corresponding mechanical properties of the item at the end of the thermal treatment. An alternating tangent approach (AT) is used to drastically reduce the computational efforts required to converge to the optimal solution associated with the maximization of the average tensile strength.",
     "keywords": ["Optimization of production lines", "Sulphur vulcanization", "Reversion", "Rheometer curve fitting", "Fourier's law of heat conduction", "Finite Element method (FEM)"]},
    {"article name": "Optimal sensor deployment in a large-scale complex drinking water network: Comparisons between a rule-based decision support system and optimization models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.03.012",
     "publication date": "08-2012",
     "abstract": "Many models or algorithms have been suggested for sensor placement in the drinking water distribution networks, such as genetic algorithms, multiobjective optimization models, and heuristic methods. Because these models or algorithms have high computational demands, however, the requirement of expensive technical computing software is unavoidable. This study presents a rule-based decision support system (RBDSS) to analyze and generate a set of sensor placement locations and compares the performance against 10 optimization models based on four indexes. Our findings show that the RBDSS demands relatively lower computational time and still exhibits outstanding performance in terms of all our indexes when dealing with a large-scale complex drinking water network.",
     "keywords": ["Rule-based decision support system", "Sensor deployment", "Water distribution system", "Graph theory", "Systems analysis"]},
    {"article name": "Development of a synthesis tool for Gas-To-Liquid complexes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.12.005",
     "publication date": "07-2012",
     "abstract": "Optimal synthesis of a Gas-To-Liquid complex is complicated due to many degrees of freedom in a highly constrained design space. One can choose between alternative, competing syngas manufacturing technologies, different types of Fischer\u2013Tropsch catalysts and reactors, with numerous connectivity options and a range of operational conditions. On the other hand, the design space is confined by equipment, operational and knowledge constraints. Furthermore, economic performance needs to be aligned with carbon and energy efficiencies. To support GTL process design a computational synthesis tool is under development. Its purpose is to find and analyse the optimum structure and operational conditions for a given market scenario. The process model covers alternative syngas generation units and Fischer\u2013Tropsch reactors with an extensive syngas recycle structure. The process units interact with the utility system, where power can be generated from off-gas and/or excess steam. The units are modelled in a reduced, input\u2013output way by algebraic equations, reflecting mass and energy balances and pressure effects. A superstructure arises when considering multiple stages for Fischer\u2013Tropsch synthesis with parallel reactors. The synthesis tool, implemented in AIMMS\u00ae, is applied to a realistic sample problem, showing profit optimisation by varying the distribution of NG to syngas generation units with different efficiencies. A sensitivity analysis is carried out by means of Singular Value Decomposition of sensitivity matrices to find dominant patterns of parametric influence on the optimum.",
     "keywords": ["Syngas manufacturing", "Fischer\u2013Tropsch", "Process synthesis", "Non-linear optimisation"]},
    {"article name": "A perspective on PSE in pharmaceutical process development and innovation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.02.022",
     "publication date": "07-2012",
     "abstract": "The pharmaceutical industry is under growing pressure to increase efficiency, both in production and in process development. This paper discusses the central role of Process Systems Engineering (PSE) methods and tools in pharmaceutical process development and innovation, and searches for answers to questions such as: Which PSE methods can be applied readily? Where is more method development needed? The paper covers key subjects for development of economically and environmentally sustainable pharmaceutical processes, including Process Analytical Technology in its broadest sense, continuous pharmaceutical manufacturing and green processes, and is illustrated with a series of short examples taken from the literature and ongoing research projects.",
     "keywords": ["ANN artificial neural network", "artificial neural network", "API active pharmaceutical ingredient", "active pharmaceutical ingredient", "CAMD computer-aided molecular design", "computer-aided molecular design", "CFD computational fluid dynamics", "computational fluid dynamics", "CP continuous processing", "continuous processing", "CPM continuous pharmaceutical manufacturing", "continuous pharmaceutical manufacturing", "CPP critical process parameter", "critical process parameter", "CQA critical quality attribute", "critical quality attribute", "DEM discrete element method", "discrete element method", "DoE design of experiments", "design of experiments", "FEM finite element method", "finite element method", "iPLS interval PLS", "interval PLS", "LCA life cycle assessment", "life cycle assessment", "LLE liquid\u2013liquid extraction", "liquid\u2013liquid extraction", "MSPC multivariate statistical process control", "multivariate statistical process control", "MW molecular weight", "molecular weight", "NCE new chemical entity", "new chemical entity", "NIR near infra-red", "near infra-red", "OED optimal experimental design", "optimal experimental design", "PAT Process Analytical Technology", "Process Analytical Technology", "PBM population balance model", "population balance model", "PCA Principal Component Analysis", "Principal Component Analysis", "PDE partial differential equation", "partial differential equation", "PLS Partial Least Squares", "Partial Least Squares", "PMI process mass intensity", "process mass intensity", "PSE Process Systems Engineering", "Process Systems Engineering", "QbD quality by design", "quality by design", "Continuous pharmaceutical manufacturing", "Design space", "PAT", "Pharmaceutical production", "Process development", "Sustainability"]},
    {"article name": "An integrated approach for dynamic flowsheet modeling and sensitivity analysis of a continuous tablet manufacturing process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.02.015",
     "publication date": "07-2012",
     "abstract": "Manufacturing of powder-based products is a focus of increasing research in the recent years. The main reason is the lack of predictive process models connecting process parameters and material properties to product quality attributes. Moreover, the trend towards continuous manufacturing for the production of multiple pharmaceutical products increases the need for model-based process and product design. This work aims to identify the challenges in flowsheet model development and simulation for solid-based pharmaceutical processes and show its application and advantages for the integrated simulation and sensitivity analysis of two tablet manufacturing case studies: direct compaction and dry granulation. The developed flowsheet system involves a combination of hybrid, population balance and data-based models. Results show that feeder refill fluctuations propagate downstream and cause fluctuations in the mixing uniformity of the blend as well as the tablet composition. However, this effect can be mitigated through recycling. Dynamic sensitivity analysis performed on the developed flowsheet, classifies the most significant sources of variability, which are material properties such as mean particle size and bulk density of powders.",
     "keywords": ["Dynamic flowsheet simulation", "Pharmaceutical manufacturing", "Sensitivity analysis", "Population balance modeling"]},
    {"article name": "A superstructure optimization approach for water network synthesis with membrane separation-based regenerators",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.02.020",
     "publication date": "07-2012",
     "abstract": "This work addresses the problem of water network synthesis. We propose a superstructure with fixed topology for a water network that consists of three layers, similar to a pooling problem: sources for reuse/recycle; regenerators for contaminants removal; and sinks for acceptance of water for reuse/recycle. The superstructure encompasses multiple freshwater sources, membrane separation-based partitioning regenerators of the industrially favored ultrafiltration and reverse osmosis, and sinks for incineration and deep ocean discharge. A mixed-integer nonlinear program is formulated based on this superstructure to determine the optimal interconnections in terms of total flowrates and contaminant concentrations. The main decisions include determining the split fractions of the source flowrates, extents of regeneration, and mixing ratios of the sources and regenerated streams subject to compliance with the maximum allowable inlet contaminant concentration limits of the sinks and discharge regulations. We also develop linear models for the membrane regenerators that admit a more general expression for the retentate stream concentration based on liquid-phase recovery factors and removal ratios. Computational studies are performed using GAMS/BARON on an industrially significant case study of a petroleum refinery water system. We incorporate linear logical constraints using 0\u20131 variables that enforce certain design and structural specifications to tighten the model formulation and enhance solution convergence. A globally optimal water network topology is attained that promotes a 27% savings equivalent to about $218,000/year reduction in freshwater use.",
     "keywords": ["Optimization", "Water reuse", "Superstructure", "Mixed-integer nonlinear programming (MINLP)", "Membrane", "Pooling problem"]},
    {"article name": "Global optimization of a MINLP process synthesis model for thermochemical based conversion of hybrid coal, biomass, and natural gas to liquid fuels",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.03.008",
     "publication date": "07-2012",
     "abstract": "A global optimization framework is proposed for a thermochemical based process superstructure to produce a novel hybrid energy refinery which will convert carbon-based feedstocks (i.e., coal, biomass, and natural gas) to liquid transportation fuels. The mathematical model for process synthesis includes simultaneous heat, power, and water integration and is formulated as a mixed-integer nonlinear optimization (MINLP) problem with nonconvex functions. The MINLP model is large-scale and includes 15,439 continuous variables, 30 binary variables, 15,406 equality constraints, 230 inequality constraints, and 335 nonconvex terms. The nonconvex terms arise from 274 bilinear terms, 1 quadrilinear term, and 60 concave cost functions. The proposed framework utilizes piecewise linear underestimators for the nonconvex terms to provide tight relaxations when calculating the lower bound. The bilinear terms are relaxed using a partitioning scheme that depends logarithmically on the number of binary variables, while the concave functions are relaxed using a linear partitioning scheme. The framework was tested on twelve case studies featuring three different plant capacities and four different feedstock-carbon conversion percentages and is able to solve each study to within a 3.22\u20138.56% optimality gap after 100 CPU hours. For 50% feedstock carbon conversion, the proposed global optimization framework shows that the break-even oil prices for liquid fuels production are $61.36/bbl for the small case study, $60.45/bbl for the medium case study, and $55.43/bbl for the large case study, while the corresponding efficiencies are 73.9%, 70.5%, and 70.1%, respectively.",
     "keywords": ["Global optimization", "Piecewise linear underestimators", "Heat, power and water integration", "Hybrid energy systems", "Mixed-integer nonlinear optimization", "Fischer\u2013Tropsch"]},
    {"article name": "Sustainable synthesis of biogas processes using a novel concept of eco-profit",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.01.010",
     "publication date": "07-2012",
     "abstract": "The objective of this contribution is to perform the sustainable mixed-integer linear programming (MILP) synthesis of biogas processes based on life cycle assessment (LCA). An aggregated model previously developed by authors for the efficient optimisation of biogas processes has been upgraded with LCA, using the novel concept of eco-profit. Eco-profit is defined as the difference between burdening (eco-cost) and unburdening (eco-benefit) the environment, where eco-cost and eco-benefit calculations are based on LCA. The advantage of eco-profit is that it is expressed as a monetary value. Therefore, eco-profit and economic profit can be merged together and the preferred solutions are those with maximal total profit. The single-and multi-objective optimisations were performed on a model of the biogas production processes. Within the former, economic, eco- and total profit were maximised separately and, within the latter, maximisation of economic profit vs. eco-profit was introduced. All the results obtained from single- and multi-criteria optimisation show that biogas production is a sustainable alternative that provides an important eco-profit.",
     "keywords": ["Biogas", "LCA", "Eco-profit", "Eco-cost", "Eco-benefit", "Optimisation"]},
    {"article name": "An optimisation framework for a hybrid first/second generation bioethanol supply chain",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.01.012",
     "publication date": "07-2012",
     "abstract": "Assessment of both economical and environmental performance of biofuel supply chains is crucial to have a complete view of the future implications of those systems. This paper presents a multi-objective, static modelling framework for the optimisation of hybrid first/second generation biofuel supply chains. Using the proposed modelling framework, different aspects are analysed including the potential GHG savings, the impact of carbon tax on the economic and environmental performance of a biofuel supply chain, the trade-off between the economic and environmental objectives and the maximum bioethanol throughput that can be achieved at different cap levels on the total supply chain cost. The trade-off between the conflicting objectives is analysed by solving the proposed multi-objective model using the \u025b-constraint method. In addition, the impact of technological learning on the economic and environmental performance of the supply chain throughout time is also analysed using a multi-period model developed based on the proposed static optimisation framework. Bioethanol production in the UK using hybrid first/second generation technologies is considered as the case study to highlight the model applicability.",
     "keywords": ["Bioethanol supply chains", "Hybrid first/second generation biofuels", "Multi-objective optimisation"]},
    {"article name": "A framework for model-based optimization of bioprocesses under uncertainty: Lignocellulosic ethanol production case",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.12.004",
     "publication date": "07-2012",
     "abstract": "This study presents the development and application of a systematic model-based framework for bioprocess optimization. The framework relies on the identification of sources of uncertainties via global sensitivity analysis, followed by the quantification of their impact on performance evaluation metrics via uncertainty analysis. Finally, stochastic programming is applied to drive the process development efforts forward subject to these uncertainties. The framework is evaluated on four different process configurations for cellulosic ethanol production including simultaneous saccharification and co-fermentation and separate hydrolysis and co-fermentation (SSCF and SHCF, respectively) technologies in different operation modes (continuous and continuous with recycle). The results showed that parameters related to pretreatment (e.g. activation energy of the reaction for glucose production, order of the reaction, etc.), hydrolysis (inhibition constant for xylose on conversion of cellulose and cellobiose, etc.) and co-fermentation (ethanol yield on xylose, inhibition constant on microbial growth, etc.), are the most significant sources of uncertainties affecting the unit production cost of ethanol with a standard deviation of up to 0.13\u00a0USD/gal-ethanol. Further stochastic optimization demonstrated the options for further reduction of the production costs with different processing configurations, reaching a reduction of up to 28% in the production cost in the SHCF configuration compared to the base case operation. Further, the framework evaluated here for uncertainties in the technical domain, can also be used to evaluate the impact of market uncertainties (feedstock prices, selling price of ethanol, etc.) and political uncertainties (such as subsidies) on the economic feasibility of lignocellulosic ethanol production.",
     "keywords": ["Uncertainty analysis", "Sensitivity analysis", "Stochastic optimization", "Bioethanol production", "Monte-Carlo simulations"]},
    {"article name": "Biomass to chemicals: Design of an extractive-reaction process for the production of 5-hydroxymethylfurfural",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.12.013",
     "publication date": "07-2012",
     "abstract": "Furanic compounds such as 5-hydroxymethylfurfural (HMF) can be obtained from sugars and have the potential to serve as substitutes of petroleum derived building blocks in the production of fuels and chemicals. In this work, we propose a process for the production of HMF from fructose based on extractive-reaction and formulate an optimization problem in order to find the operating conditions that minimize its cost of production.",
     "keywords": ["Biorefinery", "HMF", "Process design"]},
    {"article name": "Online model-based redesign of experiments with erratic models: A disturbance estimation approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.11.014",
     "publication date": "07-2012",
     "abstract": "Model-based design of experiment (MBDoE) techniques are a useful tool to maximise the information content of experimental trials when the purpose is identifying the set of parameters of a deterministic model in a statistically sound way. In a conventional MBDoE procedure, the information gathered during the evolution of an experiment is exploited only at the end of the experiment itself. Conversely, online model-based redesign of experiment (OMBRE) techniques have been recently proposed to exploit the information as soon as it is generated by the running experiment, allowing for the dynamic update of the experimental conditions to yield the most informative data in order to improve the parameter identification task. However, the effectiveness of MBDoE strategies (including OMBRE) may be severely affected by the presence of systematic modelling errors as well as by disturbances acting on the system. In this paper, a novel experiment design approach (DE-OMBRE) is presented, where a model updating policy including disturbance estimation (DE) is embedded within an OMBRE strategy in order to achieve a statistically satisfactory estimation of the model parameters as well as to estimate the possible discrepancy between the real system and the model being identified. The procedure allows reducing (or even avoiding) constraint violations, preserving the optimality of the redesign even in the presence of systematic errors and/or unknown disturbances acting on the system. Two simulated case studies of different levels of complexity are used to illustrate the benefits of the novel approach.",
     "keywords": ["Model-based design of experiment", "Parameter estimation", "Disturbance estimation", "Model updating", "Model identification"]},
    {"article name": "A new heuristic for plant-wide schedule coordination problems: The intersection coordination heuristic",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.12.014",
     "publication date": "07-2012",
     "abstract": "This work addresses the coordination heuristics of two large-scale flexible multi-stage batch (flow shop) scheduling problems, which are currently solved independently by tailored algorithms that consist of mixed-integer linear optimization and heuristics. The approach is motivated by an industrial-scale steel making process that consists of a melt shop as the first production section and a hot rolling mill as the second production section. The first section produces intermediate products, i.e. slabs, which are stored in an intermediate storage area and which are consumed by the second section. The coordination objective is to reduce the storage time of the slabs or, more generally, the storage time between two sections taking into account the objectives of the two distributed scheduling solutions. A generic model formulation for multi-stage batch schedule coordination problems is presented. Three different schedule coordination heuristics are discussed and compared for simplified case studies: the heuristic based on Lagrangean decomposition (LD), a derivative-free optimization algorithm \u2013 Multilevel Coordinate Search (MCS), the new intersection coordination heuristic (IC) and monolithic MILP model solved by CPLEX (MONO). The proposed bi-level intersection coordination heuristic uses the knowledge on the bottlenecks of the two processes to build a simplified model for the upper-level coordinator which coordinates the lower-level distributed MILP schedulers via coordination variables iteratively. The numerical comparisons show the advantage of the IC among these three coordination approaches in terms of solution quality and computational effort. The limitations of LD, MCS, MONO and IC for the plant-wide schedule coordination problem are discussed in view of the requirements of large-scale industrial schedule coordination problems.",
     "keywords": ["Production operation", "Plant-wide planning and scheduling", "MILP", "CPLEX", "Coordination", "Horizontal integration", "Large-scale scheduling problems", "Integrated steel production management"]},
    {"article name": "Integrated supply chain planning for multinational pharmaceutical enterprises",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.03.002",
     "publication date": "07-2012",
     "abstract": "The management of global supply chains is highly complex and vital for multinational pharmaceutical enterprises. Global integrated planning in multi-site, multi-echelon network of a multinational company has attracted some academic interest. However, the focus has largely been on efficient solution strategies for large problems. In this work, we develop simple yet powerful MILP model for multi-period enterprise-wide planning. We represent the entire enterprise in a seamless fashion with a granularity of individual task campaigns on each production line. Our model integrates procurement, production, and distribution along with the effects of international tax differentials, inventory holding costs, material shelf-lives, waste treatment / disposal, and other real-life factors on the after-tax profit of the company. To demonstrate the performance of our model, we solve two example problems of planning multinational pharmaceutical enterprise. For our evaluation, we consider an industrial scale planning problem for a supply chain network consisting of 34 different entities and producing 9 different products, for a period of 5 years.",
     "keywords": ["Integrated planning", "Pharmaceutical supply chains", "Multinational enterprises", "Production planning", "Procurement", "Distribution", "Transfer prices"]},
    {"article name": "Improving supply chain planning in a competitive environment",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.03.009",
     "publication date": "07-2012",
     "abstract": "This work extends the use of a Mixed Integer Linear Programming (MILP) model, devised to optimize the Supply Chain planning problem, for decision making in cooperative and/or competitive scenarios, by integrating these models with the use of the Game Theory. The system developed is tested in a case study based in previously proposed Supply Chain, adapted to consider the operation of two different Supply Chains (multi-product production plants, storage centers, and distribution to the final consumers); two different optimization criteria are used to model both the Supply Chains benefits and the customer preferences, so both cooperative and non-cooperative way of working between both Supply Chains can be considered.",
     "keywords": ["Supply chain planning", "MILP-based model", "Game theory", "Competitive supply chains"]},
    {"article name": "A comprehensive constraint programming approach for the rolling horizon-based scheduling of automated wet-etch stations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.01.005",
     "publication date": "07-2012",
     "abstract": "This contribution introduces a novel and comprehensive way of dealing with the automated wet-etch station (AWS) scheduling problem. It first analyzes the field and points out some very important problem aspects that have not been properly addressed yet. Then, an expressive constraint programming (CP) formulation that considers all types of AWS transfer device movements is proposed. The model accounts for both (i) loaded trips the robot makes to transfer wafer lots between consecutive baths, and (ii) empty movements that take place when the device moves itself from a bath, where it has dropped a wafer lot, to another bath where it is required to pick up a different lot. The CP approach is afterward generalized in order to implement an innovative rolling horizon methodology. The proposed method allows the continuous operation of the wet-etch station, minimizing the unproductive intervals that would otherwise appear between the scheduling periods that correspond to different wafer lot sets that are fed by the previous manufacturing step. The formulation has been extensively tested with problem instances of various dimensionalities in productivity maximization scenarios, in which makespan was chosen as the performance measure. The results have demonstrated that robot unloaded movements cannot be neglected; otherwise, they may lead to incorrect schedules. Furthermore, they have shown that AWS scheduling needs to be addressed as an on-line activity.",
     "keywords": ["Automated wet-etch station", "Resource-constrained scheduling", "Constraint-programming", "Semiconductor manufacturing systems", "Multiproduct batch plants"]},
    {"article name": "Efficient mathematical frameworks for detailed production scheduling in food processing industries",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.12.015",
     "publication date": "07-2012",
     "abstract": "The production scheduling of a real-world multistage food process is considered in this work. An efficient mixed integer programming (MIP) continuous-time model is proposed to address the production problem under study. The overall mathematical framework relies on an efficient modeling approach of the sequencing decisions, the integrated modeling of all production stages, and the inclusion of a set of strong tightening constraints. The simultaneous optimization of all processing stages aims at facilitating the interaction among the different departments of the production facility. Moreover, an alternative MIP-based solution strategy is proposed for dealing with large-scale food processing scheduling problems. Although this method may no guarantee global optimality, it favors low computational requirements and solutions of very good quality. Several problem instances are solved to reveal the salient computational performance and the practical benefits of the proposed MIP formulation and solution strategy.",
     "keywords": ["Production scheduling", "Mixed integer programming", "Food industry", "Ice-cream production"]},
    {"article name": "Ontological framework for enterprise-wide integrated decision-making at operational level",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.02.001",
     "publication date": "07-2012",
     "abstract": "In the domain of chemical process engineering, there is an increased interest in the integration of the enterprise hierarchical levels for decision-making purposes. At the scheduling level, decisions on the allocation of tasks to resources, sequencing and timing of tasks must be managed. However, such decisions are directly related to other enterprise actions, such as control and planning, but they are difficult to coordinate because they are modeled at different time and space scales, and their goals are not the same. In order to achieve integrated decisions supported by high quality information, there is a need to improve and develop robust computational tools and consistent models. In general, scheduling optimization approaches for decision-making differ depending on problem features, such as physical layout or time representation. Therefore, this work focuses on providing a framework based on a semantic model that captures the diversity in scheduling problem representation. Such semantic model uses the master recipe concept from the ANSI/ISA-88 standard perspective and encapsulates the scheduling decision task features. As a result, by the use of a single representation approach, any scheduling problem can be modeled and solved by its adequate optimization tool. The potential of a general model representation is presented by means of several case studies related to the scheduling function. Such case studies shed light to the model capabilities to represent different kinds and particular scheduling problems, achieving integration at the different decision support levels.",
     "keywords": ["Process scheduling", "Multiproduct plant", "Multipurpose plant", "Ontology", "Decision-levels integration"]},
    {"article name": "Modelling of pipe bend erosion by dilute particle suspensions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.02.006",
     "publication date": "07-2012",
     "abstract": "A computational fluid dynamic model coupled to a Lagrangian particle tracking routine and a number of erosion models have been used to predict the solid particle erosion in four different 90\u00b0 square cross-section bends for dilute particle-laden flow. Comparisons with experimental data over a range of conditions, including particle size, mass loading and flow Reynolds number, demonstrate good agreement with predictions of all the erosion models considered. In particular, the models are able to yield reliable predictions of erosion depth and wear location on the concave and convex walls of the bends. The slight disparities observed are most likely due to uncertainties in the particle\u2013wall collision model employed, and variability in the data. The complexity of the particle\u2013wall collision process precluded the development in this work of a model which accounted for the occurrence of erosion pockets and their subsequent influence on the fluid flow, particle trajectories and the magnitude of wear at secondary wear locations, as well as the dynamic movement of the maximum wear location. Despite this, however, and within experimental uncertainty, the models perform well, although their further extension as noted could potentially lead to improved agreement with data.",
     "keywords": ["Solid particle erosion", "Gas\u2013solid flow", "90\u00b0 Duct bend", "CFD"]},
    {"article name": "Simultaneous design of ionic liquid entrainers and energy efficient azeotropic separation processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.02.021",
     "publication date": "07-2012",
     "abstract": "A methodology and tool set for the simultaneous design of ionic liquid entrainers and azeotropic separation processes is presented. By adjusting the cation, anion, and alkyl chain length on the cation, the properties of the ionic liquid can be adjusted to design an entrainer for a given azeotropic mixture. Several group contribution property models available in literature have been used along with a newly developed group contribution solubility parameter model and UNIFAC model for ionic liquids (UNIFAC-IL). For a given azeotropic mixture, an ionic liquid is designed using a computer-aided molecular design (CAMD) method and the UNIFAC-IL model is used to screen design candidates based on minimum ionic liquid concentration needed to break the azeotrope. Once the ionic liquid has been designed, the extractive distillation column for the azeotropic mixture is designed using the driving force method with a new proposed feed stage scaling to minimize energy inputs. Along with the distillation column, an ionic liquid recovery stage is designed and simulations are used to determine the overall heat duty for the entire process for the best ionic liquid candidates. Use of a designed ionic liquid reduces material and energy requirements when compared to an ionic liquid known to experimentally break a given azeotrope but not designed using CAMD methods. The acetone\u2013methanol and ethanol\u2013water azeotropes are provided as examples.",
     "keywords": ["Computer-aided molecular design", "Ionic liquids", "Azeotropic separation processes"]},
    {"article name": "Novel MILP-based iterative method for the retrofit of heat exchanger networks with intensified heat transfer",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.02.002",
     "publication date": "07-2012",
     "abstract": "Heat transfer intensification is an effective technique for improving energy recovery in heat exchanger networks (HEN) by enhancing heat transfer without any network topology modifications. In this paper, a novel optimization method has been developed for the synthesis of intensified heat exchanger networks with a simple mixed integer linear programming (MILP) model under retrofit scenario, and a robust solution strategy based on two iteration loops has been proposed. The new method has distinctive advantages over existing design methods, as the new MILP model can effectively and significantly reduce computational difficulties associated with nonlinear formulation in existing HEN retrofit formulations. Three cases have been tested to demonstrate the validity and efficiency of the proposed approach, which requires very short computational times to obtain optimal solutions when energy saving or profit is to be maximised through retrofit.",
     "keywords": ["Heat exchanger network (HEN)", "Retrofit", "Intensified heat transfer (IHT)", "Nonlinearity", "Mixed integer linear programming (MILP)"]},
    {"article name": "Combined model approximation techniques and multiparametric programming for explicit nonlinear model predictive control",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.01.009",
     "publication date": "07-2012",
     "abstract": "This work presents a methodology to derive explicit multiparametric controllers for nonlinear systems, combining model approximation techniques and multiparametric model predictive control (mp-MPC) algorithms. Particular emphasis is given to an approach that applies a nonlinear model reduction technique, based on balancing of empirical gramians, which generates a reduced order model suitable for nonlinear mp-MPC algorithms. This approach is compared with a recently proposed method that uses a meta-modelling based model approximation technique which can be directly combined with standard multiparametric programming algorithms. The methodology is illustrated for two nonlinear models, of a distillation column and a train of CSTRs, respectively.",
     "keywords": ["Multiparametric programming", "Nonlinear MPC", "Explicit MPC", "Nonlinear model reduction", "Model approximation"]},
    {"article name": "Approximate multi-parametric programming based B&B algorithm for MINLPs",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.03.001",
     "publication date": "07-2012",
     "abstract": "In this work an improved B&B algorithm for MINLPs is proposed. The basic idea of the proposed algorithm is to treat binary variables as parameters and obtain the solution of the resulting multi-parametric NLP (mp-NLP) as a function of the binary variables, relaxed as continuous variables, at the root node of the search tree. It is recognized that solving the mp-NLP at the root node can be more computationally expensive than exhaustively enumerating all the terminal nodes of the tree. Therefore, only a local approximate parametric solution, and not a complete map of the parametric solution, is obtained and it is then used to guide the search in the tree.",
     "keywords": ["Parametric programming", "Branch & Bound", "MINLP"]},
    {"article name": "State-of-charge estimation for lithium-ion batteries under various operating conditions using an equivalent circuit model",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.02.003",
     "publication date": "06-2012",
     "abstract": "This paper describes a state-of-charge estimation methodology for lithium-ion batteries in hybrid electric vehicles. The proposed methodology is intended for SOC estimation under various operating conditions including changes in temperature, driving mode or power duty. The suggested methodology consists of a recursive estimator and employs an equivalent circuit as the electrochemical cell model. Model parameters are estimated by parameter map on experimental cell data with various temperatures and current conditions. The parameter map is developed by a least sum square error estimation method based on nonlinear programming. An adaptive estimator is employed and is based on the combination of current integration and battery model based estimation. The proposed SOC estimation methodology is demonstrated with experimental LiB pack data under various driving schedules with low and ambient temperature and sensor failure cases. Our results show that the proposed methodology is appropriate for estimating SOC under various conditions.",
     "keywords": ["State-of-charge", "Lithium-ion battery", "Equivalent circuit model", "Least squares estimator"]},
    {"article name": "Nonlinear extended output feedback control for CSTRs with van de Vusse reaction",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.02.010",
     "publication date": "06-2012",
     "abstract": "This paper developed an output-feedback control system for regulation of continuous stirred tank reactors (CSTRs) with van de Vusse reaction. The reactors are often used as benchmark representatives of nonminimum-phase processes. Control of such nonlinear processes is difficult because they exhibit the inverse response. Linear controllers usually give unsatisfactory results in this case and thus nonlinear control approaches are more suitable. The proposed control system consists of a nonlinear observer and an extended nonlinear state feedback controller. The extension consists in adding the integrator to the controller for improving steady state performance of the control system. Stability of the control system including the observer dynamics is guaranteed, thanks to the existence of an input-to-state Lyapunov function. Simulation studies are conducted to illustrate the effectiveness of the proposed control system and its robustness.",
     "keywords": ["Nonlinear process control", "Nonminimum-phase process", "Input-to-state stability", "Observer-based control", "Continuous stirred tank reactor", "van de Vusse reaction"]},
    {"article name": "Hybrid and single feedstock energy processes for liquid transportation fuels: A critical review",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.02.008",
     "publication date": "06-2012",
     "abstract": "This review provides a detailed account of the key contributions within the energy communities with specific emphasis on thermochemically based hybrid energy systems for liquid transportation fuels. Specifically, the advances in the indirect liquefaction of coal to liquid (CTL), natural gas to liquid (GTL), biomass to liquid (BTL), coal and natural gas to liquid (CGTL), coal and biomass to liquid (CBTL), natural gas and biomass to liquid (BGTL), and coal, biomass, and natural gas to liquid (CBGTL) are presented. This review is the first work that provides a comprehensive description of the contributions for the single-feedstock energy systems and the hybrid feedstock energy systems, for single stand-alone processes and energy supply chain networks. The focus is on contributions in (a) conceptual design, (b) process simulation, (c) economic analysis, (d) heat integration, (e) power integration, (f) water integration, (g) process synthesis, (h) life cycle analysis, (i) sensitivity analysis, (j) uncertainty issues, and (k) supply chain. A classification of the contributions based on the products, as well as different research groups is also provided.",
     "keywords": ["Optimization", "Hybrid feedstocks", "Supply chain", "Energy processes", "Energy systems engineering", "Transportation fuels"]},
    {"article name": "A practical approach for Generalized Predictive Control within an event-based framework",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.03.003",
     "publication date": "06-2012",
     "abstract": "This work presents a combination of the Generalized Predictive Control (GPC) algorithm with event-based sampling techniques. The proposed control scheme preserves all well-known individual advantages of GPC and event-based sampling algorithms, respectively. The main benefits of this combination are an important reduction of actuation load meanwhile the control system performance is maintained within an acceptable level. Guidelines for a tuning procedure are given and tested for a wide set of industrial process models. Furthermore, the resulting algorithm is simple to be implemented and allows to establish a tradeoff between control performance and the number of actuations. The performance of the proposed control algorithm is first verified for a first-order plus delay process and afterwards it is evaluated by using a case study based on the greenhouse temperature control problem.",
     "keywords": ["Event-based control", "Model predictive control", "Generalized Predictive Control", "Process control", "Level crossing sampling conversation"]},
    {"article name": "Integrated countercurrent reverse osmosis cascades for hydrogen peroxide ultrapurification",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.02.017",
     "publication date": "06-2012",
     "abstract": "The chemicals and materials used to manufacture and package semiconductors and printed circuit boards are called electronic chemicals. The purity of these electronic chemicals, given by the industry association Semiconductor Equipment and Materials International (SEMI), is a very compromising concern for the semiconductor industrial sector, so very strict requirements are set to avoid microelectronic devices failures because of the content of impurities of electronic chemicals. For the particular case of hydrogen peroxide as one of the most consumed wet electronic chemicals, SEMI Document C30-1110 indicates five different electronic grades defined by their limiting impurities content.The semiconductor industry is appearing as an emerging application of reverse osmosis membranes based processes. After reviewing the patents published over the last twenty years about ultrapurification for industrial production of high purity electronic grade hydrogen peroxide, the referenced separation techniques can be replaced by reverse osmosis with lower operating expenses due to energy and chemicals. This work proposes a membrane process design based on an integrated countercurrent membrane cascade, in order to determine the optimum osmosis cascade for each SEMI Grade hydrogen peroxide, with the economic profit as the objective function in the optimization strategy. The results show the benefits of the reverse osmosis process, with profit values of 20\u201385 million $/year, for a target annual production of 9000\u00a0tons of electronic hydrogen peroxide, requiring the integrated reverse osmosis cascades of two stages for the production of Grade 1 to seven stages for the strictest Grade 5.",
     "keywords": ["Reverse osmosis", "Membrane cascades", "Hydrogen peroxide", "Ultrapurification", "Wet electronic chemicals"]},
    {"article name": "Fault detection in dynamic processes using a simplified monitoring-specific CVA state space modelling approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.02.009",
     "publication date": "06-2012",
     "abstract": "State space models have been successfully used for the modelling, control and monitoring of dynamic processes with several different approaches employed to derive the state variables of the model. Typically, state-space canonical variate analysis (CVA) modelling requires the estimation of five matrices to fully parameterize the model. This paper proposes a simpler CVA state space model defined by three matrices for the specific purpose of process monitoring. A modified definition of the past vector of inputs and output is proposed in order to facilitate efficient estimation of a reduced set of state space matrices. A sequential procedure for accurate selection of the model state vector dimension is also proposed. The proposed method is applied to the benchmark Tennessee Eastman process and the results show that the proposed method gives comparable and in some cases even better performance than the established CVA state space monitoring methods.",
     "keywords": ["State space modelling", "CVA", "Dynamic models", "Fault detection", "Process monitoring"]},
    {"article name": "Design of green diesel from biofuels using computer aided technique",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.03.006",
     "publication date": "06-2012",
     "abstract": "This paper presents a systematic computer aided technique to design a sustainable (safe, environmentally friendly and economical) tailor-made \u201cgreen diesel\u201d blend that satisfies a set of desirable target properties. In this work, the software, Integrated Computer Aided System (ICAS) was used to predict the green diesel properties. The blending model is formulated to identify a set of feasible mixture blends that satisfy the desirable target properties such as density and viscosity. The blend design problem is formulated as an NLP problem and solved through GAMS. Application of the systematic technique yields several promising green diesel blends. Four final candidate blends were selected based on three key criterion, i.e. cost, sulfur content and carbon dioxide emissions. The results show that the best diesel contains 82.4% diesel, 16.6% butanol and 1% butyl levulinate. This diesel blend contributes to the reduction of CO2 emission and sulfur content by up to 15% and 17%, respectively.",
     "keywords": ["Product design", "Diesel blend", "Computer aided technique", "Butyl levulinate", "Green diesel"]},
    {"article name": "On reliable and unreliable numerical methods for the simulation of secondary settling tanks in wastewater treatment",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.02.016",
     "publication date": "06-2012",
     "abstract": "A one-dimensional model for the sedimentation-compression-dispersion process in the secondary settling tank can be expressed as a nonlinear strongly degenerate parabolic partial differential equation (PDE), which has coefficients with spatial discontinuities. Reliable numerical methods for simulation produce approximate solutions that converge to the physically relevant solution of the PDE as the discretization is refined. We focus on two such methods and assess their performance via simulations for two scenarios. One method is provably convergent and is used as a reference method. The other method is less efficient in reducing numerical errors, but faster and more easily implemented. Furthermore, we demonstrate some pitfalls when deriving numerical methods for this type of PDE and can thereby rule out certain methods as unsuitable; among others, the wide-spread Tak\u00e1cs method.",
     "keywords": ["Secondary settling tank", "Wastewater treatment plant", "Numerical method", "Method of lines", "Godunov flux", "Efficiency"]},
    {"article name": "Evaluation of the performance of a heated brine spray system by dynamic simulation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.03.007",
     "publication date": "06-2012",
     "abstract": "A cogeneration system is used to produce hot water to heat up the high concentrated brine solution produced inside an integrated industrial salt plant. A part of that high concentrated brine solution could be sprayed into the surrounding ambient air, a few metres above the solar and thermal salt recrystallization ponds to increase the global process efficiency. In this study, the dynamic model previously built of a hot brine spray system was explored in order to evaluate its behaviour. The influence on the spray system performance was studied by analysing the relevance of some operating, atmospheric conditions and spray nozzle characteristics. For the integration of this hot brine spray system model with the dynamic model of the thermal salt process, correlations were established for the total mass water evaporated in the spray system and for the temperature of sprayed brine that reaches the pond as function of seven independent variables.",
     "keywords": ["Dynamic modelling", "Simulation", "Spray system", "Ballistics theory", "gPROMS", "Salt evaporative process"]},
    {"article name": "Optimisation of LNG mixed-refrigerant processes considering operation and design objectives",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.03.005",
     "publication date": "06-2012",
     "abstract": "This paper presents a systematic analysis of optimisation formulations for the LNG process. It focuses on the construction and testing of eight objective functions with an aim to identify the most appropriate formulation. Four objective functions relate to the operational aspect of the LNG process, while four concentrate on the design aspect.It was found that the most effective operation optimisation objective function is the minimisation of the major operating cost, being compressor power (Ws). For the design objective functions, the minimisation of Net Present Value (NPV) is favoured where no restriction exists on the area available for LNG plant construction while minimising the objective function (Ws\u2013UA) is favoured in case where a limit on the plant area is imposed. Finally, a methodology is constructed for using both design and operation objective functions over the life of the LNG plant, considering gas field feed profile.",
     "keywords": ["CAPEX capital expenditure", "capital expenditure", "CBM coal bed methane", "coal bed methane", "D1 design objective function 1", "design objective function 1", "D2 design objective function 2", "design objective function 2", "D3 design objective function 3", "design objective function 3", "D4 design objective function 4", "design objective function 4", "DMR dual mixed refrigerant", "dual mixed refrigerant", "FOB freight on board", "freight on board", "HEN heat exchanger network", "heat exchanger network", "HX-301 heat exchanger stage 1", "heat exchanger stage 1", "HX-302 heat exchanger stage 2", "heat exchanger stage 2", "HX-303 heat exchanger stage 3", "heat exchanger stage 3", "J\u2013T Joule\u2013Thompson", "Joule\u2013Thompson", "LNG liquefied natural gas", "liquefied natural gas", "MCHE main cryogenic heat exchangers", "main cryogenic heat exchangers", "MR mixed refrigerant", "mixed refrigerant", "MTA minimum temperature approach", "minimum temperature approach", "NG natural gas", "natural gas", "NLP nonlinear programming", "nonlinear programming", "NPV Net Present Value", "Net Present Value", "O1 operation objective function 1", "operation objective function 1", "O2 operation objective function 2", "operation objective function 2", "O3 operation objective function 3", "operation objective function 3", "O4 operation objective function 4", "operation objective function 4", "OPEX operating expenditure", "operating expenditure", "SMR single mixed refrigerant", "single mixed refrigerant", "LNG", "Liquefied natural gas", "Mixed-refrigerant", "Optimisation", "Objective function", "Design and operation"]},
    {"article name": "A Bayesian inference based two-stage support vector regression framework for soft sensor development in batch bioprocesses",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.03.004",
     "publication date": "06-2012",
     "abstract": "Inherent process and measurement uncertainty has posed a challenging issue on soft sensor development of batch bioprocesses. In this paper, a new soft sensor modeling framework is proposed by integrating Bayesian inference strategy with two-stage support vector regression (SVR) method. The Bayesian inference procedure is first designed to identify measurement biases and misalignments via posterior probabilities. Then the biased input measurements are calibrated through Bayesian estimation and the first-stage SVR model is thus built for output measurement reconciliation. The inferentially calibrated input and output data can be further used to construct the second-stage SVR model, which serves as the main model of soft sensor to predict new output measurements. The Bayesian inference based two-stage support vector regression (BI-SVR) approach is applied to a fed-batch penicillin cultivation process and the obtained soft sensor performance is compared to that of the conventional SVR method. The results from two test cases with different levels of measurement uncertainty show significant improvement of the BI-SVR approach over the regular SVR method in predicting various output measurements.",
     "keywords": ["Soft sensor", "Batch process", "Support vector regression", "Bayesian inference", "Process and measurement uncertainty", "Fed-batch penicillin cultivation process"]},
    {"article name": "Modeling and sensitivity analysis of styrene monomer production process and investigation of catalyst behavior",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.01.014",
     "publication date": "05-2012",
     "abstract": "In this work, a fundamental kinetic model based upon the Hougen\u2013Watson non-porosity formalism was derived and used to simulate dehydrogenation and oxidation axial flow reactors. In addition, partial pressure profiles of components during styrene production process inside porous catalyst were obtained using Dusty-Gas model. The preservation equations are adopted to calculate temperature and flow profiles in the reactors filled with iron\u2013potassium promoted catalyst pellets. The presented mathematical model for ethylbenzene dehydrogenation consists of nonlinear simultaneous differential equations with multiple dependent variables. Simulation results such as selectivity and operating temperature for different conventional catalysts have been presented and compared with those of a new introduced catalyst based on Fe2O3. Comparison of simulation results with experimentally observed ones shows that the model can precisely predict behavior of the industrial unit. Furthermore, the obtained results show that application of the new introduced catalyst increase ethylbenzene conversion and decrease necessary inlet temperature.",
     "keywords": ["Styrene monomer", "Dehydrogenation reactors", "Hougen\u2013Watson method", "Dusty-Gas method", "Mathematical simulation and modeling"]},
    {"article name": "Dynamic model-based fault diagnosis for (bio)chemical batch processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.01.013",
     "publication date": "05-2012",
     "abstract": "To ensure constant and satisfactory product quality, close monitoring of batch processes is an absolute requirement in the (bio)chemical industry. Principal Component Analysis (PCA)-based techniques exploit historical databases for fault detection and diagnosis. In this paper, the fault detection and diagnosis performance of Batch Dynamic PCA (BDPCA) and Auto-Regressive PCA (ARPCA) is compared with Multi-way PCA (MPCA). Although these methods have been studied before, the performance is often compared based on few validation batches. Additionally, the focus is on fast fault detection, while correct fault identification is often considered of lesser importance. In this paper, MPCA, BDPCA, and ARPCA are benchmarked on an extensive dataset of a simulated penicillin fermentation. Both the detection speed, false alarm rate and correctness of the fault diagnosis are taken into account. The results indicate increased detection speed when using ARPCA as opposed to MPCA and BDPCA at the cost of fault classification accuracy.",
     "keywords": ["Biochemical batch processes", "Fault detection/diagnosis", "Principal Component Analysis (PCA)", "Process monitoring", "Statistical Process Control (SPC)"]},
    {"article name": "Three dimensional discrete element models for simulating the filling and emptying of silos: Analysis of numerical results",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.02.007",
     "publication date": "05-2012",
     "abstract": "The discrete element method (DEM) is a promising technique that allows the mechanical behaviour of the material stored in silos and hoppers to be studied. The present work analyses the numerical results obtained by two three-dimensional DEM models that simulate the filling and discharge of a silo for two materials: glass beads or maize grains. The aim of the present work was to assess the capacity of these models to predict the behaviour of the studied materials. To guarantee the maximum representativeness of the results, many of the simplifications usually used in DEM models were avoided. The results analysed included the vertical distributions of the normal pressure, tangential pressure and mobilised friction, the horizontal distribution of normal pressure, velocity profiles and the spatial distribution of the bulk density. The results of this analysis highlight the potential of DEM models for studying the behaviour of granular materials in silos and hoppers, provided that simplifications are minimized.",
     "keywords": ["Discrete element model", "Silo", "Hopper", "Pressures", "Flow", "Bulk density"]},
    {"article name": "Optimization of structural and operational variables for the energy efficiency of a divided wall distillation column",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.01.015",
     "publication date": "05-2012",
     "abstract": "In the present work, the optimization of different structural and process parameters of a divided wall column (DWC) for the energy efficiency has been presented. The optimal design and operation of divided wall distillation columns involve a number of variables and is a non-linear problem. Rigorous simulation of a DWC was carried out using Multifrac model of ASPEN Plus software. Box\u2013Behnken design (BBD) under response surface methodology (RSM) was used for the optimization of the parameters and to evaluate the effects of these parameters and their interactions on the energy efficiency of a DWC. The process variables were found to have significant effect on the energy efficiency of a DWC as compared to the effect of structural variables, and the predictions from the BBD optimization agree well with the results of the rigorous simulation.",
     "keywords": ["Petlyuk column", "Divided wall column (DWC)", "Process parameters", "Box\u2013Behnken design (BBD)", "Energy efficiency", "Response surface methodology (RSM)"]},
    {"article name": "Improving the selection of interior points for one-dimensional finite element methods",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.02.004",
     "publication date": "05-2012",
     "abstract": "A new strategy to improve the selection of interior points inside an element of finite element methods is proposed. The novelty is to use the element boundary information in selecting the internal points. The new strategy is compared to the classical strategy in several examples and the main benefits are qualitatively and quantitatively explained.",
     "keywords": ["Interior points selection", "Finite element methods", "One-dimensional boundary value problems"]},
    {"article name": "Towards computer-aided multiscale modelling: A generic supporting environment for model realization and execution",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.02.012",
     "publication date": "05-2012",
     "abstract": "Computer-aided multiscale modelling (CAMM) may be carried out in three consecutive stages, namely conceptual modelling, model realization, and model execution. Following earlier work on a conceptual modelling tool which aims to support the first stage of CAMM, prototypical tools for realizing conceptual models and for the execution of simulation are developed in this work, with the assumption that a multiscale simulation is to be carried out by means of integrating existing single-scale models. More specifically, the tool that supports model realisation helps modellers generate information required for executing the multiscale model. The model execution stage is in turn supported by a component-based simulation environment. Two different multiscale simulation modes, namely \u201ccoordinator driven\u201d and \u201cmaster tool driven\u201d, are identified and supported separately. Details of the design and implementation of these tools are provided. Two reactor modelling examples are used to validate these tools and to demonstrate their application.",
     "keywords": ["Multiscale modelling", "Computer-aided modelling", "Tool integration"]},
    {"article name": "A computational fluid dynamics (CFD) investigation of the flow field and the primary atomization of the close coupled atomizer",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.02.014",
     "publication date": "05-2012",
     "abstract": "The gas flow fields of four atomizers were analyzed by both k\u2013\u025b turbulence model and Reynolds stress turbulence model (RSM). The scattering angle of the main part of gas spray increased from 15.0\u00b0 to 17.4\u00b0 given by k\u2013\u025b model and 17.2\u00b0 to 19.0\u00b0 given by RSM model, as the nozzle intersection angle increased from 5\u00b0 to 65\u00b0 at the operating gas pressure of 1.0\u00a0MPa, which also moved the merging position of the annular peak. There is a high pressure area downstream of the delivery tube which contributes to the sheet breakup. Comparing the simulation results with the experimental data, it is found that RSM model is more accurate than k\u2013\u025b model and proved that computational fluid dynamics is an effective method to simulate the gas flow of the close coupled atomizer.",
     "keywords": ["Atomizer", "Intersection angle", "Gas flow field", "CFD", "Sheet breakup"]},
    {"article name": "Leapfrogging and synoptic Leapfrogging: A new optimization approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.02.011",
     "publication date": "05-2012",
     "abstract": "A novel optimization technique is introduced and demonstrated. Leapfrogging starts with a randomly located set of trial solutions (termed players) within the feasible decision variable (DV) space. At each iteration, the player with the worst objective function (OF) value is relocated to a random position within its DV-space reflection on the other side of the player with the best OF value. Test cases reveal that this simple algorithm has benefits over classic direct and gradient-based methods and particle swarm in speed of finding the optimum and in handling surface aberrations, including ridges, multi-optima, and stochastic objective functions. Potential limitations and analysis opportunities are discussed.",
     "keywords": ["Optimization", "Direct search", "Individuals", "Stochastic", "Constraints", "Nonlinear"]},
    {"article name": "Simulation-optimization approach to clinical trial supply chain management with demand scenario forecast",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.01.007",
     "publication date": "05-2012",
     "abstract": "In the pharmaceutical industry, the development activities that are required to bring a new drug to market involve considerable expense (upwards of $1 Billion) and can take in excess of 10 years. Clinical trials constitute a critically important and very expensive part of this development process as the associated supply chain encompasses producing, distributing and administering the candidate therapy to volunteer patients located in different geographic regions. A number of different approaches are being pursued to reduce clinical trial costs, including innovations in trial organization and patient pool selection. In this work, we focus our attention on improved management of the clinical supply chain. A simulation-optimization approach is presented, including patient demand simulation and demand scenario forecast, mathematical programming based planning, and discrete event simulation of the entire supply chain. Three case studies with different demand types are reported and compared to demonstrate the utility of the proposed approach.",
     "keywords": ["Clinical trial", "Supply chain", "Optimization", "MILP", "Simulation", "Demand scenario"]},
    {"article name": "Use of dynamic simulation for reactor safety analysis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.02.013",
     "publication date": "05-2012",
     "abstract": "Dynamic simulations of chemical processes are widely used to develop effective plantwide control structures that provide stable regulatory control at some desired operating condition. This paper illustrates that they can also serve a very useful role in the analysis of safety problems in the event of emergency situations. The dynamic response of the process when various failures occur is critical to the design of safety systems for the process (alarms, overrides, interlocks, safety valves and rupture disks). For example, a failure of the supply of cooling water will lead to rapid increases in pressures and temperatures in the process. Determining the rates of increase in these important variables and the time period to reach critical limits (safety response time) permits the engineer to quantitatively design effective safety systems.Chemical reactors are typically the most sensitive and potentially the most dangerous units in many processes, particularly when exothermic reactions and low per-pass reactant conversions are involved. This paper illustrates how Aspen Dynamic simulation can be used for predicting the dynamic changes in critical variables. Dynamic emergency safety simulations are presented for five processes with several types of cooled reactors (CSTR and tubular) and residence times varying from 0.16 to 60\u00a0min. Safety response times vary from several seconds to several minutes, depending on both the reactor, the system in which it is installed and the level of reactant conversion.",
     "keywords": ["Process safety", "Reactor runaway", "Emergency system"]},
    {"article name": "Rigorous dynamic models for distillation safety analysis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.02.019",
     "publication date": "05-2012",
     "abstract": "Dynamic simulations of distillation columns are widely used to develop effective control structures. The normal distillation models assume instantaneous heat transfer in the condenser and reboiler. However, when dynamic simulations are used in the analysis of safety problems in the event of emergency situations, the basic model does not accurately represent the dynamic response. Accurate response times are essential in the design of safety systems for the column. For example, a failure of the supply of cooling water will lead to rapid increases in pressures and temperatures that occur in seconds. Accurately determining the rates of increase in these important variables and the time period to reach critical limits (safety response time) permits the engineer to quantitatively design effective safety systems.This paper illustrates how rigorous condenser and reboiler models can be developed in Aspen Plus and their dynamics evaluated in Aspen Dynamics.",
     "keywords": ["Safety analysis", "Distillation simulation", "Distillation", "Dynamic models", "Aspen simulation"]},
    {"article name": "Modeling, simulation and advanced control of methanol production from variable synthesis gas feed",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.02.005",
     "publication date": "05-2012",
     "abstract": "A process relying on feedstock produced using a non-continuous energy source such as solar energy needs detailed understanding of the dynamics to define procedures for cycling and robust control design. In this work, methanol production from synthesis gas (or syngas) is studied. The syngas is (assumed to be) obtained from an intermittent upstream biomass-gasification process run by solar energy. Development of the methanol synthesis recycle-loop model is described in detail, along with several case studies performed using the steady-state and dynamic models for better understanding of the process behavior. A linear model predictive controller is designed to reject disturbances from a varying upstream syngas production process and track set point changes in the desired variables for methanol synthesis.",
     "keywords": ["Modeling", "Kinetics", "Sequential-modular", "Non-continuous", "System identification", "Model predictive control"]},
    {"article name": "Global optimization of water networks design using multiparametric disaggregation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.02.018",
     "publication date": "05-2012",
     "abstract": "We propose new mixed-integer linear programming models for the optimal design of water-using and wastewater treatment networks. These replace the original non-convex, nonlinear problems following parameterization of the concentration variables appearing in the bilinear terms resulting from the contaminant mass balances. The difference between the models lies in the numeric system used for the parameterization. We show how to perform the transformation for a generic coding and give the results for the decimal and binary systems. While the resulting MILPs are approximations of the original NLP, any desired accuracy level can be set, being the proposed models exact in the limit of an infinite number of significant digits. Through the solution of several test cases taken from the literature, we show that the value of the objective function rapidly approaches the global optimal solution. The models can also be used to initialize the NLP when solved with local optimization solvers.",
     "keywords": ["Mathematical modeling", "Bilinear terms", "Water minimization", "Water re-use", "Wastewater treatment"]},
    {"article name": "Low Order-Value Multiple Fitting for supercritical fluid extraction models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.01.018",
     "publication date": "05-2012",
     "abstract": "Low Order-Value Optimization (LOVO) is a useful tool for nonlinear estimation problems in the presence of observations with different levels of relevance. In this paper LOVO is associated with a Multiple Fitting strategy for the estimation of parameters in supercritical fluid extraction models. Experimental data of supercritical CO2 extraction of peach almond oil are considered. Multiple fitting makes it possible to impose constraints on the estimation procedure that improve the physical meaning of the parameters. A novel combination of minimization methods is used to solve problems in the LOVO setting. Numerical results are reported.",
     "keywords": ["Low Order-Value Optimization", "Supercritical fluid extraction", "Simulation", "Parameter estimation", "Sovov\u00e1's model", "Multiple fitting"]},
    {"article name": "Dual composition control and soft estimation for a pilot distillation column using a neurogenetic design",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.01.003",
     "publication date": "05-2012",
     "abstract": "Artificial neural networks exhibit a great potential for both model based control and software sensing due to their non-linear identification capabilities. This paper proposes the use of adaptive neural networks applied to the prediction of product composition starting from secondary variable measurements, and to both dual composition control and inventory control for a continuous ethanol\u2013water nonlinear pilot distillation column monitored under LabVIEW. A principal component analysis based algorithm has been applied to select the optimal net input vector for the soft sensor. Genetic algorithms are used for the automatic choice of the optimum control law based on a neural network model of the plant. The proposed real time control scheme offers a high speed of response for changes in set points and null stationary error for both dual composition control and inventory control, and reveals the potential use of this control strategy when an experimental multivariable set-up is addressed.",
     "keywords": ["Neural network", "Genetic algorithms", "PCA selection", "Neural composition estimation", "Real-time neurogenetic control"]},
    {"article name": "Identification and robustness analysis of nonlinear hybrid dynamical system concerning glycerol transport mechanism",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.01.001",
     "publication date": "05-2012",
     "abstract": "The fermentation of glycerol by Klebsiella pneumoniae is a complex bioprocess. In this paper, a generalized nonlinear hybrid dynamical system is proposed to describe this process under continuous culture based on three different transport modes of glycerol across cell membrane. In the system, the inhibitory effect of 3-hydroxypropionaldehyde to the activities of two key enzymes (glycerol dehydratase and 1,3-propanediol oxydoreductase) and to the specific cell growth rate are all taken into consideration. To infer the most reasonable transport mode of glycerol across cell membrane on the hypothesis that 1,3-propanediol passes cell membrane by both passive diffusion and active transport, a quantitative definition of biological robustness for intracellular substances is presented. Taking the presented biological robustness and relative error between experimental data and computational values as performance index, a system identification model is established. Numerical results show that it is most possible for glycerol to pass cell membrane by passive diffusion.",
     "keywords": ["Nonlinear hybrid dynamical system", "Robustness analysis", "System identification", "Continuous fermentation"]},
    {"article name": "Multiperiod production planning and design of batch plants under uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.01.008",
     "publication date": "05-2012",
     "abstract": "A two-stage stochastic multiperiod LGDP (linear generalized disjunctive programming) model was developed to address the integrated design and production planning of multiproduct batch plants. Both problems are encompassed considering uncertainty in product demands represented by a set of scenarios. The design variables are modeled as here-and-now decisions which are made before the demand realization, while the production planning variables are delayed in a wait-and-see mode to optimize in the face of uncertainty. Specifically, the proposed model determines the structure of the batch plant (duplication of units in series and in parallel) and the unit sizes, together with the production planning decisions in each time period within each scenario. The model also allows the incorporation of new equipment items at different periods. The objective is to maximize the expected net present value of the benefit. To assess the advantages of the proposed formulation, an extraction process that produces oleoresins is solved.",
     "keywords": ["Multiproduct batch plants", "Demand uncertainty", "Units in series", "Design and planning"]},
    {"article name": "Real-time production scheduling in a multi-grade PET resin plant under demand uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.01.011",
     "publication date": "05-2012",
     "abstract": "We deal with the real-time production scheduling of a continuous-process multi-grade PET resin plant. The process is surcharged by sequence-dependent changeovers, sequential processing with production and space capacity, and mixed and flexible finite intermediate storage. The management called us to develop a time responsive production scheduling tool that copes with demand uncertainty, urgent orders and increased lead times. We adopt simulation as a methodology approach and create a tool based on tangible control rules and simple production engineering methods that make dynamic analysis tractable. Our goal is to maximize the aggregate fill rate, taking into consideration the number of required equipment transitions which cause undesirable variations in base resin properties. The model is tested under a real-world six-month demand instance and ten other hypothetical scenarios. The results are compared with those of the optimal solution derived from a preexisting Mixed Integer Linear Programming model that considers short-term demand as known.",
     "keywords": ["PET resin plant", "Continuous multi-grade process", "Real-time production scheduling", "Simulation"]},
    {"article name": "A normal vector approach for integrated process and control design with uncertain model parameters and disturbances",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.01.016",
     "publication date": "05-2012",
     "abstract": "In this work the normal vector method is extended to the simultaneous treatment of parametric uncertainty and disturbances. This method ensures that desired dynamic properties hold despite parametric uncertainty by maintaining a minimal distance between the operating point and so-called critical manifolds where the process behavior changes qualitatively. Here, unknown exogeneous disturbances and uncertain model and process parameters are considered simultaneously. To address this simultaneous problem formulation, the augmented systems developed for only parameterized disturbances in previous works have to be modified and extended. A generalized formulation of the robust optimization problem results, which includes normal vector constraints on critical manifolds of steady states and of bounds on the state transient. The numerical methods are further developed to prepare for the treatment of high-dimensional problems. Illustrative case studies considering the design of a continuous mixed-suspension mixed-product removal crystallization process and the Tennessee Eastman process are presented.",
     "keywords": ["Robust optimization", "Robust stability", "Robust performance", "Grazing bifurcation", "Disturbances", "Normal vector"]},
    {"article name": "Object-oriented simulation of an Endex reactor for separation of carbon dioxide from flue emissions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.01.017",
     "publication date": "05-2012",
     "abstract": "Endex calcium looping is a novel early-stage technology for separating CO2 from flue gases that shows great potential for reducing the high cost of the regeneration step because the calcination temperature is significantly lower. In this work an Endex calcium looping system is modelled as a train of continuous stirred tank reactor (CSTR) elements mass-coupled in series and thermally coupled in parallel. A novel simulator was developed that incorporates object-oriented design to describe the Endex reactor and its components, and used to study the effects of varying the most important design and operating parameters. From a reactor design point of view, varying the heat transfer coefficient and the sorbent flow rate amounts to testing the classic tradeoff between capital and running costs: we may choose between higher capital costs (larger heat transfer coefficient) and lower running costs (low sorbent flow rate) or vice versa, for similar scrubbing and regeneration efficiencies.",
     "keywords": ["Endex", "Calcium looping", "Carbon capture", "Object-oriented simulation"]},
    {"article name": "Identification of a multivariable delta-operator stochastic state-space model with distributed time delays: Application to a rapid thermal processor",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.01.019",
     "publication date": "05-2012",
     "abstract": "A high sampling frequency is often required to monitor and control real processes. However, a high sampling frequency increases the uncertainty of the identified shift-operator discrete-time model due to the truncation error and also raises the model order by increasing the number of delay steps. This study has addressed the above two points and proposed a method to identify a compact delta-operator-based stochastic state-space model for a multiple-input multiple-output system with distributed time delays. The proposed method was applied to numerical processes to illustrate its performance and also to identify a 12-in. rapid thermal processing unit.",
     "keywords": ["Delta-operator", "Identification", "High-order ARX model", "Small sampling period", "Large delay time"]},
    {"article name": "Effects of Jacobi polynomials on the numerical solution of the pellet equation using the orthogonal collocation, Galerkin, tau and least squares methods",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.11.015",
     "publication date": "04-2012",
     "abstract": "A number of different numerical techniques in the family of weighted residual methods; the orthogonal collocation, Galerkin, tau and least squares (LSQ) methods, are used within the spectral framework to solve a linear reaction\u2013diffusion pellet problem with slab and spherical geometries. The node points are in this work taken as the roots of orthogonal polynomials in the Jacobi family. Two Jacobi polynomial parameters, \u03b1 and \u03b2, can be used to tune the distribution of the roots within the domain. The objective of this paper is thus to investigate the influence of the node point distribution within the domain adopting the weighted residual methods mentioned above. Moreover, the results obtained with the different weighted residual methods are compared to examine whether the numerical approaches show the same sensitivity to the node point distribution. The notifying findings are as follows:(i) Considering the condition number of the coefficient matrices, the different weighted residual methods do not show the same sensitivity to the roots of the polynomials in the Jacobi family. On the other hand, the simulated error obtained adopting the Galerkin, tau and orthogonal collocation methods for different \u03b1, \u03b2-combinations differ insignificantly. The condition number of the LSQ coefficient matrix is relatively large compared to the other numerical methods, hence preventing the simulation error to approach the machine accuracy. (ii) The Legendre polynomial, i.e., \u03b1\u00a0=\u00a0\u03b2\u00a0=\u00a00, is a very robust Jacobi polynomial giving on average the lowest condition number of the coefficient matrices and the polynomial also give among the best behaviors of the error as a function of polynomial order. This polynomial gives good results for small and large gradients within both slab and spherical pellet geometries. (iii) Adopting the Legendre polynomial, the Galerkin and tau methods obtain favorable lower condition numbers than the orthogonal collocation and LSQ methods.",
     "keywords": ["Pellet equation", "Jacobi polynomial", "Orthogonal collocation", "Least squares method", "Galerkin", "Weighted residual methods"]},
    {"article name": "Reduced order modelling of chemical reactors with recycle by means of POD-penalty method",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.10.001",
     "publication date": "04-2012",
     "abstract": "Spectral method based on POD are an effective approach for model reduction but variable boundary conditions are often a problem, since basis functions must satisfy boundary conditions at all times. In this work we introduce weak imposition of boundary conditions by means of a penalty method and apply the method to develop reduced order models of two different chemical reactors. A quantitative analysis shows that, by changing the values of the penalty parameters, arbitrary accuracy can be achieved at the expense of increasing CPU time. Computations can be reduced by a factor 50 at least, by maintaining still acceptable accuracy. Performance parameters of the approach appear to be model dependent.",
     "keywords": ["Model reduction", "Proper orthogonal decomposition", "Penalty method", "Tubular reactor with recycle", "Circulating fluidized bed reactor"]},
    {"article name": "Use of the Weibull equation to approximate diffusive release from particles in a closed system",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.09.012",
     "publication date": "04-2012",
     "abstract": "We consider the problem of Fickian diffusion of a solute (or heat) into or out of a suspension of particles, in a well-mixed solvent. By combining a simple numerical scheme with a Laplace transform method, we are able to efficiently solve this problem for different particle volume fractions (including accumulation of solute in the liquid phase), shapes (spheres, cubes and cylinders of different aspect ratios) and particle size distributions (assumed to be log-normal). We approximate the results by a Weibull function, and thereby provide a physical calibration for the parameters in this function when used as an approximation for our solutions. We test our calculation by measuring salt release profiles from different size distributions of agar cubes, and then use the predicted Weibull equation to deduce the diffusivity of salt in this material.",
     "keywords": ["Diffusion", "Heat transfer", "Mass transfer", "Mathematical modelling", "Particle", "Suspension"]},
    {"article name": "A comparative study of different CFD-codes for numerical simulation of gas\u2013solid fluidized bed hydrodynamics",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.12.002",
     "publication date": "04-2012",
     "abstract": "The hydrodynamics of a gas\u2013solid fluidized bed reactor were studied numerically. Computational two-dimensional results from open source software packages MFIX and OpenFOAM, and those obtained from the commercial software package Fluent were discussed and compared with numerical and experimental data existing in the literature. The gas\u2013solid flow was simulated applying the multifluid Eulerian\u2013Eulerian model, where the solid phase is treated as a continuum. The solid-phase properties were calculated by using the kinetic theory of granular flow. Momentum exchange coefficients were calculated using the Gidaspow and Syamlal\u2013O\u2019Brien drag functions. Pressure drop and bed expansion ratio predicted by the simulations were in relatively close agreement with benchmark numerical and experimental data sets in the bubbling regime. Contrary to the OpenFOAM predictions, computations with MFIX and Fluent predicted instantaneous and time-average local voidage and velocity profiles which are comparable with results from the literature.",
     "keywords": ["Multiphase flow", "Fluidization", "KTGF", "CFD", "MFIX", "Fluent", "OpenFOAM"]},
    {"article name": "Modelling a counter-diffusive reactor for methane combustion",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.12.009",
     "publication date": "04-2012",
     "abstract": "The counter diffusive reactor containing a catalyst supported on a fibre pad is often used as a radiant heater. Fuel fed from the inlet of the pad is combusted by oxygen diffusing from the reactor exit. This paper describes the development of a two dimensional model of this heater with methane as the fuel. The detailed transport equations are solved using the finite element method. The correct implementation of the boundary conditions is emphasized. The effects of external mass transfer, reaction rate and feed flow rate on the reactor performance are studied. Comparisons are made to experimental results obtained in an earlier investigation. It is shown that the primary limiting step that controls the conversion of fuel is the rate of mass transfer of oxygen through the boundary layer that develops in front of the reactor. Combustion efficiencies approaching 100% can be achieved at sufficiently high rates of reaction.",
     "keywords": ["Catalytic combustion", "Radiant heater", "Oxygen diffusion", "Conversion", "Model"]},
    {"article name": "An efficient and reliable model based on network method to simulate CO2 corrosion with protective iron carbonate films",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.11.011",
     "publication date": "04-2012",
     "abstract": "A mathematical model simulating CO2 corrosion is presented in the form of a set of non-linear, coupled differential equations. For different values of the parameters (pH, pCO2 and flow velocity) that influence the solution, the problem is numerically solved by the network method, which provides all the concentrations of the species from the steel surface to the bulk. Although the model is strongly sensitive to the above parameters, no assumptions are considered as regards the linearization of species fluxes, chemical reaction rates and current densities of the electrochemical reactions. The design of the model is explained in detail and it is run on standard electrical circuit simulation software. As expected, the corrosion rate, an important parameter to know in chemical utilities, increases for lower values of pH and higher values of flow velocity, while changes in pCO2 hardly influence this quantity. The results are very close to the experimental data.",
     "keywords": ["Corrosion", "Network simulation method", "Numerical methods"]},
    {"article name": "A space-averaged model for hollow fibre membranes filters",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.12.017",
     "publication date": "04-2012",
     "abstract": "We present a space independent model to describe the filtration process taking place in a hollow fibre membrane filter. This method is based on averaging a full three-dimensional model made by a system of PDEs describing the hydrodynamic process coupled with transport, adsorption and attachment equations. Few crucial assumptions are needed to guarantee the physical coherence of the model, but a practical way to check the fulfilment of these constraints is provided, so that one can quickly verify the applicability of the model. This work has been tested in the framework of a collaboration project with a manufacturing company to provide an optimization of both product and process. To this purpose, a program was written in Python programming language, implementing both the calibration and the forward simulation part, in order to be used by the membranes manufactures for obtaining information on the filter efficiency.",
     "keywords": ["Flow in porous media", "Modeling", "Hollow fibre", "Waste-water treatment"]},
    {"article name": "Staging of the Fischer\u2013Tropsch reactor with an iron based catalyst",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.11.009",
     "publication date": "04-2012",
     "abstract": "The Fischer\u2013Tropsch reactor is sectioned into stages based on the systematic method given by Hillestad (2010). The design functions are optimized to maximize the concentration of C11+ at the end of reactor path. The decision variables are fluid mixing, hydrogen distribution, heat transfer area distribution, coolant temperature, and catalyst concentration. With the path temperature constrained by 250\u00a0\u00b0C, staging of the reactor will increase the concentration of C11+. For a three-stage reactor, the concentration is increased by 2.50% compared to a single-stage reactor. The optimal mixing structure is plug flow to have the maximum possible conversion. A case study is conducted to separate and distribute hydrogen along the reactor path. This will reduce H2/CO at the beginning of the path and increase chain growth probability. The results show that for a three-stage reactor, the concentration of C11+ is increased by 15.93% compared to single-stage reactor.",
     "keywords": ["Reactor path", "Staging", "Fischer\u2013Tropsch synthesis", "Design functions", "Plug flow"]},
    {"article name": "Optimal design and operation of a C3MR refrigeration system for natural gas liquefaction",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.12.003",
     "publication date": "04-2012",
     "abstract": "LNG liquefaction is an energy-intensive process. Energy efficiency is the major concern in LNG liquefaction process design and operation. In this paper, a new methodology for LNG liquefaction synthesis targeting energy consumption minimization is presented. It is based on thermodynamic analysis, mathematical programming, and rigorous simulation. The procedure for MINLP (mixed-integer non-linear programming) model development and simplification is described in detail. LINDOGlobal solver in GAMS is employed to solve the MINLP problem and the optimization results are further validated through rigorous simulations. Based on the development, the optimal design and operational conditions for LNG liquefaction process can be simultaneously achieved. A C3MR LNG liquefaction process is employed to demonstrate the efficacy of the developed methodology. Comprehensive thermodynamic analysis is also conducted to give insights of the optimization results.",
     "keywords": ["Process synthesis", "Optimization", "MINLP", "LNG", "Refrigeration system"]},
    {"article name": "Computer-aided scale-up of a packed-bed tubular reactor",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.10.009",
     "publication date": "04-2012",
     "abstract": "Temperature control is crucial when designing a catalytic tubular reactor for exothermic reactions because hot spots in packed-bed tubes affect conversion, selectivity and lifespan of catalysts. To resolve the hot spot problem, a computer-aided scale-up method combining process modeling software, heat exchanger design software and computational fluid dynamics (CFD) analysis is proposed. The proposed method is composed of three steps as follows: firstly, the length and the number of tubes are determined to achieve a target production rate by the simulations of a single-tube reactor model. Secondly, the detailed geometry of a scaled-up reactor comprising multiple tubes is determined using heat exchanger design software. Finally, optimal operating conditions to control the hot spots are designated by CFD analysis. As a practical application, the method is applied to scaling up the single-tube reactor producing epichlorohydrin to a demonstration-scale reactor comprising 200 tubes so its optimal design and operating conditions are determined.",
     "keywords": ["Design", "Scale-up", "Packed-bed tubular reactor", "CFD", "Heat exchanger design software", "Process modeling software"]},
    {"article name": "Sustainability assessment of polygeneration processes based on syngas derived from coal and natural gas",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.10.006",
     "publication date": "04-2012",
     "abstract": "Polygeneration systems produce chemicals, electricity, fuel, hydrogen, etc., from one or more type of feedstock. Considering their promise to provide high material and energy conversion from natural resources, polygeneration systems are recognized as promising technologies for future chemical and power industries.Sustainability assessment of these systems can provide valuable information to designers. Embedding exergy analysis and inherent safety score that quantify the efficiency and societal aspect, respectively, to complement the widely accepted economic assessment and environmental impacts assessment in a decision tree evaluation framework provides a more comprehensive, yet fast methodology to compare related processes in terms of sustainability.In this paper two different polygeneration systems, which use coal and natural gas as feed to produce di-methyl ether and power, are compared using a comprehensive sustainability assessment methodology. The results of the assessment are used to identify the more sustainable process, taking into account the economic, environmental, societal and efficiency factors.",
     "keywords": ["Polygeneration", "Sustainability assessment", "Coal to DME", "Natural gas to DME"]},
    {"article name": "Synthesis of n-propyl propionate in a pilot-plant reactive distillation column: Experimental study and simulation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.01.004",
     "publication date": "04-2012",
     "abstract": "The catalytic esterification of n-propyl propionate using propionic acid and n-propyl alcohol as reactants via a conventional pilot plant reactive distillation column was carried-out. The strongly acidic ion-exchange resin Amberlyst46\u2122 was used as the solid acid catalyst. The influence of the reflux ratio (from 2.0 to 2.5), the molar feed ratio (from 2.0 to 2.5), and total feed (from 3.0 to 4.0\u00a0kg/h) on the purity and conversion of the species and on the composition and temperature profiles along the column was analyzed. A 23 experimental design with six actual and two simulated experiments was carried-out. With the experimental conditions obtained from the simulation results, the experiments showed that a reflux ratio of 2.0, a molar feed ratio of 2.0 and a total feed of 4\u00a0kg/h, yielded the largest propyl propionate production (1.60\u00a0kg/h). The simulated experiments were carried out using a non-equilibrium model implemented into the simulation environment Aspen Custom Modeler\u2122.",
     "keywords": ["ACM Aspen Custom Modeler", "Aspen Custom Modeler", "(D/F)mass distillate-to-feed mass ratio", "distillate-to-feed mass ratio", "EA\u2013EB simulation experiment", "simulation experiment", "E1\u2013E6 experiment", "experiment", "EQ equilibrium model", "equilibrium model", "F mass flow", "mass flow", "FR molar feed ratio", "molar feed ratio", "NEQ non-equilibrium stage model", "non-equilibrium stage model", "PI process intensification", "process intensification", "POH 1-propanol", "1-propanol", "ProAc propionic acid", "propionic acid", "ProPro n-propyl-propionate", "n-propyl-propionate", "Q composition", "composition", "RD reactive distillation", "reactive distillation", "RDC reactive distillation column", "reactive distillation column", "RR reflux ratio", "reflux ratio", "T temperature", "temperature", "TF total feed of reagents", "total feed of reagents", "VLE vapor\u2013liquid equilibrium", "vapor\u2013liquid equilibrium", "W weight", "weight", "x (mol/mol) mole fraction", "mole fraction", "X conversion", "conversion", "Propyl propionate", "Reactive distillation", "Esterification reaction", "Non-equilibrium model"]},
    {"article name": "Control of an extractive distillation process to dehydrate ethanol using glycerol as entrainer",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.01.006",
     "publication date": "04-2012",
     "abstract": "In this paper, an investigation of the design and control of an extractive distillation process to produce anhydrous ethanol using glycerol as entrainer is reported. The extractive distillation process receives the azeotropic mixture ethanol\u2013water that is fed into a dehydration column in one of the intermediate stages while at the same time glycerol is fed into one of the top stages. As overhead product high purity ethanol is withdrawn and in the bottom stream a mixture of water/glycerol is sent to a recovery column. The effects of the entrainer to feed molar ratio, reflux ratio, feed stage, feed entrainer stage and entrainer feed temperature were studied to obtain the best design with minimal energy requirements. A control scheme is developed in order to maintain stable operation for large feed disturbances. Dynamic simulations show that is possible to use only one temperature control to hold the purity specifications.",
     "keywords": ["Ethanol dehydration", "Extractive distillation", "Glycerol", "Entrainer", "Distillation control"]},
    {"article name": "Nonlinear model predictive control of fed-batch cultures of micro-organisms exhibiting overflow metabolism: Assessment and robustness",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.12.010",
     "publication date": "04-2012",
     "abstract": "Overflow metabolism characterizes cells strains that are likely to produce metabolites as, for instance, ethanol for yeasts or acetate for bacteria, resulting from an excess of substrate feeding and inhibiting the cell respiratory capacity. The critical substrate level separating the two different metabolic pathways is generally not well defined. This occurs for instance in Escherichia coli cultures with aerobic acetate formation. This work addresses the control of a lab-scale fed-batch culture of E. coli with a nonlinear model predictive controller (NMPC) to determine the optimal feed flow rate of substrate. The objective function is formulated in terms of the kinetics of the main metabolic pathways, and aims at maximizing glucose oxidation, while minimizing glucose fermentation. As bioprocess models are usually uncertain, a robust formulation of the NMPC scheme is proposed using a min\u2013max optimization problem. The potentials of this approach are demonstrated in simulation using a Monte-Carlo analysis.",
     "keywords": ["Predictive control", "Min\u2013max optimization", "Overflow metabolism", "Fermentation", "Biotechnology"]},
    {"article name": "Predictive compensation for variable network delays and packet losses in networked control systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2012.01.002",
     "publication date": "04-2012",
     "abstract": "Networked control systems (NCSs) offer many advantages over conventional control; however, they also demonstrate challenging problems such as network-induced delay and packet losses. This paper proposes an approach of predictive compensation for simultaneous network-induced delays and packet losses. Different from the majority of existing NCS control methods, the proposed approach addresses co-design of both network and controller. It also alleviates the requirements of precise process models and full understanding of NCS network dynamics. For a series of possible sensor-to-actuator delays, the controller computes a series of corresponding redundant control values. Then, it sends out those control values in a single packet to the actuator. Once receiving the control packet, the actuator measures the actual sensor-to-actuator delay and computes the control signals from the control packet. When packet dropout occurs, the actuator utilizes past control packets to generate an appropriate control signal. The effectiveness of the approach is demonstrated through examples.",
     "keywords": ["Process control", "Networked control systems (NCSs)", "Network-induced delay", "Packet loss", "Predictive compensation"]},
    {"article name": "A large-scale statistical process control approach for the monitoring of electronic devices assemblage",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.12.008",
     "publication date": "04-2012",
     "abstract": "In this paper, we present a new procedure for monitoring the assembly process of electronic devices. Monitoring the status of this operation is a challenge, as the number of quality features under monitoring is very large (order of thousands) and the number of samples available quite low (order of dozens). We propose an efficient approach for the on-line and at-line monitoring of such a process, by addressing two, hierarchically related, problems: (i) detection of faulty units (printed circuits boards with abnormal deposits); (ii) given a faulty unit, find a candidate set of solder deposits responsible for the anomaly. Our methodology is based on a latent variable framework using PCA for effectively extracting the normal behavior of the process. Both the variability in the PCA plane and around it (residuals) are considered. We have tested the proposed approach with real industrial data, and the results achieved illustrate its good discrimination ability.",
     "keywords": ["Printed circuits boards", "Principal components analysis", "Multivariate statistical process control", "Receiver operating characteristic"]},
    {"article name": "Development of virtual-labs for education in chemical process control using Modelica",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.10.010",
     "publication date": "04-2012",
     "abstract": "Virtual-labs are useful tools for education in chemical process control. A novel approach to virtual-lab implementation using the Modelica language is presented. This approach has the following advantages. Firstly, existing Modelica models can be employed to develop virtual-labs. If new models need to be programmed, the use of Modelica reduces considerably the modeling effort. Secondly, as the complete virtual-lab is described in Modelica, virtual-lab developers don\u2019t need to use programming languages. Finally, virtual-labs can be easily distributed to the users, who don\u2019t need to install any additional software. The proposed approach is based on the application of a methodology for interactive model development, and two software tools: a free Modelica library, named Interactive (http://www.euclides.dia.uned.es/Interactive), and the Dymola modeling environment. Several virtual-labs have been implemented and successfully applied to education in chemical process control. The development of a virtual-lab based on a complex model of a double-pipe heat exchanger is discussed.",
     "keywords": ["Chemical process education", "Control education", "Virtual laboratory", "Modelica", "Object-oriented modeling"]},
    {"article name": "Scheduling the cleaning actions for a fouled heat exchanger subject to ageing: MINLP formulation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.12.012",
     "publication date": "04-2012",
     "abstract": "This paper addresses the problem of scheduling the cleaning actions of a heat exchanger undergoing fouling and ageing. The existence of two discrete layers leads to the use of two cleaning methods which differ in their ability to remove the aged layer. A mixed integer nonlinear programming (MINLP) model is presented and solved for three case studies with different ageing rates. The optimal schedule consists of: (i) the timing of the cleanings and (ii) the cleaning mode in each instant.",
     "keywords": ["Ageing", "Cleaning", "Fouling", "Heat exchangers", "MINLP", "Scheduling"]},
    {"article name": "Prediction of the mutual diffusion coefficient for controlled drug delivery devices",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.12.001",
     "publication date": "04-2012",
     "abstract": "Diffusion coefficients of drugs in polymers were predicted using models based on modified free volume theory of diffusion. The descriptors used were chosen to uniquely relate the structure with desired properties. Model parameters were obtained using quantitative structure property relationships (QSPRs) developed using multiple linear regression and artificial neural networks with Bayesian regularization. Viability of the approach was established by predicting solvent diffusion coefficient in polymer for four polymer\u2013solvent systems (polystyrene\u2013toluene, polyvinylacetate\u2013toluene, polystyrene\u2013ethylbenzene and polystyrene\u2013tetrahydrofuran) and comparing with the experimental values. The model was subsequently used on three polymer\u2013drug systems (paclitaxel\u2013polycaprolactone, hydrocortisone\u2013polyvinylacetate and procaine\u2013polyvinylacetate). The predicted diffusion coefficient for Paclitaxel\u2013Polycaprolactone was used to study the release of Paclitaxel from Polycaprolactone under perfect sink condition. It is envisaged that the proposed model could be used in a reverse engineering framework to select polymers for designing the optimally controlled drug release devices.",
     "keywords": ["Controlled release devices", "QSPR, Free volume theory", "Diffusion coefficient"]},
    {"article name": "The dual-quadrature method of generalized moments using automatic integration packages",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.11.010",
     "publication date": "03-2012",
     "abstract": "Despite the several advantages of the moment based methods in solving population balances models, the lack of accuracy of their solutions is still a drawback. This work presents a solution to this problem using the DuQMoGeM, a moment based method written in a weighted residual formulation. The proposed methodology consists of using automatic integration packages in the DuQMoGeM to eliminate the integration errors in the evaluation of the moments of the population balance equation. The results show that the source of the errors in the generalized moments is related to the accuracy of the numerical integration of the aggregation and breakage terms. The use of automatic integration packages in the DuQMoGeM allows the error control of the calculated moments. However, for some problems, accurate values of a small set of moments do not guarantee the accurate representation of the number density distribution function due to the finite moment inversion problem.",
     "keywords": ["Population balance", "Multivariate PBE", "DuQMoGeM", "Aggregation", "Breakage"]},
    {"article name": "Comparison of diffusion models in the modeling of a catalytic membrane fixed bed reactor coupling dehydrogenation of ethylbenzene with hydrogenation of nitrobenzene",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.10.007",
     "publication date": "03-2012",
     "abstract": "Coupling of dehydrogenation of ethylbezene with hydrogenation of nitrobenzene in a catalytic membrane reactor can lead to a significant improvement in the conversion of ethylbenzene and production of styrene. In this work, the homogeneous reactor model for a cocurrent flow configuration is compared to two heterogeneous models based on the Fickian diffusion model and the dusty gas model for both isothermal and non-isothermal pellets. It is observed that both heterogeneous models predict a significant drop in yield and conversion compared to the homogeneous model, indicating the importance of heterogeneity. This drop is generally less severe for the dusty gas model than for the Fickian diffusion model. The assumption of isothermality causes larger deviations than the assumption of Fickian diffusion. The deviations in the predictions of the homogenous model and the heterogeneous models from those of the dusty gas model for non-isothermal pellets are \u223c6% and \u223c11%, respectively.",
     "keywords": ["Membrane reactor", "Dehydrogenation", "Hydrogenation", "Homogeneous model", "Heterogeneous model", "Dusty gas model", "Fickian diffusion model"]},
    {"article name": "Coupled simulation of an industrial naphtha cracking furnace equipped with long-flame and radiation burners",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.11.001",
     "publication date": "03-2012",
     "abstract": "A coupled reactor/furnace simulation has been conducted for a 100\u00a0kt/a SL-II naphtha cracking furnace containing both long-flame and radiation burners. The computational fluid dynamics approach was used to simulate the flow, combustion and radiative heat transfer in the furnace. The software packages COILSIM1D and SimCO were used to account for the cracking process in the reactor coils. The simulation provides for the first time detailed information about concentration, velocity, and temperature fields for these types of furnaces. Comparison of the calculated product yields against measured industrial data validates the simulation and shows that the difference with using a predefined normalized heat flux profile is limited. The results show that the design of radiation section outlet leads to an asymmetric flue gas-temperature, concentration and velocity profile. Large recirculation zones exist near the reactor tubes, making the temperature in the middle of furnace more uniform.",
     "keywords": ["Reactor/furnace simulation", "Cracking furnace", "Computational fluid dynamics", "Product yields", "Recirculation"]},
    {"article name": "crystSim: A software environment for modelling industrial batch cooling crystallization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.10.008",
     "publication date": "03-2012",
     "abstract": "Crystallization modelling is limited by the availability of methods to reliably predict mixing, temperature and concentration distributions, supersaturation, nucleation and growth kinetics for crystallization systems of industrial size. A new software environment for modelling batch cooling crystallization has been developed integrating crystallization and fluid dynamics. The mathematical model can reliably predict mixing in stirred vessels of a given geometry and its effect on the crystallization kinetics and crystal product properties thus reducing the cost and time required for the design and scale-up of industrial crystallizers. The paper describes the development and application of crystSim for modelling industrial crystallization in stirred tanks. The software is developed in Visual C# making full use of Object Oriented Programming (OOP) and multithreading.",
     "keywords": ["CDS central difference scheme", "central difference scheme", "CFD computational fluid dynamics", "computational fluid dynamics", "DC deferred correction", "deferred correction", "GUI graphic user interface", "graphic user interface", "IDE integrated development environment", "integrated development environment", "LM Lam\u2013Bremhorst", "Lam\u2013Bremhorst", "SIMPLE semi-implicit method for pressure-linked equation", "semi-implicit method for pressure-linked equation", "SIMPLEC semi-implicit method for pressure-linked equation consistent", "semi-implicit method for pressure-linked equation consistent", "SIMPLER semi-implicit method for pressure-linked equation revised", "semi-implicit method for pressure-linked equation revised", "TDMA tridiagonal matrix algorithm", "tridiagonal matrix algorithm", "TKE turbulent kinetic energy", "turbulent kinetic energy", "TI turbulent intensity", "turbulent intensity", "UD upwind scheme", "upwind scheme", "Crystallization modelling", "Mixing", "Software package", "Computational fluid dynamics (CFD)", "Object oriented programming (OOP)", "Multithreading"]},
    {"article name": "Mathematical modeling and parametric study of coke-burning process in a hydrocracker reactor",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.10.012",
     "publication date": "03-2012",
     "abstract": "A mathematical model is developed for simulation of dynamic behavior of coke-burning process in a hydrocracker reactor with an array of 4 fixed beds. The model is used for parametric study of the coke burning process. Results show that the effective parameters are inlet oxygen concentration, the flow rate of carrier gas and temperature within the beds. The effect of referred parameters on performance of regeneration of an industrial hydrocracker reactor is investigated for a real case and the optimum values of operating conditions are predicted. The optimum value for oxygen concentration during regeneration obtained as 0.85\u00a0mol%, with a predicted regeneration time of around 76\u00a0h.",
     "keywords": ["Coke-burning", "Hydrocracker", "Regeneration", "Fixed-bed reactor", "Modeling"]},
    {"article name": "Energy efficiency optimisation of wastewater treatment: Study of ATAD",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.11.016",
     "publication date": "03-2012",
     "abstract": "The aim of this paper is to minimise the energy requirement of autothermal thermophilic aerobic digestion (ATAD). To this end, a dynamic ATAD model is presented and assessed. A global sensitivity analysis was performed to identify the operating conditions with the strongest impact on the energy requirements, and thus to choose the most promising optimisation variables. The latter turned out to be the aeration flowrate, the reaction time, and the sludge flowrate. The optimisation problem was formulated following the sequential approach for dynamic optimisation, due to the discontinuous nature of ATAD. The problem was implemented in MATLAB\u00ae and solved for two case studies using the eSS algorithm, a global scatter search method that alternates with local algorithms (in our case fmincon) to refine the best solutions. The two selected full-scale case studies include a single-stage and a two-stage system. For the former, a 22% improvement of the energy requirement was achieved after optimisation, and 18% for the latter. Despite its advantages and common use in other fields, optimisation is still relatively rare in wastewater engineering. In the light of the high, rising cost of wastewater treatment, optimisation should become the norm when it comes to design and operation of wastewater treatment plants.",
     "keywords": ["Wastewater treatment", "ATAD", "Energy efficiency", "Dynamic optimisation", "Sensitivity analysis"]},
    {"article name": "Model-based analysis of micro-separators for portable direct methanol fuel-cell systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.11.005",
     "publication date": "03-2012",
     "abstract": "The applicability of capillary separation to direct methanol fuel-cell systems is studied in this article from two complementary perspectives: a three-dimensional simulation with computational fluid dynamics of a gas\u2013liquid separator, whose function is based on capillary forces rather than gravity, and a zero-dimensional model, which is integrated in the process model of a direct methanol fuel-cell system. The three-dimensional analysis indicates that an appropriate choice of construction and operation parameters allows to achieve almost perfect gas\u2013liquid separation, and that operation is not significantly influenced by orientation. The system-wide analysis indicates that the inclusion of such a capillary separator stabilises the system, allowing the use of simpler control strategies and removing the necessity of sensors difficult to implement.",
     "keywords": ["Fuel cell", "Control", "Orientation", "Capillary", "Computational fluid dynamics"]},
    {"article name": "Innovative dimethyl ether synthesis in a reactive dividing-wall column",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.11.012",
     "publication date": "03-2012",
     "abstract": "Dimethyl ether (DME) is of great industrial interest due to its use as clean fuel for diesel engines or in combustion cells, as a precursor to other organic compounds, as well as a green aerosol propellant that can effectively replace chloro-fluoro-carbons. Conventionally, high purity DME is synthesized by dehydration of methanol produced from syngas, in a process involving a catalytic fixed-bed reactor and a direct sequence of two distillation columns.The key problem of this classic process is the high investment costs for several units that require a large overall plant footprint, as well as the associated high energy requirements. To solve these problems, we propose in this work an innovative DME process based on a reactive dividing-wall column (R-DWC) that effectively integrates in one shell a reactive distillation (RD) unit with the DWC technology. The double integrated system allows the production of high-purity DME in only one unit, with minimal footprint and significantly lower costs.This study also makes a fair comparison between the reported conventional DME process and the optimally designed process alternatives based on RD and R-DWC, respectively. All processes are optimized in terms of minimal energy requirements, using the state of the art sequential quadratic programming (SQP) method implemented in AspenTech Aspen Plus. The results clearly demonstrate that the R-DWC process has superior performances as compared to the conventional or RD process: significant energy savings of 12\u201358%, up to 60% reduced CO2 emissions, as well as up to 30% lower capital investment costs.",
     "keywords": ["DWC", "Reactive distillation", "Dimethyl ether", "Methanol dehydration", "Capital and energy savings"]},
    {"article name": "Distributed parameter model for pH process including distributed continuous and discrete reactant feed",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.11.006",
     "publication date": "03-2012",
     "abstract": "In most cases the pH process is treated as a lumped parameter system for modeling, simulation and control. This approach deals not only with some advantages but also with several limits. After detailed discussion, the distributed parameter model for the pH process is derived for tubular mixers and for the in-line pH process. Although the distributed parameter approach for the pH process is more complicated than classic well-known lumped parameter pH model, it enables several new investigations. Based on the proposed model two research problems are discussed. The first problem deals with the influence of the mixer shape on the pH process dynamics. The second one presents a discussion on the identification possibilities of the reagents concentration in the distributed parameter pH process. The model discussed in the paper was explicated in more detail by the description of imperfect mixing phenomenon.",
     "keywords": ["Mathematical modeling", "Parameter identification", "Optimization", "Simulation", "pH process", "Reactant feed", "Imperfect mixing"]},
    {"article name": "Parallel tools for the bifurcation analysis of large-scale chemically reactive dynamical systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.12.016",
     "publication date": "03-2012",
     "abstract": "In this work we propose a set of tools for the parallel application of pseudo-arclength continuation to a class of systems for which the right hand side can be properly represented by a time numerically calculated evolution operator. For example, the reverse flow reactor and the reactors network with periodically switched inlet and outlet sections belong to this class of system. To conduct a dynamical analysis of these systems when the key parameters are changed, it is necessary to compute the eigenvalues of the Jacobian matrix many times. Since the Jacobian can only be obtained numerically, and this in turn takes away really significant computational power, running this operation in parallel saves real time of computation. Examples, solution lines and performance diagrams for selected systems are presented and discussed.",
     "keywords": ["Periodically forced chemical reactors", "Parameter continuation", "Bifurcation analysis", "Parallel implementation", "Parallelism"]},
    {"article name": "Improved stiction compensation in pneumatic control valves",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.09.006",
     "publication date": "03-2012",
     "abstract": "The oscillations caused by static friction (stiction) in control valves cause losses in quality and expense of raw materials. The effects of friction can be reduced through compensation techniques, until the maintenance of the valve is made in the next programmed plant shutdown. In this work, a well-known stiction compensation method that reduces variability both at process variable and valve stem movement is revisited. Limitations of this method are overcome through two proposed methods using a similar approach. The effectiveness of the proposed compensators is demonstrated using simulation examples and a flow control loop in a pilot plant.",
     "keywords": ["Stiction compensation", "Friction", "Control valve"]},
    {"article name": "Inferential MIMO predictive control of the particle size distribution in emulsion polymerization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.11.003",
     "publication date": "03-2012",
     "abstract": "A new inferential 2-step multiple input/multiple output (MIMO) model predictive control (MPC) of the particle size distribution (PSD) in emulsion polymerization processes is proposed. The bulk-like model describing the PSD is used with the material balances of initiator, radicals, monomer and surfactant. The inferential 2-step control strategy uses two measurements available online (without delay): the concentration of surfactant in the aqueous phase by conductimetry, and the concentration of monomer by calorimetry. In a first step, the optimal trajectory of surfactant concentration leading to the target PSD is calculated offline. In a second step, a multivariable model predictive control manipulates online the monomer and surfactant flow rates in order to track the precalculated surfactant concentration trajectory and to maximise the monomer concentration in the polymer particles in a constrained set-point tracking. Two control strategies are compared (nonlinear MPC and linearized MPC) with and without modelling errors.",
     "keywords": ["Model predictive control", "Nonlinear distributed parameter system", "Emulsion polymerization", "Particle size distribution", "Inferential control"]},
    {"article name": "Optimal selection of control structure using a steady-state inversely controlled process model",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.12.007",
     "publication date": "03-2012",
     "abstract": "The profitability of chemical processes strongly depends on their control systems. The design of a control system involves selection of controlled and manipulated variables, known as control structure selection. Systematic generation and screening alternative control structures requires optimization. However, the size of such an optimization problem is much larger when candidate controllers and their parameters are included and it rapidly becomes intractable. This paper presents a novel optimization framework using the notion of perfect control, which disentangles the complexities of the controllers. This framework reduces the complexity of the problem while ensuring controllability. In addition, the optimization framework has a goal-driven multi-objective function and requires only a steady-state inverse process model. Since dynamic degrees of freedom do not appear in a steady-state analysis, engineering insights are employed for developing the inventory control systems. The proposed optimization framework was demonstrated in a case study of an industrial distillation train.",
     "keywords": ["Control structure selection", "Multi-objective optimization", "Inversely controlled process model", "Perfect control"]},
    {"article name": "Optimal control and CFD modeling for heat flux estimation of a baking process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.10.011",
     "publication date": "03-2012",
     "abstract": "An optimal control method was combined with commercial CFD software for the optimization of heat flux during a baking process. The objective function was defined in terms of the transient heat flux, the temperature and the humidity of the baking product. The optimal control was achieved by the conjugate gradient method. The minimum energy consumption for a desired final baking product temperature and humidity was estimated. This study confirmed the relationship between the heat flux and baking product quality attributes and showed that optimal control models represent reliable tools to support decision making for complex processes such as baking. The proposed optimal control approach is general and could be easily extended to other processes.",
     "keywords": ["Optimal control", "CFD Multiphysics", "Baking process", "Energy consumption"]},
    {"article name": "An automata based method for online synthesis of emergency response procedures in batch processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.11.008",
     "publication date": "03-2012",
     "abstract": "Rapid response to remove (or reduce) the detrimental effects of accidents has always been an important safety issue for the chemical industries. A systematic strategy is presented in this paper to synthesize emergency response procedures in any given batch system. Specifically, two distinct sets of automata are first constructed offline to model the plant behaviors and the control specifications, respectively. On the basis of these automata, an admissible supervisor can be synthesized online for a diagnosed failure-induced system state by applying the parallel composition operation. For the purpose of identifying an efficient operating procedure to steer the system away from hazardous conditions while still maintaining an acceptable production rate, an additional set of auxiliary automata can be augmented with this supervisor to set the operation targets and to limit the total number of actuator actions. Two examples are presented in this paper to demonstrate the feasibility of the proposed approach.",
     "keywords": ["Automaton", "Batch operation", "Emergency response", "Process safety", "Supervisory control"]},
    {"article name": "Optimal production planning under time-sensitive electricity prices for continuous power-intensive processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.09.019",
     "publication date": "03-2012",
     "abstract": "Power-intensive processes can lower operating expenses when adjusting production planning according to time-dependent electricity pricing schemes. In this paper, we describe a discrete-time, deterministic MILP model that allows optimal production planning for continuous power-intensive processes. We emphasize the systematic modeling of operational transitions, that result from switching the operating modes of the plant equipment, with logic constraints. We prove properties on the tightness of several logic constraints. For the time horizon of 1 week and hourly changing electricity prices, we solve an industrial case study on air separation plants, where transitional modes help us capture ramping behavior. We also solve problem instances on cement plants where we show that the appropriate choice of operating modes allows us to obtain practical schedules, while limiting the number of changeovers. Despite the large size of the MILPs, the required solution times are small due to the explicit modeling of transitions.",
     "keywords": ["Production planning", "Power-intensive systems", "MILP models", "Time-varying electricity prices", "Air separation plants", "Cement plants"]},
    {"article name": "Rigorous scheduling of mesh-structure refined petroleum pipeline networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.11.007",
     "publication date": "03-2012",
     "abstract": "Pipeline networks represent the major mode of transportation for crude oil and refined fuels. Recent data suggest that this trend will persist in coming years. A multiproduct pipeline network can be described as a set of interconnected pipelines with several input and receiving terminals. In the most general case, it has a mesh-like configuration with alternative paths between two terminals. Pumping and delivery operations should be scheduled all at once in an integrated fashion. This work introduces a novel MILP continuous-time formulation for the scheduling of mesh pipeline networks. The pipeline operational plan is conceived as a sequence of composite pumping runs each one involving at most a batch injection at every input station. The model solution simultaneously provides the timing of batch inputs at every source, the product sequence and lot sizes at every pipeline, and the flows diverted to terminals. Three examples of growing complexity were successfully solved at low CPU times.",
     "keywords": ["Mutiproduct pipeline network", "Mesh structure", "Operational planning", "Continuous approach", "MILP formulation"]},
    {"article name": "On the combinatorial structure of discrete-time MIP formulations for chemical production scheduling",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.11.004",
     "publication date": "03-2012",
     "abstract": "We study the structure of discrete-time mixed-integer programming (MIP) models for chemical production scheduling. We discuss how chemical manufacturing facilities can be represented as dynamic networks and then converted into time-expanded networks with side constraints. Based on this representation, we show that material balance constraints of the MIP models correspond to generalized flow balances in time-expanded networks. We discuss the implications of conversion coefficients in tasks with multiple inputs and outputs. We also show that assignment constraints lead to side constraints that are equivalent to clique constraints in the time-expanded task-graph of the facility. Finally, we discuss how variable batchsizes lead to fixed charge network structures.",
     "keywords": ["Combinatorial optimization", "Dynamic network", "Time-expanded network", "Vertex packing", "Fixed-charge network"]},
    {"article name": "Integrated business and engineering framework for synthesis and design of enterprise-wide processing networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.12.011",
     "publication date": "03-2012",
     "abstract": "The synthesis and design of processing networks is a complex and multidisciplinary problem, which involves many strategic and tactical decisions at business (considering financial criteria, market competition, supply chain network, etc.) and engineering levels (considering synthesis, design and optimization of production technology, R&D, etc.), all of which have a deep impact on the profitability of processing industries.In this study, an integrated business and engineering framework for synthesis and design of processing networks is presented. The framework employs a systematic approach to manage the complexity while solving simultaneously both the business and the engineering aspects of problems, allowing at the same time, comparison of a large number of alternatives at their optimal points. The results identify the optimal raw material, the product portfolio and select the process technology for a given market scenario together with the optimal material flows through the network and calculate the corresponding performance and sustainability metrics.The framework includes a software infrastructure for integrating different methods and tools needed for problem definition, formulation and solution of the design problem as a MINLP, reducing thereby the time and cost needed to generate and solve the design/synthesis problems and providing efficient data transfer between the tools. A generic structural process model has been implemented within the framework to describe the multidimensional engineering issues allowing thereby fast and flexible model development for various production processes.A case study from vegetable oil industry is used successfully to demonstrate the applicability of the integrated framework for making optimal business and engineering decisions.",
     "keywords": ["CAPEX capital investment", "capital investment", "GOM gross operating margin", "gross operating margin", "NPV net present value", "net present value", "MMT millions metric tonnes", "millions metric tonnes", "Enterprise-wide optimization", "Integrated business and engineering", "Decisions making", "Mixed-integer non-linear program (MINLP)", "Vegetable oil", "Product portfolio management"]},
    {"article name": "Molecular geometry effects and the Gibbs\u2013Helmholtz Constrained equation of state",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.08.006",
     "publication date": "02-2012",
     "abstract": "Differences in molecular size and shape have long been known to cause difficulties the modeling and simulation of fluid mixture behavior and generally manifest themselves as poor predictions of densities and phase equilibrium, often resulting in the need to regress model parameters to experimental data. A predictive approach to molecular geometry within the Gibbs\u2013Helmholtz Constrained (GHC) framework is proposed. The novel aspects of this work include (1) the use of NTP Monte Carlo simulations coupled with center of mass concepts to determine effective molecular diameters for non-spherical molecules, and (2) the use of effective molecular diameters in the GHC equation to predict phase behavior of mixtures with components that have distinct differences in molecular size and shape. Numerical results for a CO2\u2013alkane, alkane\u2013water and CO2\u2013alkane\u2013water mixtures show that the proposed approach of combining molecular geometry with the GHC equation provides accurate predictions of liquid densities and two- and three-phase equilibrium.",
     "keywords": ["Molecular size and shape effects", "Gibbs\u2013Helmholtz Constrained equation", "NTP Monte Carlo simulations", "Center of mass", "Effective molecular diameter"]},
    {"article name": "Achieving higher accuracies for process simulations by implementing the new reference equation for natural gases",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.09.009",
     "publication date": "02-2012",
     "abstract": "For efficient design and operation of technical processes, accurate simulations are essential. Therefore, a precise representation of thermophysical properties using an adequate thermodynamic model is necessary. The GERG-2008 (Kunz et al., 2007, Kunz and Wagner, submitted for publication) is the new reference equation of state for natural gases consisting of up to 21 specific compounds. As the equation of state describes the gas and liquid phase as well as the super-critical region and the vapour\u2013liquid equilibrium with the highest accuracy available, it has high potential for accurate process-modeling. In order to implement the software available for the new equation into various common simulation tools, the GERG-2008 Property Package has been developed, which complies with the CAPE-OPEN standards specification. The influence of thermodynamic models on the simulation of the only liquefaction plant for natural gas existing in Europe is investigated. Results show significant improvements in accuracy for simulations using the new thermodynamic model.",
     "keywords": ["calc calculated", "calculated", "cp isobaric heat capacity", "isobaric heat capacity", "crit property at the critical point", "property at the critical point", "Dev deviation", "deviation", "exp experimental", "experimental", "f fugacity", "fugacity", "g Gibbs free energy", "Gibbs free energy", "h enthalpy", "enthalpy", "Ki K-factor of component i", "K-factor of component i", "LNG liquefied natural gas", "liquefied natural gas", "MCHE main cryogenic heat exchanger", "main cryogenic heat exchanger", "P power", "power", "p pressure", "pressure", "p\u2033 saturated vapour pressure", "saturated vapour pressure", "PP Property Package", "Property Package", "Q \u02d9 heat flow", "heat flow", "s entropy", "entropy", "T temperature", "temperature", "T\u2033 saturated vapour temperature", "saturated vapour temperature", "vf vapour fraction", "vapour fraction", "x mole fraction", "mole fraction", "Z compressibility factor", "compressibility factor", "\u2202 partial derivative", "partial derivative", "\u0394 difference", "difference", "\u03c6i fugacity coefficient of component i", "fugacity coefficient of component i", "\u03c1 density", "density", "\u03c1\u2032 saturated liquid density", "saturated liquid density", "GERG-2008", "Equation of state", "CAPE-OPEN", "Simulation", "Liquefied natural gas", "Liquefaction"]},
    {"article name": "RDC extraction column simulation using the multi-primary one secondary particle method: Coupled hydrodynamics and mass transfer",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.09.010",
     "publication date": "02-2012",
     "abstract": "Based on the multivariate population balance equation (PBE) and the primary secondary particle concept a mathematical model is developed for liquid extraction columns. It is extended to include the momentum balance for the dispersed phase. The resulting model is complicated by the integral source term of the PBE. To reduce this complexity, while maintaining most of the information from the continuous PBE, the concept of the primary secondary particle method is used. The effect of the number of primary particles (PP) on the final predicted solution is investigated. Numerical results show that the solution converge fast as the number of PP is increased. The terminal droplet velocity is found to be the most sensitive model parameter to the number of PP. The predicted steady state profiles (droplet diameter, holdup and the concentration profiles) along a pilot RDC extraction column are compared to the experimental data where good agreement is achieved.",
     "keywords": ["Extraction columns", "Population balance equation", "Modelling", "Momentum balance", "Primary secondary particle concept"]},
    {"article name": "Modelling and simulation of extensional-flow units in emulsion formation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.07.012",
     "publication date": "02-2012",
     "abstract": "Here we studied the emulsification process carried out in an extensional-flow unit. By means of rigorous population and momentum balances we captured the phenomenological description of the first principles occurring in such unit. The strong feature of our model approach resides in the fully mechanistic description of the governing phenomena. A population balance equation was formulated and solved to account for the disappearance and appearance of droplets at each size class. Coalescence mechanism was included to account for the instability of newly created droplets. We validated the accuracy of the results obtained from our equation-based model with experimental data obtained at pilot-plant scale. The results obtained by simulation showed that at a given set of operational conditions and pre-emulsion properties the product obtained was within the desired and narrow specifications space. As a concluding remark we suggest further exploring the design and development of extensional-flow units for structured emulsions.",
     "keywords": ["Emulsification", "Extensional flow", "Oil-in-water emulsion", "Population balance", "Momentum balance"]},
    {"article name": "Detailed steady-state simulation of tubular reactors for LDPE production: Influence of numerical integration accuracy",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.09.003",
     "publication date": "02-2012",
     "abstract": "Simulators are widely used for analyzing and optimizing the production of low density polyethylene in tubular reactors under steady state conditions. This steady state is in practice often simulated by chemical engineers using a series of CSTRs type model due to its stable behavior with respect to spatial discretization and smooth convergence for the underlying stiff model equations. Although already a large number of CSTRs is used, this number appears to be too low for the physical reality. Here, this traditional cascaded CSTR approach is compared with a plug flow type approach for a highly detailed reaction model describing the free radical copolymerization. Additionally, the influence of the discretization is rigorously investigated and quantified. It is shown that the discretization does not significantly affect the temperature and the conversion profile, but has a major impact (deviations up to 30%) on the properties which determine the end product. However, this impact of discretization is in practice often overlooked.",
     "keywords": ["Tubular reactor", "Discretization", "Differential equations", "LDPE (co)polymerization", "Steady state simulation"]},
    {"article name": "An iterative modelling approach for improving the performance of a pulsed electric field (PEF) treatment chamber",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.09.002",
     "publication date": "02-2012",
     "abstract": "An important component of the pulsed electric field (PEF) technology is the treatment chamber in which the food is exposed to high voltage pulses. Non-uniformity of the treatment, particularly with respect to the electric field distribution, is a common problem in continuous PEF treatment chambers. A previously developed and validated Multiphysics model of a pilot-scale PEF system with co-linear electrode configuration was simplified and embedded into an iterative algorithm, automatically modifying the model's treatment chamber geometry and dimensions, and evaluating the simulated process with respect to a set of performance indicators. The algorithm was capable of identifying configurations that were superior to the standard co-linear treatment chamber configuration. A 3D Multiphysics model with the identified geometrical properties was developed and a corresponding treatment chamber manufactured. The Multiphysics model was validated by comparing measured and predicted temperatures at various process conditions induced in NaCl solution and apple juice.",
     "keywords": ["PEF", "Pulsed electric fields", "Modelling", "Optimisation", "Electric field strength", "Treatment uniformity"]},
    {"article name": "Numerical simulation of three-dimensional viscoelastic planar contraction flow using the software OpenFOAM",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.09.015",
     "publication date": "02-2012",
     "abstract": "In this study, we have used a highly accurate and novel approach to solve numerically for true three-dimensional (3D) viscoelastic flows into sudden contractions. Motivation for this development has been for the advancement of 3D viscoelastic flows in complex geometries, where this new methodology is available as a general solver, written for the open source code OpenFOAM (Weller et al., 1998). The proposed approach is able to include the multiple relaxation times of differential constitutive equations, and has been performed using the Finite Volume Method (FVM), based on a Discrete Elastic Viscous Split Stress (DEVSS) technique (Fortin et al., 1997). In this work, both the Giesekus and Phan-Thien\u2013Tanner (PTT) shear-thinning models were implemented to reproduce flow through a planar 4:1 contraction, where numerical convergence was achieved for a Weissenberg number (We) of 2.9. Direct comparison with experimental data and literature involving 2D and 3D numerical simulations shows this method to be both stable and effective.",
     "keywords": ["Viscoelastic flow", "3D entry flow", "Phan-Thien Tanner model", "Giesekus model", "HWNP", "OpenFOAM"]},
    {"article name": "An efficient constraint handling method with integrated differential evolution for numerical and engineering optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.09.018",
     "publication date": "02-2012",
     "abstract": "Constrained optimization problems are very important as they are encountered in many engineering applications. Equality constraints in them are challenging to handle due to tiny feasible region. Additionally, global optimization is required for finding global optimum when the objective function and constraints are nonlinear. Stochastic global optimization methods can handle non-differentiable and multi-modal objective functions. In this paper, a new constraint handling method for use with such methods is proposed for solving equality and/or inequality constrained problems. It incorporates adaptive relaxation of constraints and the feasibility approach for selection. The recent integrated differential evolution (IDE) with the proposed constraint handling technique is tested for solving benchmark problems with constraints, and then applied to many chemical engineering application problems with equality and inequality constraints. The results show that the proposed constraint handling method with IDE (C-IDE) is reliable and efficient for solving constrained optimization problems, even with equality constraints.",
     "keywords": ["Differential evolution", "Self-adaptation", "Tabu list", "Stopping criterion", "Constraints handling", "Equality constraints"]},
    {"article name": "An efficient method for optimal design of large-scale integrated chemical production sites with endogenous uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.10.005",
     "publication date": "02-2012",
     "abstract": "Integrated sites are tightly interconnected networks of large-scale chemical processes. Given the large-scale network structure of these sites, disruptions in any of its nodes, or individual chemical processes, can propagate and disrupt the operation of the whole network. Random process failures that reduce or shut down production capacity are among the most common disruptions. The impact of such disruptive events can be mitigated by adding parallel units and/or intermediate storage. In this paper, we address the design of large-scale, integrated sites considering random process failures. In a previous work (Terrazas-Moreno et al., 2010), we proposed a novel mixed-integer linear programming (MILP) model to maximize the average production capacity of an integrated site while minimizing the required capital investment. The present work deals with the solution of large-scale problem instances for which a strategy is proposed that consists of two elements. On one hand, we use Benders decomposition to overcome the combinatorial complexity of the MILP model. On the other hand, we exploit discrete-rate simulation tools to obtain a relevant reduced sample of failure scenarios or states. We first illustrate this strategy in a small example. Next, we address an industrial case study where we use a detailed simulation model to assess the quality of the design obtained from the MILP model.",
     "keywords": ["Integrated sites", "Process reliability", "Endogenous uncertainties", "Bi-criterion optimization", "Mixed-integer linear programming", "Benders decomposition"]},
    {"article name": "Energy integration of industrial sites with heat exchange restrictions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.09.014",
     "publication date": "02-2012",
     "abstract": "Process integration methods aim at identifying options for heat recovery and optimal energy conversion in industrial processes. This paper introduces a targeting method, which includes heat exchange restrictions between process sub-systems. The problem is formulated as a MILP (mixed integer linear programming) problem, which considers not only restricted matches but also the optimal integration of intermediate heat transfer units and the energy conversion system, like heat pumping and combined heat and power production. Moreover a new mathematical formulation is presented to chose optimal heat transfer technologies. For solutions avoiding the energy penalty, the composite curves of optimal heat transfer units have to be embedded between the new generated hot and cold envelope composite curves. The application of the method is illustrated through an industrial example from the pulp and paper industry.",
     "keywords": ["Pinch analysis", "Utility integration", "Restricted matches", "Process sub-system", "Envelope composite curves", "Heat load distribution"]},
    {"article name": "Dividing wall column structure design using response surface methodology",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.07.006",
     "publication date": "02-2012",
     "abstract": "Designing dividing wall columns (DWC) \u2013 energy-efficient separators of ternary mixtures \u2013 involves multivariable problem solving. These variables interact with each other and need to be optimized simultaneously to obtain the best design. In this work, a practical method employing response surface methodology (RSM) is proposed for DWC design and optimization. The optimum DWC structure can be found in a practical manner while minimizing simulation runs. The proposed method was tested in the design and optimization of an acetic acid purification process. The RSM based optimization effectively copes with interactions between optimizing variables and its predictions agreed well with the results of rigorous simulation. The DWC system designed by the proposed method decreased total annual costs by 44.57% compared with conventional distillation.",
     "keywords": ["Distillation process", "Dividing wall column", "Structure design", "Response surface methodology"]},
    {"article name": "Hybrid simulation-optimization based approach for the optimal design of single-product biotechnological processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.07.013",
     "publication date": "02-2012",
     "abstract": "In this work, we present a systematic method for the optimal development of bioprocesses that relies on the combined use of simulation packages and optimization tools. One of the main advantages of our method is that it allows for the simultaneous optimization of all the individual components of a bioprocess, including the main upstream and downstream units. The design task is mathematically formulated as a mixed-integer dynamic optimization (MIDO) problem, which is solved by a decomposition method that iterates between primal and master sub-problems. The primal dynamic optimization problem optimizes the operating conditions, bioreactor kinetics and equipment sizes, whereas the master levels entails the solution of a tailored mixed-integer linear programming (MILP) model that decides on the values of the integer variables (i.e., number of equipments in parallel and topological decisions). The dynamic optimization primal sub-problems are solved via a sequential approach that integrates the process simulator SuperPro Designer\u00ae with an external NLP solver implemented in Matlab\u00ae. The capabilities of the proposed methodology are illustrated through its application to a typical fermentation process and to the production of the amino acid L-lysine.",
     "keywords": ["Hybrid simulation-optimization", "Mixed-integer dynamic optimization", "Biotechnological processes", "L-Lysine"]},
    {"article name": "Using fuzzy logic to tune an evolutionary algorithm for dynamic optimization of chemical processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.08.003",
     "publication date": "02-2012",
     "abstract": "Dynamic optimization of chemical processes can be carried out with evolutionary algorithms that involve many parameters. These parameters need to be given appropriate values for the algorithms to perform efficiently. This paper proposes parameter setting methods based on factorial experimentation and fuzzy logic, aimed at balancing convergence speed, robustness (consistent performance for each problem) and versatility (applicability to many different problems). The methods were tested on an existing dynamic optimisation method with at least nine tuneable parameters. The test problem set turned out to be quite demanding due to one particular problem behaving in opposite direction to the rest with respect to the most influential factor, population size. It is probable that no single tuning would be possible that will satisfy all problems. However, for the other problems, the Fuzzy Logic tuning method proposed in this paper proves to be a very promising approach.",
     "keywords": ["Evolutionary algorithm", "Parameter setting", "Process control", "Fuzzy logic", "Evolutionary optimization", "Dynamic optimization"]},
    {"article name": "Real-time inversion in large-scale water networks using discrete measurements",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.08.001",
     "publication date": "02-2012",
     "abstract": "There are significant challenges related to contamination detection and characterization within large water distribution systems. Given current sensing technology and resources, source inversion algorithms will need to rely on manual grab samples providing only a discrete positive/negative indication of the presence of contaminant. We propose an integrated real-time strategy that identifies the set of likely locations and performs additional sampling cycles to accurately identify the contamination source. We present an MILP formulation that solves the source inversion problem using discrete (positive/negative) measurements from sparse manual grab samples at limited points in time and space. The water quality model is formulated using the origin-tracking approach and is then exactly and efficiently reduced prior to the formulation of the MILP, giving a much smaller problem that is solvable in real-time settings. The formulation is tested on a water network model comprised of over 10,000 nodes and more than 150 timesteps.",
     "keywords": ["Water distribution systems", "Mixed integer linear programming", "Optimization", "Manual sampling", "Source inversion"]},
    {"article name": "Dissolved oxygen control of the activated sludge wastewater treatment process using stable adaptive fuzzy control",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.09.011",
     "publication date": "02-2012",
     "abstract": "In the operation of wastewater treatment plants a key variable is dissolved oxygen (DO) content in the bioreactors. The paper describes the development of an adaptive fuzzy control strategy for tracking the DO reference trajectory applied to the Benchmark Simulation Model n.1. The design methodology of this data-driven controller uses the Lyapunov synthesis approach with a parameter projection algorithm to construct an adaptive fuzzy controller (AFC), and guarantees the global stability of the resulting closed-loop system. To work in parallel with the AFC it is proposed a new easy to design supervisory fuzzy control with a smooth switching scheme between supervisory and nonsupervisory modes. Both controllers assume no mathematical model of the plant and may integrate human knowledge. The results of simulations show that this combined controller can learn and improve control rules resulting in accurate DO control.",
     "keywords": ["Adaptive fuzzy control", "Lyapunov synthesis", "Data-driven control", "Activated sludge process", "BSM1 benchmark"]},
    {"article name": "An MINLP model for biofouling control in seawater-cooled facilities",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.09.008",
     "publication date": "02-2012",
     "abstract": "The use of seawater as a cooling agent in industrial plants is becoming a common practice in regions with water scarcity. One of the main problems of this approach is the heat transfer reduction due to biofouling in the heat exchangers. Biocide agents and scheduled maintenances are commonly used to control the biofilm growth. Dechlorination agents are also used to reduce the biocide concentration in the water discharge. In this work, a mathematical programming approach to develop optimal policies of biocide continuous dosing, dechlorination agents continuous dosing and maintenance scheduling for a seawater-cooled power desalination plant is presented. The resulting mixed integer nonlinear programming model can be used to test different dosing policies to detect a superior one. For the case study presented, results show that daily dosing policies offer an optimum economic policy, although biweekly and monthly dosing policies might provide a suitable compromise between economic and operating issues.",
     "keywords": ["Biofouling control", "Seawater cooled plants", "Schedule maintenance", "Optimization", "MINLP model", "Biocide dosing"]},
    {"article name": "Dynamic phenomena in forced bioethanol reactors",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.11.013",
     "publication date": "02-2012",
     "abstract": "Numerical simulations have been employed to investigate the dynamic behavior of externally excited periodic bioethanol fermentors located at different parameter spaces. The fermentors are forced in the neighborhood of a static limit point. The stroboscopic Poincar\u00e9 bifurcation diagrams for the fermentors show that the forcing has significant effect on the dramatic change of the dynamics of these dissipative dynamical systems and can produce fascinating rich dynamical behavior such as: frequency locking, quasi-periodicity, chaos, intermittency, interior crisis, bistability and bubbles phenomena. It is very interesting that the period doubling scenario associated with period doubling reversals (period halving) can produce the bubble phenomenon, which may act to suppress the development of chaos. It is also shown that the period doubling reversals may act to suppress the fully developed chaos. It has been shown that the chaotic regions are harmful to the efficient production of ethanol; therefore a careful control policy is recommended.",
     "keywords": ["Bioethanol", "Bubbles phenomena", "Chaos", "Forced systems", "Fermentors", "Period doubling reversals"]},
    {"article name": "Catalyst dilution to improve dynamic controllability of cooled tubular reactors",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.07.017",
     "publication date": "02-2012",
     "abstract": "The dynamics of cooled tubular reactors are strongly affected by the overall heat-transfer coefficient U, which depends primarily on the velocity through the tubes. Higher velocities produce larger U's but increase pressure drops. Design involves trade-offs among tube diameter, tube length and number of tubes to achieve a specified pressure drop and per-pass conversion.This paper points out that another design optimization variable is available to improve the dynamic stability of cooled tubular reactors. If the catalyst activity is high, the size of the required reactor is small, which implies small heat-transfer area, and the reactor can be uncontrollable. If the catalyst is diluted with an inert solid, a bigger reactor will be required that will have more heat-transfer area and provide improved dynamic stability. This catalyst dilution strategy is explored for two reaction systems: a hypothetical chemical process and the carbonylation of dimethyl ether to produce methyl acetate.",
     "keywords": ["Reactor control", "Cooled tubular reactor", "Reactor stability", "Catalyst dilution"]},
    {"article name": "Multi-objective optimal control of chemical processes using ACADO toolkit",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.11.002",
     "publication date": "02-2012",
     "abstract": "Many practical chemical engineering problems involve the determination of optimal trajectories given multiple and conflicting objectives. These conflicting objectives typically give rise to a set of Pareto optimal solutions. To enhance real-time decision making efficient approaches are required for determining the Pareto set in a fast and accurate way. Hereto, the current paper illustrates the use of the freely available toolkit ACADO Multi-Objective (www.acadotoolkit.org) on several chemical examples. The rationale behind ACADO Multi-Objective is the integration of direct optimal control methods with scalarisation-based multi-objective methods enabling the exploitation of fast deterministic gradient-based optimisation routines.",
     "keywords": ["Multi-objective optimisation", "Dynamic optimisation", "Optimal control", "Open source", "Pareto set"]},
    {"article name": "Enhanced plant fault diagnosis based on the characterization of transient stages",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.12.006",
     "publication date": "02-2012",
     "abstract": "This paper introduces a data-based fault diagnosis system that includes an enhanced characterization of faults during transient stages. First, data under abnormal operating conditions (AOC) is projected onto a reference PCA model constructed with data under normal operating conditions (NOC). T2 and Q-statistic measures of this first PCA model are both used to detect the fault and to estimate the duration and delay of its transient evolution. After a dimensionality reduction, a second NOC PCA model is used to process data before diagnosing the faults by standard classification methods such as Artificial Neural Networks (ANN) or Support Vector Machines (SVM). A quantitative validation of the procedure has been carried out using simulated on-line data sets of the Tennessee Eastman Process (TEP). Results indicate that the incorporation of transient data in models improves the overall diagnosis performance, regardless of the particular choice between the statistical methods or the classification methods.",
     "keywords": ["On-line fault diagnosis", "Transient stages", "Tennessee Eastman Process", "PCA", "ANN", "SVM"]},
    {"article name": "Integrated production planning and scheduling optimization of multisite, multiproduct process industry",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.08.007",
     "publication date": "02-2012",
     "abstract": "The current manufacturing environment for process industry has changed from a traditional single-site, single market to a more integrated global production mode where multiple sites are serving a global market. In this paper, the integrated planning and scheduling problem for the multisite, multiproduct batch plants is considered. The major challenge for addressing this problem is that the corresponding optimization problem becomes computationally intractable as the number of production sites, markets, and products increases in the supply chain network. To effectively deal with the increasing complexity, the block angular structure of the constraints matrix is exploited by relaxing the inventory constraints between adjoining time periods using the augmented Lagrangian decomposition method. To resolve the issues of non-separable cross-product terms in the augmented Lagrangian function, we apply diagonal approximation method. Several examples have been studied to demonstrate that the proposed approach yields significant computational savings compared to the full-scale integrated model.",
     "keywords": ["Planning and scheduling integration", "Multisite production", "Continuous time formulation", "Decomposition method", "Augmented Lagrangian relaxation"]},
    {"article name": "A multi-period modelling and optimization approach to the planning of China's power sector with consideration of carbon dioxide mitigation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.09.001",
     "publication date": "02-2012",
     "abstract": "A great challenge China's power sector faces is to mitigate its carbon dioxide emissions while satisfying the ever-increasing power demand. Optimal planning of the power sector with consideration of carbon mitigation for a long-term future remains a complex task, involving many technical alternatives and an infinite number of possible plants installations, retrofitting, and decommissioning over the planning horizon. This paper presents a multi-period modelling and optimization framework for the optimal planning of China's power sector between 2010 and 2050. The planning horizon is divided into several time intervals, over which power plants of all types can be installed, retrofitted, or closed. Impacts of carbon mitigation related measures, including carbon cap and price, application of carbon capture and sequestration, are explicitly represented. A case study follows, based on real-life data of existing capacity of China's power sector in 2009, and a year-by-year development plan for China's power sector is proposed.",
     "keywords": ["Power sector", "Optimal planning", "CO2 reduction", "Cap-and-Trade", "CCS"]},
    {"article name": "Global Search Metaheuristics for planning transportation of multiple petroleum products in a multi-pipeline system",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.10.003",
     "publication date": "02-2012",
     "abstract": "The objective of this work is to develop several metaheuristic algorithms to improve the efficiency of the MILP algorithm used for planning transportation of multiple petroleum products in a multi-pipeline system. The problem involves planning the optimal sequence of products assigned to each new package pumped through each polyduct of the network in order to meet product demands at each destination node before the end of the planning horizon. All the proposed metaheuristics are combinations of improvement methods applied to solutions resulting from different construction heuristics. These improvements are performed by searching the neighborhoods generated around the current solution by different Global Search Metaheuristics: Multi-Start Search, Variable Neighborhood Search, Taboo Search and Simulated Annealing. Numerical examples are solved in order to show the performance of these metaheuristics against a standard commercial solver using MILP. Results demonstrate how these metaheuristics are able to reach better solutions in much lower computational time.",
     "keywords": ["Multi-product pipeline network", "Mixed-integer linear program", "Global Search Metaheuristics", "Taboo Search", "Simulated Annealing", "Multiple Markov Chain"]},
    {"article name": "Preliminary synthesis of work exchange networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.09.007",
     "publication date": "02-2012",
     "abstract": "In many chemical plants such as LNG, refineries, air enrichment, and ammonia compression is a major consumer of energy. In such plants, some process streams may need compression, while others may need expansion. Optimal integration of these streams may yield major savings in energy. In this work, we introduce a work exchanger network synthesis problem that is analogous to the well-known heat exchange network synthesis problem. We model the details of compressor and turbine operating curves to identify high-pressure and low-pressure streams that should be matched for work exchange via compression/turbine stages located on a single shaft. We propose a superstructure for the work exchange network configuration and develop a mixed-integer nonlinear program (MINLP) to minimize the total annualized cost for a constant speed of the single shaft. We use this preliminary model to optimize shaft speed, and demonstrate the benefits of an optimized network via an illustrative case study.",
     "keywords": ["Process optimization", "Work exchange network", "Energy integration", "Process synthesis", "Mathematical programming"]},
    {"article name": "Multiobjective strategies for New Product Development in the pharmaceutical industry",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.10.004",
     "publication date": "02-2012",
     "abstract": "New Product Development (NPD) constitutes a challenging problem in the pharmaceutical industry, due to the characteristics of the development pipeline. Formally, the NPD problem can be stated as follows: select a set of R&D projects from a pool of candidate projects in order to satisfy several criteria (economic profitability, time to market) while coping with the uncertain nature of the projects. More precisely, the recurrent key issues are to determine the projects to develop once target molecules have been identified, their order and the level of resources to assign. In this context, the proposed approach combines discrete event stochastic simulation (Monte Carlo approach) with multiobjective genetic algorithms (NSGAII type, Non-Sorted Genetic Algorithm II) to optimize the highly combinatorial portfolio management problem. In that context, Genetic Algorithms (GAs) are particularly attractive for treating this kind of problem, due to their ability to directly lead to the so-called Pareto front and to account for the combinatorial aspect. This work is illustrated with a study case involving nine interdependent new product candidates targeting three diseases. An analysis is performed for this test bench on the different pairs of criteria both for the bi- and tricriteria optimization: large portfolios cause resource queues and delays time to launch and are eliminated by the bi- and tricriteria optimization strategy. The optimization strategy is thus interesting to detect the sequence candidates. Time is an important criterion to consider simultaneously with NPV and risk criteria. The order in which drugs are released in the pipeline is of great importance as with scheduling problems.",
     "keywords": ["New Product Development", "Portfolio management", "Discrete event simulation", "Optimization", "Multicriteria genetic algorithm"]},
    {"article name": "Simultaneous process synthesis, heat, power, and water integration of thermochemical hybrid biomass, coal, and natural gas facilities",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.10.002",
     "publication date": "02-2012",
     "abstract": "A comprehensive wastewater network is introduced into a thermochemical based process superstructure that will convert biomass, coal, and natural gas to liquid (CBGTL) transportation fuels. The mixed-integer nonlinear optimization (MINLP) model includes simultaneous heat, power, and water integration that utilizes heat engines to recover electricity from waste heat and several treatment units to process and recycle wastewater. A total of 108 case studies are analyzed which consist of combinations of six coal feedstocks, three biomass feedstocks, three plant capacities, and two process superstructures. This study discusses important process topological differences between the case studies and illustrates each component of the process synthesis framework using the two medium-sized capacity case studies that have low-volatile bituminous coal and biomass feedstocks.",
     "keywords": ["Process synthesis, heat, power and water integration", "Hybrid energy systems", "Mixed-integer nonlinear optimization", "Fischer\u2013Tropsch", "Biofuels"]},
    {"article name": "Biomedical systems research\u2014New perspectives opened by quantitative medical imaging",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.07.010",
     "publication date": "01-2012",
     "abstract": "Recent advances in quantitative imaging allow unprecedented views into cellular chemistry of whole organisms in vivo. These novel imaging modalities enable the quantitative investigation of spatio-temporal reaction and transport phenomena in the living animal or the human body. This article will highlight the significant role that rigorous systems engineering methods can play for interpreting the wealth of in vivo measurements. A methodology to integrate medical imaging modalities with rigorous computational fluid dynamics entitled image-based computational fluid dynamics (iCFD) will be introduced. The quantitative analysis of biological systems with rigorous mathematical methods is expected to accelerate the introduction of novel drugs by providing a rational foundation for the systematic development of new medical therapies. Rigorous engineering methods not only advance biomedical research, but also aid the translation of laboratory research results into the bedside practice.",
     "keywords": ["CED Convection-enhanced delivery", "Convection-enhanced delivery", "CSF Cerebrospinal fluid", "Cerebrospinal fluid", "CNS Central nervous system", "Central nervous system", "CT Computed tomography", "Computed tomography", "DTI Diffusion tensor imaging", "Diffusion tensor imaging", "iCFD Image-based Computational fluid dynamics", "Image-based Computational fluid dynamics", "F-dopa 3,4-Dihydroxy-6-fluoro-DL-phenylananine Monohydrate", "3,4-Dihydroxy-6-fluoro-DL-phenylananine Monohydrate", "MRI Magnetic resonance imaging", "Magnetic resonance imaging", "NGF Nerve growth factor", "Nerve growth factor", "PET Positron emission tomography", "Positron emission tomography", "Biomedical engineering", "Quantitative medical imaging", "Image-based computational fluid mechanics", "Parameter estimation in distributed systems", "Pharmacokinetics"]},
    {"article name": "Towards computer-aided multiscale modelling: An overarching methodology and support of conceptual modelling",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.06.010",
     "publication date": "01-2012",
     "abstract": "Multiscale modelling is now widely regarded as a promising and powerful tool in various disciplines, including the broad area of process engineering. However, a multiscale model is usually much more difficult to develop than a single-scale model due to a range of conceptual, numerical, and software challenges. Currently, there is little support developed to facilitate multiscale modelling. This paper discusses the key challenges faced by computer-aided multiscale modelling (CAMM) and presents a methodology for developing a computer-based, generic and open supporting framework for multiscale modelling. Details are particularly provided on the development of a conceptual modelling tool, an important element of the envisaged tool set for CAMM. The application of this tool is illustrated by two reactor modelling examples.",
     "keywords": ["Multiscale modelling", "Computer-aided modelling", "Conceptual modelling"]},
    {"article name": "Response surface strategies in constructing statistical bubble flow models for the development of a novel bubble column simulation approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.07.014",
     "publication date": "01-2012",
     "abstract": "Bubble columns represent a substantial contribution to chemical industrial equipment and due to their complex hydrodynamics are difficult and computationally expensive to simulate. A model of the flow structure around a single bubble is required for the development of a computationally efficient alternative model. The flow structure data obtained from CFD is highly non-linear, which lends itself to a Design and Analysis of Computer Experiments (DACE) approach. The situation differs from conventional DACE applications since high resolution data is summarized, which allows different sampling criteria, correlation functions and optimization criteria to be evaluated in an assessable manner. A modified powered exponential correlation function is introduced, which in conjunction with a gradient based space-filling design resulted in an averaged 8 times smaller mean squared error compared to other correlation design combinations. Furthermore, sequentially fitting the data is shown to produce the best overall fits, when used in combination with space-filling designs.",
     "keywords": ["Bubble column", "CFD", "Computer experiment", "Kriging"]},
    {"article name": "Improved single phase modeling of propylene polymerization in a fluidized bed reactor",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.07.015",
     "publication date": "01-2012",
     "abstract": "An improved model for the production of polypropylene in a gas phase fluidized bed reactor was developed. Comparative simulation studies were carried out using the well-mixed, constant bubble size and the improved models. The improved model showed different prediction characteristics of polymer production rate as well as heat and mass transfer behavior as compared to other published models. All the three models showed similar dynamic behavior at the startup conditions but the improved model predicted a narrower safe operation window. Furthermore, the safe ranges of variation of the main operating parameters such as catalyst feed rate and superficial gas velocity calculated by the improved and well mixed models are wider than that obtained by the constant bubble size model. The improved model predicts the monomer conversion per pass through the bed which varies from 0.28 to 5.57% within the practical ranges of superficial gas velocity and catalyst feed rate.",
     "keywords": ["Mathematical modeling", "Polymerization", "Fluidized bed reactor", "Polypropylene", "Ziegler\u2013Natta catalyst"]},
    {"article name": "CFD simulation with experiments in a dual circulating fluidized bed gasifier",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.07.005",
     "publication date": "01-2012",
     "abstract": "Gas and particles hydrodynamic behaviors were investigated in a pilot-scale cold-mode riser and a bubbling fluidized bed gasifier by means of experiment and computational fluid dynamics (CFD).Six different experimental sets were conducted in the cold-rig dual fluidized bed (DFB) at different gas velocities in both the riser and the recycle chamber aeration. A two-dimensional (2D) multi-fluid Eulerian model incorporating the kinetic theory of granular flows was applied to identify unsteady-state behaviors of the fluidized bed.The CFD model predicts well the solid circulation rate in the cold-rig DFB for all the six experimental runs. A discrepancy between experiment and simulation is observed in the axial solid holdup along the riser. The simulation results demonstrate that the cold-bed simulation can be used to predict the solid circulation rate for the hot-bed operation of the DFB gasifier.",
     "keywords": ["Dual circulating fluidized-bed (DFB) gasifier", "Hydrodynamics", "Solid circulation rate", "Computational fluid dynamics (CFD)", "Simulation", "Multiphase flow"]},
    {"article name": "A note on the solution of singular boundary value problems arising in engineering and applied sciences: Use of OHAM",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.08.008",
     "publication date": "01-2012",
     "abstract": "Many problems of physical and engineering sciences are described by singular boundary value problems (SBVPs). Due to the presence of singularity, these problems pose difficulties in obtaining their solutions, and various solution schemes have been proposed to overcome these difficulties. The present work is concerned with the application of one such recently developed method, namely optimal homotopy analysis method (OHAM), to solve SBVPs. The OHAM has certain advantages: (i) it is a general method, (ii) contains an adjustable parameter to control the convergence of solution, and (iii) for certain choices of auxiliary quantities, its working resembles with those of other similar methods.The effectiveness of the OHAM has been evaluated by successfully solving two SBVPs given in recent literature as well a SBVP related to the reaction\u2013diffusion process in a spherical catalyst. The obtained results for these problems show an excellent agreement when compared with the numerical/exact/available solutions.",
     "keywords": ["Reaction\u2013diffusion process", "Effectiveness factor", "Adomian decomposition method", "Homotopy analysis method"]},
    {"article name": "A more efficient simulator of particle size distribution in slurry phase polyolefin systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.06.003",
     "publication date": "01-2012",
     "abstract": "The Segregation Approach is used as an alternative to Population Balance Modelling (PBM) to predict particle size distributions in olefin polymerisation reactors. The Segregation Approach affords simpler formulation, more explicit integration of mixing characteristics, particle growth and initial size distribution, and easier extension to unsteady-state conditions and complex kinetics.The evolution of a non-uniformly sized feed of catalyst particles in a polymer reactor system is simulated. While the two approaches yield identical predictions of the particle size distribution, in terms of computation time the Segregation model (0.09\u20131.7\u00a0s) far outperforms the PBM (1.8\u20136.2\u00a0s). This is because the sets of differential equations required by the PBM are avoided by the Segregation Approach.Based on this result, the Segregation model is described in the context of developing a model which can be used for the real-time model-based control and grade transition trajectory optimisation of olefin polymerisation processes.",
     "keywords": ["Population Balance Modelling", "Segregation Approach", "Particle size distribution", "Olefin polymerization", "Model-based control"]},
    {"article name": "A multi-scale framework for multi-phase equilibrium flash",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.07.011",
     "publication date": "01-2012",
     "abstract": "A general multi-scale framework for multi-component, multi-phase equilibrium flash calculations, which uses information at the molecular and bulk fluid length scales, is described. The multi-scale Gibbs\u2013Helmholtz constrained (GHC) EOS approach of Lucia (2010) is extended to include the use of (1) coarse grained NTP Monte Carlo simulations to gather pure component internal energies of departure, (2) a new linear mixing rule for internal energies of departure for mixtures, (3) a novel expression for partial fugacity coefficients for the GHC EOS, and (4) a novel flash algorithm that uses complex-valued compressibility factors and densities to assist in phase existence determination.Many numerical results for mixtures of CO2\u2013water, NaCl\u2013water, and CO2\u2013NaCl\u2013water are used to show that the GHC EOS flash approach is superior to all other approaches currently available. Many geometric illustrations are presented to elucidate key concepts and many experimental validations are used to substantiate claims of superiority.",
     "keywords": ["Gibbs\u2013Helmholtz constrained cubic equation of state", "Monte Carlo simulation", "Multi-scale modeling", "Multi-phase equilibrium flash"]},
    {"article name": "The HELD algorithm for multicomponent, multiphase equilibrium calculations with generic equations of state",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.07.009",
     "publication date": "01-2012",
     "abstract": "The HELD (Helmholtz free Energy Lagrangian Dual) algorithm is proposed to solve the isothermal, isobaric phase equilibrium problem (P,T flash). The flash is posed as a minimisation of the Helmholtz free energy in the volume\u2013composition space, reformulated through duality theory. The proposed solution strategy consists of: an initialisation stage, containing a stability test; a phase identification stage, in which linear and nonconvex optimisation problems are solved alternatively; and an acceleration and convergence stage. The stability test is solved with a tunneling algorithm and the nonconvex part of the second stage with a multistart approach. Examples are presented for three equations of state, SRK, SAFT-HS and SAFT-VR. Non-ideal mixtures of up to fifteen components are examined; they exhibit features such as azeotropy, liquid\u2013liquid, and liquid\u2013liquid\u2013liquid equilibria. The HELD algorithm is found to be reliable over a variety of challenging phase behaviour, converging to the best known solution in all of the calculations undertaken.",
     "keywords": ["Phase equilibrium", "Phase stability", "Helmholtz free energy", "Global optimisation", "Equations of state", "Statistical associating fluid theory (SAFT)"]},
    {"article name": "Process simulation and optimal design of membrane separation system for CO2 capture from natural gas",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.08.002",
     "publication date": "01-2012",
     "abstract": "Membrane process, a relatively new technology among other available techniques, can be used for the purpose of CO2 capture from natural gas. Over the decades, membrane performance has been described by different mathematical models, but there is limited work done in the field of process simulation where membrane models can be incorporated with other unit operations using commercially available simulator. In this paper, a two dimensional cross flow mathematical model for membrane separation has been incorporated with Aspen HYSYS as a user defined unit operation in order to optimize and design the membrane system for CO2 capture from natural gas. Parameter sensitivities, along with process economics, have been studied for different design configurations (including recycle streams and multiple stages). It has been observed that double stage with permeate recycle system gives the optimum design configuration due to minimum process gas cost involved with it.",
     "keywords": ["CO2 capture", "Membrane Process", "Membrane modeling", "Process simulation"]},
    {"article name": "Demand-driven optimization approach for biomass utilization networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.09.005",
     "publication date": "01-2012",
     "abstract": "Building Biomass Networks (B-NETs) is one of the techniques used to overcome the seasonal fluctuation in biomass supply and tapping new utilizations ways. The B-NETs are built based on a super class model for the available and possible kinds of utilization processes in the local area. Hence, the decision of selecting possible networks and scenarios can be made before using optimization methods to fix on the optimal network. In this paper, an optimization model for a Demand-driven biomass processing network is proposed. This is done through selecting alternative production paths for the same product depending on the resources availability. The unit process capacities and biomass resource availability constraints were presented to overcome their limitations in the local area. The genetic algorithms (GAs) were used in solving the problem because of their ability to deal with large search spaces and capability to calculate material flows, through networks, with no previous estimations.",
     "keywords": ["Biomass utilization networks", "Demand-driven", "Network flow", "Genetic algorithms"]},
    {"article name": "Studies on the effect of non-isothermal mixing on water-using network's energy performance",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.07.007",
     "publication date": "01-2012",
     "abstract": "This study explores the effect of different types of non-isothermal mixing on water-using network's utility consumption target, and some non-isothermal mixing rules are deduced, which can be used to estimate if utility will increase, decrease or remain unchanged after non-isothermal mixing. The energy penalty caused by heterogeneous mixing can be eliminated by decreasing the temperature approach between hot and cold streams through indirect heat transfer before mixing, so that the mixing can remain as a means of direct heat transfer when synthesizing heat-integrated water networks. Based on the non-isothermal mixing rules, one can make full use of direct heat transfer by mixing to obtain a simpler network structure and avoid the possibility of an energy penalty caused by improper non-isothermal mixing.",
     "keywords": ["Heat-integrated water networks", "Non-isothermal mixing", "Mixing rules", "Energy penalty"]},
    {"article name": "Strategies for the robust simulation of thermally coupled distillation sequences",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.06.014",
     "publication date": "01-2012",
     "abstract": "This paper presents a novel strategy for the simulation of thermally coupled distillation sequences using process simulators. First, we show that the two side stream connections involved in a \u2018thermal couple\u2019 can be accurately substituted by a combination of a material stream and heat flow; enabling a sequence of thermally coupled distillation columns to be simulated without recycle streams, similar to conventional simulations of zeotropic distillation sequences. In fact, using this method, a sequence of thermally coupled distillation columns is not more difficult to converge than other distillation systems without recycles. Furthermore, in most cases, this approach introduces negligible errors, and provides excellent starting points for rigorous simulations of actual thermally coupled systems with recycle streams.Different examples, including mixtures of hydrocarbons (C4s\u2013C5s\u2013C6s), aromatics (BTX), alcohols, non-ideal azeotropic systems (acetone, benzene, chloroform) and systems involving 4 or 5 components are presented. In addition, various thermodynamically equivalent configurations, corresponding to different alternatives for implementing this approach, are discussed.",
     "keywords": ["Distillation", "Simulation", "Thermally coupled distillation"]},
    {"article name": "Modified simple column configurations for quaternary distillations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.06.015",
     "publication date": "01-2012",
     "abstract": "Distillation is the most energy demanding, capital intensive and widely used separation method. For quaternary distillation, the conventional simple column configurations employ three columns to achieve four products with any specified purity. In this paper, the modified simple column configurations are studied which use two columns to accomplish four-component separations with any specified purity. The modified simple column configurations have the significant process intensification feature that they use less number of columns and heat exchangers. To study the performance of the modified simple column configurations different four-component mixtures with different feed compositions were analyzed. The performance was studied in terms of energy, capital cost saving and column sections function. It was demonstrated that for the composition cases considered the proposed configurations have similar or better energy performance than the traditional simple column sequences, however significant capital cost reductions are achieved in all the cases studied.",
     "keywords": ["Process intensification", "Multicomponent distillation", "Energy and capital cost saving", "Process synthesis and simulation"]},
    {"article name": "Economic and environmental strategies for process design",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.09.016",
     "publication date": "01-2012",
     "abstract": "This paper first addresses the definition of various objectives involved in eco-efficient processes, taking simultaneously into account ecological and economic considerations. The environmental aspect at the preliminary design phase of chemical processes is quantified by using a set of metrics or indicators following the guidelines of sustainability concepts proposed by IChemE (2001). The resulting multiobjective problem is solved by a genetic algorithm following an improved variant of the so-called NSGA II algorithm. A key point for evaluating environmental burdens is the use of the package ARIANE\u2122, a decision support tool dedicated to the management of plants utilities (steam, electricity, hot water, etc.) and pollutants (CO2, SO2, NO, etc.), implemented here both to compute the primary energy requirements of the process and to quantify its pollutant emissions. The well-known benchmark process for hydrodealkylation (HDA) of toluene to produce benzene, revisited here in a multiobjective optimization way, is used to illustrate the approach for finding eco-friendly and cost-effective designs. Preliminary biobjective studies are carried out for eliminating redundant environmental objectives. The trade-off between economic and environmental objectives is illustrated through Pareto curves. In order to aid decision making among the various alternatives that can be generated after this step, a synthetic evaluation method, based on the so-called Technique for Order Preference by Similarity to Ideal Solution (TOPSIS) (Opricovic & Tzeng, 2004), has been first used. Another simple procedure named FUCA has also been implemented and shown its efficiency vs. TOPSIS. Two scenarios are studied; in the former, the goal is to find the best trade-off between economic and ecological aspects while the latter case aims at defining the best compromise between economic and more strict environmental impacts.",
     "keywords": ["Multiobjective optimization", "Genetic algorithm", "Eco-efficiency", "Economic criterion", "Environmental impact"]},
    {"article name": "A systematic synthesis and design methodology to achieve process intensification in (bio) chemical processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.08.005",
     "publication date": "01-2012",
     "abstract": "Process intensification (PI) has the potential to improve existing processes or create new process options, which are needed in order to produce products using more sustainable methods. In principle, an enormous number of process options can be generated but where and how the process should be intensified for the biggest improvement is difficult to identify. In this paper the development of a systematic computer aided model-based synthesis and design methodology incorporating PI is presented. In order to manage the complexities involved, the methodology employs a decomposition-based solution approach. Starting from an analysis of existing processes, the methodology generates a set of process options and reduces their number through several screening steps until from the remaining options, the optimal is found. The application of the methodology is highlighted through a case study involving the chemo-enzymatic synthesis of N-acetyl-d-neuraminic acid (Neu5Ac).",
     "keywords": ["A N-acetyl-d-glucosamine", "N-acetyl-d-glucosamine", "Alk1 sodium hydroxide", "sodium hydroxide", "Alk2 calcium hydroxide", "calcium hydroxide", "B N-acetyl-d-manosamine", "N-acetyl-d-manosamine", "C pyruvic acid", "pyruvic acid", "Chrom chromatography", "chromatography", "Cryst crystallization", "crystallization", "D N-acetyl-d-neuraminic acid", "N-acetyl-d-neuraminic acid", "E1 N-acylglucosamine-2-epimerase", "N-acylglucosamine-2-epimerase", "E11 immobilized recombinant of E1", "immobilized recombinant of E1", "E2 N-acetyl-d-neuraminic acid aldolase", "N-acetyl-d-neuraminic acid aldolase", "E22 immobilized recombinant of E2", "immobilized recombinant of E2", "Evap evaporator", "evaporator", "ICAS Integrated Computer-Aided System", "Integrated Computer-Aided System", "ICASSIM process simulation software in ICAS", "process simulation software in ICAS", "LL liquid\u2013liquid-extraction with reactive solvent", "liquid\u2013liquid-extraction with reactive solvent", "LLM liquid\u2013liquid-extraction with methanol", "liquid\u2013liquid-extraction with methanol", "MINLP mixed integer non-linear programming", "mixed integer non-linear programming", "MoT modelling testbed, see Heitzig et al., 2011", "modelling testbed, see Heitzig et al., 2011", "NIU number of identified units", "number of identified units", "NPO number of process options", "number of process options", "OPR one-pot reactor", "one-pot reactor", "OPRE one-pot reactive extractor", "one-pot reactive extractor", "PBA/TOMAC phenylboronic acid/trioctylmethylammonium chloride", "phenylboronic acid/trioctylmethylammonium chloride", "PI process intensification", "process intensification", "Prec precipitation", "precipitation", "Ps processing steps", "processing steps", "R1 reaction 1", "reaction 1", "R2 reaction 2", "reaction 2", "S separation", "separation", "SP sub-problem", "sub-problem", "WC whole cell", "whole cell", "Methodology", "Process synthesis", "Process intensification", "Knowledge base"]},
    {"article name": "Use of glass transitions in carbohydrate excipient design for lyophilized protein formulations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.07.018",
     "publication date": "01-2012",
     "abstract": "This work describes an effort to apply methods from process systems engineering to a pharmaceutical product design problem, with a novel application of statistical approaches to comparing solutions. A computational molecular design framework was employed to design carbohydrate molecules with high glass transition temperatures and low water content in the maximally freeze-concentrated matrix, with the objective of stabilizing lyophilized protein formulations. Quantitative structure\u2013property relationships were developed for glass transition temperature of the anhydrous solute, glass transition temperature of the maximally concentrated solute, melting point of ice and Gordon\u2013Taylor constant for carbohydrates. An optimization problem was formulated to design an excipient with optimal property values. Use of a stochastic optimization algorithm, Tabu search, provided several carbohydrate excipient candidates with statistically similar property values, as indicated by prediction intervals calculated for each property.",
     "keywords": ["Molecular design", "Excipient", "Lyophilization", "Protein aggregation", "Stochastic optimization"]},
    {"article name": "Real-time product quality control for batch processes based on stacked least-squares support vector regression models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.05.015",
     "publication date": "01-2012",
     "abstract": "A novel real-time final product quality control method for batch operations based on stacked least-squares support vector regression models (stacked LSSVR) is proposed. It combines midcourse correction (MCC) and batch-to-batch control. To enhance the model prediction accuracy and generalization capability, a stacked LSSVR approach is presented. Quality control is achieved by predicting the final product quality using stacked LSSVR models and adjusting process variables at some pre-specified decision points. Then a decision is made on whether or not control action is taken at every decision point. Once the control action is expected, the manipulated variable values are calculated and the control action is taken to bring the off-spec product quality back to the target. Then a batch-to-batch control is used to overcome the model plant mismatches and unmeasured disturbances. At last, the proposed modeling and quality control strategy is illustrated on a simulated batch reactor.",
     "keywords": ["Quality control", "Least-squares support vector regression", "Optimization", "Batch processes"]},
    {"article name": "Solving dynamic optimization infeasibility problems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.07.003",
     "publication date": "01-2012",
     "abstract": "Dynamic real-time optimization (DRTO) systems sometimes fail when solving intrinsic optimization problems. There are situations where the solution is infeasible due to the initial conditions, constraint changes during operation, or even the presence of conflicts on constraint specifications. By using a goal programming approach, this work proposes a method to solve these infeasibilities by reformulating the differential-algebraic optimization problem as a multi-objective dynamic optimization problem with path constraint relaxations. Three examples were solved exploring the characteristics of such infeasibility problems. The results demonstrate the ability of the proposed method in identifying and relaxing the constraint violations, increasing the robustness of DRTO systems.",
     "keywords": ["Dynamic optimization", "Infeasibility", "Multi-objective optimization", "Real-time optimization", "Constraint relaxation"]},
    {"article name": "Model predictive control of a twin-screw extruder for thermoplastic vulcanizate (TPV) applications",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.07.001",
     "publication date": "01-2012",
     "abstract": "This paper deals with the design and experimental study of a model predictive controller (MPC) for the regulation of the melt temperature (MT) and the motor load (ML) in a pilot plant co-rotating twin-screw extruder for thermoplastic vulcanizate (TPV) applications. It is shown that these two process variables have significant influence on the crosslinking of the rubber chains by initiation of curatives such as peroxide and phenolic agents. Since the measurement of the degree of crosslinking is performed only in off-line manner, the above-mentioned control methodology can offer inferential control of the process. The manipulated variables are the screw speed (SS) and the wall temperature (WT). The previously identified multi variable model (Trifkovic et al., 2010) was used for the MPC design. In order to perform the real time control studies, the process communication was established from the local PLC (programmable logic controller) to the Advanced Control and model predictive control toolbox in MATLAB software, via OPC (OLE (Object Linking and Embedding) for Process Control) server. Both, set-point changes and disturbance effects were applied on the process for comprehensive evaluation of the controller performance.",
     "keywords": ["Twin-screw extruder", "Model predictive control", "Thermoplastic vulcanizate", "Model identification", "OPC"]},
    {"article name": "Model based control of a fed batch process used for catalyst support production",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.07.002",
     "publication date": "01-2012",
     "abstract": "In this paper, a model based control of a fed batch process used for catalyst support synthesis is proposed. The considered model is derived from mass balances and an algebraic equilibrium equation identified using titration experiments. The catalyst quality requires that three variables are controlled during the production process: pH, volume and concentration. A global control structure enabling to simultaneously control the three considered variables is given. The pH controller adjusts a reactive base flow rate while the volume and concentration controllers act on water and reactive acid flow rates, respectively. The process model is used to derive control laws that impose asymptotically stable dynamics. Representative experimental results are presented to prove the relevance of the proposed approach.",
     "keywords": ["Catalyst support", "Alumina synthesis", "pH control", "Concentration control", "Fed-batch process"]},
    {"article name": "Comparative control study of a simulated batch rectifier",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.09.013",
     "publication date": "01-2012",
     "abstract": "This article proposes a nonlinear control strategy for a batch rectifier. The hybrid control strategy comprises a state estimator, namely Luenberger-like nonlinear estimator (LNE) and the feedback linearizing controller (FLC). The observation scheme aims to estimate the imprecisely known parameter based on the available measurement. For the representative distillation unit, the state predictor is formulated using only a component material balance equation around the condenser\u2013reflux drum system. It clearly indicates the existence of a process/model mismatch and this discrepancy is efficiently taken care of by the corrector part of the LNE. In the subsequent phase, the FLC is synthesized for the example system. The derived nonlinear FLC\u2013LNE controller shows better performance compared to a traditional proportional integral (PI) law. The simple structure, straightforward design and good performance make this nonlinear controller attractive for online use.",
     "keywords": ["Batch rectifier", "Simulation", "Luenberger-like nonlinear estimator", "Feedback linearizing controller"]},
    {"article name": "Nonlinear fuzzy control of a fed-batch reactor for penicillin production",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.07.016",
     "publication date": "01-2012",
     "abstract": "The process of penicillin production is characterized by nonlinearities and parameter uncertainties that make it difficult to control. In the paper the development and testing of a multivariable fuzzy control system that makes use of type-2 fuzzy sets for the control of pH and temperature are described. The performance of the type-2 fuzzy logic control system (T2FLCS) is compared by simulation with that of a type-1 fuzzy logic control system (T1FLCS) and that of a control system with traditional proportional-integral-derivative (PID) controllers proposed in the literature. The fuzzy controllers are optimized using an ANFIS algorithm. The best results are obtained with the T2FLCS particularly when uncertainties are present in the control system.",
     "keywords": ["Fed-batch fermentor", "Non-linear systems", "Uncertainty", "Type-2 fuzzy logic controller"]},
    {"article name": "Controllability analysis of heat integrated distillation systems for a multicomponent stream",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.09.017",
     "publication date": "01-2012",
     "abstract": "Understanding the dynamic behavior of distillation columns has received considerable attention due to the fact that distillation is one of the most widely used unit operations in chemical process industries. Heat integrated distillation sequences can provide significant energy savings; however, they exhibit a complex structure which appears to affect their controllability properties. One potential solution to this problem has been suggested through the operation of heat integrated sequences under the conditions that do not provide minimum energy requirements. But, this work proved that the process under the conditions of minimum energy requirements can operate with the best controllability properties. In this work, the controllability properties were analyzed with the application of the Morari resiliency index, condition number and relative gain array, which were frequency-dependent indices. The results showed that the controllability of the best heat integrated distillation column with the minimum total annual cost was better than other sequences.",
     "keywords": ["Controllability", "TAC", "Distillation", "Sequence", "Integration", "Programming"]},
    {"article name": "Online soft sensor for hybrid systems with mixed continuous and discrete measurements",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.09.004",
     "publication date": "01-2012",
     "abstract": "Online state prediction and fault detection are typical tasks in the chemical industry. In practice it often happens that some variables, important and critical for quality control, cannot be measured online due to such restrictions as cost and reliability. An uncertainty existing in real systems allows to use a probabilistic approach to online state estimation. Such an approach is proposed in this paper. Different types of information appearing in an online diagnostic system are processed via combination of algorithms subject to probability distributions. This combination of algorithms is presented as a decomposed version of Bayesian filtering. In this paper, the proposed solution is specialized for a system with mixed both continuous and discrete-valued measurements and unobserved variables.",
     "keywords": ["Online state prediction", "Hybrid filter", "State-space model", "Mixed data"]},
    {"article name": "Bond graphs for the diagnosis of chemical processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.07.008",
     "publication date": "01-2012",
     "abstract": "The paper deals with the use of coupled bond graph as an integrated decision tool for health monitoring of chemical reactors. A bond graph model based diagnostic strategy is adopted to detect and isolate kinetic and thermodynamic drift of chemical reactors due to the appearance of secondary reactions. Furthermore, the causal and structural properties of the bond graph model are used not only for system dynamics simulation but also to design Fault Detection and Isolation (FDI) algorithms (i.e. the generation of formal fault indicators) for online supervision of faults affecting sensors, actuators, physico-chemical components and phenomena.",
     "keywords": ["Chemical process", "Bond graph", "Analytical redundancy", "Diagnosis", "Health monitoring", "Fault detection and isolation"]},
    {"article name": "Enzymatic reactor selection and derivation of the optimal operation policy, by using a model-based modular simulation platform",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.06.006",
     "publication date": "01-2012",
     "abstract": "Design and optimal operation of an enzymatic process depend on the characteristics of the free- vs. immobilized-enzyme, purification problems, materials and operation costs. When the enzyme stabilization is not fully successful, the optimal choice of the reactor type and operation mode is depending on the process and enzyme deactivation characteristics. Quick simulation of operation modes based on the process kinetics and a library of reactor models can help in comparing alternatives for getting the final decision. The analysis is exemplified for two enzymatic processes, by determining the optimal operating policy for several tested reactors: classic batch, batch with intermittent addition of enzyme, semi-batch reactor, continuously operated packed-bed column, or a mechanically agitated reactor with immobilized enzyme in small beads. Prediction of reactor performances points out the best choice for the given enzyme characteristics, and also conditions allowing to switch the production from large-volume batches to the continuous operation mode.",
     "keywords": ["Enzymatic reactor", "Optimization", "Modular simulation platform", "Starch hydrolysis", "Inulin hydrolysis"]},
    {"article name": "Simultaneous design and scheduling of a semicontinuous/batch plant for ethanol and derivatives production",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.08.004",
     "publication date": "01-2012",
     "abstract": "The interest on renewable fuels has greatly increased in the last years. Particularly, ethanol production arises as a good solution to many current economic-environmental problems. Yeast production from the ethanol residuals constitutes a sustainable alternative. Usually, this kind of plants is designed using single product campaigns. However, since yeast degradation is fast and a continuous supply must be assured, the mixed product campaign policy is the most appropriate. Besides, a stable context can be assumed to justify this approach that takes advantage of the special structure of the plant. Therefore, in this paper, a mixed integer linear programming model is formulated for simultaneous design and scheduling of a semicontinuous/batch plant for ethanol and derivatives production. The optimal plant configuration, unit sizes, number of batches of each product in the campaign and its sequencing is obtained in order to fulfill the ethanol and yeast demands minimizing the investment cost.",
     "keywords": ["Batch/semicontinuous plant design", "Scheduling", "Ethanol and yeast productions", "Mixed integer linear programming"]},
    {"article name": "Feasibility analysis of black-box processes using an adaptive sampling Kriging-based method",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.06.005",
     "publication date": "01-2012",
     "abstract": "This paper presents a new approach for performing feasibility analysis over a multivariate factor space when the explicit form of a process model is lacking or when its evaluation is expensive. Specifically, two issues are addressed: feasibility evaluation of black-box processes using Kriging and development of an adaptive sampling strategy in order to minimize sampling cost, while maintaining feasibility space accuracy. Kriging is chosen as the interpolating technique for constructing a response surface of the feasibility function as a function of the uncertain parameters when a set of input\u2013output data are available. The adaptive sampling strategy identifies critical regions and directs the search towards feasibility boundaries or where the Kriging prediction uncertainty is high. The average Kriging prediction error and cross-validation methods are used to validate the robustness of the produced model of the initial experimental design which is found to highly affect the final predicted feasible region.",
     "keywords": ["Feasibility analysis", "Black-box process", "Kriging interpolation", "Adaptive sampling"]},
    {"article name": "MILP formulations for single- and multi-mode resource-constrained project scheduling problems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.06.007",
     "publication date": "01-2012",
     "abstract": "This work presents new mixed-integer linear programming models for the deterministic single- and multi-mode resource constrained project scheduling problem with renewable and non-renewable resources. The modeling approach relies on the Resource-Task Network (RTN) representation, a network representation technique used in process scheduling problems, based on continuous time models. First, we propose new RTN-based network representation methods, and then we efficiently transform them into mathematical formulations including a set of constraints describing precedence relations and different types of resources. Finally, the applicability of the proposed formulations is illustrated using several example problems under the most commonly addressed objective, the makespan minimization.",
     "keywords": ["Project scheduling", "Single/multi-mode resource-constrained project scheduling", "Mixed-integer linear programming", "Resource-Task Network", "Project management"]},
    {"article name": "Data-driven prediction of the product formation in industrial 2-keto-l-gulonic acid fermentation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.06.012",
     "publication date": "01-2012",
     "abstract": "Mixed culture fermentation of Bacillus megaterium and Gluconobacter oxydans is widely used to produce 2-keto-l-gulonic acid (2-KGA), a key precursor for l-ascorbic acid synthesis. For such mixed cultivation, kinetic modelling is difficult because the interactions between the two strains are not well known yet. In this paper, data-driven prediction of the product formation is presented for the purpose of better process monitoring. A rolling learning-prediction approach based on neural networks is practiced to predict 2-KGA formation. Techniques associated with the approach, such as the data pretreatment and the rolling learning-prediction mechanism, are given in more detail. The validation results by using the data from commercial scale 2-KGA cultivation indicate that the prediction error is less than 5% in the later phase of fermentation and the reliable prediction time span is 8\u00a0h. The robustness of the prediction approach is further tested by adding extra noises to the process variables.",
     "keywords": ["2-Keto-l-gulonic acid", "Mixed culture", "Neural networks", "Product formation", "Rolling learning-prediction"]},
    {"article name": "Identification of economic potentials in production processes: An industrial case study",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.06.011",
     "publication date": "12-2011",
     "abstract": "Process improvement is mainly triggered by quality, safety, environmental considerations or economic potentials. First, ideas for problem solving and the related economic potential need to be analyzed before proceeding to measure, cost and risk assessment.Economic process improvement potentials and options for realization are identified using a Three Stage Method developed in BASF's Process Engineering department. The method is applied to a case study involving two products and a process improvement option.The main tool for this analysis is based on systems technology and material and energy flow analysis. Cost analysis and allocation are based on the results of material and energy flow analysis and provides a basis for developing engineering solutions. The method and the tools are applicable to a wide range of scales including global interactions of production processes and supply chains.",
     "keywords": ["Chemical process", "Cost allocation", "Process optimization", "Process systems engineering", "Material flow analysis", "System analysis"]},
    {"article name": "Equivalence of on-lattice stochastic chemical kinetics with the well-mixed chemical master equation in the limit of fast diffusion",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.05.008",
     "publication date": "12-2011",
     "abstract": "Well-mixed and lattice-based descriptions of stochastic chemical kinetics have been extensively used in the literature. Realizations of the corresponding stochastic processes are obtained by the Gillespie stochastic simulation algorithm and lattice kinetic Monte Carlo algorithms, respectively. However, the two frameworks have remained disconnected. We show the equivalence of these frameworks whereby the stochastic lattice kinetics reduces to effective well-mixed kinetics in the limit of fast diffusion. In the latter, the lattice structure appears implicitly, as the lumped rate of bimolecular reactions depends on the number of neighbors of a site on the lattice. Moreover, we propose a mapping between the stochastic propensities and the deterministic rates of the well-mixed vessel and lattice dynamics that illustrates the hierarchy of models and the key parameters that enable model reduction.",
     "keywords": ["Master equation", "Time scale separation", "Stiffness", "Kinetic Monte Carlo", "Gillespie algorithm", "Chemical kinetics"]},
    {"article name": "Hybrid modeling for the prediction of leaching rate in leaching process based on negative correlation learning bagging ensemble algorithm",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.02.012",
     "publication date": "12-2011",
     "abstract": "For predicting the leaching rate in hydrometallurgical process, it is very necessary to use an accurate mathematical model in leaching process. In this paper, a mechanism model is proposed for description and analysis of heat-stirring-acid leaching process. Due to some modeling errors existed between mechanism model and actual system, a hybrid model composed of mechanism model and error compensation model is established. A new support vector regression (SVR) bagging ensemble algorithm based on negative correlation learning (NCL) is investigated for solving the problem of error compensation. The sample of the next component learner is rebuilt continuously with this algorithm to improve the ensemble errors, and the optimum ensemble result also can be obtained. Simulation results indicate that the proposed hybrid model with the new algorithm has a better prediction performance in leaching process than other models.",
     "keywords": ["Leaching process", "Hybrid model", "Negative correlation learning", "Support vector regression", "Simulation"]},
    {"article name": "An integrated computational fluid dynamics\u2013process model of natural circulation steam generation in a coal-fired power plant",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.04.003",
     "publication date": "12-2011",
     "abstract": "A semi-detailed 1D process model for steam generation in a natural circulation boiler (thermosyphon loop) is linked to a detailed 3D computational fluid dynamics (CFD) model of the coal-fired furnace. The CFD model has been validated against typical data from a 500\u00a0MWe subcritical power plant. The heat flux distribution data from the CFD model are regressed into a function of height and used to drive the process model. The complex physics occurring in the furnace are coupled with the thermosyphon steam loop, resulting in circulation flows of around 4 times the feed flow. The steam side heat transfer coefficients are predicted in the process model and so the overall heat transfer coefficient for use in the CFD simulation can be re-evaluated as a function of height. The recalculated heat flux distribution is almost identical to the original, because the dominant resistance to heat flow is on the furnace side.",
     "keywords": ["Thermosyphon", "Boiler", "Power", "Process simulation", "CFD"]},
    {"article name": "Enabling a commercial computational fluid dynamics code to perform certain nonlinear analysis tasks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.03.008",
     "publication date": "12-2011",
     "abstract": "In this work we enable the commercial computational fluid dynamics code Fluent, to successfully trace a complete solution branch, even past turning points. Here the so-called Recursive Projection Method (RPM) is implemented as a computational shell \u201cwrapped\u201d around Fluent, in conjunction with a pseudo-arc-length method for convergence on the unstable branch. The case study is a mixed convection flow in a stagnation point chemical vapor deposition (CVD) reactor. Multiple steady states coexist over a range of inlet Reynolds numbers, due to the competition of the two dominant physical mechanisms: forced and free convection. Continuation on the solution branch reveals a curve consisting of a stable branch, dominated by free convection, followed, past the first turning point, by an unstable branch. Past a second turning point, follows another stable branch dominated by forced convection. Taking the problem a step further, it is augmented with a chemical model describing the deposition of silicon (Si) from silane (SiH4), silylene (SiH2) and hydrogen (H2). The solution branch does not alter since the gas mixture is dilute and the carrier gas, in this case nitrogen (N2), and the precursor, in this case SiH4, are of similar molar masses; the concentration differences cannot lead to solutal convection. Results for the mass fraction distribution inside the reactor and the film growth rates are reported in all parts of the solution branch.",
     "keywords": ["Computational fluid dynamics", "Unstable solutions", "Solution tracing", "Recursive Projection Method", "Chemical vapor deposition"]},
    {"article name": "CFD analysis of low temperature water gas shift reactor",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.02.011",
     "publication date": "12-2011",
     "abstract": "In the design of water gas shift reactors, the performance of catalysts is not known a priori and hence having a general kinetic expression will be of much help. Computational Fluid Dynamic study was carried out to investigate the performance of a packed bed reactor for different feed compositions using five commonly used types of macro kinetic models. User Defined Functions were developed for the reaction rate to predict the CO conversion in the reactor. The effects of temperature and time factor on CO conversion were studied. The Langmuir\u2013Hinshelwood model gave the best prediction for H2 rich mixtures. The Temkin model was better for higher CO concentrations, whereas the other models gave large deviations for the fixed bed reactor.",
     "keywords": ["WGSR", "Packed bed reactor", "Langmuir\u2013Hinshelwood model", "Temkin model", "CO conversion", "CFD"]},
    {"article name": "Development and implementation of a polydispersed multiphase flow model in OpenFOAM",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.04.011",
     "publication date": "12-2011",
     "abstract": "The two-phase flow solver implemented in the open-source OpenFOAM code was extended to a multiphase flow formulation (n dispersed and one continuous phases) and then coupled to the population balance equation (PBE) solution by the Direct Quadrature Method of Moments (DQMOM), originating a polydispersed multiphase flow solver. Although each dispersed phase has its own velocity field, the present implementation considers only the interfacial momentum exchange between the continuous and the dispersed phases. The multiphase flow formulation was described and the details of the PBE\u2013CFD coupling algorithms in OpenFOAM were provided. The implementation of the multiphase flow code was verified and evaluated against the original OpenFOAM two-phase flow solver for flow through a 2D backward facing step, using simplified breakage and aggregation kernels. The computational cost of both codes were compared for serial and parallel simulations.",
     "keywords": ["Computational fluid dynamics", "Population balance", "Multiphase Flow", "DQMOM", "OpenFOAM"]},
    {"article name": "Stochastic modeling and multi-objective optimization for the APECS system",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.02.003",
     "publication date": "12-2011",
     "abstract": "The Advanced Process Engineering Co-Simulator (APECS), developed at the U.S. Department of Energy's (DOE) National Energy Technology Laboratory, is an integrated software suite that enables the process and energy industries to optimize overall plant performance with respect to complex thermal and fluid flow phenomena. The APECS system uses the process-industry standard CAPE-OPEN (CO) interfaces to combine equipment models and commercial process simulation software with powerful analysis and virtual engineering tools. The focus of this paper is the CO-compliant stochastic modeling and multi-objective optimization capabilities provided in the APECS system for process optimization under uncertainty and multiple and sometimes conflicting objectives. The usefulness of these advanced analysis capabilities is illustrated using a simulation and multi-objective optimization of an advanced coal-fired, gasification-based, zero-emissions electricity and hydrogen generation facility with carbon capture.",
     "keywords": ["Stochastic modeling", "Multi-objective optimization", "Clean coal technologies", "Energy systems"]},
    {"article name": "Implementation of the waste reduction (WAR) algorithm utilizing flowsheet monitoring",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.02.004",
     "publication date": "12-2011",
     "abstract": "Environmental metric software can be used to evaluate the sustainability of a chemical based upon data from the chemical process used to manufacture it. An obstacle to the development of environmental metric software for use in chemical process modeling software has been the inability to obtain information about the process directly from the model. There have been past attempts to develop environmental metrics that make use of the process models, but there has not been an integrated, standardized approach to obtaining the process information required for calculating metrics. As a result, environmental evaluation packages are largely limited to use in a single simulation package, further limiting the development and adoption of these tools.This paper proposes a standardized mechanism for obtaining process information directly from a process model using a strongly integrated interface set, called flowsheet monitoring. The flowsheet monitoring interface provides read-only access to the unit operation and streams within the process model, and can be used to obtain the material flow data from the process streams. This material flow data can then be used to calculate process-based environmental metrics. The flowsheet monitoring interface has been proposed as an extension of the CAPE-OPEN chemical process simulation interface set.To demonstrate the capability of the flowsheet monitoring interfaces, the US Environmental Protection Agency (USEPA) WAste Reduction (WAR) algorithm is demonstrated in AmsterCHEM's COFE (CAPE-OPEN Flowsheeting Environment). The WAR add-in accesses the material flows and unit operations directly from the process simulator and uses flow data to calculate the potential environmental impact (PEI) score for the process. The WAR algorithm add-in is included in the latest release of COCO Simulation Environment, available from http://www.cocosimulator.org/.",
     "keywords": ["Process simulation", "Sustainability metrics", "Flowsheet monitoring", "WAR algorithm", "CAPE-OPEN"]},
    {"article name": "Simulation of a supercritical carbon dioxide extraction plant with three extraction vessels",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.04.002",
     "publication date": "12-2011",
     "abstract": "Although SuperCritical (SC) Fluid Extraction (SCFE) has been successfully applied commercially the last three decades, there is no systematic procedure or computational tool in the literature to scale-up and optimize it. This work proposes an algorithm to simulate dynamics in a multi-vessel (\u22653) high-pressure SCFE plant where extraction vessels operate in batches, and is thus forced to use simulated-countercurrent flow configuration to improve efficiency. The algorithm is applied to a three-vessel SCFE plant using a shrinking-core model to describe inner mass transfer in the substrate. As example, the extraction of oil from pre-pressed seeds using SC CO2 at 313\u00a0K and 30\u00a0MPa is simulated. After three cycles the process reaches a pseudo-steady-state condition that simplifies the estimation of plant productivity. Use of a three- instead of two-vessel SCFE plant increases oil concentration in the stream exiting the plant and decreases CO2 usage at the expense of increasing extraction time.",
     "keywords": ["Coupled partial differential equations", "Industrial scale", "Mathematical modeling", "Packed bed", "Simulation", "Supercritical fluid extraction"]},
    {"article name": "A model-based nucleation study of the combined effect of seed properties and cooling rate in cooling crystallization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.11.002",
     "publication date": "12-2011",
     "abstract": "Crystallization is a widely used unit operation for the production of pharmaceuticals, fertilizers, and fine chemicals. A commonly used crystallization operational objective is to produce large crystals under minimum nucleation rates. For cooling crystallization there are two key methods for minimizing nucleation, programmed cooling and seeding. In this paper, we evaluate the cooling and seeding methods through the detailed modeling of nucleation phenomena coupled with a population balance and dynamic optimization of this mathematical formulation. Extensive simulation results showed that initial seeding parameters, as proclaimed by others, do have a significant effect on the final product crystal size distribution. It also showed the significance of a combined seeding\u2013cooling approach where a joint cooling and seeding optimization gives superior performance to just optimizing the seed. Importantly, the developed model was highly instrumental in rapidly determining optimal combined seeding\u2013cooling profiles via dynamic model-based optimizations.",
     "keywords": ["Crystallization", "Seeding", "Seed chart", "Cooling", "Population balance", "Optimization", "Modeling"]},
    {"article name": "Numerical assessment of diffusion\u2013convection\u2013reaction model for the catalytic abatement of phenolic wastewaters in packed-bed reactors under trickling flow conditions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.013",
     "publication date": "12-2011",
     "abstract": "Computational fluid dynamics (CFD) modeling of trickle-bed reactors with detailed interstitial flow solvers has remained elusive mostly due to the extreme CPU and memory intensive constraints. Here, we developed a comprehensible and scalable CFD model based on the conservative unstructured finite volume methodology to bring new insights from the perspective of catalytic reactor engineering to gas\u2013liquid\u2013solid catalytic wet oxidation. First, the heterogeneous flow constitutive equations of the trickle bed system have been derived by means of diffusion\u2013convection\u2013reaction model coupled within a Volume-of-Fluid framework. The multiphase model was investigated to gain further evidence on how the effect of process variables such as liquid velocity, surface tension and wetting phenomena affect the overall performance of high-pressure trickle-bed reactor. Second, as long as the application of under-relaxation parameters, mesh density, and time stepping strategy play a major role on the final corroboration, several computational runs on the detoxification of liquid pollutants were validated accordingly and evaluated in terms of convergence and stability criteria. Finally, the analysis of spatial mappings for the reaction properties enables us to identify the existence of relevant dry zones and unveil the channeling phenomena within in the trickle-bed reactor.",
     "keywords": ["Computational fluid dynamics", "Trickle-bed reactor", "Multiphase flow", "Catalytic wet oxidation", "Total organic carbon", "Temperature"]},
    {"article name": "A sharp cut algorithm for optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.11.010",
     "publication date": "12-2011",
     "abstract": "In this paper, we introduce a new cutting plane algorithm which is computationally less expensive and more efficient than Kelley\u2019s algorithm. This new cutting plane algorithm uses an intersection cut of three types of cutting planes. We find from numerical results that the global search method formed using successive linear programming and a new intersection set is at least twice as fast as Kelley\u2019s cutting planes. The necessary mathematical analysis and convergence theorem are provided. The key findings are illustrated via optimization of a cascade of three CSTRs.",
     "keywords": ["02.60.Pn", "Cutting plane", "Successive linear programming", "Sharp cut", "Convergence theorem"]},
    {"article name": "Using redundancy to strengthen the relaxation for the global optimization of MINLP problems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.035",
     "publication date": "12-2011",
     "abstract": "In this paper we present a strategy to improve the relaxation for the global optimization of non-convex MINLPs. The main idea consists in recognizing that each constraint or set of constraints has a meaning that comes from the physical interpretation of the problem. When these constraints are relaxed part of this meaning is lost. Adding redundant constraints that recover that physical meaning strengthens the relaxation. We propose a methodology to find such redundant constraints based on engineering knowledge and physical insight.",
     "keywords": ["Global optimization", "Reduction constraints", "Tight formulations"]},
    {"article name": "Analysis of the particle swarm algorithm in the optimization of a three-phase slurry catalytic reactor",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.06.001",
     "publication date": "12-2011",
     "abstract": "The Particle Swarm Optimization (PSO) method was employed to optimize an industrial chemical process characterized by being difficult to be optimized by conventional deterministic methods. The chemical process is a three phase catalytic slurry reactor (tubular geometry) in which the reaction of the hydrogenation of o-cresol producing 2-methyl-cyclohexanol is carried out. The optimization problem was formulated considering as input variables the operating conditions of the reactor and as objective function the maximization of productivity, subject to the environmental constraint of conversion. The process was represented by a multivariable non-linear rigorous mathematical model and in order to solve the optimization problem, the performance of the PSO algorithm was evaluated considering four sets of parameters values suggested by the literature. PSO demonstrated to be efficient and robust to solve the constrained optimization problem, independently of the values of the PSO parameters. The solution of the rigorous mathematical model of the reactor was associated with a high computational burden, and although the PSO algorithm presented high rate of convergence, the attempt to make possible the optimization in a timeframe suitable to real time applications failed because the algorithm lost robustness (fraction of the number of runs the algorithm reached the optimization goal) when run with a reduced number of function evaluations. Therefore, if this type of application is desired, simplified mathematical models with fast and simple numerical methods must be preferred.",
     "keywords": ["Multiphase reactors", "Optimization", "PSO"]},
    {"article name": "A new Lagrangian decomposition approach applied to the integration of refinery planning and crude-oil scheduling",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.03.026",
     "publication date": "12-2011",
     "abstract": "The aim of this paper is to introduce a methodology to solve a large-scale mixed-integer nonlinear program (MINLP) integrating the two main optimization problems appearing in the oil refining industry: refinery planning and crude-oil operations scheduling. The proposed approach consists of using Lagrangian decomposition to efficiently integrate both problems. The main advantage of this technique is to solve each problem separately. A new hybrid dual problem is introduced to update the Lagrange multipliers. It uses the classical concepts of cutting planes, subgradient, and boxstep. The proposed approach is compared to a basic sequential approach and to standard MINLP solvers. The results obtained on a case study and a larger refinery problem show that the new Lagrangian decomposition algorithm is more robust than the other approaches and produces better solutions in reasonable times.",
     "keywords": ["Refinery planning", "Crude-oil scheduling", "Mixed-integer nonlinear programming", "Lagrangian decomposition"]},
    {"article name": "Investigating the use of path flow indicators as optimization drivers in batch process retrofitting",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.12.008",
     "publication date": "12-2011",
     "abstract": "Improvement of batch processes has gained much attention in industry due to the competitive market and stricter environmental legislation. This paper focuses on a process flowsheet decomposition based methodology resulting in path flow indicators which are able to highlight process alternatives with an improved performance. The novel aspects introduced in this methodology are the implementation of a new path flow indicator category that focuses on unit occupancy time and the multi-objective process assessment in order to reveal sustainable retrofitting actions. Furthermore, the retrofit alternatives are not only classified according to the diverse objective functions but also to the differences observed in the path flow indicator matrix of the generated retrofit alternatives compared to the base case. This classification enables a more detailed analysis of the retrofit alternative impact and illustrates the potential of path flow indicators as optimization drivers. The methodology is exemplified in an industrial batch process case study.",
     "keywords": ["Retrofit method", "Path flow indicators", "Chemical batch process", "Sustainable process alternatives"]},
    {"article name": "A sustainability root cause analysis methodology and its application",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.05.004",
     "publication date": "12-2011",
     "abstract": "In the design of chemical/energy production systems, a major challenge is to identify the bottleneck issues and improve its sustainability effectively. Due to the multi-dimensional feature of sustainability, how to account for the impacts of various design factors and the cause-and-effect relationships can be very difficult. This paper will present a sustainability root cause analysis method based on the combination of Pareto Analysis and Fishbone diagram. The sustainability of the process is assessed incorporating economic, environmental, societal and efficiency concerns.This methodology is able to help the designers focus the attention on the most important fundamental causes, discover opportunities for sustainability improvement and provide critical guidance to design for sustainability. The efficacy of this methodology will be demonstrated through a case study on a biodiesel production technology.",
     "keywords": ["Root cause analysis", "Sustainability analysis", "The Pareto analysis", "Fishbone diagram", "Biodiesel"]},
    {"article name": "Numerical optimization of heat recovery steam cycles: Mathematical model, two-stage algorithm and applications",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.04.015",
     "publication date": "12-2011",
     "abstract": "Most of the advanced integrated energy systems need a heat recovery steam cycle (HRSC), either fired or unfired, that recovers the waste heat from gas turbines and process units in order to generate electric power and supply mechanical power to compressors, heat to endothermic processes, and steam to external users. The key feature of such HRSCs is the integration between the heat recovery steam generator (HRSG) and the external heat exchangers. This paper presents a rigorous mathematical programming model, a linear approximation, and a two-stage algorithm for optimizing the design of integrated HRSGs and HRSCs, simultaneously considering the HRSG together with the heat recovery steam network and the intensive steam cycle variables. A detailed application of the methodology is described for an integrated gasification combined cycle plant with CO2 capture and results for other interesting plants are reported. A significant efficiency gain is obtained with respect to usual practice designs.",
     "keywords": ["Heat recovery steam cycle", "Heat recovery steam generator", "Utility systems", "Heat integration", "Bi-level optimization", "Linear programming", "Derivative-free optimization"]},
    {"article name": "Wastewater minimization in multipurpose batch plants with a regeneration unit: Multiple contaminants",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.04.008",
     "publication date": "12-2011",
     "abstract": "Wastewater minimization can be achieved by employing water reuse opportunities. This paper presents a methodology to address the problem of wastewater minimization by extending the concept of water reuse to include a wastewater regenerator. The regenerator purifies wastewater to such a quality that it can be reused in other operations. This further increases water reuse opportunities in the plant, thereby significantly reducing freshwater demand and effluent generation. The mathematical model determines the optimum batch production schedule that achieves the minimum wastewater generation within the same framework. The model was applied to two case studies involving multiple contaminants and wastewater reductions of 19.2% and 26% were achieved.",
     "keywords": ["Wastewater minimization", "Regeneration", "Multiple contaminants"]},
    {"article name": "Synthesis of water networks considering the sustainability of the surrounding watershed",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.03.021",
     "publication date": "12-2011",
     "abstract": "This paper presents a new mathematical programming model for the optimal synthesis of recycle and reuse networks considering simultaneously the integration of the water network system and the surrounding watershed to satisfy process and environmental constraints. The model considers the optimal location of the new industrial facility to integrate its wastewater discharge to the environment with the surrounding watershed through a disjunctive formulation. The pollutants discharged for the new plant are tracked simultaneously with the other discharges to the watershed (i.e., residential, sanitary, industrial and extractions), and the natural phenomena that affect the composition of the watershed (i.e., evaporation, filtration, etc.), in addition to the chemical reactions that are carried out in the rivers. The objective function consists in minimizing the total annual cost that is constituted by the installation of the new plant cost (including the transportation for raw materials, products and services, as well as the land cost), the wastewater treatment costs (including the piping cost) and the fresh sources cost. Two example problems were used to show the applicability of the proposed methodology.",
     "keywords": ["Material flow analysis", "Optimization", "Optimal location of a new plant", "Watersheds", "Sustainable systems", "Mass integration", "Recycle and reuse networks", "Water integration"]},
    {"article name": "Dual-objective optimization of integrated water/wastewater networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.04.010",
     "publication date": "12-2011",
     "abstract": "The dual-objective optimization of an integrated water/wastewater network (IWWN) is addressed by targeting for minimum fresh water consumption at the same time with operating costs reduction. An IWWN is a recycle system composed of two oriented graphs, the first encoding the water-using units (WUs) and the second, the treatment units (TUs). Although internal recycles are forbidden ab initio for the WUs graph, external recycles from the appropriate TU to the WU whose inlet restrictions are met by the partially treated water are encouraged. The corresponding mathematical model was written. A synthetic example is proposed and analyzed under several scenarios with respect to the fresh water consumption, the magnitude of internal and treated water reuse and the investment/operating costs related to the active pipes network. A comparison is made regarding the differences in network topology and fresh water consumption implied by different points from the Pareto front (PF).",
     "keywords": ["Integrated wastewater network", "Fresh water minimization", "Costs optimization", "Multi-objective optimization", "Pareto front"]},
    {"article name": "Identification of uncertain MIMO Wiener and Hammerstein models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.05.013",
     "publication date": "12-2011",
     "abstract": "Several approaches can be found in the literature to perform the identification of block oriented models (BOMs). In this sense, an important improvement is to achieve robust identification to cope with the presence of uncertainty.In this work, two special and widely used BOMs are considered: Hammerstein and Wiener models. The models herein treated are assumed to be described by parametric representations. The approach introduced in this work for the identification of the multiple input\u2013multiple output (MIMO) uncertain model is performed in a single step. The uncertainty is described as a set of parameters which is found through the solution of an optimization problem.A distillation column simulation model is presented to illustrate the robust identification approach. This process is an interesting benchmark due to its well-known nonlinear dynamics. Both Hammerstein and Wiener models are used to represent this plant in the presence of uncertainty. A comparative study between these models is established.",
     "keywords": ["Wiener models", "Hammerstein models", "Robust identification"]},
    {"article name": "Modelling and dynamic optimization of thermal cracking of propane for ethylene manufacturing",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.05.010",
     "publication date": "12-2011",
     "abstract": "In tubular reactors inside a cracking furnace, heat transfer, thermal cracking reactions and coke buildup take place and closely interact with each other. It is important to understand the process and optimize its operation. A 1-dimensional (1D) pseudo-dynamic model was developed based on first principle and implemented in gPROMS\u00ae. Coke buildup inside the tube wall was also accounted for. The model was validated dynamically. The impact of process gas temperature profile, and constant tube outer wall temperature profile on product yields and coking rate are assessed. Finally, dynamic optimization was applied to the operation of this tubular reactor. The effects of coking on reduction of production time and the decoking cost have been considered. The tube outer wall temperature profile and steam to propane ratio in the feed were used as optimization variables. Dynamic optimization investigation indicates that it can improve operating profit by 13.1%.",
     "keywords": ["Mathematical modelling", "Dynamic optimization", "Tubular reactor", "Case study", "Ethylene", "Thermal cracking"]},
    {"article name": "Multicriteria dynamic optimization of an emulsion copolymerization reactor",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.05.014",
     "publication date": "12-2011",
     "abstract": "A multicriteria optimization approach based on an evolutionary algorithm has been developed to determine the optimal control policy for a fed-batch emulsion copolymerization reactor, particularly for styrene and butyl acrylate in the presence of n-C12 mercaptan as chain transfer agent. The process model was elaborated and validated experimentally in order to predict the global monomer conversion, the number and weight average molecular weights, the particle size distribution and the residual monomers mass fraction. The process objectives were to produce core\u2013shell particles (hard core and smooth shell) with specific end-use properties and high productivity. This has been achieved by the maximization of the monomers overall conversion at the end of the process and the minimization of the error between the glass transition temperature and a designed profile subject to a set of operational constraints. The nondominated Pareto solutions obtained were ranked according to a decision making aid method based on a decision maker preferences and experience using multi-attribute utility theory. Finally, the best solution was implemented experimentally.",
     "keywords": ["Emulsion copolymerization", "Chain-transfer agent", "Core\u2013shell morphology", "Multiobjective optimization", "Decision aid method", "Experimental implementation"]},
    {"article name": "Energy efficient control of a BTX dividing-wall column",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.03.024",
     "publication date": "12-2011",
     "abstract": "Dividing-wall column (DWC) is considered nowadays the new champion in distillation, as it can bring substantial reduction in the capital invested as well as savings in the operating costs. This work presents the simulation results of energy efficient control and dynamics of a dividing-wall column (DWC). In order to allow a fair comparison of the results with previously published references, the case-study considered here is the industrially relevant ternary separation of the mixture benzene\u2013toluene\u2013xylene (BTX) in a DWC. Rigorous simulations were carried out in Aspen Plus and Aspen Dynamics. Several conventional control structures based on PID control loops (DB/LSV, DV/LSB, LB/DSV, LV/DSB) were used as a control basis. These control structures were enhanced by adding an extra loop controlling the heavy component composition in the top of the prefractionator, by using the liquid split as an additional manipulated variable, thus implicitly achieving minimization of energy requirements. The results of the dynamic simulations show relatively short settling times and low overshooting especially for the DB/LSV and LB/DSV control structures. Moreover, the energy efficient control proposed in this work allows the operation of DWC with minimum energy requirements or maximum purity of products.",
     "keywords": ["Dividing-wall column", "Petlyuk", "Energy efficiency", "Dynamic optimization", "PID loops", "DWC"]},
    {"article name": "Expected energy analysis for industrial process planning problem with fuzzy time parameters",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.05.012",
     "publication date": "12-2011",
     "abstract": "Industrial process planning is to make an optimal decision in terms of resource allocation. The planning objective can be to minimize the time required to complete a task, maximize customer satisfaction by completing orders in a timely fashion and minimize the cost required to complete a task. Based on time and energy consumption in an industrial process planning problem, a novel energy analysis method is proposed to solve it. According to different constraints and credibility theory, typical expected value models of energy for it are presented. In addition, a hybrid intelligent optimization algorithm integrating fuzzy simulation, neural network and genetic algorithm is provided for solving the proposed expected value models. Some numerical examples are also given to illustrate the proposed concepts and the effectiveness of the used algorithm.",
     "keywords": ["Energy analysis", "Industrial process planning", "Fuzzy simulation", "Expected value", "Fuzzy optimization"]},
    {"article name": "Temporal and spatial Lagrangean decompositions in multi-site, multi-period production planning problems with sequence-dependent changeovers",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.004",
     "publication date": "12-2011",
     "abstract": "We address in this paper the optimization of a multi-site, multi-period, and multi-product planning problem with sequence-dependent changeovers, which is modeled as a mixed-integer linear programming (MILP) problem. Industrial instances of this problem require the planning of a number of production and distribution sites over a time span of several months. Temporal and spatial Lagrangean decomposition schemes can be useful for solving these types of large-scale production planning problems. In this paper we present a theoretical result on the relative size of the duality gap of the two decomposition alternatives. We also propose a methodology for exploiting the economic interpretation of the Lagrange multipliers to speed the convergence of numerical algorithms for solving the temporal and spatial Lagrangean duals. The proposed methods are applied to the multi-site multi-period planning problem in order to illustrate their computational effectiveness.",
     "keywords": ["Lagrangean decomposition", "Production planning", "Temporal decomposition", "Spatial decomposition"]},
    {"article name": "Resource-constrained production planning in semicontinuous food industries",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.04.012",
     "publication date": "12-2011",
     "abstract": "The resource-constrained production planning problem in semicontinuous multiproduct food industries is addressed. In particular, the case of yogurt production, a representative food process, in a real-life dairy facility is studied in detail. The problem in question is mainly focused on the packing stage, whereas timing and capacity constraints are imposed with respect to the batch stage to ensure the generation of feasible production plans. A novel mixed discrete/continuous-time mixed-integer linear programming model, based on the definition of families of products, is proposed. Timing and sequencing decisions are taken for product families rather than for products; thus, reducing significantly the model size. Additionally, material balances are realized for every particular product, permitting the detailed optimization of inventory and operating costs. Packing units operate in parallel and share resources. Qualitative as well as quantitative objectives are considered. Several industrial case studies, including also some unexpected events scenarios, have been solved to optimality.",
     "keywords": ["Production planning", "Resource-constrained planning", "Re-planning", "MILP", "Food industry", "Dairy industry"]},
    {"article name": "Novel genetic algorithm for short-term scheduling of sequence dependent changeovers in multiproduct polymer plants",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.05.002",
     "publication date": "12-2011",
     "abstract": "Polymer plants generally operate to produce different grades of product from the same reactor. Such systems commonly require short-term scheduling to meet market demand. One important requirement in continuous-time scheduling of such systems is to satisfy a variety of constraints, including identifying feasible sequences of the predecessor and successor jobs to effectively handle changeovers. In this study, a new genetic algorithm (GA) is proposed to solve such job sequencing problems. The proposed GA uses real-coded chromosome to represent job orders and their sequences in the schedule. The novelty is that the representation ensures that all constraints are satisfied a priori, except the sequence constraint which is handled by penalizing violations. Three important problems relevant to polymer industry are solved to obtain optimal schedules. The first deals with the sequencing constraint between individual product orders, the second with sequencing constraint between groups of product orders, while the third incorporates batching with scheduling.",
     "keywords": ["Grades", "Multi-objective optimization", "Real-coded genetic algorithm", "Linear programming", "Stochastic modeling"]},
    {"article name": "A novel optimization method to automated wet-etch station scheduling in semiconductor manufacturing systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.02.014",
     "publication date": "12-2011",
     "abstract": "This work addresses the short-term scheduling of one of the most critical stages in the semiconductor industry, the automated wet-etch station (AWS). An efficient MILP-based computer-aided tool is developed in order to achieve a proper synchronization between the activities of sequential chemical and water baths and limited automated wafer's lot transfer devices. The major goal is to find the optimal integrated schedule that maximizes the whole process productivity without generating wafer contamination. Several examples are successfully solved to illustrate the capabilities of the proposed method.",
     "keywords": ["MILP-based approach", "Short-term scheduling problem", "Semiconductor manufacturing system (SMS)", "Automated wet-etch station (AWS)", "Semiconductor wafer fabrication"]},
    {"article name": "A CP formulation for scheduling multiproduct multistage batch plants",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.043",
     "publication date": "12-2011",
     "abstract": "The short-term scheduling of multiproduct multistage batch plants is tackled in this paper by means of a constraint programming (CP) methodology. This approach, consisting of both a model and a search strategy, easily handles different features found in industrial environments: finite unit ready times, dissimilar parallel equipment at each stage, sequence-dependent changeovers, topology constraints, forbidden job-equipment assignments, order release times, as well as renewable resources limitations. It can also address various interstage storage and operational policies: UIS, NIS/ZW, NIS/UW, and mixed ones. Besides, it introduces two simple and efficient search methodologies based on domain knowledge, whose great impact on the computational performance is shown. The approach was extensively tested by means of several examples having various difficulty degrees. It rendered good computational results for a variety of interstage storage policies and objective functions. Moreover, this work shows that the default depth-first search strategy does not perform well for scheduling problems.",
     "keywords": ["Short-term scheduling", "Multiproduct multistage batch plants", "Constraint programming", "Search strategies"]},
    {"article name": "Integrated campaign planning and resource allocation in batch plants",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.03.025",
     "publication date": "12-2011",
     "abstract": "Operational planning in batch plants has been studied extensively in the literature. However, the focus has largely been on developing better formulations and solution approaches for large examples. This work attempts to capture the key aspects of the industrial planning activity such as interactions among the planner and other stakeholders and the effect of resource allocation on process performance. We present a simple mathematical formulation for integrated resource allocation and campaign planning in multiproduct batch plants. Our model enables decision support pertaining to campaign scheduling, sequence-dependent changeovers, key resource allocations, scheduled maintenance, inventory profiles with safety stock limitations, and new product introductions. To demonstrate the performance of our mathematical model, we consider a case study from a typical specialty chemical plant from the lube industry. We validate our approach using a series of dynamic business and market scenarios with planning horizons of up to 2 years.",
     "keywords": ["Production planning", "Medium-term", "Campaigns", "Multiproduct batch plants", "Resource allocation", "MILP"]},
    {"article name": "The multi-echelon vehicle routing problem with cross docking in supply chain management",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.03.028",
     "publication date": "12-2011",
     "abstract": "Multi-echelon distribution networks are quite common in supply chain and logistics. Deliveries of multiple items from factories to customers are managed by routing and consolidating shipments in warehouses carrying on long-term inventories. On the other hand, cross-docking is a logistics technique that differs from warehousing because products are no longer stored at intermediate depots. Instead, cross-dock facilities consolidate incoming shipments based on customer demands and immediately deliver them to their destinations. Hybrid strategies combining direct shipping, warehousing and cross-docking are usually applied in real-world distribution systems. This work deals with the operational management of hybrid multi-echelon multi-item distribution networks. The goal of the N-echelon vehicle routing problem with cross-docking in supply chain management (the VRPCD-SCM problem) consists of satisfying customer demands at minimum total transportation cost. A monolithic optimization framework for the VRPCD-SCM based on a mixed-integer linear mathematical formulation is presented. Computational results for several problem instances are reported.",
     "keywords": ["Supply chain management", "Logistics", "Distribution networks", "Vehicle routing and scheduling", "Cross-docking"]},
    {"article name": "An integrated knowledge-based approach for modelling biochemical reaction networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.03.030",
     "publication date": "12-2011",
     "abstract": "This paper presents a new methodology for constructing cellular network topologies by searching for new binding species and new reactions catalysed by the enzymes present. Our technique is knowledge-based and integrates several steps. Starting from a pre-determined list of enzymes in the system it (i) generates lists of binding species, (ii) constructs a reaction network using these species and (iii) finds pathways through this network, which link different substrates (raw materials) with target metabolites (pathway products). Graph-theory-based analysis of the two-dimensional structures of known binding species is used to compute pharmacophores, the structures and functional groups binding at the corresponding enzymes\u2019 sites. New binding species are obtained by searching in appropriate databases for existing compounds, which contain these pharmacophores. Reactions are constructed by generating all possible combinations of the binding species identified and by testing the feasibility (i.e. the ability to conserve atomic/molecular mass) of each constructed reaction. Generated reactions are required to be linearly independent in order to minimise the complexity of subsequent steps. Finally, pathways through the reaction network are computed to assess important reactions and metabolites for a given process. Our integrated procedure has been applied to two illustrative systems, the glycolysis and the citric acid cycle in Homo sapiens and Saccharomyces cerevisiae, respectively. New binding species and reactions were found for the enzymes involved. It was observed that some enzymes are very specific and only catalyse a small number of very similar reactions. Pathways were also constructed and analysed to demonstrate the relative importance of the metabolites involved.",
     "keywords": ["Metabolic networks", "Pharmacophores", "Graph-theory", "Reaction generation", "Reaction pathways", "Mechanism mis-match"]},
    {"article name": "On electrical load tracking scheduling for a steel plant",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.03.006",
     "publication date": "12-2011",
     "abstract": "Nolde and Morari (2010) study a steel manufacturing scheduling problem where the tasks must be scheduled such that electricity consumption matches to a pre-specified periodic energy chart. They propose a continuous time integer linear programming formulation to solve the problem. In this note, we present an alternative continuous time formulation, focused on the relative positions of tasks and time periods, that improves significantly the computation time.",
     "keywords": ["Steel plant", "Mixed integer linear programming", "Load tracking"]},
    {"article name": "Three-dimensional modeling of fluid catalytic cracking industrial riser flow and reactions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.12.014",
     "publication date": "11-2011",
     "abstract": "A three-dimensional and two-phase flow model to predict the dynamic behavior of a fluid catalytic cracking (FCC) industrial reactor was developed in this work. The study took into account heat transfer and chemical reactions. A four-lump model was proposed to represent the catalytic cracking reactions in which the heavy oil (gas oil) is converted into gasoline and light hydrocarbon gases. Gas acceleration inside the reactor due to molar expansion and a model to describe undesirable catalyst deactivation by coke deposition on its surface were also considered. An Eulerian description of the phases was used to represent the two-phase flow. A commercial CFD code (Ansys CFX version 11.0) was used to obtain the numerical data. Appropriate functions were implemented inside the CFX code to model the heterogeneous kinetics and catalyst deactivation. Results show nonuniform tendencies inside the reactor, emphasizing the importance of using three-dimensional models in FCC process predictions.",
     "keywords": ["Numerical simulation", "Gas\u2013solids flow", "Nonuniformity", "FCC", "Riser", "Cracking reactions"]},
    {"article name": "Rising of 3D catalyst particles in a natural convection dominated flow by a parallel DNS method",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.02.013",
     "publication date": "11-2011",
     "abstract": "We investigate the problem of particulate flows with heat transfer by parallel direct numerical simulation (DNS). Among other heat transfer problems, we examine in detail the case of a 3D spherical catalyst rising in an enclosure due to natural convection though it is heavier than the suspending fluid. Natural convection is created by heat transferred from the warmer particle to the fluid. Heat is assumed to be produced at a constant rate in the particle bulk. As expected, there exists a critical production rate that leads to the rising of the catalyst. Compared to the 2D circular cylinder counterpart, momentum and heat transfers are slower in 3D and the spherical catalyst rises for a lower production rate. At the numerical level, we employ a Distributed Lagrange Multiplier/Fictitious Domain formulation together with an operator-splitting algorithm to solve the coupled problem. Two families of Lagrange multiplier are introduced to relax the velocity and temperature constraints respectively. As suggested in Wachs (2009), particle collisions are handled by an efficient Discrete Element Method granular solver. As it is, the model is restricted to the case of homogeneous temperature over the particles. From a computational viewpoint, this work might be regarded as an extension of the method proposed in our previous contributions Dan and Wachs, 2010, Yu et al., 2006 to distributed computing with our new parallel code PeliGRIFF.1 This opens up new possibilities to study a broad range of applications in 3D and to get more insight in the comprehension of particulate flows with heat transfer. In particular, we examine how a bed of spherical catalysts can be self-fluidized as a result of the heat produced in the particles bulk, mimicking an exothermic catalyst reaction in a chemical engineering reactor.",
     "keywords": ["Particulate flow", "Distributed Lagrange Multiplier/Fictitious Domain method", "Discrete Element Method", "Heat transfer", "Catalysts", "Parallel computing"]},
    {"article name": "On the representation of QMOM as a weighted-residual method\u2014The dual-quadrature method of generalized moments",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.05.017",
     "publication date": "11-2011",
     "abstract": "Quadrature-closed moment-based methods for the solution of the population balance equation were analyzed. The QMOM was shown to be a particular case of a method based on generalized moments (QMoGeM). Then, a weighted-residual method (WRM) based on the generalized moments was derived (WRMoGeM). If it is closed by the Gauss\u2013Christoffel quadrature, the resulting method (QWRMoGeM) was shown to be identical to the QMoGeM, giving the correct representation of QMOM as a WRM.The WRMoGeM formulation was used to derive a new method (DuQMoGeM) that employs two quadrature rules, one for discretizing the particulate system and other to accurately integrate the integrals in the equations for the generalized moments. A Galerkin version of this method was implemented and used to solve several examples with known analytical solutions. The DuQMoGeM solutions for breakage and aggregation problems were shown to be more accurate than the QMOM solutions.",
     "keywords": ["Population balance", "Simulation", "Multiphase flow", "Multiphase reactors", "Quadrature-based methods", "Weight-residual methods"]},
    {"article name": "Numerical aspects and implementation of population balance equations coupled with turbulent fluid dynamics",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.04.001",
     "publication date": "11-2011",
     "abstract": "In this paper, we present numerical techniques for one-way coupling of CFD and Population Balance Equations (PBE) based on the incompressible flow solver FeatFlow which is extended with Chien\u2019s Low-Reynolds number k \u2013 \u025b turbulence model, and breakage and coalescence closures. The presented implementation ensures strictly conservative treatment of sink and source terms which is enforced even for geometric discretization of the internal coordinate. The validation of our implementation which covers wide range of computational and experimental problems enables us to proceed into three-dimensional applications as, turbulent flows in a pipe and through a static mixer. The aim of this paper is to highlight the influence of different formulations of the novel theoretical breakage and coalescence models on the equilibrium distribution of population, and to propose an implementation strategy for three-dimensional one-way coupled CFD\u2013PBE model.",
     "keywords": ["Breakage", "Coalescence", "Computational fluid dynamics", "Dispersion", "Population balances", "Static mixer"]},
    {"article name": "Performance of numerical integrators on tangential motion of DEM within implicit flow solvers",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.02.017",
     "publication date": "11-2011",
     "abstract": "This study examines the behaviour of continuous time integration schemes over discontinuities in the tangential forces typical in an oblique impact within a Discrete Element Simulation (DEM). High order schemes are associated low error and efficient computation, however, for DEM this is not always the case. The simulations consist of a particle impacting tangentially with a plane and sliding along it, this makes the numerical integration independent of errors from the normal force integration. Three possible force regimes that occur in the tangential motion of an oblique impact are explored; frictional, elastic and elastic-to-frictional. Tests are conducted to explore the effects of the location of the discontinuity within the time step and to examine scheme order through varying time step resolution. For certain scenarios the tangential motion contains elastic and then frictional forces, this presents a second discontinuity between these force regimes. The effects of this second discontinuity are also presented. It was found that all schemes were limited to 1st order by at least one of the conditions tested. The Symplectic Euler is recommended as it is found to be of generally higher accuracy than other 1st order schemes in these tests, as was found in a similar study regarding normal impacts.",
     "keywords": ["Particle", "Discrete element method", "Tangential force", "Numerical integration", "Implicit flow solvers"]},
    {"article name": "A perturbation approach for consistent initialization of index-1 explicit differential\u2013algebraic equations arising from battery model simulations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.003",
     "publication date": "11-2011",
     "abstract": "Estimation of consistent initial conditions is very crucial for the successful solution of differential\u2013algebraic equation (DAE) systems that arise in many fields of science and engineering. In this paper, an efficient perturbation approach for initialization of DAE systems of index-1 is proposed and implemented for DAE models governing batteries. In addition, different existing solvers are compared for consistent initialization of DAE systems. The proposed approach does not necessarily require a nonlinear solver for initialization and builds on the applicability and usability of robust and efficient explicit, linearly implicit and semi-implicit integrators in time. Three different problems are presented wherein the proposed approach is observed to work for a wider range of inconsistent initial conditions compared to other existing generally used routines. It is also observed that the present approach is computationally efficient compared to the other existing approaches in a given environment.",
     "keywords": ["DAE", "Initialization", "Consistent initial condition", "Perturbation approach", "Index-1 DAEs"]},
    {"article name": "Solution strategies for multistage stochastic programming with endogenous uncertainties",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.11.013",
     "publication date": "11-2011",
     "abstract": "In this paper, we present a generic mixed-integer linear multistage stochastic programming (MSSP) model considering endogenous uncertainty in some of the parameters. To address the issue that the number of non-anticipativity (NA) constraints increases exponentially with the number of uncertain parameters and/or its realizations, we present a new theoretical property that significantly reduces the problem size and complements two previous properties. Since one might generate reduced models that are still too large to be solved directly, we also propose three solution strategies: a k-stage constraint strategy where we only include the NA constraints up to a specified number of stages, an iterative NAC relaxation strategy, and a Lagrangean decomposition algorithm that decomposes the problem into scenarios. Numerical results for two process network examples are presented to illustrate that the proposed solution strategies yield significant computational savings.",
     "keywords": ["Multistage stochastic programming", "Endogenous uncertainties", "Non-anticipativity constraints", "Decomposition techniques", "Planning under uncertainty", "Process networks"]},
    {"article name": "Application of empirical modelling in multi-layers membrane manufacturing",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.03.007",
     "publication date": "11-2011",
     "abstract": "Multistructured membranes based on ultrafine fibers of polymethylmethacrylate-co-methacrylic acid (PMMA-co-MAA) and TiO2 nanoparticles have been obtained by electrohydrodynamic (EHD) technologies, for active filter media manufacturing. Process optimization of the nanofibers based layers has been investigated by response surface methodology (RSM) in order to predict the domain of the parameters where the smallest fiber diameter can be achieved. A quantitative relationship between electrospinning parameters and the responses (mean diameter and standard deviation) was established and then the final multi-layers structure of nanofibers and nanoparticles has been achieved for a controlled and robust process. The nanostructured membranes have been characterized by SEM imaging, EDAX, TGA analysis and water vapour permeability and their photocatalytic activity has been tested on VOCs degradation.",
     "keywords": ["Electrospinning", "Nanofibers", "RSM", "Process optimization", "Photocatalysis", "Nanoparticles"]},
    {"article name": "MOSAIC a web-based modeling environment for code generation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.03.022",
     "publication date": "11-2011",
     "abstract": "In this work, a new modeling environment called MOSAIC is presented, which combines well known concepts such as equation-based modeling, use of symbolic mathematic language, and code generation. Moreover, the proposed tool follows a new modeling approach for the re-use of single equations and the support of different naming conventions. The modeling is done strictly in the documentation level. In analogy to nomenclatures given in documentation or the literature, the notation is introduced as a mandatory modular model element. The model information is stored in XML and MathML, and code generation for different programming languages is used to transform the generally defined models into executable programs or suitable code fragments for the solution or use in various numerical environments. Furthermore, MOSAIC is provided as a Software as a Service.The result is a software tool that allows for modeling in the documentation level, promotes the reuse of model elements, and supports centralized cooperation on the Internet. Since symbolic mathematic formulations are a key issue in this project, a convention for the formulation of such expressions is motivated and proposed. Additionally, a new modeling concept using notations as model element is introduced. The implementation of these approaches in MOSAIC is discussed based on the example of the dynamic modeling and the simulation of a condenser.",
     "keywords": ["Modular modeling", "Code generation", "XML", "MathML", "Software as a service", "Equation based modeling", "Modeling environment"]},
    {"article name": "Simulation of freezing step in vial lyophilization using finite element method",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.10.009",
     "publication date": "11-2011",
     "abstract": "Determination of heat transfer and temperature profile during freezing step is fundamental to predict the final structure of a lyophilized product. The aim of this study was to develop and optimize a dynamic model based on finite element method for simulation of freezing step in order to study final product morphology in both vertical and radial directions. Different factors have been taken into account: chamber pressure, shelf temperature, vial shape, initial solution temperature, nucleation temperature and phase changes. The dynamic axisymmetric model proposed could simulate temperature of each point in the vial and position of liquid\u2013solid interface, without necessity of fitting parameters or questionable assumptions. In addition, the model was extended to predict the average crystal size in each element and the influence of different factors was examined.",
     "keywords": ["Finite element method", "Mannitol", "Simulation", "Lyophilization", "Heat transfer", "Freeze-drying"]},
    {"article name": "Simulation of fuel ethanol production from potato tubers",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.11.003",
     "publication date": "11-2011",
     "abstract": "Semi-continuous fuel ethanol production (FEP), based on the utilization of potato overstocks, including either acid (process I) or enzymatic (process II) hydrolysis of potato starch on the industrial scale was developed. The FEP simulations were developed by help of the Aspen Plus\u00ae software using both experimental and literature data. The main goals were to analyze the influence of starch conversion kinetics on overall process design, to compare two FEP processes with respect to their technical benefits and limitations, to use the mass and energy balances for determining the consumption of materials and energy and to evaluate FEP environmental impact. From the technical assessment, both processes were shown to be feasible for producing high quality fuel ethanol. The analysis showed that the process I was less complex since it required fewest and smallest process equipment units than the process II, making it a competitive alternative for commercial FEP form potato overstocks.",
     "keywords": ["Acid hydrolysis", "Aspen Plus\u00ae", "Enzymatic hydrolysis", "Ethanol", "Production design", "Potato tuber"]},
    {"article name": "Efficient and accurate numerical simulation of nonlinear chromatographic processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.10.002",
     "publication date": "11-2011",
     "abstract": "Models for chromatographic processes consist of nonlinear convection-dominated partial differential equations (PDEs) coupled with some algebraic equations. A high resolution semi-discrete flux-limiting finite volume scheme is proposed for solving the nonlinear equilibrium dispersive model of chromatography. The suggested scheme is capable to suppress numerical oscillations and, hence, preserves the positivity of numerical solutions. Moreover, the scheme has capability to accurately capture sharp discontinuities of chromatographic fronts on coarse grids. The performance of the current scheme is validated against other flux-limiting schemes available in the literature. The case studies include single-component elution, two-component elution, and displacement chromatography on non-movable (fixed) and movable (counter-current) beds.",
     "keywords": ["Nonlinear chromatographic models", "Finite volume schemes", "Discontinuous solutions", "Delta shocks", "Displacement chromatography"]},
    {"article name": "Computer modeling of isothermal crystallization in short fiber reinforced composites",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.11.011",
     "publication date": "11-2011",
     "abstract": "Computer modeling and simulation for the isothermal crystallization of short fiber reinforced composites with athermal nucleation is presented. The \u201cpixel coloring\u201d technique which is capable of capturing the morphology evolution and calculating the crystallization kinetics is implemented. A parametric study is used to explore the influences of fibers on the crystallization in the reinforced system. The results indicate that the fibers depress the crystallization rate when compared to that of neat polymers or accelerate the crystallization rate by providing nucleation sites. The constraining effect is mainly dependent on fiber content while the enhancing effect is mainly determined by fiber surface and fiber nucleation density. Fiber orientation has almost no effect on the crystallization kinetics but changes the ultimate morphology. Fiber length and fiber diameter have different influences on crystallization kinetics and mean size of spherulites. Results show that fiber diameter affects the crystallization much more than that of fiber length.",
     "keywords": ["Crystallization", "Morphology", "Simulation", "Short fiber reinforced composite", "Isothermal"]},
    {"article name": "A stochastic approach for the prediction of PSD in crystallization processes: Analytical solution for the asymptotic behavior and parameter estimation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.007",
     "publication date": "11-2011",
     "abstract": "Recently, a novel stochastic formulation based on the Fokker\u2013Planck equation (FPE) for the description of anti-solvent mediated crystal growth process was proposed. Here, we further expand these results by analyzing the asymptotic (end of the batch) solution of the FPE for the CSD. In this regard, the analytical solution of the stationary FPE is exploited for predicting the end of the batch CSD as function of the model parameters. Furthermore, the availability of such analytical solution is used to simplify and diminish the computational burden of the parameter estimation problem. Two alternative approaches for parameter estimation are discussed based on the use of the analytical solution of the FPE and of the dynamic of the logistic equation (deterministic component of the FPE approach). Validations against experimental data for the NaCl\u2013water\u2013ethanol anti-solvent crystallization system are presented.",
     "keywords": ["Particle size measurement", "Brownian motion", "Process parameter estimation", "Probabilistic models", "Stochastic modeling"]},
    {"article name": "Rational design of heating elements using CFD: Application to a bench-scale adiabatic reactor",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.11.005",
     "publication date": "11-2011",
     "abstract": "This article explores the use of computational fluid dynamics (CFD) modelling for designing an oven which enables the adiabatic operation of a chemical reactor at bench-scale. For accomplishing this scope, the oven consists of electrical heating elements, air circulation system and a control loop that uses the temperature inside the reactor as set-point for the reactor wall temperature. Depending on the spatial configuration of the air flow and the heating elements, as well as the air flow rate, different temperature profiles within a given oven section are obtained, being appropriate those leading to uniform reactor wall temperatures and fast dynamic response. The use of CFD allows, by obtaining temperature maps within the oven, the selection of appropriate configurations. The optimal configuration adopted has been experimentally validated in a lab-scale adiabatic reactor working with both particulated and monolithic catalyst beds.",
     "keywords": ["CFD", "Heat losses compensation", "Un-steady state", "Lab-scale testing"]},
    {"article name": "Optimising the porous structure of heterogeneous reforming catalysts with a globular model of the grain",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.06.004",
     "publication date": "11-2011",
     "abstract": "The theory of a globular grain model was used to construct an algorithm for analysing the problem of how the porous structure of heterogeneous catalysts affects their efficiency in model reforming reactions. Our analysis has substantiated the undeniable advantage of applying the bidispersive grain structure since its effectiveness is many times as high as the effectiveness of the monodispersive grain structure. Apparently, the method described in the present paper will be equally efficient when applied to heterogeneous catalysts made use of in other technological processes.",
     "keywords": ["Reforming", "Mathematical model", "Catalyst", "Porous structure", "Optimisation"]},
    {"article name": "Optimization of industrial CSTR for vinyl acetate polymerization using novel shuffled frog leaping based hybrid algorithms and dynamic modeling",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.04.004",
     "publication date": "11-2011",
     "abstract": "Industrial scale continuous stirred tank reactor (CSTR) for production and grade transitions of poly vinyl acetate (PVAc) at the different reactor sizes was investigated. Such reactor is known to show oscillatory behavior and to have periodic limit points, particularly at high molecular weights. Four efficient novel hybrid optimization methods which use variable population size genetic algorithm (VPGA), bacterial optimization algorithm (BO) and shuffled frog leaping method (SFL) were introduced for this kind of reactors. These algorithms can reliably find dynamically stable points with desired conditions for PVAc production. The dynamic modeling combined with the new hybrid optimization methods were used to obtain the optimal operating conditions which increased the outlet flow rate by 17%.",
     "keywords": ["Hybrid optimization methods", "Variable population size genetic algorithm", "Bacterial optimization", "Shuffled frog leaping", "Polymerization"]},
    {"article name": "Graph theory augmented math programming approach to identify minimal reaction sets in metabolic networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.05.006",
     "publication date": "11-2011",
     "abstract": "Bioprocesses are of growing importance as an avenue to produce chemicals. Microorganisms containing only desired catalytic and replication capabilities in their metabolic pathways are expected to offer efficient processes for chemical production. Realizing such minimal cells is the holy grail of metabolic engineering. In this paper, we propose a new method that combines graph-theoretic approaches with mixed-integer liner programming (MILP) to design metabolic networks with minimal reactions. Existing MILP based computational approaches are computationally complex especially for large networks. The proposed graph-theoretic approach offers an efficient divide-and-conquer strategy using the MILP formulation on sub-networks rather than considering the whole network monolithically. In addition to the resulting improvement in computational complexity, the proposed method also aids in identifying the key reactions to be knocked-out in order to achieve the minimal cell. The efficacy of the proposed approach is demonstrated using three case studies from two organisms, Escherichia coli and Saccharomyces cerevisiae.",
     "keywords": ["Metabolic engineering", "Systems biology", "Strain improvement", "Minimal cell", "Flux Balance Analysis", "Optimization"]},
    {"article name": "A calculation procedure for a heat exchanger and bypass equipment",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.02.002",
     "publication date": "11-2011",
     "abstract": "An efficient, technological solution for controlling the heat transfer performance of a heat exchanger is to install on process streams a bypass circuit, whose flow rate is determined in order to keep the outlet temperature of process fluids at target value. Control on outlet temperature is often necessary when working conditions undergo transients and modifications. In this manuscript, a calculation procedure for a heat exchanger and bypass equipment is described, along with relevant numerical model and solving technique. For developing the heat exchanger model in detail and validating the procedure against observed results, the specific case of a process waste heat boiler provided with a bypass is considered. The numerical model is based on mass, momentum and energy balance equations and is applied to a mono-dimensional and stationary type problem. The procedure can be conceptually extended to other specific applications and applied either to rating or design calculations.",
     "keywords": ["Shell-and-tube heat exchanger", "Bypass", "Heat transfer control", "Waste heat boiler"]},
    {"article name": "A bilevel optimization method for simultaneous synthesis of medium-scale heat exchanger networks based on grouping of process streams",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.11.009",
     "publication date": "11-2011",
     "abstract": "In this work we present a bilevel optimization method for simultaneous synthesis of heat exchanger networks (HENS). The results show that compared to a similar method using the same superstructure the presented method can provide comparative solutions to HENS problems with reduced calculation effort especially for larger problems. Hence the value of this work is a good way of simplifying a HENS problem so that HENS problems can be solved efficiently with good results. The presented method combines four submodels into an overall method by using grouping of streams, aggregate streams and bilevel optimization. The idea of the method is to decompose the set of binary variables i.e. the variables that define the existence of heat exchanger matches, into two separate problems. Three different HENS examples from the literature are solved.",
     "keywords": ["Heat exchanger network synthesis", "Synheat model", "Bilevel optimization", "MINLP", "Grouping of process streams", "Aggregate streams"]},
    {"article name": "Reactive pressure swing batch distillation by a new double column system",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.002",
     "publication date": "11-2011",
     "abstract": "Ethyl-acetate is generally produced by the esterification reaction of ethanol with acetic-acid. Since the reaction is equilibrium-limited, the use of reactive distillation is an attractive option. A new double-column system is suggested for ethyl-acetate production by applying reactive pressure swing batch distillation. The system was investigated by a feasibility study based on the analysis of reactive and non-reactive residue curve maps. Two different process options were found to be feasible. In the first option, the reactive column operates at the lower pressure (1.01\u00a0bar), and the non-reactive column that removes ethyl-acetate from the system operates at the higher pressure (10\u00a0bar). The second option allows the reactive column to operate at the higher pressure (10\u00a0bar), and the non-reactive column removes water at the lower pressure (1.01\u00a0bar). The first process option was studied by rigorous simulation based on less simplifying assumptions using a professional dynamic simulator. The influence of the most important operation parameters was also studied.",
     "keywords": ["Pressure swing batch distillation", "Reactive batch distillation", "Ethyl-acetate production", "Feasibility study", "Reactive residue curve map", "Dynamic simulation"]},
    {"article name": "Simultaneous mixed-integer dynamic optimization for environmentally benign electroplating",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.07.004",
     "publication date": "11-2011",
     "abstract": "Hoist scheduling, especially cyclic hoist scheduling (CHS), is used to maximize the manufacturing productivity of electroplating processes. Water-reuse network design (WRND) for the electroplating rinsing system targets the optimal water allocation, such that fresh water consumption and wastewater generation are minimized. Currently, there is still a lack of studies on integrating CHS and WRND technologies for electroplating manufacturing. In this paper, a multi-objective mixed-integer dynamic optimization (MIDO) model has been developed to integrate CHS and WRND technologies for simultaneous consideration of productivity and water use efficiency for environmentally benign electroplating. The orthogonal collocation method on finite elements is employed to convert the MIDO problem into a mixed-integer nonlinear programming (MINLP) problem. The efficacy of the methodology is demonstrated by solving a real electroplating example. It demonstrates that the computational methods of production scheduling, process design, and dynamic optimization can be effectively integrated to create economic and environmental win\u2013win situations for the electroplating industry.",
     "keywords": ["Hoist scheduling", "Water-reuse network design", "Environmentally benign manufacturing"]},
    {"article name": "Applying a dual extended Kalman filter for the nonlinear state and parameter estimations of a continuous stirred tank reactor",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.12.010",
     "publication date": "11-2011",
     "abstract": "The extended Kalman filter (EKF) provides an efficient method for generating approximate maximum-likelihood estimates of the states or parameters of discrete-time nonlinear dynamical systems. In this paper, we consider the dual-estimation problem, the so-called dual EKF, in which both the states of a dynamical system and its parameters are estimated simultaneously, given only noisy observations. The main contribution of this paper is to show the efficacy of a proposed simplified dual-EKF technique (which in this work will be referred to as the dual EKF-2) in comparison with the conventional joint EKF. This has been demonstrated by conducting simulation studies on a CSTR which has been dynamically simulated using the HYSYS simulation package. Extensive analysis revealed that, not only the dual-EKF approach can achieve optimal state- and parameter-estimation performances comparable to the joint EKF, but also it has the main advantage of carrying out separate estimations of the states and parameters.",
     "keywords": ["Dual estimation", "Dual EKF", "Joint EKF", "State and parameter constraints", "CSTR"]},
    {"article name": "Dynamic modelling of a rotary kiln for calcination of titanium dioxide white pigment",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.03.029",
     "publication date": "11-2011",
     "abstract": "A rigorous one-dimensional dynamic model of a rotary kiln for calcination of titanium dioxide white pigment is developed. The regenerative heat transfer in the kiln wall is described by a new mixed numerical/analytical approach. The model is validated by means of a dynamic test case representing a 15-day period of plant operation. The required process data are captured within a measurement campaign on a kiln run by TRONOX Pigments GmbH, Krefeld-Uerdingen, Germany. The predictive accuracy of the model is estimated by means of a sensitivity analysis considering uncertainties of model parameters and measured input values. The actual agreement between simulation and measurement results is significantly better than indicated by the sensitivity analysis.",
     "keywords": ["Titanium dioxide", "Sulphate method", "Calcination", "Rotary kiln", "Dynamic modelling"]},
    {"article name": "Soft Sensor design for a Topping process in the case of small datasets",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.12.009",
     "publication date": "11-2011",
     "abstract": "In this paper, a new strategy to cope with the identification of nonlinear models of industrial processes, when a limited number of experimental data is available, is proposed. The approach is intended to improve the generalization capabilities of the model and it is based on the integration of bootstrap resampling, noise injection and neural model stacking. A number of algorithms to stack the first level neural models are also compared. The method proposed has been applied to develop a Soft Sensor for the estimation of the Freezing Point of Kerosene in an atmospheric distillation unit (Topping) working in a refinery in Sicily, Italy. The improvements obtained thanks to the strategy proposed, with respect to a classical neural model, are shown in the paper.",
     "keywords": ["DCS Distributed Control Systems", "Distributed Control Systems", "DNN Diffusion Neural Network", "Diffusion Neural Network", "MLP Multi Layer Perceptron", "Multi Layer Perceptron", "NN Neural Networks", "Neural Networks", "OLDS Original Learning Datasets", "Original Learning Datasets", "PCA Principal Component Analysis", "Principal Component Analysis", "PCR Principal Component Regression", "Principal Component Regression", "PLS Partial Least Square", "Partial Least Square", "RBF Radial Basis Function", "Radial Basis Function", "SVM Support Vector Machines", "Support Vector Machines", "SS Soft Sensor", "Soft Sensor", "TCU Thermal Cracking Unit", "Thermal Cracking Unit", "TDS Test Datasets", "Test Datasets", "Nonlinear systems identification", "Soft Sensors", "Model stacking", "Distillation columns", "Neural models", "Bootstrap resampling"]},
    {"article name": "Multivariate statistical monitoring of the aluminium smelting process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.03.001",
     "publication date": "11-2011",
     "abstract": "This paper describes the development of a new \u2018cascade\u2019 monitoring system for the aluminium smelting process that uses latent variable models. This system is based on the changes of variability patterns within a feeding cycle which are used to provide indications of faults and their possible causes. The system has been tested offline using 31 data sets. The performance of the system to detect an anode effect has been compared with a typical latent variable model that monitors the change of behaviour at every time instant. The results show that the \u2018cascade\u2019 monitoring system is able to detect abnormal events. It was possible to relate each event with specific patterns associated with abnormalities thus facilitating later fault diagnosis.",
     "keywords": ["Process monitoring", "Multiway principal component analysis (MPCA)", "Aluminium electrolysis"]},
    {"article name": "Cost reduction of the wastewater treatment plant operation by MPC based on modified ASM1 with two-step nitrification/denitrification model",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.03.031",
     "publication date": "11-2011",
     "abstract": "The Activated Sludge Model No. 1 (ASM1) considers that nitrification and denitrification are single step processes and nitrite nitrogen (NO2\u2013N), which is an intermediate for the two processes, is not accounted for. The first part of this paper presents the development of an enhanced ASM1 with two step nitrification/denitrification processes and its implementation in the Benchmark Simulation Model No.1 wastewater treatment plant (WWTP). The secondary settler was considered to be reactive in order to achieve a better fit between the simulation model and the behavior of the real WWTP. The second part presents the investigation of Model Predictive Control approach for the advanced control of the WWTP. Two control strategies are implemented for the wastewater treatment plant and they are analyzed from the perspective of the benefits brought to the WWTP operation. The proposed control strategy shows a reduction of the operational costs and the improvement of the effluent quality index.",
     "keywords": ["MPC", "Nitrification", "Denitrification", "Reactive secondary settler"]},
    {"article name": "Effect of monomer feed and production rate on the control of molecular weight distribution of polyethylene in gas phase reactors",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.04.014",
     "publication date": "11-2011",
     "abstract": "The control of the polymer molecular weight distribution via altering the monomer to hydrogen molar ratio in fluidized bed reactors is examined. The molar ratio is altered by manipulating the monomer and hydrogen feed rates using nonlinear model predictive controller. The simulation revealed promising results however a trade-off between utilizing both the monomer and hydrogen flows simultaneously and the hydrogen flow exclusively exists. Utilization of the monomer and hydrogen flows together favors the rapid transition to the required MWD but at the expense of higher purge and inconsistent production rate. Exclusive use of the hydrogen intake leads to steady production rate and less consumption of the purge. However, longer time is needed to achieve the desired MWD. The same phenomena are observed when discrete mechanism for product withdrawal is implemented.",
     "keywords": ["Molecular weight distribution", "Nonlinear model predictive control", "Polymerization reactor control", "Polyethylene", "Fluidized-bed reactor"]},
    {"article name": "Considerations on nonlinear model predictive control techniques",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.04.009",
     "publication date": "11-2011",
     "abstract": "The nonlinear model predictive control (NMPC) is an on-line application based on nonlinear convolution models. It is an appealing control methodology, but it is difficult to implement and its solution is not so performing since it unavoidably means to solve a usually large-scale, constrained, and multidimensional optimization. To increase the difficulty, this optimization problem is subject to computationally heavy differential and algebraic constraints constituting the same convolution model and the least squares nature of the objective function easily leads to narrow valleys and multimodality issues.Beyond a short review of the state-of-the-art, the paper is aimed at highlighting the possibility to exploit at best the intrinsic features of the specific system one is going to control using the NMPC. The idea is to give the NMPC the possibility to automatically select the best combination of algorithms (differential solvers and optimizers) in accordance with the specific problem to be solved. From this perspective, the NMPC could be easily extended to many scientific fields traditionally far from process systems and computer-aided process engineering and the user has not to worry about which specific differential solvers and optimizers are needed to solve his/her problem.",
     "keywords": ["Process control", "Model predictive control", "Model-based engineering", "Dynamic optimization", "Conscious NMPC"]},
    {"article name": "Optimization issues of the broke management system in papermaking",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.12.012",
     "publication date": "11-2011",
     "abstract": "This paper presents an optimization strategy for the design and operation of a broke management system in a papermaking process. A stochastic model based on a two-state Markov process is presented for the broke system and a multiobjective and bi-level stochastic optimization model is developed featuring (i) a multiobjective operational subproblem for the optimization of the broke dosage and (ii) a multiobjective design problem formulation. An efficient optimization strategy is proposed for the operational subproblem along with a simulation based Pareto optimal solution for the design problem, and illustrated with a detailed case study.",
     "keywords": ["Process design and control", "Multiobjective optimization", "Papermaking process", "Broke management", "Stochastic process"]},
    {"article name": "Risk conscious solution of planning problems under uncertainty by hybrid multi-objective evolutionary algorithms",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.044",
     "publication date": "11-2011",
     "abstract": "We consider the risk conscious solution of planning problems with uncertainties in the problem data. The problems are formulated as two-stage stochastic mixed-integer models in which some of the decisions (first-stage) have to be made under uncertainty and the remaining decisions (second-stage) can be made after the realization of the uncertain parameters. The uncertain model parameters are represented by a finite set of scenarios. The risk conscious optimization problem under uncertainty is solved by a stage decomposition approach using a multi-objective evolutionary algorithm which optimizes the expected scenario costs and the risk criterion with respect to the first-stage decisions. The second-stage scenario decisions are handled by mathematical programming. Results from numerical experiments for two real-world problems are shown.",
     "keywords": ["Planning under uncertainty", "Risk conscious planning", "Stage decomposition", "Hybrid algorithm", "Multi-objective evolutionary algorithm"]},
    {"article name": "A novel rolling horizon strategy for the strategic planning of supply chains. Application to the sugar cane industry of Argentina",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.04.006",
     "publication date": "11-2011",
     "abstract": "In this article, we propose a new method to reduce the computational burden of strategic supply chain (SC) planning models that provide decision support for public policy makers. The method is based on a rolling horizon strategy where some of the integer variables in the mixed-integer programming model are treated as continuous. By comparing with rigorous solutions, we show that the strategy works efficiently. We illustrate the capabilities of the approach presented by its application to a SC design problem related to the sugar cane industry in Argentina. The case study involves determining the number and type of production and storage facilities to be built in each region of the country so that the ethanol and sugar demand is fulfilled and the economic performance is maximized.",
     "keywords": ["Supply chain management (SCM)", "Bioethanol", "Sugar cane industry", "Rolling horizon"]},
    {"article name": "Quantifying polymer structural component evolution using X-ray scattering and mixed-integer network component analysis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.03.027",
     "publication date": "11-2011",
     "abstract": "In this work we present a novel computational approach for the extraction of underlying polymer structural component signatures and corresponding structural evolution through decomposition of multivariate X-ray scattering (SAXS/WAXS) datasets. Without assumptions based on structural geometry, this mixed-integer network component analysis (NCA) methodology generates a reduced set of component scattering signatures and component fraction evolution. Structural models are then assigned to each component based on a generalized expression for scattering from multi-phase materials. The methodology is applied systematically to the study of ethylene/alpha-olefin copolymer isothermal crystallization. The decomposition generates component signatures defining structures of varying extent within the sample but with constant average local structure. For WAXS datasets, these components can be correlated to crystalline and amorphous regions, while for SAXS datasets they can be correlated to ordered and disordered crystalline lamellae. These model choices agree with structures observed in the literature and are confirmed by comparison to reference crystallinity data.",
     "keywords": ["Network component analysis", "Mixed-integer nonlinear programming", "Chemometrics", "X-ray scattering", "Ethylene/alpha-olefin copolymers", "Crystallization"]},
    {"article name": "Modeling and optimization of product design and portfolio management interface",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.06.009",
     "publication date": "11-2011",
     "abstract": "The paper presents modeling and analysis of product design and product portfolio management (PD\u2013PM) domains interaction using an integrated simulation\u2013optimization model. To represent the interactions, the product design phase is modeled as a discrete-scenario static system. The goal of this work is to develop a decision support framework that relies on product design\u2013product portfolio management integration in order to aid product design planning and design execution. We utilize dependency matrix approach to illustrate domain relation between the product design and product portfolio management domains, and to facilitate their integration. Hence, the process integration model utilizes iterative effects, and their attendant processing duration and costs, to pattern domain interaction. An industrial case study is used to illustrate the application and utility of the proposed approach.",
     "keywords": ["Domain mapping matrix", "Simulation optimization", "Product design", "Monte-Carlo simulation"]},
    {"article name": "A common approach to the computation of coarse-scale steady states and to consistent initialization on a slow manifold",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.12.002",
     "publication date": "10-2011",
     "abstract": "We present a simple technique for the computation of coarse-scale steady states of dynamical systems with time scale separation in the form of a \u201cwrapper\u201d around a fine-scale simulator. We discuss how this approach alleviates certain problems encountered by comparable existing approaches, and illustrate its use by computing coarse-scale steady states of a lattice Boltzmann fine scale code. Interestingly, in the same context of multiple time scale problems, the approach can be slightly modified to provide initial conditions on the slow manifold with prescribed coarse-scale observables. The approach is based on appropriately designed short bursts of the fine-scale simulator whose results are used to track changes in the coarse variables of interest, a core component of the equation-free framework.",
     "keywords": ["Slow manifold", "Coarse-graining", "Time-stepper", "Initialization"]},
    {"article name": "CFD prediction of fluid flow and mixing in stirred tanks: Numerical issues about the RANS simulations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.12.007",
     "publication date": "10-2011",
     "abstract": "This work is aimed at verifying the effect of numerical issues on the RANS-based predictions of single phase stirred tanks. In particular, the effect of grid size and discretization schemes on global parameters, mean velocity, turbulent dissipation rate and homogenization is considered. Although contradictory results have been reported so far on the capability of RANS methods in fluid mixing, the most widely accepted conclusion is that adequate values are generally to be expected for the predicted mean flow quantities, while much less confidence must be put on the calculated turbulent quantities and related phenomena. The results obtained in this work partially revise this last statement and demonstrate that firm conclusions on the limits of RANS simulations can be drawn only after careful verification of numerical uncertainties. The simulation results are discussed and compared to the literature experimental data and to original passive tracer homogenization curves determined with planar laser induced fluorescence.",
     "keywords": ["Mixing", "Turbulence", "Homogenization", "PLIF", "CFD", "Numerical error"]},
    {"article name": "Image based meshing of packed beds of cylinders at low aspect ratios using 3d MRI coupled with computational fluid dynamics",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.03.017",
     "publication date": "10-2011",
     "abstract": "CFD is a valuable tool for understanding the flow and pressure drop in packed beds. However, determining the geometry can be complex. One possible method is to use a non-invasive imaging method such as MRI, however, problems occur in processing complex geometries when using traditional commercial meshing software. This work focuses on the use of image based meshing software originally developed for the field of computational biomechanics, to create geometries from 3d MRI scans of packed beds for use with computational dynamics. For this work we focus on disordered packed beds of cylinders at low aspect ratios and Reynolds numbers of Re\u00a0=\u00a01431\u20135074 (based on particle diameter and superficial velocity). We compare CFD studies with experimental data performed on the actual scanned beds and compare these with the correlation proposed by Eisfeld and Schnitzlein (2001). Computational data is shown to correlate well with experimental and theoretical results.",
     "keywords": ["Packed bed", "Pressure drop", "MRI", "CFD"]},
    {"article name": "Numerical analysis of the global identifiability of reaction\u2013diffusion systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.03.012",
     "publication date": "10-2011",
     "abstract": "This article describes a numerical approach which allows for the analysis of the parametric identifiability of non-linear systems. It is shown that the analysis of global identifiability of N parameters of a system can be reduced to finding a global minimum of a specially designed 2N-dimensional function. We discuss the structure of this function and its physical interpretation. As an example we consider identifiability of a diffusion\u2013reaction system and discuss experimental conditions for better identifiability of this system.",
     "keywords": ["Inverse problems", "Identifiability", "Reaction\u2013diffusion equations"]},
    {"article name": "Development of a new model for multiple effect evaporator system",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.11.001",
     "publication date": "10-2011",
     "abstract": "A new simplified scalable mathematical model, based on concepts of stream analysis, temperature paths and internal heat exchange, has been developed for synthesis of a multiple effect evaporator systems. In this model, fresh feed is assumed to be composed of product and number of condensate streams, which come out from different effects and these are treated as separate streams. For the present work a septuple effect flat falling film evaporator system, used for concentrating black liquor in an Indian Kraft Pulp and Paper mill, has been considered. This system is being operated under backward sequence with condensate-, feed- and product-flashing as well as steam splitting in first two effects. The set of linear algebraic equations for this model are self-generated through programming and is solved simultaneously using Gaussian Elimination Method with partial pivoting. Results of the present approach are validated with published model and industrial data.",
     "keywords": ["Mathematical model", "Multiple effect evaporator", "Stream analysis", "Temperature path"]},
    {"article name": "Robust extensions for reduced-space barrier NLP algorithms",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.11.014",
     "publication date": "10-2011",
     "abstract": "Reduced-space barrier NLP algorithms are particularly useful for optimization of large structured systems with few degrees of freedom. Such optimization algorithms are often applied on process models developed within equation oriented process simulators. By partitioning the search direction into tangential and normal steps, these methods can exploit the structure of the equality constraints and adjust the remaining degrees of freedom in a lower dimensional space. Moreover, as shown in previous work, the barrier approach extended with a novel filter linear search algorithm has global and fast local convergence properties. However, convergence properties of the reduced-space barrier algorithm require regularity assumptions. In particular, the method may fail in the presence of linearly dependent active constraints. To deal with these questions, we modify the reduced-space barrier method in two ways. First, as the filter line search requires a feasibility restoration step, we develop and analyze an improved algorithm for this step, which is tailored to the reduced-space method. In addition, a dimension change procedure is proposed to address decomposition of problems with linearly dependent constraints. Finally, both approaches are implemented within a reduced-space version of IPOPT and numerical tests demonstrate the performance of the proposed modifications.",
     "keywords": ["Global convergence", "Feasibility restoration", "Interior-point method", "Reduced-space method", "Nonlinear programming"]},
    {"article name": "Separation network design with mass and energy separating agents",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.10.013",
     "publication date": "10-2011",
     "abstract": "The mathematical model developed in this paper deals with simultaneous synthesis of the integrated separation network, where both mass separating agents (MSAs) and energy separating agents (ESAs) are taken into account. The proposed model formulation is believed to be superior to the available ones. Traditionally, the tasks of optimizing ESA-based and MSA-based processes were either performed individually or studied on a heuristic basis. In this work, both kinds of processes are incorporated into a single comprehensive flowsheet and a novel state-space superstructure with multi-stream mixings is adopted to capture all possible network configurations. By properly addressing the issue of interactions between the MSA and ESA subsystems, lower total annualized cost (TAC) can be obtained by solving the corresponding mixed-integer nonlinear programming (MINLP) model. A benchmark problem already published in the literature has been investigated to demonstrate how better conceptual designs can be generated by our proposed approach.",
     "keywords": ["Simultaneous synthesis", "Process integration", "Separation network design", "State-space superstructure", "MINLP model"]},
    {"article name": "Comparing three configurations of the externally heat-integrated double distillation columns (EHIDDiCs)",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.11.008",
     "publication date": "10-2011",
     "abstract": "In terms of separation of a binary mixture of ethylene and ethane, three configurations of externally heat-integrated double distillation columns (EHIDDiCs), including a symmetrical EHIDDiC (S-EHIDDiC), an asymmetrical EHIDDiC (A-EHIDDiC), and a simplified asymmetrical EHIDDiC (SA-EHIDDiC), are compared with respect to aspects related to process design and controllability. It has been found that the A-EHIDDiC and SA-EHIDDiC are superior to the S-EHIDDiC in terms of thermodynamic efficiency as well as in terms of process dynamics and controllability. As for the comparison between the A-EHIDDiC and SA-EHIDDiC, the latter shows somewhat comparable behaviors with the former in terms of process design and controllability. These results demonstrate that the asymmetrical configuration should generally be favored over the symmetrical one for the development of the EHIDDiC. It is feasible to approximate external heat integration using three heat exchangers between the high- and low-pressure distillation columns involved.",
     "keywords": ["Distillation", "Heat integration", "Process synthesis", "Process design", "Process controllability"]},
    {"article name": "Glycerol ethers synthesis from glycerol etherification with tert-butyl alcohol in reactive distillation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.016",
     "publication date": "10-2011",
     "abstract": "This paper presents a study of glycerol etherification with tert-butyl alcohol catalyzed by Amberlyst 15 in reactive distillation (RD). A thermodynamic analysis is firstly investigated by applying three group contribution methods, to determine the equilibrium composition by minimization of the Gibbs free energy and to compare the predicted values against measured data. Next, the kinetic model parameters are regressed by matching measured data from an autoclave reactor. The activity based Langmuir\u2013Hinshelwood model is found to give the best representation of the reaction rate data. The regressed kinetic rate expressions are also compared against independently measured data in fixed bed reactors reported in the literature and found to give a good match. Finally, using the developed models, it is shown by simulation as well as verification by experiments, that the suitable RD configuration for the production of glycerol ethers in RD is the one consisting of 6 rectifying stages and 6 reaction stages without stripping stage.",
     "keywords": ["DTBG di tert-butyl ether of glycerol", "di tert-butyl ether of glycerol", "Gly glycerol", "glycerol", "IB isobutylene", "isobutylene", "LH Langmuir\u2013Hinshelwood kinetic model", "Langmuir\u2013Hinshelwood kinetic model", "LH-A Langmuir\u2013Hinshelwood based on activity model", "Langmuir\u2013Hinshelwood based on activity model", "MTBG mono tert-butyl ether of glycerol", "mono tert-butyl ether of glycerol", "PL Power Law model", "Power Law model", "PL-A Power Law based on activity model", "Power Law based on activity model", "PL-X Power Law based on mole fraction model", "Power Law based on mole fraction model", "TBA tert-butyl alcohol", "tert-butyl alcohol", "tert tertiary", "tertiary", "TTBG tri tert-butyl ether of glycerol", "tri tert-butyl ether of glycerol", "Etherification", "Kinetic", "Equilibrium conversion", "Group contribution", "Tert-butyl ether of glycerol", "Reactive distillation"]},
    {"article name": "Dynamic modeling and validation of absorber and desorber columns for post-combustion CO2 capture",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.10.001",
     "publication date": "10-2011",
     "abstract": "The paper evaluates, by modeling and simulation, carbon dioxide capture in aqueous solution of mono-ethanolamine (MEA) in packed absorption columns to be used in power sector for reducing greenhouse gas emissions. The mathematical model of carbon dioxide absorption and rich amine regeneration process includes transfer processes: mass and heat to study the coupled effect of temperature and concentration on the rate of absorption. The reaction kinetics and the vapor\u2013liquid equilibrium (VLE) are other important parts of the model. The present dynamic mathematical model can be used to analyze the absorption rate, to understand the micro level interaction of various processes taking place inside the absorption and desorption column, and to improve the overall design of the system.The aim of the project was to validate the absorber and desorber models, as well as to understand the dynamic behavior of the whole capture-regeneration steps.",
     "keywords": ["Post-combustion CO2 capture", "Absorption", "Desorption", "Dynamic model"]},
    {"article name": "An efficient sparse approach to sensitivity generation for large-scale dynamic optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.10.008",
     "publication date": "10-2011",
     "abstract": "In this work, an efficient approach is proposed for the combined step-wise state and sensitivity integration tailored to the orthogonal collocation in finite elements integration method. The presented algorithm can be adapted to any one step integration method where gradients are available from the solution of the discretized system. Moreover, it is completely based on sparse matrix calculus, which makes it especially suitable for large-scale equation systems with a relative small number of independent variables. The relative computational effort in comparison to a state integration is small with a linear increase w.r.t. the number of independent variables. The performance of the developed algorithm has been tested on two case studies: the parameter estimation of a stiff implicit differential equation system with parameters directly connected to the differential states, and the dynamic optimization of a stiff general DAE system.",
     "keywords": ["First-order sensitivities", "Integration of fully implicit dynamic systems", "Orthogonal collocation on finite elements", "Large sparse equation systems", "Differential algebraic equation systems", "Single shooting"]},
    {"article name": "Modelling, simulation and control of an industrial, semi-batch, emulsion-polymerization reactor",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.05.016",
     "publication date": "10-2011",
     "abstract": "This paper presents an emulsion-polymerization model that is designed for an industrial, semi-batch reactor. The model consists of a reaction model and a calorimetry model, and as such enables us to predict the reactor temperature and the batch-output parameters, i.e., the conversion, the solids content and the viscosity. The model was validated on real-plant data and used in the analysis and design of the reactants dosing control. The control strategy proposed is valid for cases where evaporative cooling is either the only or an additional way to remove the heat of the reaction. It consists of an initiator and monomer dosing control, using the reactor temperature as a controlled variable. The simulation results and the real-plant testing show that the proposed reactants dosing control significantly reduces the variations in the reactor temperature and at the same time results in more uniform batch results.",
     "keywords": ["Emulsion polymerization", "Semi-batch industrial reactor", "First-principle model", "Calorimetry model", "gPROMS", "Control"]},
    {"article name": "Plant-wide control strategy applied to the Tennessee Eastman process at two operating points",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.11.006",
     "publication date": "10-2011",
     "abstract": "This work presents a new plant-wide control strategy able to be applied on large scale chemical plants. It is based on an extension of the non square relative gain array (NRG) theoretical concepts, introduced by Chang and Yu (1990), and the generalized relative disturbance gain (GRDG) presented in Chang and Yu (1992). The extension of the NRG is useful for searching the best group of controlled variables (CVs) independently of the problem dimensionality. Meanwhile, the extension of the GRDG allows configure the loops pairing by considering the trade-off between servo and regulator behavior. It can be done thanks to define a proper function, named net load effect, accounting both set point and disturbances effects. Even though these concepts are not new, the main contribution of this paper is the selection of the adequate objective function. It is mathematically expressed in a new way, in terms of Frobenius norm of specific matrices related with the models of the plant and very useful for evaluating the process interaction. Then, it drives the search supported by genetic algorithms (GA), which evaluates all the possible combinations of input\u2013output variables. It allows to solve successfully and with less computational effort the combinatorial optimization problem, even though the high dimension usually involved in large scale chemical plants. The use of the relative gain array (RGA) can also be considered for pairing purpose, but in some cases it could drive to a less effective structure. The use of relative normalized gain array (RNGA) for pairing the selected CVs with the most suitable MVs is able to lead to best control structures only if a dynamic model of the plant is available. Therefore, it must be emphasized that this approach is developed for working in cases where only steady-state plant information is available. However, if a dynamic model is disposable too the algorithm is extended to use it. In addition, a mathematical demonstration is presented so as to understand why is possible to find a well conditioned control structure. The methodology is tested in the Tennessee Eastman (TE) process at the base case proposed by Downs and Vogel (1992), and at an optimized working point presented by Ricker (1995). Both working points show two quite different scenarios. Thus, a set of dynamic simulations for both cases and the hardware requirements compared to the previous suggested are given to proof the capacity of this approach.",
     "keywords": ["Plant-wide control", "Optimum energy consumption", "Disturbance rejection", "Controllability"]},
    {"article name": "Dynamic hybrid simulation of batch processes driven by a scheduling module",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.04.007",
     "publication date": "10-2011",
     "abstract": "Simulation is now a CAPE tool widely used by practicing engineers for process design and control. In particular, it allows various offline analyses to improve system performance such as productivity, energy efficiency, waste reduction, etc. In this framework, we have developed the dynamic hybrid simulation environment PrODHyS whose particularity is to provide general and reusable object-oriented components dedicated to the modeling of devices and operations found in chemical processes. Unlike continuous processes, the dynamic simulation of batch processes requires the execution of control recipes to achieve a set of production orders. For these reasons, PrODHyS is coupled to a scheduling module (ProSched) based on a MILP mathematical model in order to initialize various operational parameters and to ensure a proper completion of the simulation. This paper focuses on the procedure used to generate the simulation model corresponding to the realization of a scenario described through a particular scheduling.",
     "keywords": ["Dynamic hybrid simulation Environment", "Batch processes", "Scheduling", "Petri nets", "Mixed Integer Linear Programming", "Object oriented modeling"]},
    {"article name": "A Lagrangian relaxation approach for a multi-mode inventory routing problem with transshipment in crude oil transportation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.005",
     "publication date": "10-2011",
     "abstract": "An inventory routing problem in crude oil transportation is studied, in which crude oil is transported from a supply center to multiple customer harbors to satisfy their demands over multiple periods. In the problem, a heterogeneous fleet of tankers consisting of tankers owned by a distributor and tankers rented from a third party, a pipeline, and multiple types of routes are considered; both inventory level and shortage level at each customer harbor are limited. The objective is to determine for each period over a given time horizon the number of tankers of each type to be rented/returned at the supply center, the number of tankers of each type to be dispatched on each route, and the quantity of crude oil flowing through the pipeline that minimizes the total logistics cost.After formulating the problem as a mixed integer programming problem, a Lagrangian relaxation approach is developed for finding a near optimal solution of the problem. The approach is also applied to a variant of the problem in which both fully and partially loaded tankers are allowed in the transportation of crude oil. Numerical experiments show that this approach outperforms an existing meta-heuristic algorithm, especially for the instances of large sizes.",
     "keywords": ["Crude oil transportation", "Logistics", "Inventory routing problem", "Optimization", "Lagrangian relaxation"]},
    {"article name": "Extending the resource task network for industrial applications",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.010",
     "publication date": "10-2011",
     "abstract": "The discrete time resource-task network (RTN) model is a generalized mixed-integer linear programming model used in scheduling optimization problems. This paper presents several extensions to the RTN that have been used at The Dow Chemical Company. One RTN extension allows for more realistic demand fulfillment: customer orders can be filled in their entirety at a distinct time point, rather than over several time periods. Modifications are proposed that allow tasks to interact not just with resources, but also with the bounds on those resources, which allows for an efficient method of modeling storage. The concept of multiple extents is introduced to extend the functionality of a single task and thereby reduce the overall size of the model and improve computation time. An alternative formulation of the RTN is also introduced in spatial rather than temporal coordinates, which allows applicability to a different class of problems such as payload optimization.",
     "keywords": ["Resource-task-network", "Scheduling", "MILP", "Payload optimization"]},
    {"article name": "Simultaneous data reconciliation and joint bias and leak estimation based on support vector regression",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.06.002",
     "publication date": "10-2011",
     "abstract": "Process data measurements are important for process monitoring, control, optimization, and management decision making. However, process data may be heavily deteriorated by measurement biases and process leaks. Therefore, it is significant to simultaneously estimate biases and leaks with data reconciliation. In this paper, a novel strategy based on support vector regression (SVR) is proposed to achieve simultaneous data reconciliation and joint bias and leak estimation in steady processes. Although the linear objective function of the SVR approach proposed is robust with little computational burden, it would not result in the maximum likelihood estimate. Therefore, to ensure accurate estimates, the maximum likelihood estimate is applied based on the result of the SVR approach. Simulation and comparison results of a linear recycle system and a nonlinear heat-exchange network demonstrate that the proposed strategy is effective to achieve data reconciliation and joint bias and leak estimation with superior performances.",
     "keywords": ["Data reconciliation", "Support vector regression", "Parameter estimation", "Gross error detection"]},
    {"article name": "Performance of integration schemes in discrete element simulations of particle systems involving consecutive contacts",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.09.008",
     "publication date": "10-2011",
     "abstract": "In this investigation, which is a follow-up study extending earlier work (Kruggel-Emden, Sturm, Wirtz, & Scherer, 2008), a realistic assessment of the performance of integration schemes in systems of moving particles and consecutive contacts is conducted. Linear contact models are applied throughout this work as they allow for an analytical solution of consecutive oblique impacts. The many-particle systems considered are the discharge of particles from a hopper and particle movement in a shaken container. Results for many-particle systems are robust with respect to the applied integration method and step size once particle interactions are resolved with a sufficient number of steps. The integration schemes are also evaluated based on consecutive particle/wall contacts. Integration of consecutive contacts in a discrete element framework implies repeatedly solving non-continuous systems of differential equations. Various termination conditions for the normal force models and adaptive time stepping for one-step integration methods are investigated. The effect of softened contacts on particle trajectories is discussed. Based on these insights, recommendations for the most accurate integration schemes are made.",
     "keywords": ["Realistic assessment", "Integration schemes", "Discrete element method", "Consecutive contacts"]},
    {"article name": "Optimization framework for the simultaneous process synthesis, heat and power integration of a thermochemical hybrid biomass, coal, and natural gas facility",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.041",
     "publication date": "09-2011",
     "abstract": "A thermochemical based process superstructure and its mixed-integer nonlinear optimization (MINLP) model are introduced to convert biomass (switchgrass), coal (Illinois #6), and natural gas to liquid (CBGTL) transportation fuels. The MINLP model includes simultaneous heat and power integration utilizing heat engines to recover electricity from the process waste heat. Four case studies are presented to investigate the effect of CO2 sequestration (CCS) and greenhouse gas (GHG) reduction targets on the process topology along with detailed parametric analysis on the role of biomass and electricity prices. Topological similarities for the case studies include selection of solid/vapor-fueled gasifiers and iron-catalyzed Fischer-Tropsch units that facilitate the reverse water\u2013gas-shift reaction. The break-even oil price was found to be $57.16/bbl for CCS with a 50% GHG reduction, $62.65/bbl for CCS with a 100% GHG reduction, $82.68/bbl for no CCS with a 50% GHG reduction, and $91.71 for no CCS with a 100% GHG reduction.",
     "keywords": ["Process synthesis", "Heat and power integration", "Hybrid energy systems", "Thermochemical", "Mixed-integer nonlinear optimization"]},
    {"article name": "Design and control of energy integrated SOFC systems for in situ hydrogen production and power generation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.02.006",
     "publication date": "09-2011",
     "abstract": "This paper studies the design and operation of energy integrated solid oxide fuel cell (SOFC) systems for in situ hydrogen production and power generation. Two configurations are considered: one where the hot effluent stream from the fuel cell is used directly to provide heat to the endothermic reforming reaction, and another where the hot effluent streams are mixed and combusted in a catalytic burner before the energy integration. A comparative evaluation of the two configurations is presented in terms of their design, open-loop dynamics and their operation under linear multi-loop controllers.",
     "keywords": ["SOFC", "Methane reforming", "Energy integration", "Process control"]},
    {"article name": "Optimization of IGCC processes with reduced order CFD models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.018",
     "publication date": "09-2011",
     "abstract": "Integrated gasification combined cycle (IGCC) plants have significant advantages for efficient power generation with carbon capture. Moreover, with the development of accurate CFD models for gasification and combined cycle combustion, key units of these processes can now be modeled more accurately. However, the integration of CFD models within steady-state process simulators, and subsequent optimization of the integrated system, still presents significant challenges. This study describes the development and demonstration of a reduced order modeling (ROM) framework for these tasks. The approach builds on the concepts of co-simulation and ROM development for process units described in earlier studies. Here we show how the ROMs derived from both gasification and combustion units can be integrated within an equation-oriented simulation environment for the overall optimization of an IGCC process. In addition to a systematic approach to ROM development, the approach includes validation tasks for the CFD model as well as closed-loop tests for the integrated flowsheet. This approach allows the application of equation-based nonlinear programming algorithms and leads to fast optimization of CFD-based process flowsheets. The approach is illustrated on two flowsheets based on IGCC technology.",
     "keywords": ["Co-simulation", "PCA", "Reduced order modeling", "IGCC", "Process optimization", "CFD"]},
    {"article name": "Integrated design and control under uncertainty: Embedded control optimization for plantwide processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.02.016",
     "publication date": "09-2011",
     "abstract": "High performance processes should operate close to design boundaries and specification limits, while still guaranteeing robust performance without design constraint violations. Since design chemical process is operating close to tighter boundaries safely; much attention has been devoted to integrating design and control, in which the design decisions, dynamics, and control performance are considered simultaneously in some optimal fashion. However, rigorous methods for solving design and control simultaneously lead to challenging mathematical formulations which easily become computationally intractable. In an earlier paper of our group, a new mathematical methodology to reduce the combinatorial complexity of integrating design and control was introduced (Malcolm et al., 2007). We showed that substantial problem size reduction can be achieved by embedding control for specific process designs. In this paper, we extend the embedded control methodologies to plantwide flowsheet. The case study for the reactor-column flowsheet will demonstrate the current capabilities of the methodology for integrating design and control under uncertainty.",
     "keywords": ["Integrated design and control", "Uncertainty", "Global optimization", "Dynamic systems", "Embedded control mechanism"]},
    {"article name": "A multi-paradigm modeling framework for energy systems simulation and analysis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.05.005",
     "publication date": "09-2011",
     "abstract": "The modern world energy system is highly complex and interconnected and the effects of energy policies may have unintended consequences. Modeling and analysis tools can therefore be crucial to gaining insight into the interactions between system components and formulating policies that will shape the future energy system. We present in this work a multi-paradigm modeling framework that allows for the continual adjustment and refinement of energy system models as the understanding of the system under study increases. This flexible and open framework allows for the consideration of different levels of model aggregation, timescales and geographic considerations within the same model through the use of different modeling formalisms. We also present a case study of the combined California natural gas and electricity systems that illustrates how the framework may be used to account for the significant uncertainty that exists within the system.",
     "keywords": ["Electricity systems", "Energy systems modeling", "Multi-paradigm modeling", "Agent-based modeling"]},
    {"article name": "Optimal design and global sensitivity analysis of biomass supply chain networks for biofuels under uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.02.008",
     "publication date": "09-2011",
     "abstract": "Bio-fuels represent promising candidates for renewable liquid fuels. One of the challenges for the emerging industry is the high level of uncertainty in supply amounts, market demands, market prices, and processing technologies. These uncertainties complicate the assessment of investment decisions. This paper presents a model for the optimal design of biomass supply chain networks under uncertainty. The uncertainties manifest themselves as a large number of stochastic model parameters that could impact the overall profitability and design. The supply chain network we study covers the Southeastern region of the United States and includes biomass supply locations and amounts, candidate sites and capacities for two kinds of fuel conversion processing, and the logistics of transportation from the locations of forestry resources to the conversion sites and then to the final markets.To reduce the design problem to a manageable size the impact of each uncertain parameter on the objective function is computed for each end of the parameter's range. The parameters that cause the most change in the profit over their range are then combined into scenarios that are used to find a design through a two stage mixed integer stochastic program. The first stage decisions are the capital investment decisions including the size and location of the processing plants. The second stage recourse decisions are the biomass and product flows in each scenario. The objective is the maximization of the expected profit over the different scenarios. The robustness and global sensitivity analysis of the nominal design (for a single nominal scenario) vs. the robust design (for multiple scenarios) are analyzed using Monte Carlo simulation over the hypercube formed from the parameter ranges.",
     "keywords": ["Biorefinery", "Supply chain networks", "Uncertainty", "Robustness", "Mixed integer linear programming", "Global sensitivity analysis"]},
    {"article name": "Optimal design of a multi-product biorefinery system",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.042",
     "publication date": "09-2011",
     "abstract": "In this paper we propose a biorefinery optimization model that can be used to find the optimal processing route for the production of ethanol, butanol, succinic acid and blends of these chemicals with fossil fuel based gasoline. The approach unites transshipment models with a superstructure, resulting in a Mixed Integer Non-Linear Program (MINLP). We consider a specific problem based on a network of 72 processing steps (including different pretreatment steps, hydrolysis, fermentation, different separations and fuel blending steps) that can be used to process two different types of feedstock. Numerical results are presented for four different optimization objectives (maximize yield, minimize costs, minimize waste and minimum fixed cost), while evaluating different cases (single product and multi-product).",
     "keywords": ["Biomass", "Biorefinery", "Ethanol", "Succinic acid", "Butanol", "Fuel blends", "Optimization", "Mixed-Integer Non-Linear Program (MINLP)"]},
    {"article name": "A decision support tool for strategic planning of sustainable biorefineries",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.05.011",
     "publication date": "09-2011",
     "abstract": "In this paper we formulate, implement, and test a model for technology and product portfolio design for a multi-product multi-platform biorefining enterprise. The model considered is an MILP financial planning model with the objective of maximizing the stakeholder value. Integer variables are used to select appropriate feedstocks, technologies, and products, material and capacity balances are used to design capacity and set production targets, while cash balances are used to describe investment and operations financing. Stakeholder value is described as the shareholder value with monetized environmental implications in terms of emissions mitigation costs and credits. Process integration schemes utilizing emissions are considered to reduce the emissions load and add to the bottom-line. A preliminary process design and product portfolio is provided as a result. Advantages of process integration are quantified using a central utilities facility and effluent recycles. Sensitivity analysis is conducted to determine important parameters that shape the objective function.",
     "keywords": ["Sustainability", "Technology assessment", "Integrated biorefinery", "Product portfolio selection", "Strategic planning and optimization", "Stakeholder value"]},
    {"article name": "Spatially explicit multi-objective optimisation for design and planning of hybrid first and second generation biorefineries",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.020",
     "publication date": "09-2011",
     "abstract": "Climate change mitigation has become a binding driver in biofuels production. First generation bioethanol, initially indicated as the most competitive option, is now incurring in ever increasing discredits forcing the transition towards more sustainable productions (i.e. second and third generation technologies). This paper addresses the strategic design and planning of corn grain- and stover-based bioethanol supply chains through first and second generation technologies. A Mixed Integer Linear Programming framework is proposed to optimise the environmental and financial performances simultaneously. Multi-period, multi-echelon and spatially explicit features are embodied within the formulation to steer decisions and investments through a global approach. A demonstrative case study is proposed involving the future Italian biomass-based ethanol production. Results show the effectiveness of the optimisation tool at providing decision makers with a quantitative analysis assessing the economic and environmental performance of different design configuration and their effect in terms of technologies, plant sizes and location, and raw materials.",
     "keywords": ["Bioethanol supply chain", "First and second generation", "Multi-objective optimisation", "Capacity planning"]},
    {"article name": "Energy optimization of hydrogen production from lignocellulosic biomass",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.03.002",
     "publication date": "09-2011",
     "abstract": "In this paper we address the conceptual design for the production of hydrogen from switchgrass. The process is modeled as a mixed-integer non linear programming problem (MINLP) for a superstructure embedding two different gasification technologies, direct and indirect, and two reforming modes, partial oxidation or steam reforming, gas cleaning and a water gas shift reactor (WGSR) with membrane separation is used to obtain pure hydrogen. Given the small number of structural alternatives, the problem is solved by constraining the binary variables of the MINLP so as to select each gasifier and reforming mode yielding four NLP's. Next, the energy is integrated, and finally, an economic evaluation is performed. It is shown that indirect gasification with steam reforming is the preferred technology providing higher production yields than the ones reported in the literature for hydrogen from natural gas and at a potentially lower and promising production cost 0.67$/kg.",
     "keywords": ["Energy", "Biofuels", "Alternative fuels", "Fuel cells", "Water"]},
    {"article name": "Integrated conceptual design of solar-assisted trigeneration systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.03.014",
     "publication date": "09-2011",
     "abstract": "This work is aimed at the development of a systematic procedure for energy conservation through the integrated design of trigeneration systems (combined cooling, heating, and power \u2013 \u201cCCHP\u201d) while incorporating solar energy as a renewable form of energy with low GHG emissions. The focus is on developing preliminary screening and targets that guide the conceptual design of a trigeneration system. Absorption refrigeration is used to utilize excess process heat and external energy in the form of fossil and solar energy. To account for the seasonal fluctuation in collected solar energy, the decision-making horizon is discretized into multiple periods. An extended transshipment representation is developed to embed design configurations. Next, a nonlinear programming formulation is developed. The solution of the optimization formulation determines the optimal levels of power, external heating, external cooling, heat integration, mix of fossil/solar energy forms to be supplied to the process, and the scheduling of the system operation.",
     "keywords": ["Process integration", "Solar energy", "Design", "Optimization", "Energy conservation", "Sustainability"]},
    {"article name": "Integration and management of renewables into Total Sites with variable supply and demand",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.02.009",
     "publication date": "09-2011",
     "abstract": "Reducing CO2 emissions could be achieved by maximising heat recovery and increasing the share of renewables in the primary energy mix. Process Integration has developed over the years into a credible process system engineering tool. One of its important developments has been Total Site Heat Integration, which has combined the heating and cooling requirements of individual processes unlocking, allowing better integration. The current paper presents an extension of the Total Site methodology covering industrial, residential, service, business and agricultural customers and the incorporation of renewable energy sources (solar, wind, biomass, and some types of waste), accounting for the often substantial variability on the supply and demand sides and for the use of non-isothermal utilities. It further applies the extension of the heat cascade principle with inclusion of heat storage and minimises the heat waste and carbon footprint of the considered sites. This is illustrated with a comprehensive case study.",
     "keywords": ["Total Site Integration", "Energy management", "Integration of renewables", "Total Site Heat Cascade", "Varying energy supply", "Varying energy demand"]},
    {"article name": "Investment portfolios under uncertainty for utilizing natural gas resources",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.04.005",
     "publication date": "09-2011",
     "abstract": "Numerous reasons including lower carbon and sulfur emissions have led to the rapid growth of natural gas (NG) demand. However, more than one-third of world NG reserves are stranded, i.e., either remote (e.g., offshore) or in regions with saturated markets. This reality makes the investment decisions complex and uncertain for NG field developers. In this study, we consider the case of a company that wishes to develop a stranded natural gas reserve for some potential nearby markets under uncertain prices of crude oil and feed gas, and demands of liquefied NG (LNG), compressed NG (CNG), and gas-to-liquid (GTL) products. We present a 2-stage stochastic mixed-integer linear program (MILP) that yields maximum-ENPV (expected NPV) decisions on production capacities, market allocations, and delivery vessels. The small model size allows us to consider many stochastic scenarios in our scenario-based approach. We illustrate our approach using several examples.",
     "keywords": ["Portfolio optimization", "LNG", "CNG", "GTL", "Gas utilization", "Natural gas", "Stranded fields"]},
    {"article name": "Disaggregation\u2013aggregation based model reduction for refinery-wide optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.04.016",
     "publication date": "09-2011",
     "abstract": "In this paper, reduced nonlinear refinery models are developed by generating and using input\u2013output data from a process simulator. In particular, rigorous process models of continuous catalytic reformer (CCR) and naphtha splitter units are used for generating the data. To deal with complexity associated with large amounts of data, that is usually available in the refineries, a disaggregation\u2013aggregation based approach is presented. The data is split (disaggregation) into smaller subsets and reduced artificial neural network (ANN) models are obtained for each of the subset. These ANN models are then combined (aggregation) to obtain an ANN model which represents all the data originally generated. The disaggregation step can be carried out within a parallel computing platform. Refinery optimization studies are carried out to demonstrate the applicability and the usefulness of the proposed model reduction approach.",
     "keywords": ["Refinery-wide optimization", "Artificial neural network", "Disaggregation\u2013aggregation", "Model reduction", "Parallel computing"]},
    {"article name": "Simulation and exergoeconomic analysis of a dual-gas sourced polygeneration process with integrated methanol/DME/DMC catalytic synthesis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.015",
     "publication date": "09-2011",
     "abstract": "Polygeneration energy systems have drawn great attention because of their superiority over conventional stand-alone plants in energy efficiency and emissions control. Although many approaches have been made to the process design of polygeneration energy systems, most of them use coal gasification as the only gas source and produce only a single type of chemical product. In this paper, we present a dual-gas sourced process design which utilizes, besides synthetic gas (syngas) from coal gasification, coke oven gas as gas sources and co-produces methanol, dimethyl ether, and dimethyl carbonate via an integrated catalytic synthesis procedure. A process simulation based on detailed chemical kinetics is presented to illustrate its feasibility. Results of an exergoeconomic analysis are provided to indicate the exergy loss in each functional block within the process. Based on the analysis, an exergy cost distribution diagram is presented, showing the production cost of a product over each functional block.",
     "keywords": ["Dual-gas sourced polygeneration", "Integrated catalytic synthesis", "Exergoeconomic analysis", "Exergy cost distribution", "Process simulation"]},
    {"article name": "Rigorous-simulation pinch-technology refined approach for process synthesis of the water\u2013gas shift reaction system in an IGCC process with carbon capture",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.05.001",
     "publication date": "09-2011",
     "abstract": "Integrated gasification combined cycle (IGCC) technology is becoming increasingly more competitive among advanced power generation systems suitable for carbon capture. As an emerging technology, many different IGCC process configurations have been heuristically proposed to meet even more aggressive economic and environmental goals. One attractive design combines gasification with a water\u2013gas shift (WGS) reaction system, pressure swing adsorption, and chemical-looping combustion (CLC) for CO2 removal prior to feeding the fuel gas to the combined cycle for power production. The WGS reaction step is required to convert CO to CO2 and the extent of conversion is determined by the degree of carbon capture required in the CLC step. As a first towards optimizing the overall energy efficiency of this IGCC process, we apply heat exchanger network synthesis (HENS) to the WGS reaction system. This particular part of the process was chosen because of its evident integration potential (steam required for the WGS reactions can be generated by recovering energy released by the same reactions) and the influence of some of the gasifier parameters (temperature and pressure) on its performance and on all the subsequent parts of the process. After generating alternative designs using Aspen Energy Analyzer (AEA), the HENS problem was formulated in the sequential-modular Aspen Plus simulator using a process superstructure approach and solved by mixed integer nonlinear programming (MINLP) algorithms. The HENS capability is implemented as CAPE-OPEN (CO) compliant unit operation and makes use of MINLP algorithms, namely Generalized Bender's Decomposition (GBD), Outer Approximation (OA), Equality Relaxation (ER), Augmented Penalty (AP), and Simulated Annealing (SA). This MINLP-based HENS was used in the CO-compliant Aspen Plus simulator to obtain a design for the WGS reaction system that provided a cost of energy for the IGCC system with CO2 capture that was 28% lower than the base case.",
     "keywords": ["IGCC system", "Carbon capture", "Chemical looping", "MINLP process synthesis"]},
    {"article name": "Operating reserve policies with high wind power penetration",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.03.004",
     "publication date": "09-2011",
     "abstract": "The rapid increase in installed wind generation capacity in the United States has raised concerns about electricity system reliability because of the intermittent and variable nature of the wind power. To help manage the wind power variability new ways of determining the amount of operating reserve capacity that must be kept available are necessary. A two-stage stochastic programming approach for the unit commitment problem had been introduced to mimic the short-term power operation decision process, i.e., day-ahead unit commitment and hour-ahead economic dispatch. Within this framework, a new formulation is proposed to determine the spinning and non-spinning reserve levels for large-scale systems over a 24-h optimization horizon with economic considerations. The proposed model is then applied to a large-scaled California test system. Simulation results illustrate the impact of stochastic wind power generation behavior and increased installed wind capacity on operating reserve requirements and the system cost as a whole.",
     "keywords": ["Operating reserve optimization", "Wind power", "Stochastic programming", "Unit commitment and economic dispatch", "Mixed integer and linear programming"]},
    {"article name": "Modeling, simulation and experimental validation of a PEM fuel cell system",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.03.013",
     "publication date": "09-2011",
     "abstract": "The aim of this work is the development and experimental validation of a detailed dynamic fuel cell model using the gPROMS modeling environment. The model is oriented towards optimization and control and it relies on material and energy balances as well as electrochemical equations including semi-empirical equations. For the experimental validation of the model a fully automated and integrated hydrogen fuel cell testing unit was used. The predictive power of the model has been compared with the data obtained during load change experiments. A sensitivity analysis has been employed to reveal the most critical empirical model parameters that should be estimated using a systematic estimation procedure. Model predictions are in good agreement with experimental data under a wide range of operating conditions.",
     "keywords": ["PEM fuel cell", "Parameter estimation", "Dynamic modeling", "Experimental validation"]},
    {"article name": "Cascade refrigeration system synthesis based on exergy analysis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.02.015",
     "publication date": "09-2011",
     "abstract": "Refrigeration systems are very important to chemical/petrochemical process industries because their performances are closely related to product quality, energy usage efficiency, and plant profitability. Hitherto, the optimal synthesis of a cascade refrigeration system with multiple refrigerants and multiple temperature levels presents considerable challenges and systematic studies combined with thermodynamic insights and mathematical-programming approaches in this area are still lacking. In this paper, a general methodology for the optimal synthesis of such cascade refrigeration system to maximize the energy efficiency has been developed. The exergy-temperature chart combined with the exergy analysis is presented to comprehensively analyze the thermodynamic nature of a refrigeration system, which provides a solid foundation for the conceptual design/retrofit of the complex refrigeration system. An exergy-embedded MINLP model has also been developed for the optimal synthesis of a general cascade refrigeration system. The efficacy of the developed methodology is demonstrated through a case study on the retrofit of a cascade refrigeration system for an ethylene plant.",
     "keywords": ["Refrigeration system", "Process synthesis", "MINLP", "Exergy analysis", "Energy saving"]},
    {"article name": "On mathematical modeling of solar hydrogen production in monolithic reactors",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.03.019",
     "publication date": "09-2011",
     "abstract": "The hydrogen production based on the exploitation of the water splitting reaction and of solar energy on monolith type reactors is a novel and very promising process. Several trade offs between reaction efficiency and material stability appear during the operation of the process rendering necessary advanced optimization and control using appropriate mathematical models. The complexity of the geometric and operating features of the process prevents from the use of a general model for the optimization studies so a model splitting in several size scales procedure is proposed. It is shown that in case of silicon carbide as monolith material the one dimensional single channel model is the most appropriate to be used for the process operation optimization studies. The above general principles are described in detail in the present paper to shed light to the particular modeling problem with the emphasis given in the work performed in our laboratory.",
     "keywords": ["Solar hydrogen", "Water splitting", "Mathematical modeling", "Monolithic reactors"]},
    {"article name": "Design and optimization of hydrogen storage units using advanced solid materials: General mathematical framework and recent developments",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.03.005",
     "publication date": "09-2011",
     "abstract": "This review presents the general mathematical framework of modeling, design and optimization of hydrogen storage using advanced solid materials. The emphasis is given on metal hydride storage tanks, since these systems have been well studied in the literature, both theoretically and experimentally, and are expected to offer significant advantages when current research and development efforts succeed in commercializing the required technology. Enhanced cooling during hydrogen filling of the storage tank is found to be essential to improve hydrogen storage time requirements. For this reason several innovative design strategies for heat exchanger configurations are presented and evaluated in terms of process design and performance improvement. Finally, control and optimization of certain operating conditions can also have a significant impact in hydrogen storage operation.",
     "keywords": ["BC boundary condition", "boundary condition", "CF cooling fluid", "cooling fluid", "DAE differential algebraic equation", "differential algebraic equation", "IPDAE integral partial differential algebraic equation", "integral partial differential algebraic equation", "MH metal hydride", "metal hydride", "MHST metal hydride storage tank", "metal hydride storage tank", "Hydrogen storage", "Metal hydride tanks", "Mathematical modeling", "Optimization"]},
    {"article name": "Modeling and simulation of lithium-ion batteries",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.05.007",
     "publication date": "09-2011",
     "abstract": "In this work the dynamic one-dimensional modeling and simulation of Li ion batteries with chemistry LixC6\u2212\u2212 LiyMn2O4 is presented. The model used is robust in terms of electrochemical variables prediction rather than only the electrical ones. This enables us to analyze the internal behavior of the battery under different discharge rates. The method of lines (MOL) was used for predicting the behavior from the model without any loss of exactitude for regular geometries. The boundary conditions were modified to achieve a better convergence of the solver. The simulation results were compared to experimental data from the research literature. Some examples of application are also presented that include the simulation for the optimization of design parameters, the evaluation of the behavior of the battery under dynamic discharge rates simulating real simplified conditions of operation and the simulation of the parallel discharge of different capacity pairs of batteries.",
     "keywords": ["Lithium-ion batteries", "Modeling", "Simulation", "Energy"]},
    {"article name": "A combined heuristic and indicator-based methodology for design of sustainable chemical process plants",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.03.015",
     "publication date": "08-2011",
     "abstract": "The current emphasis on sustainable production has prompted chemical plants to minimize raw material and energy usage without compromising on economics. While computer tools are available to assist in sustainability assessment, their applications are constrained to a specific domain of the design synthesis problem. This paper outlines a design synthesis strategy that integrates two computer methodologies \u2013 ENVOPExpert and SustainPro \u2013 for simultaneous generation, analysis, evaluation, and optimization of sustainable process alternatives. ENVOPExpert diagnoses waste sources, identifies alternatives, and highlights trade-offs between environmental and economic objectives. This is complemented by SustainPro which evaluates the alternatives and screens them in-depth through indicators for profit and energy, water, and raw material usage. This results in accurate identification of the root causes, comprehensive generation of design alternatives, and effective reduction of the optimization search space. The framework is illustrated using an acetone process and a methanol and dimethyl ether production case study.",
     "keywords": ["Sustainable development", "Process simulation", "Heuristic analysis", "Indicator analysis", "Energy and water minimization", "Sustainability metrics"]},
    {"article name": "A mixed-integer programming approach to strategic planning of chemical centres: A case study in the UK",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.014",
     "publication date": "08-2011",
     "abstract": "Chemical centres provide great potential to tackle the worldwide energy and environmental issues via integrated chemical synthesis and heat and power generation. However, planning of chemical centres still involves many formidable challenges, including locating production sites, arrangement of transportation, and selection of appropriate technologies. These problems become further complicated when considering the geographic situation of a region under study.In this paper, we propose a multi-period mixed-integer programming (MIP) approach to the optimal planning of chemical centres. The planning horizon is firstly divided into several time intervals, and the planning region is represented by a grid. Then a superstructure representation is developed to capture all available logistic and technical options. Based on the superstructure representation, an MIP problem is developed, and by solving it an optimal planning strategy can be obtained. A real-life case study for the UK follows, where the UK is divided into a grid of 34 cells.",
     "keywords": ["Strategic planning", "Process integration", "Chemical synthesis", "Emissions reduction", "Superstructure", "Mixed-integer programming"]},
    {"article name": "A shortcut method for the preliminary synthesis of process-technology pathways: An optimization approach and application for the conceptual design of integrated biorefineries",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.04.013",
     "publication date": "08-2011",
     "abstract": "Synthesis and screening of technology alternatives is a key process-development activity in the process industries. Recently, this has become particularly important for the conceptual design of biorefineries. This work introduces a shortcut method for the synthesis and screening of integrated biorefineries. A structural representation (referred to as the chemical species/conversion operator) is introduced. It is used to track individual chemicals while allowing for the processing of multiple chemicals in processing technologies. The representation is used to embed potential configurations of interest. An optimization approach is developed to screen and determine optimum network configurations for various technology pathways using simple data. The solution to the optimization formulation provides a quick and effective method for screening and interconnecting the technological pathways and to distributing the flows over the network. Case studies are solved to illustrate the applicability of the proposed approach.",
     "keywords": ["Biorefinery", "Biofuel", "Design", "Renewables", "Reaction pathway", "Screening"]},
    {"article name": "Optimal design for sustainable bioethanol supply chain considering detailed plant performance model",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.008",
     "publication date": "08-2011",
     "abstract": "The always increasing energy demand combined with the declining availability of fossil fuels is driving forces for the investigation of renewable energy sources. In this context, bioethanol is considered as one of the most appropriate solutions for short term gasoline substitution. Then, the motivation of this work is to propose a MINLP optimization model for a sustainable design and behavior analysis of sugar/ethanol supply chain (SC). A detailed model for ethanol plant design is embedded in the SC model, and therefore plant and SC designs are simultaneously obtained. Yeast production and residue recycles are taken into account in order to assess the environmental impact. The inclusion of sustainability issues in the model produces both economic and operative changes in SC and plant designs. The simultaneous optimization of these elements allows the evaluation of several compromises among design and process variables. These issues are highlighted throughout the evaluated studied cases.",
     "keywords": ["SC optimization", "Plant design", "Sugar/ethanol production", "Sustainable processes"]},
    {"article name": "Optimal energy supply network determination and life cycle analysis for hybrid coal, biomass, and natural gas to liquid (CBGTL) plants using carbon-based hydrogen production",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.019",
     "publication date": "08-2011",
     "abstract": "A mixed-integer linear optimization formulation is developed to analyze the United States energy supply chain network for the hybrid coal, biomass, and natural gas to liquids (CBGTL) facilities. Each state is discretized into octants and each octant centroid serves as a potential location of one facility. The model selects the optimal locations of CBGTL facilities, the feedstock combination, and size of each facility that gives the minimum overall production cost. Two case studies are presented to investigate the effects of various technologies and hydrogen prices. The CBGTL network is capable to supply transportation fuel demands for the country at a cost between $15.68 and $22.06/GJ LHV ($76.55\u2013$112.91/bbl crude oil) of produced liquid fuels for both case studies. Life cycle analysis on each facility in the supply chain network shows that the United States fuel demands can be fulfilled with an excess of 50% emissions reduction compared to petroleum based processes.",
     "keywords": ["Energy supply chain", "Hybrid energy system", "Transportation fuel", "Life cycle analysis", "Mixed integer linear optimization"]},
    {"article name": "An optimization framework for cost effective design of refueling station infrastructure for alternative fuel vehicles",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.03.018",
     "publication date": "08-2011",
     "abstract": "Historically, gasoline and diesel fuels have been used for transportation, but the possible decline of oil supplies in the future is forcing nations to consider alternative fuels. Most technically and economically feasible alternative fuels have a lower energy density than gasoline, which results in a shorter range for these vehicles. This necessitates a greater need for convenient access to refueling facilities for alternative fuel vehicles. Since infrastructure development is expensive, there is a need to direct investments towards the establishment of refueling facilities in areas which result in maximum impact. This can be addressed by locating facilities at sites which service as many vehicles as possible. This work deals with the use of mathematical programming for determining the best locations for establishing alternative transportation fuel stations. The objective was to site the refueling stations at locations which maximize the number of vehicles served, while staying within budget constraints. The model used here is a modified form of the flow interception facility location model. For the case study we used the transportation network of Alexandria, Virginia, as a test bed for our model. Origin-destination travel demand data for this city is simulated through a transportation simulator to determine the routes taken by individual vehicles. The results are then compared with the service level offered by conventional gasoline refueling stations already located in the city. This work integrates the use of transportation modeling with mathematical programming for the solution of a complex large-scale problem on a real-life transportation network.",
     "keywords": ["Green transportation", "Facility location models", "Flow interception problem", "Microscopic transportation models", "Decision analysis", "Alternative fuels", "Plug-in hybrids"]},
    {"article name": "Silicon solar cell production",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.04.017",
     "publication date": "08-2011",
     "abstract": "A significant role can be played by the systems engineering community in the optimization of the production process for silicon solar cells. Many of the techniques utilized for cell manufacturing are of recent origin and the amount of experience in the industry as a whole is limited. Some of the individual processes and steps are poorly adapted for continuous production since they were designed for micro-electronics applications rather than photovoltaics. Only very recently has the industry grown to the point where intermediate products, such as solar grade silicon, solar silicon wafers, solar cells and solar panels are commodities having global market potential. Finally, industry consolidation has generated large commercial entities which can better take advantage of tools from process systems engineering. The chemical and process systems and engineering communities can contribute to this booming industry by providing methods for improved control, process optimization and retro-fitting of existing processes, as well as encouraging process innovation and scale-up. This paper describes the complete production process for solar cells, highlights challenges relevant to systems engineering, and overviews work in three distinct areas: the application of real time optimization in silicon production, the development of scale-up models for a fluidized bed poly-silicon process and a new process concept for silicon wafer production.",
     "keywords": ["Silicon", "Solar cells", "Process systems engineering", "Supply chain management", "Modeling", "Simulation", "Control"]},
    {"article name": "Bi-objective optimization approach to the design and planning of supply chains: Economic versus environmental performances",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.03.009",
     "publication date": "08-2011",
     "abstract": "Traditionally the design of supply chains has been based on economic objectives. However, as societal environment concerns grows, environmental aspects are also emerging at academic and industry levels as decisive factors within the supply chain management context. The investment towards logistics structures that considers both economic and environmental performances is nowadays an important and current research topic.This paper addresses the planning and design of supply chain structures for annual profit maximization, while considering environmental aspects. The latter are accounted for through the Eco-indicator methodology. Profit and environmental impacts are balanced using an optimization approach adapted from symmetric fuzzy linear programming (SFLP), while the supply chain is modelled as a mixed integer linear programming (MILP) optimization problem using the Resource-Task-Network (RTN) methodology. The obtained model applicability is validated through the solution of a set of supply chain problems.",
     "keywords": ["Multi-objective optimization", "Fuzzy approach", "Economic versus environment performances"]},
    {"article name": "A novel MILP-based objective reduction method for multi-objective optimization: Application to environmental problems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.02.001",
     "publication date": "08-2011",
     "abstract": "Multi-objective optimization has recently emerged as a useful technique in sustainability analysis, as it can assist in the study of optimal trade-off solutions that balance several criteria. The main limitation of multi-objective optimization is that its computational burden grows in size with the number of objectives. This computational barrier is critical in environmental applications in which decision-makers seek to minimize simultaneously several environmental indicators of concern. With the aim to overcome this limitation, this paper introduces a systematic method for reducing the number of objectives in multi-objective optimization with emphasis on environmental problems. The approach presented relies on a novel mixed-integer linear programming formulation that minimizes the error of omitting objectives. We test the capabilities of this technique through two environmental problems of different nature in which we attempt to minimize a set of life cycle assessment impacts. Numerical examples demonstrate that certain environmental metrics tend to behave in a non-conflicting manner, which makes it possible to reduce the dimension of the problem without losing information.",
     "keywords": ["Environmental engineering", "Life cycle assessment", "Multi-objective optimization", "Objective reduction"]},
    {"article name": "Bi-objective minimization of environmental impact and cost in utility plants",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.05.003",
     "publication date": "08-2011",
     "abstract": "A methodology to minimize potential environmental impact and operating cost in the selection of the operating conditions of a steam and power plant is presented. A bi-objective mixed integer nonlinear programming problem is formulated and solved in GAMS. Different strategies are implemented successfully to generate the Pareto curve such as: minimum distance to the utopia point, \u025b-constraint, weighted sum and global criterion. An analysis of the Pareto curve allows the identification of two regions where it is cheaper and more expensive respectively, to reduce the potential environmental impact, providing relevant information to support a decision making process. The economical valorisation of greenhouse gases emissions reduction was also carried out, showing the region of the Pareto curve in which the income would compensate the increment in operating cost, leading to a reduction of the potential environmental impact with no extra cost.",
     "keywords": ["Bi-objective", "Minimisation", "Environmental impact", "Cost", "Utility plant"]},
    {"article name": "A multi-objective analysis for the retrofit of a pulverized coal power plant with a CO2 capture and compression process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.03.020",
     "publication date": "08-2011",
     "abstract": "The long term sustainability of fossil energy systems depends on reducing their carbon footprint and freshwater consumption. Much of the United States is or will be experiencing water shortages in the near future. Since power generation accounts for about a third of all freshwater use, reducing freshwater requirements will be of increasing importance. In addition, recent reports indicate that adding a carbon capture system may double water consumption. Thus, when designing a carbon capture and compression system, it is important to consider not only the direct costs, but also the increased environmental burden associated with increased freshwater requirements. To address these interrelated sustainability issues, a modular framework for multi-objective analysis was developed and demonstrated by minimizing freshwater consumption and levelized cost of electricity for the retrofit of a hypothetical 550 MW subcritical pulverized coal power plant with an MEA-based carbon capture and compression system.",
     "keywords": ["Carbon capture", "Power", "Water minimization", "Multi-objective optimization", "Simulation", "Modeling"]},
    {"article name": "Advanced simulation environment for clean power production in IGCC plants",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.02.005",
     "publication date": "08-2011",
     "abstract": "Oxygen-blown biomass integrated gasification combined cycle (IGCC) plants are one of the most promising options for clean energy generation with CO2 abatement potential. However, the integrated nature of IGCC leads to difficult design problems. In this study, we present an advanced simulation environment for the preliminary design and retrofit of IGCC plants. We describe the modelling approach, the model validation strategy and the plant behaviour, as determined by sensitivity analyses. The simulation environment uses Pareto curves to examine various co-gasification and co-production case studies in terms of technical, economic and environmental performance. It serves as a decision support tool in the design stage, which can be used to explore ways to improve plant performance and to analyse the influence of raw materials and the unit\u2019s operational parameters. The test and validation results are discussed.",
     "keywords": ["Integrated gasification combined cycle (IGCC)", "Biomass gasification", "Co-production", "Process simulation", "Sensitivity analysis"]},
    {"article name": "Optimization of CO2 capture process with aqueous amines using response surface methodology",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.03.016",
     "publication date": "08-2011",
     "abstract": "Amine is one of candidate solvents that can be used for CO2 recovery from the flue gas by conventional chemical absorption/desorption process. In this work, we analyzed the impact of different amine absorbents and their concentrations, the absorber and stripper column heights and the operating conditions on the cost of CO2 recovery plant for post-combustion CO2 removal. For each amine solvent, the optimum number of stages for the absorber and stripper columns, and the optimum absorbent concentration, i.e., the ones that give the minimum cost for CO2 removed, is determined by response surface optimization. Our results suggest that CO2 recovery with 48\u00a0wt% DGA requires the lowest CO2 removal cost of $43.06/ton of CO2 with the following design and operating conditions: a 20-stage absorber column and a 7-stage stripper column, 26\u00a0m3/h of solvent circulation rate, 1903\u00a0kW of reboiler duty, and 99\u00b0C as the regenerator-inlet temperature.",
     "keywords": ["CO2 capture", "Amines", "Optimization", "CO2 removal cost", "Response surface methodology"]},
    {"article name": "Modeling, simulation and control of an internally heat integrated pressure-swing distillation process for bioethanol separation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.03.011",
     "publication date": "08-2011",
     "abstract": "Ethanol produced from biomass is one of the most promising renewable fuels. However, most of the proposed separation methods use volatile organic compounds as solvents. Because such solvents are environmentally hazardous, new, efficient and sustainable ways of alcohol purification process ought to be developed. Ideally, no solvent should be required for the high purity purification of alcohol aqueous solutions. Therefore, in this work the modeling, simulation and control of an internally heat integrated pressure-swing distillation (IHIPSD) process to separate the ethanol/water azeotropic binary system into a high purity ethanol stream are addressed. By means of the proposed separation process, high-purity ethanol, suitable for use in the transportation sector, is obtained. Despite the high interaction between the column sections, the results show that the proposed separation process can be operated smoothly with an array of PI controllers. Moreover, the purity of ethanol is maintained with the control structure proposed in the face of upsets.",
     "keywords": ["Pressure-swing distillation process", "Biofuel ethanol", "Process modeling", "Simulation", "Control", "Internal heat integration"]},
    {"article name": "Energy, water and process technologies integration for the simultaneous production of ethanol and food from the entire corn plant",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.02.007",
     "publication date": "08-2011",
     "abstract": "This paper presents simultaneous integration of different technologies such as the traditional dry-grind process to obtain ethanol from grain with the gasification of the corn stover followed by either syngas fermentation or catalytic mixed alcohols synthesis. The optimal integrated process when using the entire corn plant (18\u00a0kg/s of grain and 10.8\u00a0kg/s of stover) is the one in which the dry-grind technology to process corn grain is integrated with the catalytic path for the corn stover due to the improved integration of energy, requiring only 17\u00a0MW of energy, 50\u00a0MW of cooling and 1.56\u00a0gal/gal of freshwater, for an ethanol production cost of 1.22\u00a0$/gal. However, the production cost decreases as we only use stover to produce ethanol, while the grain is used for food due to the lower cost of the stover and the more favorable energy balance of the ethanol production process from gasification.",
     "keywords": ["Bioethanol", "Energy and food production", "Integrated process synthesis", "Heat, water and technology integration", "Process synthesizer MIPSYN"]},
    {"article name": "A global optimal formulation for the water integration in eco-industrial parks considering multiple pollutants",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.03.010",
     "publication date": "08-2011",
     "abstract": "A mathematical programming formulation for the water integration in eco-industrial parks considering streams with several pollutants is presented. The formulation is based on a superstructure that allows the wastewater reuse in the same plant, the water exchange with different plants, and a shared set of interceptors that must be selected to determine the network configuration that satisfies process equipments and environmental constraints. The model formulation considers wastewater with several pollutants, and optimizes the network according to the minimum total annual cost, which includes the costs of fresh water, piping and regeneration. A new discretization approach is also proposed to handle the large set of bilinear terms that appear in the model in order to yield a near global optimal solution. The results obtained in several examples show considerable savings with respect to the solutions of the individual plant integration policy commonly employed for these types of problems.",
     "keywords": ["Water integration", "Eco-industrial parks", "Recycle/reuse networks", "Optimization", "Convex discretization", "Inter-plant water integration"]},
    {"article name": "Sequential methodology for integrated optimization of energy and water use during batch process scheduling",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.05.009",
     "publication date": "08-2011",
     "abstract": "Though commonly encountered in practice, energy and water minimization simultaneously during batch process scheduling has been largely neglected in literature. In this paper, we present a novel framework for incorporating simultaneous energy and water minimization in batch process scheduling. The overall problem is decomposed into three parts \u2013 scheduling, heat integration, and water reuse optimization \u2013 and solved sequentially. Our approach is based on the precept that in any production plant, utilities (energy and water) consumption is subordinate to the production target. Hence, batch scheduling is solved first to meet an economic objective function. Next, alternate schedules are generated through a stochastic search-based integer cut procedure. For each resulting schedule, minimum energy and water reuse targets are established and networks identified. As illustrated using two well-known case studies, a key feature of this approach is its ability to handle problems that are too complex to be solved using simultaneous methods.",
     "keywords": ["Multi-purpose plant", "Scheduling", "Continuous-time formulation", "Heat integration", "Water reuse synthesis", "Wastewater minimization"]},
    {"article name": "Water sustainability: A systems engineering approach to restoration of eutrophic Lakes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.03.003",
     "publication date": "08-2011",
     "abstract": "We address restoration of eutrophic lakes and reservoirs through the formulation of dynamic optimization problems subject to complex PDAE systems representing biogeochemical processes in the water bodies. The model includes phytoplankton, zooplankton, fish, nutrients, DO, particulate and dissolved carbon dynamics. The PDAE has been transformed into an ordinary differential algebraic equation system by spatial discretization into two water layers. An optimal control problem for the implementation of three different restoration techniques and their combinations has been formulated within a control vector parameterization approach. Numerical results for the different problems provide optimal profiles for tributary deviation flowrate through a nearby wetland, aeration rates and fish removal rates, as restoration strategies.",
     "keywords": ["Eutrophication", "Optimal control", "Dynamic optimization", "Restoration", "Biomanipulation"]},
    {"article name": "Oil spill response planning with consideration of physicochemical evolution of the oil slick: A multiobjective optimization approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.009",
     "publication date": "08-2011",
     "abstract": "This paper addresses the optimal planning of oil spill response operations under economic and responsive criteria, with consideration of oil weathering process. The economic criterion is measured by total cost, while the measure of responsiveness is the time span of the entire response operations. A bi-criterion, multiperiod mixed-integer linear programming (MILP) model is developed that simultaneously predicts the optimal time trajectories of oil volume and slick area, transportation profile, response resource utilization levels, cleanup schedule, and coastal protection plan. The MILP model integrates with the prediction of an oil weathering model that accounts for oil physicochemical properties, spilled amount, hydrodynamics, and weather conditions. The multi-objective optimization model is solved with the epsilon-constraint method and produces a Pareto optimal curve that reveals how the optimal total cost and response operations change under different specifications of responsiveness. We present two illustrative examples for oil spill incidents in the Gulf of Mexico and New England.",
     "keywords": ["Planning", "Oil spill response", "MILP", "Multi-objective optimization", "ODE"]},
    {"article name": "GPU simulations for risk assessment in CO2 geologic sequestration",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.03.023",
     "publication date": "08-2011",
     "abstract": "A main concern for any CO2 sequestration system is whether it may leak CO2 over a long-term time horizon. The outcome depends on the competition between sequestration and leakage processes. Leakages may occur from failure of manmade material or through faults in the formations above the reservoir. A simple and computationally efficient simulator was constructed based on the CQUESTRA model (LeNeveu, 2008). To assess the risk associated with uncertainty in the values of uncertain parameters in this model, thousands of runs were carried out with the simulator on a general-purpose graphics processing unit (GPU). The GPU implementation was up to 64 times faster compared to a CPU implementation. In the absence of active faults around a single injection well, the model suggests that leakages of more than 1% of the total CO2 are unlikely during the 1000 year period after dissipation of temperature and pressure transients associated with injection. Leakage amounts for ten leaky wells are considerably higher, suggesting the critical importance of monitoring equipment after sequestration.",
     "keywords": ["CO2 sequestration", "Risk assessment", "Monte Carlo simulation", "GPU parallel computing"]},
    {"article name": "Systematic mesh development for 3D CFD simulation of fixed beds: Single sphere study",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.12.006",
     "publication date": "07-2011",
     "abstract": "To develop and validate meshes for computational fluid dynamics (CFD) simulations of transport in fixed beds, a single particle is often used as a test case. We present results for drag coefficient (CD) and heat transfer Nusselt number (Nu) for flow past a sphere, focusing on high flow rates typical of industrial steam reformers (400\u00a0<\u00a0Re\u00a0<\u00a020,000). Over this range, good predictions of CD were obtained using large eddy simulation (LES) to capture vortex shedding and wake dynamics, with a mesh refined downstream from the sphere. The small time-steps and high cell count required make this too expensive for fixed beds. Nu can be accurately calculated using a Reynolds-averaged Navier\u2013Stokes (RANS) method with shear-stress transport (SST) k\u2013\u03c9 closure provided the mesh at the particle surface is fine enough and covers most of the boundary layer. Single sphere simulations of heat transfer are more useful for fixed bed mesh development than drag coefficient calculations.",
     "keywords": ["Computational fluid dynamics", "Sphere", "Drag coefficient", "Heat transfer", "Turbulent flow", "Packed bed"]},
    {"article name": "Surface B-splines fitting for speeding up the simulation of adsorption processes with IAS model",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.08.003",
     "publication date": "07-2011",
     "abstract": "The ideal adsorbed solution theory (IAS) is commonly used in process simulation for predicting multicomponent adsorption equilibrium. However, this model requires a significant computational effort which translates in time consuming simulations.A methodology using surface B-splines fitting was developed for speeding the prediction of the multicomponent equilibrium with IAS theory. As a case study, a breakthrough of an enantiomer bi-component mixture in a fixed bed was simulated and a speed-up of 57.9% was achieved while maintaining the accuracy of IAS multicomponent equilibrium prediction.",
     "keywords": ["Adsorption", "IAS", "Simulation", "B-splines"]},
    {"article name": "Comparison of Eulerian\u2013Lagrangian and Eulerian\u2013Eulerian method for dilute gas\u2013solid flow with side inlet",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.09.001",
     "publication date": "07-2011",
     "abstract": "A Eulerian\u2013Lagrangian method is implemented to simulate turbulent two-phase gas\u2013solid riser flow, using a mean-field/probability density function (PDF) method. The mean-field method is applied to the gas phase while the PDF method is applied to the solid phase. Using the PDF method for the solid phase is advantageous as there is no need for closure models for the convection term in the momentum equation contrary to the situation where the mean-field method is used. The present method is implemented to investigate the influence of a side inlet on the flow pattern in a dilute gas\u2013solid riser configuration. When applying the Eulerian\u2013Lagrangian method, a perfect correspondence with the experimental observations is obtained. On the contrary, when using the Eulerian\u2013Eulerian method significant differences with the experimental data are observed.",
     "keywords": ["Gas\u2013Solid flow", "Eulerian\u2013Lagrangian", "Eulerian\u2013Eulerian", "Stochastic differential equations (SDEs)", "Finite Volume", "Reynolds Average"]},
    {"article name": "Direct simulations of spherical particle motion in Bingham liquids",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.09.002",
     "publication date": "07-2011",
     "abstract": "The present work deals with the development of a direct simulation strategy for solving the motion of spherical particles in a Bingham liquid. The simulating strategy is based on a lattice-Boltzmann flow solver and the dual-viscosity Bingham model. Validation of the strategy is first performed for single phase (lid-driven cavity flow) and then for two phase flows. Lid-driven cavity flow results illustrate the flow's response to an increase of the yield stress. We show how the settling velocity of a single sphere sedimenting in a Bingham liquid is influenced by the yield stress of the liquid. The hydrodynamic interactions between two spheres are studied at low and moderate Reynolds number. At low Reynolds number, two spheres settle with equal velocity. At moderate Reynolds number, the yield effects are softened and the trailing sphere approaches the leading sphere until collision occurs.",
     "keywords": ["Bingham liquid", "Simulation", "Lattice-Boltzmann", "Lid-driven cavity", "Sedimentation"]},
    {"article name": "New trends in building numerical programs",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.07.004",
     "publication date": "07-2011",
     "abstract": "This manuscript focuses the attention on the possibility that some basic programs for solving numerical problems have to be revised.The fundamental case of linear system solution and related concepts is proposed as example of the different ways to approach a problem using object-oriented programming and procedural approaches. The different points of view shows that the traditional approach adopted to deem the conditioning of a system as well as all existing programs to solve linear systems need to be revised. A brief discussion deals with the possibility of parallelizing programs for personal computers and with interaction of parallel computing and object-oriented programming.",
     "keywords": ["Object-oriented programming", "Parallel computing", "Linear systems", "Condition number"]},
    {"article name": "Simultaneous calculation of chemical and phase equilibria using convexity analysis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.07.019",
     "publication date": "07-2011",
     "abstract": "Chemical and phase equilibria were considered for closed multicomponent reactive systems at: (a) constant pressure and temperature; (b) constant pressure and enthalpy. Equilibrium at constant P and T was found by minimization of G, while equilibrium at constant P and H was found by maximization of S or minimization of \u2212S, all with respect to the number of moles of each component in each phase. Both cases could be handled as optimization problems, satisfying the restrictions imposed by mole or atom balances, and non-negativity of number of moles. Convexity analyses were carried out, and the conditions were found in order to guarantee global minimum, for one liquid phase, one gas phase, and a number of solid phases. The minimum point was then found either by analytical methods or by direct minimization methods. These strategies were tested for a number of cases, with good results.",
     "keywords": ["Chemical and phase equilibrium", "Gibbs free energy minimization", "Entropy maximization", "Convexity analysis"]},
    {"article name": "A systematic method to create reaction constraints for stoichiometric matrices",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.07.024",
     "publication date": "07-2011",
     "abstract": "Modeling rate-controlled chemically reactive systems in biocatalysis, fuel combustion, material science, and chemical process engineering involves the quantification and exploitation of interactions between many chemical species. These dynamic chemical systems, having relatively few limiting reactions, can be conceived as a series of snapshots where reactions have fixed extents but otherwise idle. Since the reactions affect the stoichiometric matrix of the internal constraints, such constrained equilibrium states cannot be defined in terms of conventional atomic mass balances.A systematic method for obtaining generalized equilibrium constraints for reaction mechanisms of arbitrary complexity is presented. Reaction matrices are converted into entity conservation matrices using row operations. The simultaneously introduced virtual components enable Gibbs energy calculations for complex reaction schemes including organic systems and enzyme-catalyzed biochemical transformations having multiple limiting reactions. Classical Gibbs energy minimization, which would otherwise readily model phase transformations and solvent interactions, is thereby made accessible to these emerging application fields.",
     "keywords": ["Reaction constraint", "Virtual component", "Gibbs energy minimization", "Entity conservation matrix", "Rate-controlled constrained equilibrium"]},
    {"article name": "A thermodynamic equilibrium reactor model as a CAPE-OPEN unit operation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.07.016",
     "publication date": "07-2011",
     "abstract": "A single-phase equilibrium reactor model has been developed, that calculates the reaction equilibrium at a fixed pressure by minimization of the Gibbs free energy at constant temperature, or a maximization of entropy at constant enthalpy. An automatic procedure of determining the reactions and stoichiometry is applied. The reactor model has been implemented in compliance with the CAPE-OPEN standards, and can therefore run in multiple simulation environments using different thermodynamic engines. Care must be taken by the unit operation not to evaluate thermodynamic properties at conditions for which the thermodynamic server may not provide answers. Specifically, it is important to evaluate thermodynamic properties only at mole fractions in the [0,1] region. A projection algorithm has been applied to ensure this. The reactor model has been tested in different simulation environments and is available in the free-of-charge COCO simulator suite.",
     "keywords": ["CAPE-OPEN", "Reactor model", "Process-simulation", "Chemical equilibrium"]},
    {"article name": "Synthesis of multipass heat exchanger networks based on pinch technology",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.08.005",
     "publication date": "07-2011",
     "abstract": "The multipass heat exchanger is the most common type of heat transfer equipment used in heat exchanger networks (HENs) by the chemical process industries. There are many methods that have been proposed for the synthesis of HENs with multipass heat exchangers, which are mostly derived from the FT design method. In this paper, an alternative new method to synthesis multipass HENs is presented based on the classical pinch technology. In the multipass heat exchanger, both countercurrent and co-current flow are involved. For the co-current flow, composite curves and problem tables are modified, and compared with that of the countercurrent flow. A proper minimum temperature difference is also selected considering the energy-capital cost trade-offs, and then a multipass HEN is synthesized. Results of the case study demonstrate that the new approach meets operating requirements and minimizes the total cost successfully.",
     "keywords": ["Heat exchanger networks", "Pinch technology", "Multipass", "Co-current", "Countercurrent"]},
    {"article name": "Systematic integration of LCA in process systems design: Application to combined fuel and electricity production from lignocellulosic biomass",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.11.012",
     "publication date": "07-2011",
     "abstract": "This paper presents a methodology to integrate life cycle assessment (LCA) in thermo-economic models used for the optimal conceptual design of energy conversion systems. It is illustrated by an application to a thermo-economic model developed for the multi-objective optimization of combined synthetic natural gas (SNG) and electricity production from lignocellulosic biomass. The life cycle inventory (LCI) is written as a function of the parameters of the thermo-economic model. In this way, the obtained environmental indicators from the life cycle impact assessment (LCIA) are calculated as a function of the decision variables of process design. The LCIA results obtained with the developed methodology are compared with the results obtained by a conventional LCA of the same process. Then, a multi-objective environomic (i.e. thermodynamic, economic, environmental) optimization of the process superstructure is performed. The results highlight the important effects of process configuration, integration, efficiency and scale on the environmental impacts.",
     "keywords": ["Process systems design", "Biofuels", "Synthetic natural gas", "Life cycle assessment", "Optimization"]},
    {"article name": "Towards the modelling and control of NOx emission in a fluidized bed sludge combustor",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.07.015",
     "publication date": "07-2011",
     "abstract": "Sludge incineration is a widely used technology because of its large volume reduction and complete organic destruction. However, incineration does encounter a number of drawbacks, among which are notably carbon monoxide (CO) and nitrogen oxides (NOx) emissions. Carbon monoxide emissions are efficiently avoided by oxygen regulation in the furnace. Nitrogen oxide (NOx) formation, however, is very complex and not well known. This paper proposes a dynamic model of sludge combustion in an industrial fluidized bed combustor mainly focused on NOx formation. The model is tuned with industrial data. A control strategy is proposed to improve on the current industrial regulation, simultaneously ensuring CO and NOx control, yet without decreasing the combustion efficiency. The results are sufficiently acceptable for this process to be carried out in a real incinerator plant.",
     "keywords": ["Sludge", "Incineration", "NOx formation", "Modelling", "Control"]},
    {"article name": "Experimental and theoretical investigation of parametric sensitivity and dynamics of a continuous stirred tank reactor for acid catalyzed hydrolysis of acetic anhydride",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.09.005",
     "publication date": "07-2011",
     "abstract": "The continuous stirred tank reactor is a dynamic system exhibiting nonlinear behavior such as multiplicity and oscillations and, in certain range of operating conditions, may exhibit a parametric sensitivity where small changes in one or more of the input parameters lead to large changes in the output variable. In the present work, hydrolysis of acetic anhydride reaction system was used to demonstrate the existence of parametric sensitivity with respect to the input parameter, the cooling water flow rate. The applications of parametric sensitivity analysis were used for detection of parametric sensitivity in a continuous stirred tank reactor using catalyses hydrolysis of acetic anhydride reaction system. Also, theoretical investigation revealed that the effect of wall capacitance has definite influence on the dynamics of continuous stirred tank reactor. The continuous stirred tank reactor showed parametric sensitivity both in the regions of uniqueness and multiplicity, and a mathematical model was developed for the reactor. The numerically simulated results are in satisfactory agreement with the experimental data.",
     "keywords": ["Continuous stirred tank reactor (CSTR)", "Parametric sensitivity", "Modeling", "Acid catalyzed hydrolysis"]},
    {"article name": "An impedance model for blood flow in the human arterial system. Part I: Model development and MATLAB implementation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.09.006",
     "publication date": "07-2011",
     "abstract": "An impedance model capable of predicting the time-dependent blood pressure and flow profiles in all of the vessels in the human arterial network has been developed. The model is based on a Womersley-type one-dimensional in space approximation of the pulsatile flow of a viscous fluid within elastic vessels. Nominal values from the literature are used to provide the input aortic pressure wave, the geometric dimensions of large arteries, various blood properties, vessel elasticity, etc. The necessary information to characterize the smaller arteries, arterioles and capillaries is taken from a physical scaling model [West, G. (1999). The origin of scaling laws in biology. Physica Acta, 263, 104\u2013113]. The parameters, input setup, and the subsequent solution to the model equations have been efficiently implemented within MATLAB, which also allows for a variety of output information displays. The MATLAB implementation also allows for a comprehensive sensitivity analysis of the results to various input parameter values to be effortlessly obtained.",
     "keywords": ["MATLAB", "Recursive function", "Blood flow model", "Pulsatile flow", "Arterial network", "Sensitivity analysis"]},
    {"article name": "Adaptation and application of the In Situ Adaptive Tabulation (ISAT) procedure to reacting flow calculations with complex surface chemistry",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.10.005",
     "publication date": "07-2011",
     "abstract": "In this article, the In Situ Adaptive Tabulation (ISAT) procedure, originally developed for the efficient computation of homogeneous reactions in chemically reacting flows, is adapted and demonstrated for reacting flow computations with complex heterogeneous (or surface) reactions. The treatment of heterogeneous reactions within a reacting flow calculation requires solution of a set of nonlinear differential algebraic equations at boundary faces/nodes, as opposed to the solution of an initial value problem for which the original ISAT procedure was developed. The modified ISAT algorithm, referred to as ISAT-S, is coupled to a three-dimensional unstructured reacting flow solver, and strategies for maximizing efficiency without hampering accuracy and convergence are developed. These include use of multiple binary tables, use of dynamic tolerance values to control errors, and periodic deletion and/or re-creation of the binary tables. The new procedure is demonstrated for steady-state catalytic combustion of a methane\u2013air mixture on platinum using a 24-step reaction mechanism with 19 species, and for steady-state three-way catalytic conversion using a 61-step mechanism with 34 species. Both reaction mechanisms are first tested in simple 3D channel geometry with reacting walls, and the impact of various ISAT parameters is investigated. It is found that the temperature of the reacting wall dictates the retrieval rate from the ISAT table. As a final step, the catalytic combustion mechanism is demonstrated in an laboratory-scale monolithic catalytic converter geometry with 57 channels discretized using 354,300 control volumes (4.6 million unknowns) after employing quarter symmetry. For this particular case, the use of ISAT-S resulted in reduction of the overall CPU time from 19.3 to 13.6\u00a0h. For all of the cases considered, the reduction in the time taken to perform surface chemistry calculations alone was found to be a factor of 5\u201311.",
     "keywords": ["Surface reaction", "Complex chemistry", "Computation", "ISAT", "CFD", "Reacting flow"]},
    {"article name": "Microstructure-based mathematical modelling and spectroscopic imaging of tablet dissolution",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.07.008",
     "publication date": "07-2011",
     "abstract": "A computer model was developed with the objective of modelling pharmaceutical tablet dissolution with non-swelling excipients. The model, which allows the explicit description of the tablets heterogeneous microstructure, was verified using Fourier Transform Infrared (FTIR) spectroscopic imaging and UV\u2013visible spectroscopy. Two parametric studies were conducted using the model for binary tablets. One investigated the effect of particle size and tablet composition on component release times and the other studied the effect of changing the diffusivity and solubility of one component on the release times of both. Tablets containing Polyethylene Glycol (PEG) 4000 and nicotinamide were used in the experiments. Physical properties of pure components were obtained from literature and also experiments using tablets containing 100% and 10% (w/w) nicotinamide. The model was then used to predict the dissolution of a tablet containing 40% (w/w) nicotinamide which matched the experiment with a mean squared error of 0.08%.",
     "keywords": ["ATR FT-IR spectroscopy", "Dissolution", "Imaging", "Mathematical modelling", "Pharmaceuticals", "Simulation"]},
    {"article name": "Effect of the diffusion phenomena in the catalyst grain on the selectivity of a system of parallel-consecutive reactions involved in the process of n-heptane dehydrocyclization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.12.001",
     "publication date": "06-2011",
     "abstract": "Analytical relations were derived for analyzing the selectivity of consecutive-parallel reactions occurring under conditions of the reforming process. With these relations it is also possible to determine how the shape of the catalyst grain, as well as the kinetic and diffusion phenomena that govern the process, affects the efficiency of the desired final products.",
     "keywords": ["n-Heptane dehydrocyclization", "Mathematical modelling", "Kinetics", "Porous catalyst", "Catalyst selectivity", "Optimization"]},
    {"article name": "Numerical methods for solving two-dimensional aggregation population balance equations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.08.002",
     "publication date": "06-2011",
     "abstract": "The cell average technique (CAT) and the fixed pivot (FP) method for solving two-dimensional aggregation population balance equations using a rectangular grid were implemented in Kumar et al. (2008). Recently, Chakraborty and Kumar (2007) have studied the FP scheme for the same problem on two different types of triangular grids and found that the method shows better results for number density as compared to the rectangular grids. However, they did not discuss the results for higher moments. Therefore, our first aim in this work is to compare different moments calculated by the FP technique on rectangular and triangular meshes with the analytical moments. Further we introduce a new mathematical formulation of the CAT for the two different types of triangular grids as considered by Chakraborty and Kumar (2007). The new formulation is simple to implement and gives better accuracy as compared to the rectangular grids. Three different test problems are considered to analyze the accuracy of both schemes by comparing the analytical and numerical solutions. The new formulation shows good agreement with the analytical results for number density and higher moments.",
     "keywords": ["Population balance", "Aggregation", "Cell average", "Fixed pivot", "Particle"]},
    {"article name": "Numerical analysis of CO2 concentration and recovery from flue gas by a novel vacuum swing adsorption cycle",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.07.026",
     "publication date": "06-2011",
     "abstract": "The objective of this work is to study the feasibility of carbon dioxide concentration and recovery from flue gases using a novel VSA cycle without rinse step with the aid of mathematical modeling. A theoretical model based on conservation equations (following the one proposed by Da Silva and Rodrigues (2001)) is used for an initial evaluation of the process performance. Activated carbon is the adsorbent considered, simulating adsorption equilibrium and kinetics with the equations proposed by Kikkinides, Yang, and Cho (1993). According to the simulated results, it is possible to recover carbon dioxide with high purity (>93%) from a mixture with 13% carbon dioxide at 40\u00a0\u00b0C, with higher recovery (>90%) and a lower power consumption (<0.12\u00a0kWh/kgCO2) than other processes with rinse step reaching the same purity. Although these results are theoretical, they show the potential advantages of this process for CO2 capture.",
     "keywords": ["Carbon dioxide capture", "Simulation", "Power consumption", "Equalization", "Vacuum swing adsorption"]},
    {"article name": "Accuracy and convergence rate of steady-state simulation of one-dimensional, reactive gas flow with molar expansion",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.02.010",
     "publication date": "06-2011",
     "abstract": "A coupled and a decoupled solution method are applied to the steady-state simulation of one-dimensional, reactive gas flow. The accuracy of the numerical steady-state solution depends to a significant extent on the molar expansion of the gas. The convergence rate of the coupled solution method is almost independent of molar expansion. The convergence rate of the decoupled solution method is at least 50% lower for strong molar expansion than for nonexistent molar expansion. By adding a transport equation for molar mass, the convergence rate of the decoupled solution method becomes almost independent of molar expansion. The results for the decoupled solution method apply to other methods in which composition is determined on a fixed flow field. A tool is presented to quantify the perturbation of the flow field by the chemical reactions.",
     "keywords": ["Steady-state", "Reactive flow", "Accuracy", "Convergence rate", "Molar mass", "Solution method"]},
    {"article name": "Time representations and mathematical models for process scheduling problems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.07.007",
     "publication date": "06-2011",
     "abstract": "During the last 15 years, many mathematical models have been developed in order to solve process operation scheduling problems, using discrete or continuous-time representations. In this paper, we present a unified representation and modeling approach for process scheduling problems. Four different time representations are presented with corresponding strengthened formulations that rely on exploiting the non-overlapping graph structure of these problems through maximum cliques and bicliques. These formulations are compared, and applied to single-stage and multi-stage batch scheduling problems, as well as crude-oil operations scheduling problems. We introduce three solution methods that can be used to achieve global optimality or obtain near-optimal solutions depending on the stopping criterion used. Computational results show that the multi-operation sequencing time representation is superior to the others as it allows efficient symmetry-breaking and requires fewer priority-slots, thus leading to smaller model sizes.",
     "keywords": ["Time representations", "Batch scheduling", "Crude-oil scheduling", "Mixed-integer linear programming"]},
    {"article name": "Modeling and multi-objective optimization of cyclone separators using CFD and genetic algorithms",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.07.017",
     "publication date": "06-2011",
     "abstract": "In the present study, multi-objective optimization of cyclone separators is performed at three steps. At the first step, pressure drop (\u0394p) and the cut-point (D50) in a set of cyclone separators are numerically investigated using CFD techniques. Two meta-models based on the evolved group method of data handling (GMDH) type neural networks are obtained, at the second step, for modeling of \u0394p and D50 with respect to geometrical design variables. Finally, using obtained polynomial neural networks, multi-objective genetic algorithms are used for Pareto based optimization of cyclone separators considering two conflicting objectives, \u0394p and D50.It is shown that some interesting and important relationships as useful optimal design principles involved in the performance of cyclones can be discovered by Pareto based multi-objective optimization of the obtained polynomial meta-models.",
     "keywords": ["Cyclone separators", "Gas\u2013solid", "Multi-objective optimization", "CFD", "GMDH"]},
    {"article name": "A new method of locating all pinch points in nonideal distillation systems, and its application to pinch point loci and distillation boundaries",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.11.007",
     "publication date": "06-2011",
     "abstract": "A new method for automatically finding all of the pinch points in a user-specified composition space in nonideal distillations system at any reflux is presented. It does not rely on the solution of ODEs, and neither knowledge of the system topology, nor rigorous simulation is required. Moreover, the method can be applied to any column section, even those within complex configurations. The method works on the principle of a systematic search over an area to find where the conditions for a pinch point are satisfied; this includes nodes outside of the mass balance triangle, which, while physically impossible, do provide useful information. This principle is extended to reflux-parameterised pinch point loci and to finding distillation boundaries accurately. Nonidealities are modelled with the NRTL model, although any model can be used. Only ternary systems have been considered, but the method can extend to higher order systems.",
     "keywords": ["Distillation", "Node", "Pinch point locus", "Column profile map", "Difference point equation"]},
    {"article name": "Design of extractive distillation for the separation of close-boiling mixtures: Solvent selection and column optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.12.005",
     "publication date": "06-2011",
     "abstract": "A practical methodology for the design and optimization of extractive distillation is proposed in this work. The extractive distillation is generally applied to the separation of close-boiling mixtures, which by conventional distillation is difficult to separate. The design and optimization of extractive distillation is more complex than that of the conventional distillation when considering the selection of suitable solvent to enhance the separation. Currently, the solvent selection can be effectively handled by the assistance of the computer-aided molecular design (CAMD) approach. The selection result may however be inconclusive due to the lack of accurate or missing parameters in the property model. In this work, the experimental verification and the property parameter determination were proved to be necessary as an additional step to achieve a successful and reliable design. The overall design methodology was illustrated through an industrial separation of C8-Aromatics mixture.",
     "keywords": ["C8-Aromatics", "Extractive distillation", "Process design and optimization"]},
    {"article name": "Analysis of the stability and controllability of chemical processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.12.011",
     "publication date": "06-2011",
     "abstract": "Chemical processes constitute strongly nonlinear systems and, for such systems, multiple steady-state solutions typically exist. In addition, the various steady state solutions are likely to differ in terms of stability and phase behaviors, which is an important consideration for practical applications. A chemical process is used in this paper to demonstrate how to analyze process stability and controllability. Finally, the conclusion is drawn that overall system stability and phase behaviors should be considered because the individual unit operations or subsystems differ from the total system in terms of these features. Therefore, the analysis of stability and controllability of process systems is important in terms of the design of inherently safer processes.",
     "keywords": ["Chemical process", "Steady state solution stability", "Controllability", "Zero dynamics", "Minimum phase"]},
    {"article name": "A direct sampling particle filter from approximate conditional density function supported on constrained state space",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.07.022",
     "publication date": "06-2011",
     "abstract": "Constraints on the state vector must be taken into account in the state estimation problem. Recently, acceptance/rejection and projection methods are proposed in the particle filter framework for constraining the particles. A weighted least squares formulation is used for constraining samples in unscented and ensemble Kalman filters. In this paper, direct sampling from an approximate conditional probability density function (pdf) is proposed. It is obtained by approximating the a priori pdf as a Gaussian. The support of the conditional density is a subset of the intersection of two supports, the 3-sigma bounds of the priori Gaussian and the constrained state space. A direct sampling algorithm is proposed for handling linear and nonlinear equality and inequality constraints. The algorithm uses the constrained mode for nonlinear constraints.",
     "keywords": ["State estimation", "Particle filter", "Maximum a posteriori estimation"]},
    {"article name": "Parallel computing approaches to sensor network design using the value paradigm",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.07.014",
     "publication date": "06-2011",
     "abstract": "One previous paradigm was based on sensor positioning selection to allow key variable observability. Later, the problem was posed as a minimum cost problem subject to sensor network performance (precision and gross error robustness first and accuracy as a replacement later). In a previous paper (Nguyen and Bagajewicz, submitted for publication), we presented a new paradigm in sensor network design, one that maximizes value, defined as economic value of information minus its cost. We presented two methods to solve the problem and we concluded that their performance was acceptable for small and medium size problems, but not for large problems. In this paper we use parallel computing approaches for large problems.",
     "keywords": ["Instrumentation design", "Sensor network design", "Parallel computing"]},
    {"article name": "Novel soft sensor method for detecting completion of transition in industrial polymer processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.09.003",
     "publication date": "06-2011",
     "abstract": "Soft sensors are widely used to estimate process variables that are difficult to measure online. In polymer plants that produce various grades of polymers, the quality of products must be estimated using soft sensors in order to reduce the amount of off-grade material. However, during grade transition, the predictive accuracy deteriorates because the state in polymer reactors is unsteady, causing the values of process variables to differ from the steady-state values used to construct regression models. Therefore, we have proposed to construct models that detect the completion of transition to ensure that the polymer quality evaluated after transition conforms to the predicted one. By using these models and regression models constructed for each product grade, the polymer quality can be predicted with high accuracy, selecting a regression model appropriately. The proposed method was applied to industrial plant data and was found to exhibit higher predictive performance than traditional methods.",
     "keywords": ["Soft sensor", "Transition", "k-Nearest neighbor method", "Support vector machine", "Range-based approach", "One-class support vector machine"]},
    {"article name": "Learning patterns in combinatorial protein libraries by Support Vector Machines",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.017",
     "publication date": "06-2011",
     "abstract": "Recent advances such as directed evolution and high throughput experiments can generate recombinant protein libraries and screen them for properties of interest. However it is impractical to span the theoretical range of combinatorial library and hence predictive models using the limited experimental data are of invaluable use. In this work, we have developed a novel machine learning strategy using Support Vector Machine (SVM) to predict the folding nature of recombinant proteins from Cytochrome P450 family using available experimental data. The folding-status is determined by an empirical energy model based on pair-wise interactions. It is shown that applying similarity-kernel function to the SVM formulation enables inclusion of many body interaction terms without additional computational effort. This approach can be generalized to other recombinant families and different properties of interest. The inferences derived by analyzing the data using the new method are in agreement with published results.",
     "keywords": ["SVM", "Recombinant proteins", "Combinatorial protein libraries", "Kernel methods", "Folding classification"]},
    {"article name": "Modeling and design of transdermal drug delivery patches containing an external heating device",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.006",
     "publication date": "06-2011",
     "abstract": "Process modeling and design concepts were implemented to aid in the manufacturing of heat-enhanced transdermal drug-delivery systems. The simulated prototype consists of a corticosterone-loaded polymer patch applied to the skin and connected to a heating device in which an exothermic reaction occurs. To achieve a desired transdermal flux of 1.2\u00a0\u00d7\u00a010\u22125\u00a0mg/cm2\u00a0h, this contribution focuses on the influences of the (1) initial reaction rate (\u2212rA0), (2) mass of filler material in the device (m), (3) initial concentration (C0) of medicament in the patch and (4) overall heat transfer coefficient (U). A regression technique yielded the following results: \u2212rA0\u00a0=\u00a03.000\u00a0\u00d7\u00a010\u22122\u00a0kg/m3\u00a0s, m\u00a0=\u00a01.251\u00a0\u00d7\u00a010\u22128\u00a0kg, U\u00a0=\u00a06.124\u00a0\u00d7\u00a010\u00a0J/m2\u00a0K\u00a0s and C0\u00a0=\u00a01.966\u00a0\u00d7\u00a010\u22121\u00a0kg/m3. When m was fixed at 12.5\u00a0g, the optimum design required the following specifications: rA0\u00a0=\u00a02.765\u00a0\u00d7\u00a010\u22122\u00a0kg/m3\u00a0s, U\u00a0=\u00a01.402\u00a0\u00d7\u00a0103\u00a0J/m2\u00a0K\u00a0s and C0\u00a0=\u00a01.941\u00a0\u00d7\u00a010\u22121\u00a0kg/m3. The priority (Si) of the input factors (i) in reaching the target delivery rate is: S C 0 > S \u2212 r A 0 > S m > S U .",
     "keywords": ["Transdermal", "Nonlinear pharmacokinetics", "Mathematical model", "Heat", "Optimization", "Process design"]},
    {"article name": "On the improvement of performance of bioreactors through periodic forcing",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.09.004",
     "publication date": "06-2011",
     "abstract": "The paper investigates the periodic operation of a continuous bioreactor described by an unstructured model for which the yield coefficient is linearly dependant on the substrate. The model was studied by a number of authors who showed its ability to predict oscillatory behavior for some range of dilution rates. The paper investigates the different dynamic behaviors predicted by the model when the substrate feed concentration is periodically varied. It is shown that when a region of stable limit cycle is forced with a relatively high forcing frequency, the behavior of the process alternates between periodic and complex chaotic behavior via a number of mechanisms of transition, including period doubling and intermittency. It is shown that the time-average performance of the bioreactor can be substantially improved by an appropriate selection of the forcing amplitude. The effect of the position of the center of forcing on the nature of the emerging patterns are also studied.",
     "keywords": ["Bioreactor", "Unstructured model", "Periodic", "Forcing", "Optimization"]},
    {"article name": "Off-line model reduction for on-line linear MPC of nonlinear large-scale distributed systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.023",
     "publication date": "05-2011",
     "abstract": "Model predictive control (MPC) is an efficient method for the controller design of a large number of processes. However, linear MPC is often inappropriate for controlling nonlinear large-scale systems, while non-linear MPC can be computationally costly. The resulting optimization-based procedure can lead to local minima due to the, non-convexities that non-linear systems can exhibit. To overcome the excessive computational cost of MPC application for large-scale nonlinear systems, model reduction methodology in conjunction with efficient system linearizations have been exploited to enable the efficient application of linear MPC for nonlinear distributed parameter systems (DPS). An off-line model reduction technique, the proper orthogonal decomposition (POD) method, combined with a finite element Galerkin projection is first used to extract accurate non-linear low-order models from the large-scale ones. Trajectory Piecewise-Linear (TPWL) methodologies are subsequently developed to construct a piecewise linear representation of the reduced nonlinear model, both in a static and in a dynamic fashion. Linear MPC, based on quadratic programming, can then be efficiently performed on the resulting low-order, piece-wise affine system. Our combined methodology is readily applicable in combination with advanced MPC methodologies such as multi-parametric MPC (MP-MPC) (Pistikopoulos, 2009). The stabilisation of the oscillatory behaviour of a tubular reactor with recycle is used as an illustrative example to demonstrate our methodology.",
     "keywords": ["Model reduction", "Model predictive control", "Distributed systems", "Proper orthogonal decomposition", "Trajectory piecewise-linear"]},
    {"article name": "Non-linear model approximation and reduction by new input-state Hammerstein block structure",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.037",
     "publication date": "05-2011",
     "abstract": "In this paper, the focus will be on approximating nonlinear large scale mathematical model of process systems using full order block-structured model. Further, the objective is to achieve a reduced order model for the nonlinear large model with reduced computational complexity, while at the same time being the good approximation of nonlinear model. The modeling approach used for this purpose is block structure models. Input\u2013output Hammerstein structure referred to the classical Hammerstein model has been extended to new Hammerstein structure making use of states and inputs, hence called as input-state (IS) Hammerstein structure. In this paper it is shown that expansion of Taylor series leads to IS-Hammerstein structure. The accuracy of the approximation is improved by including higher order (second order) approximation. The input-state Hammerstein structure provides opportunities for model reduction in context of reducing the computational load by order reduction of states and Jacobians. IS-full order Hammerstein model has been implemented on a case study from the process industry namely the high purity distillation column. Within the operational domain of a process, the IS-Hammerstein structure provides a reduced order mode that can be used for online application purposes (i.e., optimization, model predictive control, etc.).",
     "keywords": ["Nonlinear model approximation", "Block structure models", "Hammerstein structure", "Reduced order states & Jacobians", "High purity distillation column", "CSTR"]},
    {"article name": "Scheduling and control decision-making under an integrated information environment",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.025",
     "publication date": "05-2011",
     "abstract": "The complexity of decision-making in process industries and the need of highly competitive organizations require new supporting tools to coordinate and optimize the information flow among decision levels. This work presents a framework for integrating the scheduling and control decision levels by means of an ontology, which allows and coordinates the information exchange among the different modeling paradigms/conventions currently used for the enterprise-wide optimization (EWO). The scheduling of two multiproduct batch plants with increasing complexity is presented for illustrating the proposed working procedure.",
     "keywords": ["Process control and scheduling", "Multiproduct batch plants", "Ontology", "Decision-levels integration"]},
    {"article name": "Optimization-based design of reactive distillation columns using a memetic algorithm",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.038",
     "publication date": "05-2011",
     "abstract": "The design optimization of reactive distillation columns (RDC) is characterized by complex nonlinear constraints, nonlinear cost functions, and the presence of many local optima. The standard approach is to use MINLP solvers that work on a superstructure formulation where structural decisions are represented by discrete variables and lead to an exponential increase in the computational effort. The mathematical programming (MP) methods which solve the continuous sub-problems provide only one local optimum which depends strongly on the initialization. In this contribution a memetic algorithm (MA) is introduced and applied to the global optimization of four different formulations of a computational demanding real-world design problem. An evolution strategy addresses the global optimization of the design decisions, while continuous sub-problems are efficiently solved by a robust MP solver. The MA is compared to MINLP techniques. It is the only algorithm that finds the global solution in reasonable times for all model formulations.",
     "keywords": ["Conceptual design optimization", "Reactive distillation columns", "Memetic algorithm"]},
    {"article name": "Kalman-based strategies for Fault Detection and Identification (FDI): Extensions and critical evaluation for a buffer tank system",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.045",
     "publication date": "05-2011",
     "abstract": "This paper is concerned with the application of Kalman filter based methods for Fault Detection and Identification (FDI). The original Kalman based method, formulated for bias faults only, is extended for three more fault types, namely the actuator or sensor being stuck, sticky or drifting. To benchmark the proposed method, a nonlinear buffer tank system is simulated as well as its linearized version. This method based on the Kalman filter delivers good results for the linear version of the system and much worse for the nonlinear version, as expected. To alleviate this problem, the Extended Kalman Filter (EKF) is investigated as a better alternative to the Kalman filter. Next to the evaluation of detection and diagnosis performance for several faults, the effect of dynamics on fault identification and diagnosis as well as the effect of including the time of fault occurrence as a parameter in the diagnosis task are investigated.",
     "keywords": ["Kalman filter", "Fault Detection and Identification (FDI)", "Process safety", "Process control", "Non-linear systems"]},
    {"article name": "A multi-level simulation approach for the crude oil loading/unloading scheduling problem",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.030",
     "publication date": "05-2011",
     "abstract": "An integrated approach for refinery production scheduling and unit operation optimization problems is presented. Each problem is at a different decision making layer and has an independent objective function and model. The objective function at the operational level is an on-line maximization of the difference between the product revenue and the energy and environmental costs of the main refinery units. It is modeled as an NLP and is constrained by ranges on the unit's operating condition as well as product quality constraints. The production scheduling layer is modeled as an MILP with the objective of minimizing the logistical costs of unloading the crude oil over a day-to-week time horizon. The objective function is a linear sum of the unloading, sea waiting, inventory, and setup costs. The nonlinear simulation model for the process units is used to find optimized refining costs and revenue for a blend of two crudes. Multiple linear regression of the individual crude oil flow rates within the crude oil percentage range allowed by the facility is then used to derive linear refining cost and revenue functions. Along with logistics costs, the refining costs or revenue are considered in the MILP scheduling objective function. Results show that this integrated approach can lead to a decrease of production and logistics costs or increased profit, provide a more intelligent crude schedule, and identify production level scheduling decisions which have a tradeoff benefit with the operational mode of the refinery.",
     "keywords": ["Supply chain management", "Refinery scheduling", "Petroleum supply chain", "Scheduling control"]},
    {"article name": "A generic multi-dimensional model-based system for batch cooling crystallization processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.029",
     "publication date": "05-2011",
     "abstract": "A generic multi-dimensional modelling framework for crystallization processes has been developed to study various aspects of batch cooling crystallization operations and modelling options. The framework contains a generic crystallizer model from which a wide range of problem-system specific models can be created through a model generation procedure. The modelling framework allows one to study a wide range of chemical (crystallization) systems as well as different crystallizer operation phases (scenarios) through choices of different forms of models for phenomena such as nucleation, crystal growth, agglomeration and breakage. Applications of the modelling framework are highlighted through: (i) a paracetamol crystallization case study illustrating the ability of the modelling framework to develop and further extend models and to switch between different chemical systems; and (ii) a potassium dihydrogen phosphate (KDP) case study to demonstrate how the model complexity can be changed by switching between one-dimensional and two-dimensional descriptions.",
     "keywords": ["Crystallization", "Generic modelling framework", "Multi-dimensional model", "Paracetamol", "Potassium dihydrogen phosphate", "Population balance model"]},
    {"article name": "Convex/concave relaxations of parametric ODEs using Taylor models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.031",
     "publication date": "05-2011",
     "abstract": "This paper presents a discretize-then-relax method to construct convex/concave bounds for the solutions of a wide class of parametric nonlinear ODEs. The algorithm builds upon Taylor model methods recently developed for verified solution of parametric ODEs. To enable the propagation of convex/concave state bounds, a new type of Taylor model is introduced, in which convex/concave bounds for the remainder term are computed in addition to the usual interval bounds. At each time step, a two-phase procedure is applied: a priori convex/concave bounds that are valid over the entire time step are calculated in the first phase; then, pointwise-in-time convex/concave bounds at the end of the time step are obtained in the second phase. This algorithm is implemented in an object-oriented manner using templates and operator overloading. It is demonstrated and compared to other available approaches on a selection of problems from the literature.",
     "keywords": ["Interval analysis", "Taylor models", "Convex relaxations", "Ordinary differential equations", "Global optimization", "Dynamic optimization"]},
    {"article name": "A mixed integer optimisation approach for integrated water resources management",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.032",
     "publication date": "05-2011",
     "abstract": "In areas lacking substantial freshwater resources, the utilisation of alternative water sources, such as desalinated seawater and reclaimed water, is a sustainable alternative option. This paper presents an optimisation approach for the integrated management of water resources, including desalinated seawater, wastewater and reclaimed water, for insular water deficient areas. The proposed mixed integer linear programming (MILP) model takes into account the subdivided regions on the island, the subsequent localised needs for water use (including water quality) and wastewater production, as well as geographical aspects. In addition, the integration of potable and non-potable water systems is considered. The optimal water management decisions, including the location of desalination, wastewater treatment, and reclamation plants, as well as the conveyance infrastructure for desalinated water, wastewater and reclaimed water, are obtained by minimising the annualised total capital and operating costs. Finally, the proposed approach is applied to two Greek islands: Syros and Paros-Antiparos, for case study and scenario analysis.",
     "keywords": ["Integrated water resources management", "MILP", "Desalination", "Wastewater treatment", "Water reclamation"]},
    {"article name": "APOGEE: Global optimization of standard, generalized, and extended pooling problems via linear and logarithmic partitioning schemes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.026",
     "publication date": "05-2011",
     "abstract": "Our recent work globally optimized two classes of large-scale pooling problems: a generalized pooling problem treating the network topology as a decision variable and an extended pooling problem incorporating environmental regulations into constraints. The pooling problems were optimized using a piecewise linear scheme that activates appropriate under- and overestimators with a number of binary decision variables that scales linearly with the number of segments in the piecewise relaxation. Inspired by recent work Vielma and Nemhauser, 2010, Vielma et al., 2010a, we introduce a formulation for the piecewise linear relaxation of bilinear functions with a logarithmic number of binary variables and computationally compare the performance of this new formulation to the best-performing piecewise relaxations with a linear number of binary variables. We have unified our work by developing APOGEE, a computational tool that globally optimizes standard, generalized, and extended pooling problems. APOGEE is freely available to the scientific community at helios.princeton.edu/APOGEE/.",
     "keywords": ["Large-scale optimization", "Global optimization", "MINLP", "Quadratically constrained quadratic programs", "Pooling problem", "EPA Complex Emissions Model"]},
    {"article name": "Modelling and simulation of particle re-suspension in a turbulent square duct flow",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.040",
     "publication date": "05-2011",
     "abstract": "The ability of a Reynolds-averaged Navier\u2013Stokes (RANS) approach, coupled with a Lagrangian particle tracking technique, to predict particle re-suspension rates in a high Reynolds number duct flow has been assessed for spherical particles over a range of sizes, with results compared with predictions based on large eddy simulation. In general, there is reasonable agreement between the two predictive techniques in regards to the locations where maximum re-suspension rates occur in the lower half of the duct, with both methods predicting the preferential re-suspension of smaller particles. The main difference between the approaches is in the magnitude of the re-suspension rate, with RANS predicting a greater variability across the duct. These differences are attributable to the method used to derive instantaneous fluid velocities, required by the Lagrangian particle tracking technique, from the RANS solutions, coupled with smaller inaccuracies due to the turbulence model employed as the basis of the RANS solutions.",
     "keywords": ["Particles", "Re-suspension", "Square duct", "LES", "RANS"]},
    {"article name": "Modelling the drop coalescence at the interface of two liquids",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.022",
     "publication date": "05-2011",
     "abstract": "Several processes involve two immiscible liquid phases where the drops of the dispersed phase are surrounded by a continuous phase. The drops coalesce for joining their mother phase. This phenomenon occurs in many industrial mixing, separation, as well as in environmental processes. This paper is focused on the problem of the coalescence of a single drop at the interface with its mother phase. An original model characterizing the mechanism of coalescence is proposed and then validated by comparison with experimental data both from the literature and from the authors.",
     "keywords": ["Drop shape", "Film drainage", "Coalescence mechanism", "Coalescence time"]},
    {"article name": "Effects of fouling on performance of retrofitted heat exchanger networks: A thermo-hydraulic based analysis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.027",
     "publication date": "05-2011",
     "abstract": "The energy recovery performance of crude pre-heat trains (PHTs) in oil refineries is typically impaired by deposition over time of fouling on the thermal surfaces. Such time varying effects are normally not considered in the design or retrofit of heat exchangers networks. In this paper, the importance of taking into account such effects is demonstrated, by means of a case study. An existing industrial PHT network is simulated using a dynamic, distributed mathematical model for shell-and-tube heat exchangers undergoing crude oil fouling. To systematically assess the impact of fouling at the network level, several key performance indicators are proposed and used to analyse three retrofit options aimed at maximising overall heat recovery. Simulation results show that network designs that maximise energy recovery at steady state are not the best when fouling occurs. It is concluded that a proper retrofit design must include consideration of time varying fouling effects.",
     "keywords": ["Crude oil fouling", "Refinery pre heat train", "Heat exchanger network", "Shell and tube heat exchanger", "Modelling"]},
    {"article name": "Development and validation of a dynamic model for regeneration of passivating baths using membrane contactors",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.021",
     "publication date": "05-2011",
     "abstract": "This work aims at the development of a dynamic model for the mathematical description of facilitated transport separation processes carried out in membrane contactors where mass transport phenomena are coupled with chemical reactions. A general model that takes into account the description of all possible mass transport steps and interfacial chemical reactions is initially presented, allowing its application to a wide range of separation processes and operation conditions. The analysis of the specific system under study, regeneration of trivalent chromium spent passivating baths by removal of zinc using the emulsion pertraction technology, allowed to define several assumptions obtaining simplified models with minimum number of uncertain parameters and mathematical complexity. The final equations and parameters were validated with experimental data reported in a previous work (Urtiaga, Bringas, Mediavilla, & Ortiz, 2010).",
     "keywords": ["Passivating baths", "Membrane contactors", "Zinc separation", "Emulsion pertraction technology", "Facilitated transport"]},
    {"article name": "A kinetic approach to the mathematical model of fixed bed gasifiers",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.036",
     "publication date": "05-2011",
     "abstract": "This work presents a comprehensive mathematical model of a fixed bed gasifier, where heat and mass transport resistances and chemical kinetics are accounted for both at the reactor and the particle scale. A multistep kinetic model of devolatilization of solid fuels, such as coals, plastics, biomasses and wastes has been employed and validated. The kinetic model of refuse derived fuels (RDF) and wastes is simply based on a linear combination of the devolatilization models of its main constituents. Ligno-cellulosic and plastic materials, together with ash and moisture, allow to account for the high heterogeneity of RDF. Successive gas phase reactions of the released species are described with a detailed kinetic scheme. Furthermore, an accurate description of heat and mass transport between gas and solid phases allows the proper characterization of combustion and gasification of the solid fuel at the particle and reactor scale. The mathematical model of a counterflow fixed bed reactor is then applied first to discuss the importance of heat transfer resistances at the particle scale, then to describe coal and biomass gasification. This work summarizes several facets of this problem with validations and examples and it allows to evaluate feasibility and limitations of the proposed approach.",
     "keywords": ["Gasifier", "Detailed kinetics", "Coals", "Biomasses", "Wastes"]},
    {"article name": "On maximizing biodiesel mixing ratio based on final product specifications",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.034",
     "publication date": "05-2011",
     "abstract": "A model predicting 12 properties of diesel\u2013biodiesel mixtures was developed. This was based on existing correlations capable of providing quality characteristics for the mixtures. The model was also used to maximize the biodiesel fraction in the diesel\u2013biodiesel mixtures, while taking into consideration all product quality specifications as they are defined by Greek Legislation. The properties examined were density, viscosity, cloud point, pour point, volatility at temperatures 250\u00a0\u00b0C, 350\u00a0\u00b0C and 360\u00a0\u00b0C, cetane index, cetane number, sulfur, water, higher heating value, flash point and cold filter plugging point (CFPP). The model was evaluated for mixtures between two diesel types (normal diesel and Shell extra diesel) and four biodiesel types (i.e. biodiesel produced from different vegetable oils). The model was developed in MATLAB and the corresponding biodiesel optimization studies were carried out with the MATLAB's optimization toolbox.",
     "keywords": ["Biodiesel", "Product quality", "Optimization", "Prediction"]},
    {"article name": "Cell-based dynamic heat exchanger models\u2014Direct determination of the cell number and size",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.033",
     "publication date": "05-2011",
     "abstract": "Large amounts of thermal energy are transferred between fluids for heating or cooling in industry as well as in the residential and service sectors. Typical examples are crude oil preheating, ethylene plants, pulp and paper plants, breweries, plants with exothermic and endothermic reactions, space heating, and cooling or refrigeration of food and beverages. Heat exchangers frequently operate under varying conditions. Their appropriate use in flexible heat exchanger networks as well as maintenance/reliability related calculations requires adequate models for estimating their dynamic behaviour. Cell-based dynamic models are very often used to represent heat exchangers with varying arrangements. The current paper describes a direct method and a visualisation technique for determining the number of the modelling cells and their size.",
     "keywords": ["Dynamic heat exchanger modelling", "Cell models", "Heat exchanger networks", "Controllability"]},
    {"article name": "Separation of butanol from acetone\u2013butanol\u2013ethanol fermentation by a hybrid extraction\u2013distillation process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.028",
     "publication date": "05-2011",
     "abstract": "The alternative fuel butanol can be produced via acetone\u2013butanol\u2013ethanol (ABE) fermentation from biomass. The high costs for the separation of ABE from the dilute fermentation broth have so far prohibited the industrial-scale production of bio-butanol. In order to facilitate an effective and energy-efficient product removal, we suggest a hybrid extraction\u2013distillation downstream process with ABE extraction in an external column. By means of computer-aided molecular design (CAMD), mesitylene is identified as novel solvent with excellent properties for ABE extraction from the fermentation broth. An optimal flowsheet is developed by systematic process synthesis which combines shortcut and rigorous models with rigorous numerical optimization. Optimization of the flowsheet structure and the operating point, consideration of heat integration, and the evaluation of the minimum energy demands are covered. It is shown that the total annualized costs of the novel process are considerably lower compared to the costs of alternative hybrid or pure distillation processes.",
     "keywords": ["Butanol", "ABE fermentation", "Solvent", "Extraction", "Downstream process", "Process synthesis"]},
    {"article name": "Simulated preparation of supported porous catalyst and evaluation of its reaction-transport properties",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.039",
     "publication date": "05-2011",
     "abstract": "In this contribution, mathematical model for the description of solvent evaporation and noble metal precursors crystallization in a porous medium on nano-scale is presented. The methodology is based on the volume-of-fluid method and the model is validated by comparing the numerical simulation results with analytical solutions for evaporation from a single pore and for particle growth and Ostwald ripening of two freely suspended particles in a saturated solution. Impact of process conditions (initial concentration of the metal precursors, temperature, transport properties, nucleation) on the resulting porous catalyst is studied.",
     "keywords": ["Porous catalyst", "Catalyst design", "Crystallization", "Drying", "Volume-of-fluid method", "Mathematical modeling"]},
    {"article name": "Process intensification in duplex pressure swing adsorption",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.024",
     "publication date": "05-2011",
     "abstract": "As an alternative to the Pressure Swing Adsorption (PSA) based on the Skarstrom cycle or its variants, a novel two-bed PSA \u2013 called duplex PSA \u2013 has been proposed by Hirose and independently by Leavitt to get both products of high purities. A modified duplex PSA has been presented to achieve process intensification, that is, to enhance the product purities and productivities. Simulation studies were carried out to explore the attainable product purities and possible process intensification for CO2 capture with the original and modified duplex PSA. The volume reduction of beds that can be realized with modified duplex PSA is about 100\u201350 times the original duplex PSA depending upon the product purities.",
     "keywords": ["Process intensification", "Duplex-PSA", "CO2 capture", "Gas separation"]},
    {"article name": "An alternate computational architecture for advanced process engineering",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.06.010",
     "publication date": "04-2011",
     "abstract": "An algorithm for equation-oriented (EO) flowsheeting to which the interfacing of external modular procedures is readily allowed is presented. In the algorithm, all the process units in a flowsheet are solved simultaneously in an EO environment while taking advantage of the external software as it is. The algorithm is the basis for the process simulator MIKAN. The simulator consists of two parts, namely EO main, which is the simulator's executive, and add-on blocks to incorporate external procedures. EO main also serves to solve separation processes. The PBM (Pseudo-Binary-Mixture)-based algorithm for separation processes (Ishii & Otto, 2008) is fully exploited in EO main. The equations of the entire system are composed of a set of equations for separation processes and sets of equations representing the input\u2013output relation of each add-on block. The input\u2013output relation is obtained by numerical perturbation where all the component flow rates are perturbed collectively. Although the proposed perturbation is simple and significantly less expensive, it is very reliable since the interactions among the input variables affecting the output are fully accounted for.Robustness, flexibility and efficiency of the new algorithm have been confirmed by its implementation. A serious drawback with the EO approach is the difficult accessibility to the user when incorporating new or external process models. Easy accessibility is an important attribute of the new algorithm, even though the simulator's executive is highly integrated and complex. It is significant that the derivative information at various levels of a flowsheet is easily obtained in the algorithm. The novel algorithmic capabilities of the algorithm provide a robust platform for the next stage of advanced process engineering.",
     "keywords": ["Equation-oriented flowsheeting", "Numerical perturbation", "Modular procedures", "PBM concept", "Process engineering", "Separation processes"]},
    {"article name": "Numerical simulation and validation of gas-particle rectangular jets in crossflow",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.09.007",
     "publication date": "04-2011",
     "abstract": "This paper presents a numerical study of a gas-particle flow in three inclined rectangular jets in crossflow. The predicted gas phase velocities and particle phase velocities are validated against previously reported experimental data. Two turbulence models, the standard k\u2013\u025b model and Shear Stress Transfer (SST) model, are used to model the gas phase turbulence. This work shows that both models provide acceptable predictions of the gas flow and mixing generated by the three jets. Neither model could accurately reproduce the jet core and the flow near bottom wall. The particle phase in this flow comprises a large number of small particles. Thus particles follow the gas phase flow closely and any errors in the turbulence model and gas flow predictions are passed on to the particle phase simulation. This paper also includes a literature review on rectangular jets in crossflow and gas-particle laden jets in crossflow.",
     "keywords": ["Gas-particle flow", "Jet in crossflow", "Standard k\u2013\u025b model models", "SST model", "Lagrangian model"]},
    {"article name": "Evaluation of potassium chloride emissions applying the Discrete Particle Method (DPM)",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.06.003",
     "publication date": "04-2011",
     "abstract": "An increasing price for primary energy makes thermal conversion of biomass a very attractive alternative to produce energy. However, emissions emanating from combustion for example are of concern due to legal limits and impact components of the combustor negatively by corrosion. One of the major emission components is potassium chloride. In a vapor atmosphere it may form a very aggressive environment that will corrode boilers. Therefore, the object of this study is to evaluate the kinetic data describing the formation of potassium chloride during switchgrass combustion. Experimental data that records the combustion process and the emission formation versus time, carried out by the National Renewable Energy Institute in Colorado (US), was used to evaluate the kinetic data.The approach is based on the Discrete Particle Method (DPM) that solves the coupled differential conservation equations for mass and energy of a switchgrass particle. Processes considered were heating-up, pyrolysis, combustion of switchgrass and evaporation of potassium chloride. Thus, the conversion process of switchgrass is described sufficiently accurate in particular the evolution of temperature. The evaporation of potassium chloride is approximated by an Arrhenius-like expression including a pre-exponential factor and an activation energy. These parameters were determined by a least-square method so that the deviation between the measured data and the predictions was minimized. The kinetic data determined yielded good agreement between experimental data and predictions.",
     "keywords": ["Biomass combustion", "Alkali emission", "Kinetic parameters", "Optimization"]},
    {"article name": "An improved genetic algorithm based on a novel selection strategy for nonlinear programming problems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.06.014",
     "publication date": "04-2011",
     "abstract": "Genetic algorithm is a heuristic population-based search method that incorporates three primary operators: crossover, mutation and selection. Selection operator plays a crucial role in finding optimal solution for constrained optimization problems. In this paper, an improved genetic algorithm (IGA) based on a novel selection strategy is presented to handle nonlinear programming problems. Each individual in selection process is represented as a three-dimensional feature vector composed of objective function value, the degree of constraints violations and the number of constraints violations. We can distinguish excellent individuals through two indices according to Pareto partial order. Additionally, IGA incorporates a local search (LS) process into selection operation so as to find feasible solutions located in neighboring areas of some infeasible solutions. Experimental results over a set of benchmark problems demonstrate that proposed IGA has better robustness, effectiveness and stableness than other algorithm reported in literature.",
     "keywords": ["Genetic algorithms", "Nonlinear programming problems", "Constraint-handling", "Non-dominated solution", "Optimization"]},
    {"article name": "A discrete time formulation for batch processes with storage capacity and storage time limitations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.10.004",
     "publication date": "04-2011",
     "abstract": "This paper extends the conventional discrete time mixed integer linear programming (MILP) formulation for scheduling multiproduct/multipurpose batch processes by introducing storage capacity and storage time limitations. For this purpose, storage vessels are explicitly modeled on which material flows are defined, and storage capacity and storage time constraints are expressed. The approach is shown to be effective in modeling the scheduling problem in a variety of storage configurations such as single/multiple dedicated and multipurpose storage vessels. In a numerical study, cases where storage capacity and storage time limitations have significant impacts on scheduling production and storage operations are highlighted.",
     "keywords": ["Multiproduct/multipurpose batch processes", "Storage capacity", "Storage time"]},
    {"article name": "Dynamics of liquid\u2013liquid systems based on linear thermodynamics of irreversible processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.06.015",
     "publication date": "04-2011",
     "abstract": "The use of non-equilibrium models for integrated processes involving liquid\u2013liquid systems has increased in recent years. These processes often exhibit complex dynamic behavior. These dynamical systems still pose many open questions, e.g. with regard to the sources of multiple steady states (MSS). This article analyzes the effect of mass transfer on the MSS of these systems. A generalized non-equilibrium modeling approach based on linear thermodynamics of irreversible processes (LTIP) is presented, and the dynamics of the system is studied systematically. It is shown that the non-linearity present in even the simplest non-ideal activity model acts as a source for MSS. The parameters that affect the solubility, e.g. temperature, can play a critical role on the existence of MSS in the system. A geometrical visualization of the MSS is also illustrated.",
     "keywords": ["Phase splitting", "Multiple steady states", "Liquid\u2013liquid system", "Non-equilibrium model", "Bifurcation analysis"]},
    {"article name": "Aeration control of a wastewater treatment plant using hybrid NMPC",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.07.021",
     "publication date": "04-2011",
     "abstract": "In the operation of wastewater treatment plants a key variable is dissolved oxygen (DO) content in the bioreactors. As oxygen is consumed by the microorganisms, more oxygen has to be added to the water in order to comply with the required minimum dissolved oxygen concentration. This is done using a set of aerators working on/off that represents most of the plant energy consumption. In this paper a hybrid nonlinear predictive control algorithm is proposed, based on economic and control aims. Specifically, the controller minimizes the energy use while satisfying the time-varying oxygen demand of the plant and considering several operation constraints. A parameterization of the binary control signals in terms of occurrence time of events allows the optimization problem to be re-formulated as an nonlinear programming (NLP) problem at every sampling time. Realistic simulation results considering real perturbations data sets for the inlet variables are presented.",
     "keywords": ["Hybrid nonlinear Model Predictive Control", "Wastewater treatment plants", "Real-time optimization"]},
    {"article name": "Leak detection in gas pipeline networks using an efficient state estimator. Part-I: Theory and simulations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.10.006",
     "publication date": "04-2011",
     "abstract": "Dynamic simulation models can be used along with flow and pressure measurements, for on-line leak detection and identification in gas pipeline networks. In this two part paper, a methodology is proposed for detecting and localizing leaks occurring in gas pipelines. The main features of the proposed methodology are: (i) it is applicable to both single pipelines and pipeline networks and (ii) it considers non-ideal gas mixtures. In order to achieve the desired computational efficiency for on-line deployment, an efficient state estimation technique based on a transfer function model, previously developed by the authors, is embedded in a hypothesis testing framework. In Part-I of this paper, a detailed description of the methodology is presented, and its performance is evaluated using simulations on two illustrative pipeline systems. The proposed method is shown to perform satisfactorily even with noisy measurements and during transient conditions, provided there is sufficient redundancy in the measurements.",
     "keywords": ["Gas pipeline networks", "Leak detection", "Dynamic simulation", "State estimation", "Transfer function", "Generalized likelihood ratio method"]},
    {"article name": "Leak detection in gas pipeline networks using an efficient state estimator. Part II. Experimental and field evaluation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.10.011",
     "publication date": "04-2011",
     "abstract": "In Part-I of this two part paper, a method is proposed for on-line leak detection and identification in gas pipeline networks using flow and pressure measurements. Simulations on two illustrative networks were used to demonstrate the applicability of the proposed method. In this paper, the performance of the proposed leak detection and identification methodology was evaluated using experiments with compressed air on a laboratory scale network. The on-line applicability of the proposed methodology was demonstrated through field level leak detection tests carried out on a 204.7\u00a0km long pipeline in India, supplying natural gas to a power plant. The laboratory and field tests demonstrated that the proposed methodology can be used for quick on-line detection of leaks, and locating the leaks reasonably accurately.",
     "keywords": ["Gas pipeline networks", "Leak detection", "Dynamic simulation", "State estimation", "Field tests"]},
    {"article name": "Data flow modeling, data mining and QSAR in high-throughput discovery of functional nanomaterials",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.04.018",
     "publication date": "04-2011",
     "abstract": "Metal oxide nanoparticles are promising materials in applications for fuel cells, gas sensors and fine chemical catalysis. Their functionality depends excessively on composition, structure as well as synthesis and processing conditions. Continuous hydrothermal flow synthesis (CHFS) reactors are an effective technology to make nanoceramics. In order to increase sample throughput of CHFS, a manual high-throughput continuous hydrothermal (HiTCH) flow synthesis process capable of formulating scores of samples per day was developed. More recently, a fully automated nanoceramics synthesis platform called RAMSI (rapid automated synthesis instrument) based on the HiTCH synthesis technology was developed. When large numbers of nanoceramics are made and formulated into appropriate libraries, automated analytical instruments can be used to allow collection of a large amount of useful data. This paper describes the information flow management system of RAMSI (as well as CHFS) and the data mining system for supporting discovery, QSAR (quantitative structure\u2013activity relationship) modeling and DoE (design of experiments). Case studies demonstrating the use of the high-throughput data mining system are presented. These include clustering of Raman spectra, interpretation of X-ray diffraction (XRD) measurements, and QSAR model building linking XRD data and photocatalytic properties. A genetic algorithm method for DoE is also presented that can guide the experiments to search optimal XRD patterns.",
     "keywords": ["Data mining", "QSAR", "Design of experiments", "Genetic algorithm", "Nanoparticle", "High-throughput"]},
    {"article name": "A framework for work process modeling in the chemical industries",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.10.012",
     "publication date": "04-2011",
     "abstract": "During the life cycle of a chemical product and its associated production process, diverse types of work processes are performed, such as design and operational processes. In this contribution, a framework for the modeling of work processes in the chemical industries is presented which comprises an iterative modeling procedure, an extensible modeling language for work processes, and software tools for its practical application. The framework supports the iterative creation of work process models for diverse applications such as work process simulation or enactment and the subsequent export of the models in suitable application tools. Ease of use has been a key design criterion for the framework. Internally, the framework builds on semantic technologies to provide substantial support for model checking and model transformations.",
     "keywords": ["Work process modeling", "Design process", "Operational process", "Modeling language", "Modeling tool", "Ontologies"]},
    {"article name": "Information integration in chemical process engineering based on semantic technologies",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.12.003",
     "publication date": "04-2011",
     "abstract": "During the design phase of a chemical plant, information is created by various software tools and stored in heterogeneous formats, such as technical documents, CAE databases, or simulation files. Eventually, these scattered information items need to be merged and consolidated. However, there is no efficient computer support for this task available today. While existing technologies like XML are capable of handling the structural and syntactic differences between the heterogeneous formats, these technologies cannot resolve any semantic incompatibilities. For this reason, information integration is still largely performed manually \u2013 a task which is both tedious and error-prone. Semantic technologies based on ontologies have been identified as an appropriate means to establish semantic interoperability. This contribution presents an ontology-based approach for information integration in chemical process engineering. The underlying knowledge base, which is based on the formal ontology OntoCAPE, is presented, and the design and implementation of a prototypical integration software are described. Further, the application of the software prototype in a large industrial use case is reported.",
     "keywords": ["Semantic technologies", "Ontology-based software", "OntoCAPE", "Information integration", "Chemical engineering design process"]},
    {"article name": "Support for modeling and monitoring of engineering design processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.12.004",
     "publication date": "04-2011",
     "abstract": "Engineering design processes are highly complex and dynamic inasmuch unforeseen changes occur frequently at process runtime. In this paper we present the process management system PROCEED that aids process managers and process participants in planning and enacting the work processes in plant design projects. It is based on the commercial lifecycle engineering tool Comos. PROCEED exceeds the state of the art in process management in several ways. The models used to represent running design processes incorporate aspects of project plans and workflow instances to reflect the current planning and execution states of work processes. Workflow definitions are used to define best practices for repetitive process parts and are enacted to guide process participants. PROCEED ensures consistent changes of process models even at process runtime. A process manager can resort to numerous progress measures in order to get exact quantitative information about the performance of a process.",
     "keywords": ["Engineering design process management", "Progress measurement", "Project status analysis", "Correctness checks"]},
    {"article name": "Tools for consistency management between design products",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.011",
     "publication date": "04-2011",
     "abstract": "Consistency between different design documents is most important, as these documents are usually elaborated by different persons. Consistency means that structures within different documents \u201cmatch\u201d each other and also the values of corresponding attributes. Interactive and incremental tools for building up and maintaining consistency \u2013 so-called integrators \u2013 install fine-grained relations (semantic links) between objects of these documents. Such integrators have been investigated for some time in different application domains (Nagl, 1996).This paper deals with two extensions of integrators (K\u00f6rtgen, 2009a, K\u00f6rtgen, 2009b). Both are devoted to speed up the (re-)integration process. The first extension makes use of the still valuable information of links damaged due to changes of documents. The second reuses consistent documents of past projects, to find out big and suitable related parts of design documents or of corresponding underlying knowledge. This paper is written for a chemical engineer and, therefore, emphasizes the tool functionality side.",
     "keywords": ["Consistency between design results", "Semantic links between objects of different documents", "Incremental and interactive integrators", "Links generated by tools"]},
    {"article name": "Application integration within an integrated design environment",
     "doi": "https://doi.org/10.1016/j.compchemeng.2011.01.012",
     "publication date": "04-2011",
     "abstract": "IMPROVE can also be seen as a big software tool integration project of given, extended, or new tools into one integrated environment for collaborative design. Thereby, six integration problems occur on the software level, which are sketched in this paper. Two of them are discussed in more detail: a special architecture editor was realized helping to transform abstract architecture situations to complex and concrete ones, reflecting the final software solution. The second one and the focus of this paper is wrapper construction, in order to get access to single tools as parts of the environment.A method is presented for wrapper construction (for interfaces and services) on different layers. Tools support this construction, which can be applied by non-specialists. As many tools are integrated, mechanical wrapper construction is of big importance for the quality of the environment, as well as for the efficiency of the process to get it.",
     "keywords": ["Tool and application integration", "Integration problems", "Methodological and efficient wrapper construction", "Construction support by tools", "Architecture modeling"]},
    {"article name": "Thermophysical and thermochemical properties on-demand for chemical process and product design",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.12.013",
     "publication date": "03-2011",
     "abstract": "The article provides a perspective of chemical process and product design on-demand, plus its implementation and impact in addressing modern challenges faced by the chemical industry. The concepts of Global Information Systems in Science and Engineering in application to the field of thermodynamics as well as Dynamic Data Evaluation for thermophysical and thermochemical properties are discussed as underlying principles for implementation of chemical process and product design on-demand.",
     "keywords": ["Dynamic data evaluation", "Global Information Systems", "On demand", "Process design", "Product design", "Thermochemical properties", "Thermophysical properties"]},
    {"article name": "A new homotopy for seeking all real roots of a nonlinear equation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.04.007",
     "publication date": "03-2011",
     "abstract": "A new continuation method, which applies a new homotopy that is a combination of the fixed-point and Newton homotopies (FPN), is developed for seeking all real solutions to a nonlinear equation, written as f(x)\u00a0=\u00a00, without having to specify a bounded interval. First, the equation to be solved is multiplied by (x\u00a0\u2212\u00a0x0), where x0 is the starting value, which is set to zero unless the function does not exist at x0, in which case x0 becomes a tracking initiation point that can be set arbitrarily to any value where the function does exist. Next, the new function, (x\u00a0\u2212\u00a0x0)f(x)\u00a0=\u00a00, is incorporated into the FPN homotopy. The initial step establishes a single bifurcation point from which all real roots can be found. The second step ensures a relatively simple continuation path that consists of just two branches that stem from the bifurcation point and prevents the formation of any isola. By tracking the two branches of the homotopy path, all real roots are located. Path tracking is carried out with MATLAB, using the continuation toolbox of CL_MATCONT, developed by Dhooge et al. (2006), based on the work of Dhooge, Govaerts, and Kuznetsov (2003), which applies Moore\u2013Penrose predictor-corrector continuation to track the path, using convergence-dependent step-size control to negotiate turning points and other sharp changes in path curvature. This new method has been applied, without failure, to numerous nonlinear equations, including those with transcendental functions. As with other continuation methods, f(x)must have twice-continuous derivatives.",
     "keywords": ["Nonlinear equation", "Homotopy", "Continuation", "All real roots", "Bifurcation", "Path tracking"]},
    {"article name": "Numerical solution of a multi-dimensional batch crystallization model with fines dissolution",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.03.016",
     "publication date": "03-2011",
     "abstract": "In this article a mathematical model for two-dimensional batch crystallization process with fines dissolution is presented. The fines dissolution is useful for improving the quality of a product and facilitates the downstream process like filtration. The crystals growth rates can be size-dependent and a time-delay in the recycle pipe is incorporated in the model. The high resolution finite volume schemes, originally derived for general systems in divergence form, are used to solve the resulting model. The schemes have already been used for the simulation of complex problems in gas dynamics and were found to be computationally efficient, accurate, and robust. The numerical test problems of this manuscript verify the capability of the proposed schemes for solving batch crystallization models.",
     "keywords": ["Population balances", "Batch crystallization processes", "Fines dissolution", "Multi-dimensional problems", "High resolution schemes"]},
    {"article name": "The moment method for one-dimensional dynamic reactor models with axial dispersion",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.03.017",
     "publication date": "03-2011",
     "abstract": "A polynomial approximation method for calculating state profiles for plug-flow reactors is extended to one-dimensional reactor models that include axial dispersion. The method is based on the conservation of reactor state profile moments along the spatial dimension. The moments are then transformed analytically into a polynomial approximation at each timestep. The boundary conditions of the parabolic partial differential equation are given special attention. It is shown that the Danckwerts boundary conditions are an appropriate set of boundary conditions for flow problems with axial dispersion in closed-closed geometries. A significant feature of the present method is that boundary conditions of the partial differential equation model to be solved are implicitly satisfied via the moment transformation, while the polynomial profile in the numerical approximation does not have to satisfy the boundary conditions exactly. The method is tested in two cases: startup of a tubular reactor and fixed-bed adsorber involving axial dispersion.",
     "keywords": ["Dynamic model", "Axial dispersion", "Moment transformation", "Polynomial approximation", "Numerical methods"]},
    {"article name": "Towards multiscale modelling in product engineering",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.05.009",
     "publication date": "03-2011",
     "abstract": "A concept of multiscale modelling of product manufacturing based on integration of three modelling methods currently applied at different scales of length and time: process system modelling, computational fluid dynamics and computational chemistry was presented. Major features of the three key types of modelling in the chemical and process industries were briefly described. The first applications and mutual benefits of joint use of two of the three approaches were presented along with the perspectives for the full integration of all three methods. The crucial role of a universal interface, such as the CAPE-OPEN standard, was emphasized.",
     "keywords": ["Multiscale modelling", "CFD", "Process system modelling", "Computational chemistry", "CAPE-OPEN"]},
    {"article name": "Novel bound contraction procedure for global optimization of bilinear MINLP problems with applications to water management problems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.04.010",
     "publication date": "03-2011",
     "abstract": "We propose a new method to obtain the global optimum of MINLP problems containing bilinearities. Our special method that contracts the bounds of one variable at a time allows reducing the gap between a linear lower bound and an upper bound obtained solving the original problem. Unlike some methods based on variable partitioning, our bound contraction procedure does not introduce new integers or intervals. We illustrate the method by applying it to water management problems.",
     "keywords": ["Mathematical programming", "Optimization", "Global optimization"]},
    {"article name": "A global optimization method based on multi-unit extremum-seeking for scalar nonlinear systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.04.003",
     "publication date": "03-2011",
     "abstract": "Finding the global optimum of a nonlinear function is a challenging task that could involve a large number of functional evaluations. In this paper, an algorithm that uses tools from the domain of extremum-seeking is shown to provide an efficient deterministic method for global optimization. Extremum-seeking schemes typically find the local optimum by controlling the gradient to zero. In this paper, the multi-unit framework is used, where the gradient is estimated by finite difference for a given offset between the inputs. The gradient is pushed to zero by an integral controller. It is shown that if the offset is reduced to zero, the system can be made to converge to the global optimum of nonlinear continuous static, scalar maps. The result is extended to constrained problems where a switching control strategy is employed. Several illustrative examples are presented and the proposed method is compared with other methods of global optimization.",
     "keywords": ["Global optimization", "Multi-unit optimization", "Extremum-seeking control", "Real-time optimization", "Convergence"]},
    {"article name": "Computational energy-based redesign of robust proteins",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.04.005",
     "publication date": "03-2011",
     "abstract": "The robustness of a system is a property that pervades all aspects of Nature. The ability of a system to adapt itself to perturbations due to internal and external agents, to aging, to wear, to environmental changes is one of the driving forces of evolution. At the molecular level, understanding the robustness of a protein has a great impact on the in silicon design of polypeptide chains and drugs; the chance of computationally checking the ability of a protein to preserve its structure and function in the native state can lead to the design of new compounds that can work in a living cell more effectively. Inspired by the well known robustness analysis framework used in Electronic Design Automation, we introduced a notion of robustness for proteins and two dimensionless quantities: the energetic yield and the energetic relative entropy. We used the energetic yield in order to quantify the robustness of a protein, and to detect sensitive regions and sensitive residues in the protein, whereas we adopted the energetic relative entropy to measure the discrepancy between two potential energy distributions. Subsequently, we implemented a new robustness-centred protein design algorithm called Robust-Protein-Design (RPD); the aim of the algorithm is to discover new conformations with a specific function with high yield values. We performed an extensive characterization of the robustness property of many peptides, proteins, and drugs. Moreover, we found that robustness and relative entropy are conflicting objectives which constitute a trade-off useful as design principle for new proteins and drugs.Finally, we used the RPD algorithm on the Crambin protein (1CRN); the obtained results confirm that the algorithm was able to find out a Crambin-like protein that is 23% more robust than the wild type.",
     "keywords": ["Protein robustness", "Robust-protein-design algorithm", "Energetic relative entropy"]},
    {"article name": "Integrated solvent and process design using a SAFT-VR thermodynamic description: High-pressure separation of carbon dioxide and methane",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.06.016",
     "publication date": "03-2011",
     "abstract": "The increasing importance of natural gas as an energy source poses separation challenges, due to the high pressures and high carbon dioxide concentrations of many natural gas streams. A methodology for computer-aided molecular and process design (CAMPD) applicable to such extreme conditions is presented, based on the integration of process and cost models with an advanced molecular-based equation of state, the statistical associating fluid theory for potentials of variable range (SAFT-VR). The approach is applied to carbon dioxide capture from methane using physical absorption. The search for an optimal solvent is focused on n-alkane blends. A simple flowsheet is optimised using two objectives: maximum purity and maximum net present value. The best equipment sizes, operating conditions, and average chain length of the solvent (the n-alkane) are identified, indicating n-alkane solvents offer a promising alternative. The proposed methodology can readily be extended to wider classes of solvents and to other challenging processes.",
     "keywords": ["Solvent and process design", "CAMD", "SAFT", "CO2 capture", "Natural gas"]},
    {"article name": "Efficient meta-modelling of complex process simulations with time\u2013space-dependent outputs",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.05.013",
     "publication date": "03-2011",
     "abstract": "Process simulations can become computationally too complex to be useful for model-based analysis and design purposes. Meta-modelling is an efficient technique to develop a surrogate model using \u201ccomputer data\u201d, which are collected from a small number of simulation runs. This paper considers meta-modelling with time\u2013space-dependent outputs in order to investigate the dynamic/distributed behaviour of the process. The conventional method of treating temporal/spatial coordinates as model inputs results in dramatic increase of modelling data and is computationally inefficient. This paper applies principal component analysis to reduce the dimension of time\u2013space-dependent output variables whilst retaining the essential information, prior to developing meta-models. Gaussian process regression (also termed kriging model) is adopted for meta-modelling, for its superior prediction accuracy when compared with more traditional neural networks. The proposed methodology is successfully validated on a computational fluid dynamic simulation of an aerosol dispersion process, which is potentially applicable to industrial and environmental safety assessment.",
     "keywords": ["Computer experiments", "Design of experiments", "Gaussian process", "Kriging model", "Meta-model", "Principal component analysis"]},
    {"article name": "A robust monitoring tool for distributed parameter plug flow reactors",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.06.001",
     "publication date": "03-2011",
     "abstract": "This paper deals with the design of an interval observer for plug flow reactors that are modeled by first-order hyperbolic partial differential equations. This model is considered to obtain the dynamics of an auxiliary variable that are further represented by a set of ordinary differential equations by means of the method of characteristics. Regarding these equations, the structure of the interval observer is deduced. This proposed estimation scheme is valid at a certain fixed specific axial point, is robust in the face of both uncertain parameters and load disturbances and is successfully implemented through simulations on an isothermal plug flow reactor within which a consecutive reaction is taking place and on a nonisothermal plug flow reactor with lateral feed where an irreversible reaction is carried out.",
     "keywords": ["ODE's ordinary differential equations", "ordinary differential equations", "PDE's partial differential equations", "partial differential equations", "OCM orthogonal collocation method", "orthogonal collocation method", "FDM finite difference method", "finite difference method", "MC method of characteristics", "method of characteristics", "EED estimation error dynamics", "estimation error dynamics", "Interval observers", "Distributed parameter systems", "Chemical reactors", "Partial differential equations", "Convective transport", "Method of characteristics"]},
    {"article name": "Reconciling continuum and non-continuum data with industrial application",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.06.004",
     "publication date": "03-2011",
     "abstract": "In order to perform data reconciliation, it is important that noises in the data have well-defined distributions. The motivation behind this study was to enable the comparison between a discrete and continuous data set so that means can be compared for gross error over the short term; this required that local variables exhibit similar distributions.A case study was done on a system where non-continuum loads from a dump truck were to be reconciled with two downstream continuum weightometers. An algorithm was developed using the binomial distribution and time delay in order to simulate the effect of the dump pocket.Regression analysis based on principal components was used to evaluate the performance of the smoothing algorithm and to determine the most likely maximum hopper capacity that locates between the two weightometers.",
     "keywords": ["Data smoothing", "Binomial-normal data comparison", "Principal component regression"]},
    {"article name": "Finite-sample comparison of robust estimators for nonlinear regression using Monte Carlo simulation: Part I. Univariate response models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.04.009",
     "publication date": "03-2011",
     "abstract": "Classical least squares can be strongly affected due to the inevitable occurrence of departures from its model assumptions, most notably those from the distributional assumptions. Robust estimators, on the other hand, will resist them. Unfortunately, the multiplicity of alternative robust regression estimators that have been suggested in the literature over the years is a source of confusion for practitioners of regression analysis. Moreover, little is known about their small-sample performance in the nonlinear regression setting, in particular on the chemical engineering field. A simulation study comparing six such estimators (namely LMS, LTS, LTD, MM-, \u03c4 -, and L p -norm) together with the usual least squares estimator is presented. The results obtained provide guidance as to the choice of an appropriate estimator.",
     "keywords": ["Robust regression", "Nonlinear model", "Monte Carlo", "Finite-sample robustness"]},
    {"article name": "An Artificial Neural Network approximation based decomposition approach for parameter estimation of system of ordinary differential equations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.06.005",
     "publication date": "03-2011",
     "abstract": "In this work a new approach for parameter estimation which is based upon decomposing the problem into two subproblems is proposed, the first subproblem generates an Artificial Neural Network (ANN) model from the given data and then the second subproblem uses the ANN model to obtain an estimate of the parameters. The analytical derivates from the ANN model obtained from the first subproblem are used for obtaining the differential terms in the formulation of the second subproblem. This greatly simplifies the parameter estimation problem. The key advantage of the proposed approach is that solution of a large optimization problem requiring high computational resources is avoided and instead two smaller problems are solved. This approach is particularly useful for large and noisy data sets and nonlinear models where ANN models are known to perform quite well and therefore plays an important role in the solution of the overall parameter estimation problem.",
     "keywords": ["Parameter estimation", "Global optimization", "Artificial Neural Network"]},
    {"article name": "Long-term scheduling of a single-unit multi-product continuous process to manufacture high performance glass",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.06.011",
     "publication date": "03-2011",
     "abstract": "In this paper we address the long-term scheduling of a real world multi-product single stage continuous process for manufacturing glass. This process features long minimum run lengths, and sequence dependent changeovers of the order of days, with high transition costs. The long-term scheduling involves extended time horizons that lead to large scale mixed-integer linear programming (MILP) scheduling models. In order to address the difficulties posed by the size of the models, three different rolling horizon algorithms based on different models and time aggregation techniques are developed. The models are based on the continuous time slot MILP model, and on the traveling salesman model proposed by Erdirik-Dogan and Grossmann (2008). Due to the particular characteristics of the process under study, several new features, including minimum run lengths and changeovers across due dates, are proposed. The performance and characteristics of the proposed rolling horizon algorithms are discussed for one industrial example.",
     "keywords": ["Planning", "Scheduling", "Multi-product continuous plants", "MILP", "Glass production"]},
    {"article name": "Modeling and simulation of heat and mass transfer during drying of solids with hemispherical shell geometry",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.05.007",
     "publication date": "02-2011",
     "abstract": "A heat and mass transfer model was proposed to describe the moisture and temperature evolution during drying of solid products with hemispherical shell geometry (HSG). The dimensionless form of the model was numerically solved for both several drying conditions and values of a geometrical factor related with the inner radius of the HSG to obtain their moisture and temperature profiles. In addition, average drying kinetics were calculated from the volume integration of local moisture values. A theoretical and numerical approach was used to develop a mass transfer analogy between the proposed HSG and a simpler flat slab-shaped product. These analogies provide simple mathematical expressions for drying process simulation and estimation of diffusion coefficients in solids with the proposed geometry, and may be applicable to other mass and heat transfer operations. Furthermore, the presented procedure may be used to develop similar expressions in other non-traditional or dissection geometries.",
     "keywords": ["Drying", "Heat transfer", "Mass transfer", "Mathematical modeling", "Hemispherical shell", "Simulation"]},
    {"article name": "Strategy for predicting effective transport properties of complex porous structures",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.06.002",
     "publication date": "02-2011",
     "abstract": "Measurements of effective transport properties of porous media, such as effective diffusivity and permeability, are well established by several experimental techniques. Effective transport properties can be also calculated from the spatially 3D reconstructed porous media, where the morphology characteristics required for the reconstruction are obtained from electron microscopy images. Here we demonstrate the reconstruction of porous alumina catalyst carrier with bimodal pore size distribution. Multi-scale concept is employed for the computation of effective diffusivity and permeability of reconstructed porous media and calculated effective transport properties are compared with transport parameters experimentally determined in Graham diffusion and simple permeation cell. The limitations of current state-of-the-art reconstruction techniques for porous media with broad pore size distribution are discussed. We show that the contribution of nano-pores towards the total diffusion flux is significant and cannot be neglected, but it is reasonable to neglect the contribution of nano-pores towards the sample permeability.",
     "keywords": ["Porous media", "Pore space reconstruction", "Effective diffusivity", "Darcy's permeability", "Multi-scale modeling"]},
    {"article name": "Numerical and technological properties of bubble column bioreactors for aerobic processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.03.015",
     "publication date": "02-2011",
     "abstract": "This paper describes the method of modelling and numerical simulation of bubble column bioreactors in which the process of aerobic biodegradation of carbonaceous substrate occurs. Such bioreactors belong to systems with distributed state variables. Determining steady states of such objects results in solving non-linear boundary-value problems.The bioreactors with axial dispersion and piston flow with biomass recirculation were analysed. The effectiveness of selected algorithms used to determine the steady states of such bioreactors were compared; the numerical properties of mathematical models of analysed bioreactors were specified, the branches of steady states in bioreactors with axial dispersion and piston flow were determined and the application areas of such bioreactors were defined.",
     "keywords": ["Bubble column bioreactors", "Modeling", "Numerical simulation"]},
    {"article name": "On scalable analytical models for heap leaching",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.09.009",
     "publication date": "02-2011",
     "abstract": "In this paper we present analytical models suitable for scaling up the heap leaching process of solid reactants from porous pellets. The models are based on first order ordinary differential equations together with some constitutive relations derived from models based on ordinary and partial differential equations and other relations based on insight. The models are suitable for applications in which the scale-up is neccesary. This approach allows to obtain accurate solutions for actual industry heap leaching operations. Novelty of this approach is the simple form of the models and its accuracy as compared with more complex models. Due to the models simplicity, they can be used for analysis, design, control and optimization of heap leaching processes without mathematical complexities. The models include the effect of heap height, particle sizes, flow rates, and several operation-design variables. Finally, some numerical experiments which confirm our theory are presented.",
     "keywords": ["Heap leaching", "Analytical models", "Differential equations-based models", "Scale-up"]},
    {"article name": "Application of ANN and EA for description of metal ions sorption on chitosan foamed structure\u2014Equilibrium and dynamics of packed column",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.05.012",
     "publication date": "02-2011",
     "abstract": "In this study, a multi-component sorption equilibria calculation, with application of artificial neural network (ANN) and identification of adsorption dynamics model using evolutionary algorithm (EA), is presented.Equilibrium experiments were carried out to estimate sorptivity of a new form of a chitosan foamed structure and its selectivity towards Cu(II), Zn(II) and Cr(VI) ions. In the case of single ions, it was found that in the whole range of concentrations, experimental data were well described by the Langmuir\u2013Freundlich equation. In the case of a multi-component mixture the application of a neural MLP network was proposed. Calculations with the use of MLP enabled description of sorption isotherms for when one, two and three ions were present at the same time in the solution. The network also enabled an analysis of sorption of the selected ion, taking into account the effect of its concentration on the sorption of other ions. This assessment would not be possible in an experimental way only.A universal mathematical model of adsorption in a packed column is proposed in this paper. The model includes mass balances for fluid and adsorbent as well as a sorption kinetics. The effect of these, is a system of two partial differential equations. Additionally, the distance and time are composed in one relevantly defined variable. The proposed transformations convert the system of partial differential equations to a system of ordinary equations, which enables analytical solution of the equations system. Also, calculation of a concentration distribution within the solution and adsorbent, dependent on the distance from inlet, and process duration, is achieved. The data obtained in the measurements for Cu(II), Ni(II) and Zn(II) ions, were then compared with those obtained from the model using EA for identification of model coefficients.",
     "keywords": ["Heavy metal ions", "Adsorption", "Artificial neural network", "Evolutionary algorithm", "Modeling"]},
    {"article name": "Optimization of Petlyuk sequences using a multi objective genetic algorithm with constraints",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.10.007",
     "publication date": "02-2011",
     "abstract": "In this work we use genetic algorithms to optimize Petlyuk sequences using a rigorous design model. A multi objective genetic algorithm (GA) with constraints was formulated and interconnected with the Aspen Plus process simulator to obtain each data point during the search process. In addition to providing more energy-efficient designs than some reported structures, two relevant trends were observed from the results of the case studies; one had to do with the feed location to the prefractionator as a function of the mixture properties, and the other one with optimal structures requiring four interconnecting stages instead of the two normally used for Petlyuk sequences. An application for the separation of azeotropic mixtures is also included. The optimal placement of vapor\u2013liquid interconnections is again shown to be different for each interconnecting stream. The GA showed a robust performance, and was practically independent on the initial values for the search variables.",
     "keywords": ["Distillation", "Thermally coupled distillation", "Petlyuk column", "Optimization", "Genetic algorithms"]},
    {"article name": "Iterative optimization of the economic efficiency of an industrial process within the validity area of the static plant model and its application to a Pulp Mill",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.10.010",
     "publication date": "02-2011",
     "abstract": "Optimization of the steady state economic efficiency of an industrial process is a specific task because the decision variables of the optimization (setpoints of the control system) affect the process through the control strategy. Thus, the effects of saturation of a control system must be taken into account when the gradient of the objective function is estimated and the necessary optimality conditions are checked. In particular, because the optimality conditions cannot be checked directly in the presence of active constraints on the manipulated variables, approximations of the steady state values of the manipulated variables as functions of the setpoints (static plant model) are needed in order to be able to evaluate the optimality conditions. In this paper an iterative method for optimization of the plant profit rate is proposed avoiding the control saturation and is applied to the Pulp Mill benchmark model optimization. Three different static models describing the steady state values of the manipulated variables are constructed and used in the optimization. The results of the optimization are presented and compared against the straightforward single-step optimization of the plant economic efficiency.",
     "keywords": ["Plantwide optimization", "Economic efficiency", "Pulp and paper production", "Industrial application"]},
    {"article name": "Optimal storage design for a multi-product plant: A non-convex MINLP formulation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.04.002",
     "publication date": "02-2011",
     "abstract": "We discuss a tank design problem for a multi product plant, in which the optimal cycle time and the optimal campaign size are unknown. A mixed-integer nonlinear programming (MINLP) formulation is presented, where non-convexities are due to the tank investment cost, storage cost, campaign setup cost and variable production rates. The objective of the optimization model is to minimize the sum of the production cost per ton per product produced. A continuous-time mathematical programming formulation is proposed and several extensions are discussed. The model is implemented in GAMS and computational results are reported for the two global MINLP solver BARON and LINDOGlobal as well as several nonlinear solvers available in GAMS.",
     "keywords": ["Mixed-integer nonlinear programming", "Global optimization", "Storage design", "Cycle time", "Campaign length", "Lot sizing problem", "Continuous-time model"]},
    {"article name": "A bio-inspired algorithm based on membrane computing and its application to gasoline blending scheduling",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.01.008",
     "publication date": "02-2011",
     "abstract": "For the purpose of applying membrane computing as a global optimization technique, a bio-inspired algorithm based on membrane computing (BIAMC) is proposed to solve both constrained and unconstrained problems. The membrane structure used in BIAMC is a network of membranes that is inspired by the Golgi apparatus. In the process of approaching to the optimum solution, the objects containing a tentative solution are evolved by the rewriting rule in the parallel identical membranes and synthesized by the novel rules of target indication, transition and abstraction in the membrane of quasi-Golgi. The information transfers according to directions defined by the communication rule. Eight well-known unconstrained and constrained functions are used for performance testing. Then we apply the proposed algorithm with two schemes to solving a typical nonlinear optimization of gasoline blending and scheduling problem. The results show that the proposed approach can find optimal or close-to-optimal solutions efficiently.",
     "keywords": ["Membrane computing", "Membrane structure", "Constraints handling", "Gasoline blending scheduling"]},
    {"article name": "Process structure change detection by eigenvalue-based method",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.05.011",
     "publication date": "02-2011",
     "abstract": "This paper proposes a novel eigenvalue-based method for process structure change detection. Based on this method, the number of components can be automatically determined, and the noise variance can also be estimated under limited data samples. Different from traditional methods, eigenvalues of the sampled data covariance matrix are used for structure change detection. Due to the difficulty in modeling the distribution of the calculated eigenvalues, the well-known one-class classification approach: support vector data description (SVDD) method is employed. To test the performance of the proposed method, two case studies are provided.",
     "keywords": ["Structure change detection", "Eigenvalue-based", "One-class classification", "Support vector data description"]},
    {"article name": "A hyperspectral imaging sensor for on-line quality control of extruded polymer composite products",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.07.020",
     "publication date": "02-2011",
     "abstract": "This study examines the ability of chemometrics methods, namely multivariate image analysis (MIA) and Grey Level Co-occurrence Matrix analysis (GLCM), to extract meaningful information from visible and near-infrared spectral images of extruded wood/plastic composite materials for predicting spatio-temporal variations in their properties. The samples were produced under varying process and feed conditions according to designed experiments. Mechanical properties of the samples were measured using standard analytical methods both during steady-state and dynamic transition periods. A Bootstrap-PLS regression technique was first used for selecting the spectral bands (i.e. wavelengths) that were the most highly correlated with the material properties. In a second step, a more parsimonious PLS regression model was built between the spectral and textural features extracted from the lower dimensional spectral images and the corresponding quality properties of each sample. The imaging sensor was able to simultaneously monitor 7 properties in both steady-state operation and during transitions.",
     "keywords": ["Spectral imaging", "PLS", "Polymer composite", "Extrusion", "Wavelength selection"]},
    {"article name": "A methodology for the simultaneous design and control of large-scale systems under process parameter uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.05.010",
     "publication date": "02-2011",
     "abstract": "This work presents a simultaneous design and control methodology for large-scale systems. The approach is based on the identification of an uncertain model from a first-principle process model. Using the identified uncertain model, a Structured Singular Value (SSV) analysis is used to estimate the realizations in the disturbance set that generates the worst-case variability and constraint violations. Then, simulations of the first-principle process model are performed with the critical disturbance profile as input to estimate the actual worst-case output variability and the worst-case variations in the process constraints. Since the proposed methodology is formulated as a nonlinear constrained optimization problem, it avoids the computationally expensive task of solving dynamic optimization problems, making it suitable for application to large-scale systems. The proposed methodology was tested on the Tennessee Eastman process to show that a redesign of the major process units in the process could significantly reduce the costs of this plant.",
     "keywords": ["Design and control", "Parameter uncertainty", "Structured Singular Value"]},
    {"article name": "Finding a trade-off between observability and economics in the fault detection of chemical processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.04.006",
     "publication date": "02-2011",
     "abstract": "This paper presents a methodology to quantitatively gauge the potential economical loss due to unobserved faults when standard statistical monitoring charts are used. It is shown that in closed loop operation, a shorter time for detection may result from retuning the controller at the expense of higher product variability. Accordingly, an optimization approach is proposed for finding a trade-off between the economic losses resulting from lack of detection and losses resulting from higher product variability. In order to account for faults with different frequency contents, the method is applied in the frequency domain. The proposed optimization based methodology is later validated in the time domain.",
     "keywords": ["Observability", "Fault detection", "Average run length", "PCA", "Economic significance"]},
    {"article name": "Fault diagnosis with automata generated languages",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.10.003",
     "publication date": "02-2011",
     "abstract": "A SDG-based simulation procedure is proposed in this study to qualitatively predict the effects of one or more fault propagating in a given process system. These predicted state evolution behaviors are characterized with an automaton model. By selecting a set of on-line sensors, the corresponding diagnoser can be constructed and the diagnosability of every fault origin can be determined accordingly by inspection. Furthermore, it is also possible to define a formal diagnostic language on the basis of this diagnoser. Every string (word) in the language is then encoded into an IF-THEN rule and, consequently, a comprehensive fuzzy inference system can be synthesized for on-line diagnosis. The language generation steps are illustrated with a series of simple examples in this paper. The feasibility and effectiveness of this approach has been tested in extensive numerical simulation studies.",
     "keywords": ["Safety", "Chemical processes", "Systems engineering", "Simulation", "Fault diagnosis", "Automata"]},
    {"article name": "Evaluation of decision fusion strategies for effective collaboration among heterogeneous fault diagnostic methods",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.05.004",
     "publication date": "02-2011",
     "abstract": "Numerous methodologies for fault detection and identification (FDI) in chemical processes have been proposed in literature. However, it is extremely difficult to design a perfect FDI method to efficiently monitor an industrial-scale process. In this work, we seek to overcome this difficulty by using multiple heterogeneous FDI methods and fusing their results so that the strengths of the individual FDI methods are combined and their shortcomings overcome. Several decision fusion strategies can be used for this purpose. In this paper, we study the relative benefits of utility-based and evidence-based decision fusion strategies. Our results from a lab-scale distillation column and the popular Tennessee Eastman challenge problem show that in situations where no single FDI method offers adequate performance, evidence-based fusion strategies such as weighted voting, Bayesian, and Dempster\u2013Shafer based fusion can provide (i) complete fault coverage, (ii) more than 40% increase in overall fault recognition rate, (iii) significant improvement in monitoring performance, and (iv) reduction in fault detection and diagnosis delays.",
     "keywords": ["Process monitoring", "Supervision", "Bayesian probability", "Classifier"]},
    {"article name": "Petri-net models for comprehensive hazard analysis of MOCVD processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.05.006",
     "publication date": "02-2011",
     "abstract": "Reducing contamination level is of primary importance for the safety and efficiency of a MOCVD process. Off-line fault identification is one of the basic tasks that must be performed in hazard analysis to identify potential operational problems. For illustration convenience, the scope of present study is limited to the purge-gas purifier of the process. A systematic step-by-step procedure is proposed in this paper to construct Petri nets for modeling the purification system. Efficient hazard assessment studies have been performed by simulating the fault propagation behaviors on the basis of the system model. A comprehensive list of possible fault origins has been thoroughly examined to demonstrate the effectiveness of the proposed approach.",
     "keywords": ["MOCVD", "Adsorption", "Safety", "Petri-net model", "Simulation"]},
    {"article name": "Optimal scheduling of continuous plants with energy constraints",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.05.008",
     "publication date": "02-2011",
     "abstract": "This work addresses the scheduling of continuous single stage multiproduct plants with parallel units and shared storage tanks. Processing tasks are energy intensive and we consider time-dependent electricity pricing and availability together with multiple intermediate due dates, handled as hard constraints. A new discrete-time aggregate formulation is proposed to rapidly plan the production levels. It is combined with a continuous-time model for detailed scheduling as the essential part of a rolling-horizon algorithm. Their computational performance is compared to traditional discrete and continuous-time full-space formulations with all models relying on the Resource-Task Network (RTN) process representation. The results show that the new models and algorithm can generate global optimal schedules much more efficiently than their counterparts in problems involving unlimited power availability. Under restricted power, the aggregate model underestimates the electricity cost, which may cause the rolling-horizon approach to converge to a suboptimal solution, becoming the discrete-time model a better approach.",
     "keywords": ["Production planning", "Mixed-integer linear programming", "Non-uniform time grid", "Continuous-time model", "Resource-Task Network"]},
    {"article name": "Outlier detection in large data sets",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.11.004",
     "publication date": "02-2011",
     "abstract": "In this paper we propose a method for correctly detecting outliers based on a new technique developed to simultaneously evaluate mean, variance and outliers. This method is capable of self-regulating its robustness to suit the experimental data set under analysis, so as to overcome shortcomings of: (i) nonrobust methods such as the least sum of squares; (ii) the need of the user in defining a trimmed sub-set of experimental points such as in least trimmed sum of squares; and (iii) the possibility to read the data set only once to evaluate the mean, variance, and outliers of a population by preserving robustness.",
     "keywords": ["Outliers", "Reliable parameter estimation", "Robustness", "Large data sets"]},
    {"article name": "Review of adaptation mechanisms for data-driven soft sensors",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.07.034",
     "publication date": "01-2011",
     "abstract": "In this article, we review and discuss algorithms for adaptive data-driven soft sensing. In order to be able to provide a comprehensive overview of the adaptation techniques, adaptive soft sensing methods are reviewed from the perspective of machine learning theory for adaptive learning systems. In particular, the concept drift theory is exploited to classify the algorithms into three different types, which are: (i) moving windows techniques; (ii) recursive adaptation techniques; and (iii) ensemble-based methods. The most significant algorithms are described in some detail and critically reviewed in this work. We also provide a comprehensive list of publications where adaptive soft sensors were proposed and applied to practical problems. Furthermore in order to enable the comparison of different methods to standard soft sensor applications, a list of publicly available data sets for the development of data-driven soft sensors is presented.",
     "keywords": ["Data-driven soft sensing", "Process industry", "Adaptation", "Incremental learning", "Online prediction", "Process monitoring", "Soft sensor case studies", "Review"]},
    {"article name": "Modeling and simulation of an oxy-fuel combustion boiler system with flue gas recirculation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.05.001",
     "publication date": "01-2011",
     "abstract": "In this paper, a mathematical model of an oxy-fuel combustion boiler system with flue gas recirculation is investigated. The reduction of CO2 emission from coal-fired power plants is an important research issue in alleviating the global warming. The entire dynamics are decomposed in two main parts; fire-side dynamics and water-side dynamics. The fire-side dynamics consist of the mass and energy balance equations in the furnace (combustion process) and the flue gas dynamics represented by the mass balance equations of five gases (O2, CO2, SO2, H2O and NO2). The water-side dynamics include a drum pressure equation and a steam temperature equation. To validate the developed models, the real experimental data in Karakas, Koumanakos, et al. (2007) are used. To investigate the local behavior near an operating point, a linearization method at its steady-state condition is pursued. The time responses of the entire dynamics using step inputs (the oxygen mass flow rate, the coal mass flow rate, the primary air mass flow rate, etc.) are also discussed.",
     "keywords": ["ESP electrostatic precipitator", "electrostatic precipitator", "RFG recycled flue gas", "recycled flue gas", "FG flue gas", "flue gas", "ASU air separation unit", "air separation unit", "RC raw coal", "raw coal", "Oxy-fuel combustion boiler", "Coal power plant", "Modeling", "Simulation", "Mass and energy balance", "Model validation"]},
    {"article name": "Characteristics of liquid and tracer dispersion in trickle-bed reactors: Effect on CFD modeling and experimental analyses",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.06.006",
     "publication date": "01-2011",
     "abstract": "The characteristics of mechanical dispersion of tracer and liquid are analyzed using CFD modeling and experimental results from the literature. The most significant differences are underlined and their impact is discussed further. When compared to uniform liquid distribution, the more complicated flow conditions in liquid source measurements are considered to have a significant effect on result analysis and should be paid more attention to. Modeling of mechanical dispersion of liquid using CFD is discussed. Finally, liquid source dispersion cases are simulated and the results are compared to the experimental liquid as well as tracer dispersion results.",
     "keywords": ["Liquid dispersion", "Packed beds", "CFD", "Porous media", "Multiphase flow"]},
    {"article name": "Dimension reduction of bivariate population balances using the quadrature method of moments",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.06.012",
     "publication date": "01-2011",
     "abstract": "Crystallization models with direction-dependent growth rates give rise to multi-dimensional population balance equations (PBE) that require a high computational cost. We propose a model reduction based on the quadrature method of moments (QMOM). Using this method a two-dimensional population balance is reduced to a system of one-dimensional advection equations. Despite the dimension reduction the method keeps important volume dependent information of the crystal size distribution (CSD). It returns the crystal volume distribution as well as other volume dependent moments of the two-dimensional CSD. The method is applied to a model problem with direction-dependent growth of barium sulphate crystals, and shows good performance and convergence in these examples. We also compare it on numerical examples to another model reduction using a normal distribution ansatz approach. We can show that our method still gives satisfactory results where the other approach is not suitable.",
     "keywords": ["Crystallization", "Direction-dependent growth", "Multi-dimensional population balances", "Quadrature method of moments", "Upwind-MUSCL scheme", "Second order finite volume scheme"]},
    {"article name": "Identification of semi-parametric hybrid process models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.05.002",
     "publication date": "01-2011",
     "abstract": "Hybrid models are mathematical models that comprise both mechanistic and black-box or data-driven components. Typically, the parameters in the mechanistic part of a hybrid model (if any) are assumed to be known. However in this research, a two-level approach is proposed for the identification of hybrid models where some parameters in the mechanistic part of the model are unknown. At the first level, the black-box component is identified using a regularization method with given values for the regularization and mechanistic parameters. At the second level, the regularization and mechanistic parameters are determined simultaneously and optimized according to a specific criterion placed on the predictive performance of the hybrid model. This approach is tested through the modelling of a toluene nitration process, where a support vector machine (SVM) model is used to represent the chemical kinetics, with the mass transfer-related mechanistic parameters being estimated simultaneously. The case study shows that good results can be obtained in terms of both the prediction of the process variables of interest and the estimates of the mechanistic parameters, when the measurement error in the training data is small whilst when the magnitude of the measurement error increases, the accuracy of the estimates of the mechanistic parameters decreases. However, the predictive performance of the resulting hybrid model in the latter case is still acceptable, and can be much better than that attained from the application of a pure black-box model under certain extrapolation conditions.",
     "keywords": ["Hybrid modelling", "Model identification", "Semi-parametric models"]},
    {"article name": "A real-time algorithm for moving horizon state and parameter estimation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.07.012",
     "publication date": "01-2011",
     "abstract": "A moving horizon estimation (MHE) approach to simultaneously estimate states and parameters is revisited. Two different noise models are considered, one with measurement noise and one with additional state noise. The contribution of this article is twofold. First, we transfer the real-time iteration approach, developed in Diehl et al. (2002) for nonlinear model predictive control, to the MHE approach to render it real-time feasible. The scheme reduces the computational burden to one iteration per measurement sample and separates each iteration into a preparation and an estimation phase. This drastically reduces the time between measurements and computed estimates. Secondly, we derive a numerically efficient arrival cost update scheme based on one single QR-factorization. The MHE algorithm is demonstrated on two chemical engineering problems, a thermally coupled distillation column and the Tennessee Eastman benchmark problem, and compared against an Extended Kalman Filter. The CPU times demonstrate the real-time applicability of the suggested approach.",
     "keywords": ["State estimation", "Parameter estimation", "Moving horizon estimation", "Differential-algebraic equations", "Real-time optimization", "Ternary distillation", "Tennessee\u2013Eastman process"]},
    {"article name": "On-line estimation in a distributed parameter bioreactor: Application to the gluconic acid production",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.05.005",
     "publication date": "01-2011",
     "abstract": "This work presents a methodology which exploits the underlying biochemical structure of bioprocesses to estimate concentrations in aerobic fermenters from oxygen measurements. Although a number of estimators have been proposed over the years in the literature, the methodology proposed in this work is able to operate in transient conditions while does not require the knowledge of the growth kinetics. In addition, it can be also applied to fermenters where the spatial distribution of the concentrations is relevant. In this case, we propose a systematic approach to optimally locate the sensors based on the use of reduced order models. This method allows the reconstruction of the oxygen concentrations from a limited number of sensors. Finally, the methodology proposed will be illustrated on a horizontal tubular reactor for the production of gluconic acid by free-growth of Aspergillus niger.",
     "keywords": ["On-line indirect estimation", "Aerobic fermentation", "Tubular (bio)reactors", "Optimal sensor location"]},
    {"article name": "A knowledge-based simulation-optimization framework and system for sustainable process operations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.08.004",
     "publication date": "01-2011",
     "abstract": "Design and operation of chemical plants involves a combination of synthesis, analysis and evaluation of alternatives. Such activities have traditionally been driven by economic factors first, followed by engineering, safety and environmental considerations. Recently, chemical companies have embraced the concept of sustainable development, entailing renewable feed materials and energy, non-toxic and biodegradable products, and waste minimization or even elimination at source. In this paper, we introduce a knowledge-based simulation-optimization framework for generating sustainable alternatives to chemical processes. The framework has been developed by combining different process systems engineering methodologies \u2013 the knowledge-based approach for identifying the root cause of waste generation, the hierarchical design method for generating alternative designs, sustainability metrics, and multi-objective optimization \u2013 into one coherent simulation-optimization framework. This is implemented as a decision-support system using Gensym's G2 and the HYSYS process simulator. We illustrate the framework and system using the HDA and biodiesel production case studies.",
     "keywords": ["Waste minimization", "Knowledge-based system", "Process graph", "Simulation-optimization", "Process synthesis", "Multi-objective decision-making"]},
    {"article name": "Optimal location of measurements for parameter estimation of distributed parameter systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.04.014",
     "publication date": "01-2011",
     "abstract": "Systematic methodologies for the optimal location of spatial measurements, for efficient estimation of parameters of distributed systems, are investigated. A review of relevant methods in the literature is presented, and a comparison between the results obtained with three distinctive existing techniques is given. In addition, a new approach based on the Proper Orthogonal Decomposition (POD), to address this important problem is introduced and discussed with the aid of illustrative benchmark case studies from the literature. Based on the results obtained here, it was observed that the method based on the Gram determinant evolution (Vande Wouwer et al., 2000), does not always produce accurate results. It is strongly dependent on the behaviour of sensitivity coefficients and requires extensive calculations. The method based on max\u2013min optimisation (Alonso, Kevrekidis, Banga, & Frouzakis, 2004) assigns optimal sensor locations to the positions where system outputs reach their extrema values; however, in some cases it produces more than one optimal solution. The D-optimal design method, Uci\u0144ski (2003, June 18\u201320), produces as results the optimal number and spatial positions of measurements based on the behaviour (rather than the magnitude) of the sensitivity functions. Here we show that the extrema values of POD modes can be used directly to compute optimal sensor locations (as opposed e.g. to Alonso, Kevrekidis, et al., 2004, where PODs are merely used to reduce the system and further calculations are needed to compute sensor locations). Furthermore, we demonstrate the equivalence between the extrema of POD modes and of sensitivity functions. The added value of directly using PODs for the computation of optimal sensor locations is the computational efficiency of the method, side-stepping the tedious computation of sensitivity coefficient Jacobian matrices and using only system responses and/or experimental results directly. Furthermore, the inherent combination of model reduction and sensor location estimation in this method becomes more important as the complexity of the original distributed parameter system increases.",
     "keywords": ["Sensitivity coefficients", "Gram determinant", "D-optimal design", "Measurement points", "Max\u2013min optimisation", "Proper Orthogonal Decomposition"]},
    {"article name": "A partial Granger causality based method for analysis of parameter interactions in bioreactors",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.07.013",
     "publication date": "01-2011",
     "abstract": "In this article, we utilize the concept of partial Granger causality to study the penicillin production process under several operating conditions. We propose a graph-theoretic template (causal network) based method for intelligent process monitoring. We validate our results with the aid of existing knowledge and available literature. The proposed method is quite general and can be extended to analyze several physical, chemical or biological systems.",
     "keywords": ["Bioreactors", "Graph theory", "Partial Granger causality", "Process monitoring"]},
    {"article name": "Design of a modified sequential probability ratio test (SPRT) for pipeline leak detection",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.06.009",
     "publication date": "01-2011",
     "abstract": "A classical SPRT likelihood test for sequential independently distributed data is often used in pipeline mass balance leak detection to distinguish between true leaks and false alarms in the minimum time consistent with a user defined error tolerance. However such time series data would not be expected to be independent, especially as it is often moving averaged to remove noise and unwanted transients. In this paper a modified SPRT test is derived using a simple Gaussian Markov process to model a correlated time series. Application of the modified test to correlated time series data is shown to reduce false alarms below that of a classical SPRT.",
     "keywords": ["Pipeline leak detection", "CUSUM", "SPRT"]},
    {"article name": "Effective fault detection and isolation using bond graph-based domain decomposition",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.07.033",
     "publication date": "01-2011",
     "abstract": "The problem of fault detection and isolation in complex plants can be effectively addressed by a hierarchical strategy involving successive narrowing of the search space of potential faults. A bond graph network is one means of achieving a hierarchical strategy based on the physical domains present in the plant. First, the multivariate statistical method of principal component analysis is used to reduce the data dimension. Second, a discrete wavelet transform is applied to abstract the dynamics at different scales. Thirdly, the Mahalanobis distance is applied to calculate the confidence level. Following a conclusion of the existence of a fault, isolation is achieved by comparing the time scale at which the violation occurred to the time scale associated with a physical domain. In the final step, a Bayesian network is employed to describe the conditional dependence between faulty domains and fault signatures. Two examples are presented to demonstrate these concepts.",
     "keywords": ["Wastewater treatment plant", "Principal component analysis", "Wavelet transform", "Bayesian network"]},
    {"article name": "Mathematical modeling for simultaneous design of plants and supply chain in the batch process industry",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.06.008",
     "publication date": "01-2011",
     "abstract": "Most supply chain design models have focused on the integration problem, where links among nodes must be settled in order to allow an efficient operation of the whole system. At this level, all the problem elements are modeled like black boxes, and the optimal solution determines the nodes allocation and their capacity, and links among nodes. In this work, a new approach is proposed where decisions about plant design are simultaneously made with operational and planning decisions on the supply chain. Thus, tradeoffs between the plant structure and the network design are assessed. The model considers unit duplications and the allocation of storage tanks for plant design. Using different sets of discrete sizes for batch units and tanks, a mixed integer linear programming model (MILP) is attained. The proposed formulation is compared with other non-integrated approaches in order to illustrate the advantages of the presented simultaneous approach.",
     "keywords": ["Supply chain design", "Batch process design", "MILP", "Simultaneous optimization"]},
    {"article name": "Scheduling multi-product tree-structure pipelines",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.03.018",
     "publication date": "01-2011",
     "abstract": "The problem discussed in this paper is short-term scheduling of distribution of petroleum derivatives from a single oil refinery to a number of depots through a tree-structure pipeline. Scheduling product batches in pipelines is a very complex task with many constraints to be considered. Batches of refined products and grades are pumped back-to-back in the pipeline, often with no separation device between batches. In this work a continuous-time, MILP problem representation for tree-structure pipelines is proposed. The approach is successfully applied to a number of pipeline scheduling problems, including a real-world problem. The data and experimental results are reported.",
     "keywords": ["Scheduling", "Tree-structure pipeline", "MILP formulation", "Oil industry"]},
    {"article name": "Derivation of optimal operating policies under safety and technological constraints for the acetoacetylation of pyrrole in a semi-batch catalytic reactor",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.05.003",
     "publication date": "01-2011",
     "abstract": "A large number of optimization strategies have been developed for semi-batch reactors (SBR) with a fixed or free terminal time, with or without considering various sources of uncertainty. Such strategies account for safety constraints rather empirically determined in the form of parameter thresholds, while safety indices are seldom integrated in the optimization objective function. The present work illustrates how the runaway boundaries, and their confidence region associated to the parameter uncertainty, can be evaluated using the process model and a generalized sensitivity criterion, and how they can be included in the SBR optimization. A concrete example is provided for the SBR used for the acetoacetylation of pyrrole with diketene in homogeneous catalysis, a process known to be of high risk due to the very exothermic side-reactions. While previous studies approached the isothermal SBR, the present work is focused on optimizing the non-isothermal SBR operation.",
     "keywords": ["Semi-batch reactor", "Optimization", "Safety boundaries", "Pyrrole", "Diketene"]},
    {"article name": "Towards an integrated design of biofuels and their production pathways",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.07.035",
     "publication date": "12-2010",
     "abstract": "Future mobility strongly depends on liquid energy carriers, which need to be produced sustainably from whole plants. This therefore deals with the identification of novel fuel components with promising properties for engine application. However, in contrast to frequently followed approaches, we aim at the preservation of the molecular structures present in the biorenewables during biofuel production. Thus, the design of novel fuels as well as new synthesis routes from platform chemicals to fuels need to be explored. To this end, (i) molecular structures of fuels exhibiting promising properties are predicted by means of Computer-Aided Molecular Design (CAMD) while (ii) alternative production pathways are identified, classified and assessed by Reaction Network Flux Analysis (RNFA). This contribution therefore not only introduces both of these methodologies, but also presents how these fields can be linked in integrated product and process design for the identification of promising biofuel molecules of the future.",
     "keywords": ["Biofuels", "Integrated product-process design", "CAMD", "Reaction Network Flux Analysis"]},
    {"article name": "Optimal design of reliable integrated chemical production sites",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.07.027",
     "publication date": "12-2010",
     "abstract": "Since plants that form the process network are subjected to fluctuations in product demand or random mechanical failures, design decisions such as adding redundant units and increasing storage between units can increase the flexibility and reliability of an integrated site. In this paper, we develop a bi-criterion optimization model that captures the trade-off between capital investment and process robustness in the design of an integrated site. Design decisions considered are increases in process capacity, introduction of parallel units, and addition of intermediate storage. The mixed-integer linear programming (MILP) formulation proposed in this paper includes the representation of the material levels in the intermediate storage by means of a probabilistic model that captures the effects of the discrete, uncertain events. We also integrate a superstructure optimization with stochastic modeling techniques such as continuous-time Markov chains. The application of the proposed model is illustrated with two example problems.",
     "keywords": ["Reliability", "Flexibility", "Mixed-integer programming", "Markov chains", "Stochastic programming"]},
    {"article name": "Robust integration of real time optimization with linear model predictive control",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.06.017",
     "publication date": "12-2010",
     "abstract": "Here, we study the stable integration of real time optimization (RTO) with model predictive control (MPC) in a three layer structure. The intermediate layer is a quadratic programming whose objective is to compute reachable targets to the MPC layer that lie at the minimum distance to the optimum set points that are produced by the RTO layer. The lower layer is an infinite horizon MPC with guaranteed stability with additional constraints that force the feasibility and convergence of the target calculation layer. It is also considered the case in which there is polytopic uncertainty in the steady state model considered in the target calculation. The dynamic part of the MPC model is also considered unknown but it is assumed to be represented by one of the models of a discrete set of models. The efficiency of the methods presented here is illustrated with the simulation of a low order system.",
     "keywords": ["Real time optimization", "Model predictive control", "Robust stability"]},
    {"article name": "Effect of seed loading and cooling rate on crystal size and shape distributions in protein crystallization\u2014A study using morphological population balance simulation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.06.020",
     "publication date": "12-2010",
     "abstract": "A morphological population balance model is applied to crystallization of hen-egg white lysozyme for investigation of the effect of seed loading and cooling rate on supersaturation, and crystal size and shape distributions. Growth rates of individual faces and final crystal size and shape distributions were examined under varied seeding and cooling conditions. It was found that for growth only crystallization, desired crystal size and shape can be obtained by coordinative manipulation of the seed loading and cooling rate: low seed loading and high cooling rate lead to large crystals of low aspect ratio, but care has to be taken to avoid nucleation and major shape change such as width becoming larger than the length. The interesting results not only demonstrate the effectiveness of morphological population balance simulation for protein crystallization but also provide useful knowledge for process optimization and control.",
     "keywords": ["Morphological population balance model", "Crystal shape distribution", "Protein crystallization", "Seed loading", "Cooling rate", "Hen-egg white lysozyme"]},
    {"article name": "Stochastic dynamic predictions using Gaussian process models for nanoparticle synthesis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.07.023",
     "publication date": "12-2010",
     "abstract": "Gaussian process modeling (also known as kriging) is an empirical modeling approach that has been widely applied in engineering for the approximation of deterministic functions, due to its flexibility and ability to interpolate observed data. Despite its statistical properties, Gaussian process models (GPM) have not been employed to describe the dynamics of stochastic systems with multiple outputs. Our paper presents a methodology to construct approximate models for multivariate stochastic dynamic simulations using GPM, by combining ideas from design of experiments, spatial statistics and dynamic systems modeling. The methodology is the first application in dynamic systems modeling that combines parameter and state uncertainty propagation in Gaussian process models. We apply the methodology in the prediction of a dynamic size distribution during the synthesis of nanoparticles. The method is robust to the simulation noise, and is able to learn the dynamics using a small number of sequentially designed samples of the nanoparticle simulation.",
     "keywords": ["Gaussian process model", "Dynamic systems modeling", "Spatial statistics", "Reduced-order model", "Nanoparticle synthesis"]},
    {"article name": "Modeling the kinetics of the coalescence of water droplets in crude oil emulsions subject to an electric field, with the cellular automata technique",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.07.006",
     "publication date": "12-2010",
     "abstract": "In this study, the concept of cellular automata is applied in an innovative way to simulate the separation of phases in a water/oil emulsion. The velocity of the water droplets is calculated by the balance of forces acting on a pair of droplets in a group, and cellular automata is used to simulate the whole group of droplets. Thus, it is possible to solve the problem stochastically and to show the sequence of collisions of droplets and coalescence phenomena. This methodology enables the calculation of the amount of water that can be separated from the emulsion under different operating conditions, thus enabling the process to be optimized. Comparisons between the results obtained from the developed model and the operational performance of an actual desalting unit are carried out. The accuracy observed shows that the developed model is a good representation of the actual process.",
     "keywords": ["Cellular automata", "Optimization of water use", "Emulsion", "Coalescence", "Crude oil desalting"]},
    {"article name": "Online identification of adsorption isotherms in SMB processes via efficient moving horizon state and parameter estimation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.07.005",
     "publication date": "12-2010",
     "abstract": "In this paper, a concept to identify the adsorption isotherm of Simulated Moving Bed processes online during the operation is proposed. The influence of model parameters on the measurements is analyzed by a sensitivity analysis which enables to identify the set of parameters that can be estimated simultaneously. The parameters are estimated in real-time by a moving horizon state and parameter estimation scheme. Numerical simulations of validated models for separation problems with nonlinear isotherms of Langmuir type are presented. Furthermore, it is it shown that a structural plant/model mismatch in the void fraction can be compensated to a large extent by modifying the Henry coefficients of the adsorption isotherm such that the estimated model can be used in the context of online optimizing control.",
     "keywords": ["Simulated Moving Bed chromatography", "Adsorption isotherm identification", "Moving horizon estimation", "Real-time application", "Real-time iteration"]},
    {"article name": "Viscoelastic fluid analysis in internal and in free surface flows using the software OpenFOAM",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.07.010",
     "publication date": "12-2010",
     "abstract": "Synthetic polymer products are of great importance in several industrial sectors, such as for production of packaging, parts of appliances, electronics, cars and food processing industries. Due to the increasing demand for this kind of material, reduction of waste and increase of quality has become a key issue in polymer industry. In this sense modeling and simulation of processing operations appears as a fundamental tool, leading to better understanding of how the rheological properties of polymers affect their processability and final product quality, and reducing time and costs related to the development of processes and products. This work presents some basic results that aims to validate a developed methodology for internal viscoelastic fluid flows, which was developed in a previous work in the OpenFOAM computational fluid dynamics package and also will be showed a extension of this methodology for analysis of free surface viscoelastic fluid flows, using the VOF methodology. A classical flow phenomena used in the rheology literature to present the concept of viscoelastic effects was simulated, i.e., the die swell experiment. The results obtained using Giesekus model showed the great potential of the developed formulation, once phenomena observed experimentally were reproduced in the simulations.",
     "keywords": ["Viscoelastic flow", "VOF", "Free surface flow", "OpenFOAM", "Die swell"]},
    {"article name": "Development of an optimal operation strategy in a sequential batch reactor (SBR) through mixed-integer particle swarm dynamic optimization (PSO)",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.07.030",
     "publication date": "12-2010",
     "abstract": "Dynamic optimization in SBRs represents an enormous challenge in order to save time and energy. As the non-convexities presented by these systems limit the application of deterministic techniques, stochastic contributions to meet global optimization become crucial. A PSO algorithm in order to minimize the aeration demand in a SBR was developed. The network size, sequencing and stages duration, were assumed as the decision variables for the dynamic MINLP problem. Two kinds of PSO algorithms (relaxed and mixed-integer) were applied in order to find the best way for taking into account the mixed-integer nature. Stochastic optimization improved the results obtained from a sequential shooting method/NLP, and mixed-integer PSO resulted in the best structure solving the MINLP. Despite that, and in order to assure the most robust and reliable solution, the assessment of both PSO formulations must be considered. PSO results have given an optimal operation policy of easy implementation.",
     "keywords": ["SBR", "Dynamic optimization", "Particle swarm optimization"]},
    {"article name": "Real time optimization (RTO) with model predictive control (MPC)",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.07.001",
     "publication date": "12-2010",
     "abstract": "This paper studies a simplified methodology to integrate the real time optimization (RTO) of a continuous system into the model predictive controller in the one layer strategy. The gradient of the economic objective function is included in the cost function of the controller. Optimal conditions of the process at steady state are searched through the use of a rigorous non-linear process model, while the trajectory to be followed is predicted with the use of a linear dynamic model, obtained through a plant step test. The main advantage of the proposed strategy is that the resulting control/optimization problem can still be solved with a quadratic programming routine at each sampling step. Simulation results show that the approach proposed may be comparable to the strategy that solves the full economic optimization problem inside the MPC controller where the resulting control problem becomes a non-linear programming problem with a much higher computer load.",
     "keywords": ["Real time optimization", "Model predictive control", "Fluid catalytic converter", "Integration of control and optimization"]},
    {"article name": "IPL2 and 3 performance improvement method for process safety using event correlation analysis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.07.029",
     "publication date": "12-2010",
     "abstract": "Alarm management efforts have recently intensified, and many tools based on the guidelines in EEMUA 191 have been used to analyze alarm system performance at chemical sites. However, attempts to improve alarm systems using conventional methods have not made satisfactory progress, because they focused on evaluating current performance without providing information that would be useful for improving and rationalizing the alarm system. In this paper a novel method using event correlation analysis is proposed as a means of improving IPL2 and 3 performance, including alarm systems and operator actions. This method's effectiveness was evaluated with data from an alarm system improvement project at a chemical site.",
     "keywords": ["EEMUA", "IPL", "Alarm management", "Event correlation", "Clustering", "Knowledge extraction"]},
    {"article name": "Image-based classification of paper surface quality using wavelet texture analysis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.06.013",
     "publication date": "12-2010",
     "abstract": "The characteristics of paper surface play a central role in the overall quality of paper produced in modern paper machines. Among the surface features, paper formation, i.e., the level of homogeneity in the distribution of fibres on the surface of paper, is a key quality parameter, being currently monitored off-line, at low sampling rates relatively to the high production speeds achieved with modern paper machines. Therefore, in this paper, we address the problem of assessing the quality of paper formation, on-line, in situ, in an autonomous, efficient, objective and fast way, using features derived from images collected by a specially designed sensor, coupled with proper classification methodologies. The results obtained clearly demonstrate the potential of the proposed assessment approach either for the more complex three-class classification problem as well as for less demanding, but still important in practice, two-class \u201cAccept\u201d/\u201cReject\u201d or \u201cPass\u201d/\u201cFail\u201d problem.",
     "keywords": ["Wavelet texture analysis", "Paper formation", "Classification", "Feature extraction", "Variable selection"]},
    {"article name": "On the topological modeling and analysis of industrial process data using the SOM",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.07.002",
     "publication date": "12-2010",
     "abstract": "In this paper, we overview and discuss the implementation of topology-based approaches to modeling and analyzing industrial process data. Emphasis is given to the representation of the data obtained with the self-organizing map (SOM). The methods are used in visualizing process measurements and extracting relevant information by exploiting the topological structure of the observations. Benefits of the SOM with industrial data are presented for a set of process measurements measured in an industrial gas treatment plant. The practical goal is to identify significant operational modes and most sensitive process variables before developing an alternative control strategy. The results confirmed that the SOM-based approach is capable of providing valuable information and offers possibilities for direct application to other process monitoring tasks.",
     "keywords": ["Process monitoring", "Process supervision", "Self-organizing maps"]},
    {"article name": "Combined mass and energy integration in process design at the example of membrane-based gas separation systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.06.019",
     "publication date": "12-2010",
     "abstract": "This paper presents an approach for combined mass and energy integration in process synthesis and illustrates it at the thermochemical production of crude synthetic natural gas (SNG) from lignocellulosic biomass and its separation in a membrane cascade. Based on a general process superstructure, the design problem is decomposed into non-linear unit models whose energy and mass balances are used as constraints in mixed integer linear programming (MILP) that targets the maximum combined production of fuel, heat and power. The flowsheet structure and its operating conditions are thereby considered as complicating decision variables in an overall non-linear and non-continuous optimisation problem that is addressed with an evolutionary, multi-objective optimisation algorithm. In a process that uses its waste and intermediate product streams to balance the heat demand, such a formulation allows for identifying intensified, overall optimal flowsheets by considering all aspects of the process design.",
     "keywords": ["CHP combined heat and power", "combined heat and power", "(FIC)FB (fast internally circulating) fluidised bed", "(fast internally circulating) fluidised bed", "HEN heat exchanger network", "heat exchanger network", "MI(N)LP mixed integer (non-)linear programming", "mixed integer (non-)linear programming", "SNG synthetic natural gas", "synthetic natural gas", "Energy integration", "Mass integration", "Process integration", "Membrane system design", "Process design", "SNG"]},
    {"article name": "A model-based methodology for simultaneous design and control of a bioethanol production process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.07.003",
     "publication date": "12-2010",
     "abstract": "In this work, a framework for the simultaneous solution of design and control problems is presented. Within this framework, two methodologies are presented, the integrated process design and controller design (IPDC) methodology and the process-group contribution (PGC) methodology. The concepts of attainable region (AR), driving force (DF), process-group (PG) and reverse simulation are used within these methodologies. The IPDC methodology is used to find the optimal design-control strategy of a process by locating the maximum point in the AR and DF diagrams for reactor and separator, respectively. The PGC methodology is used to generate more efficient separation designs in terms of energy consumption by targeting the separation task at the largest DF. Both methodologies are highlighted through the application of two case studies, a bioethanol production process and a succinic acid production process. In the final discussion, the results are put in context.",
     "keywords": ["Flowsheet modeling", "Systematic framework", "Driving force", "Attainable region", "Integrated process design and controller design", "Process-group"]},
    {"article name": "Reverse problem formulation approach to molecular design using property operators based on signature descriptors",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.07.009",
     "publication date": "12-2010",
     "abstract": "In this work, an algorithm has been developed for the solution of property based molecular design problems. The recently introduced concept of molecular signature descriptors has been used to design molecules that meet the property targets corresponding to a required process performance. It has been shown that a variety of topological indices (TI) of molecules can be represented in terms of molecular signatures. Signatures can be used to represent different molecular groups if the property targets can be calculated using group contribution models as well. Therefore, the developed algorithm has the ability to combine a variety of property models based on group contribution expressions and topological indices based QSAR/QSPR's to track different property targets in molecular design. This algorithm utilizes molecular property operators formed from signatures for solving the reverse problem of obtaining the molecular structures that satisfy the property targets estimated during the process design step. The principles of graph theory are incorporated to ensure that the design provides feasible molecular structures. Since the molecular operators are formed based on molecular signatures, property models based on different TIs can be represented on the same property platform. Techniques have been developed to describe all TIs with a single signature height. The accuracy of this method depends only on how well the actual property\u2013TI relationships are estimated. Since many TIs can be used to describe each property, this algorithm generally provides reliable results. In addition to physical properties, a wide variety of biological activities can be tracked using the correlations with TIs. This contribution will illustrate the developed methods and highlight their use through a case study.",
     "keywords": ["Reverse problem formulation", "Molecular signature", "Molecular design"]},
    {"article name": "A new algorithm for global optimization: Molecular-Inspired Parallel Tempering",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.06.018",
     "publication date": "12-2010",
     "abstract": "A novel stochastic algorithm for global optimization, Molecular-Inspired Parallel Tempering (MIPT), is presented. MIPT incorporates some basic features of molecular dynamics simulation into the Parallel Tempering formulation. In MIPT, molecules move in the decision-variable-space as the result of different forces: repulsion, friction and random forces. Two different types of molecules are considered: explorers and refiners. Explorers present lower friction and are subject to repulsion forces causing them to move faster towards low molecular density regions. Refiner molecules achieve better values of the objective function and are subject to larger friction forces restricting their motion to a narrow region around their current position. The efficiency of MIPT is tested in five challenging case studies and compared with other established, well-known optimization methods. The results demonstrate that new MIPT is a competitive and efficient algorithm, reaching the global optimum with 100% success ratio in most cases, without requiring much computational effort.",
     "keywords": ["Global optimization", "Molecular-Inspired Parallel Tempering", "Stochastic optimization"]},
    {"article name": "An equation-oriented approach for handling thermodynamics based on cubic equation of state in process optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.07.028",
     "publication date": "12-2010",
     "abstract": "This paper deals with handling of the cubic equation of state (CEOS) in equation-oriented (EO) process optimization. The roots of the CEOS are generally computed by subroutine-based calculations that involve logical if-else conditions. This approach can lead to nonsmoothness and convergence issues when used with gradient-based solvers in an EO framework. In this work, we propose a new general EO approach for selecting the appropriate root of the CEOS by incorporating derivative constraints specific to the desired (vapor or liquid) phase. We prove that these constraints are capable of isolating the liquid, middle and vapor root. The derivative constraints are tested using the EO phase-equilibrium formulation of Gopal and Biegler (1999) which is suitably extended for handling CEOS and relaxing the derivative constraints in the event of disappearance of phases. Using numerical examples involving simulation of flash vessels and optimization of distillation column, it is demonstrated that the proposed EO formulation always selects the appropriate roots, handles missing phases on distillation trays and flash units, and is fairly robust to different starting points in the phase envelopes.",
     "keywords": ["Cubic equation of state", "Flash calculations", "Complementarity constraints", "Nonlinear programming", "Process optimization", "Simulation"]},
    {"article name": "Multiparametric programming based algorithms for pure integer and mixed-integer bilevel programming problems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.07.032",
     "publication date": "12-2010",
     "abstract": "This work introduces two algorithms for the solution of pure integer and mixed-integer bilevel programming problems by multiparametric programming techniques. The first algorithm addresses the integer case of the bilevel programming problem where integer variables of the outer optimization problem appear in linear or polynomial form in the inner problem. The algorithm employs global optimization techniques to convexify nonlinear terms generated by a reformulation linearization technique (RLT). A continuous multiparametric programming algorithm is then used to solve the reformulated convex inner problem. The second algorithm addresses the mixed-integer case of the bilevel programming problem where integer and continuous variables of the outer problem appear in linear or polynomial forms in the inner problem. The algorithm relies on the use of global multiparametric mixed-integer programming techniques at the inner optimization level. In both algorithms, the multiparametric solutions obtained are embedded in the outer problem to form a set of single-level (M)(I)(N)LP problems \u2013 which are then solved to global optimality using standard fixed-point (global) optimization methods. Numerical examples drawn from the open literature are presented to illustrate the proposed algorithms.",
     "keywords": ["Integer and mixed-integer bilevel programming", "Multiparametric programming", "Global optimization", "Reformulation linearization technique"]},
    {"article name": "Linking marketing and supply chain models for improved business strategic decision support",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.07.018",
     "publication date": "12-2010",
     "abstract": "A supply chain (SC) model incorporating business strategic decision components is an important tool for gaining a competitive edge in todays global market. Enterprise models of this type must encompass not only the SC, but also the demand chain since understanding the market is crucial for developing good business policies. To operate effectively, marketing activities must be coordinated with other corporate functional areas. Specifically, managers should evaluate the trade-off between marketing and SC decisions to enhance the performance of the overall metric: the shareholders value. Recently, there has been significant progress in developing marketing science models for reaching quantitatively based marketing decisions. In this work, we build on these developments to formulate a mathematical model that accounts for the main relevant business functionalities. The result is a MINLP formulation which optimizes the SC and marketing strategic decisions in an integrated fashion. Moreover, a financial model that evaluates the enterprise value is also incorporated.",
     "keywords": ["Enterprise wide optimization", "Marketing models", "Business decision support systems"]},
    {"article name": "Efficient bulk maritime logistics for the supply and delivery of multiple chemicals",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.07.031",
     "publication date": "12-2010",
     "abstract": "Many multinational chemical companies (MNCs) manage the inventories of several raw materials at their worldwide sites. Maritime transportation plays a key role in this chemical logistics. In this paper, we address an inventory service problem in which a chemical MNC uses a fleet of multi-parcel ships with dedicated compartments to move multiple chemicals continually among its internal and external production and consumption sites. The objective is to ensure continuity of operation at all sites by maintaining adequate inventory levels of all raw materials. We develop a novel multi-grid continuous-time mixed-integer linear programming (MILP) formulation based (Susarla, Li, & Karimi, 2010) for this chemical logistics problem. Our model allows limited jetties at each site, non-zero transfer times, variable load/unload quantities, transfer task sequencing, etc. In contrast to the literature, it needs no separate estimates for arrivals at each site. Several examples are solved to illustrate the efficiency of our proposed formulation.",
     "keywords": ["Routing", "Scheduling", "Mixed-integer linear programming", "Transportation", "Chemical logistics", "Supply chains", "Inventory management"]},
    {"article name": "Reactive scheduling framework based on domain knowledge and constraint programming",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.07.011",
     "publication date": "12-2010",
     "abstract": "Industrial environments frequently face disruptive events. This contribution presents a support framework, aimed at addressing the repair-based reactive scheduling problem. It is based on an explicit object-oriented domain representation and a constraint programming (CP) approach. When an unforeseen event occurs, the framework captures the in-progress agenda status, as well as the event effect on it. Based on this information, a rescheduling problem specification is developed. Tasks to be rearranged are recognized and the set of the most suitable rescheduling action types (e.g. shift-jump, reassign, freeze) is identified. Since a given specification may lead to several solutions, the second stage relies on a CP model to address the problem just defined. To create such model, action types are automatically transformed into constraints. Provided that good quality schedules can be reached in low CPU times, alternative solution scenarios focusing on stability and regular performance measures can be posed for each problem.",
     "keywords": ["Reactive scheduling", "Batch plants", "Decision support systems", "Knowledge-based scheduling", "Constraint programming"]},
    {"article name": "Modeling and optimization with Optimica and JModelica.org\u2014Languages and tools for solving large-scale dynamic optimization problems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.11.011",
     "publication date": "11-2010",
     "abstract": "The Modelica language, targeted at modeling of complex physical systems, has gained increased attention during the last decade. Modelica is about to establish itself as a de facto standard in the modeling community with strong support both within academia and industry. While there are several tools, both commercial and free, supporting simulation of Modelica models few efforts have been made in the area of dynamic optimization of Modelica models. In this paper, an extension to the Modelica language, entitled Optimica, is reported. Optimica enables compact and intuitive formulations of optimization problems, static and dynamic, based on Modelica models. The paper also reports a novel Modelica-based open source project, JModelica.org, specifically targeted at dynamic optimization. JModelica.org supports the Optimica extension and offers an open platform based on established technologies, including Python, C, Java and XML. Examples are provided to demonstrate the capabilities of Optimica and JModelica.org.",
     "keywords": ["Modelica", "Optimica", "Dynamic optimization", "Model predictive control"]},
    {"article name": "Modified Adomian Decomposition Method and computer implementation for solving singular boundary value problems arising in various physical problems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.02.035",
     "publication date": "11-2010",
     "abstract": "In this paper, we present an efficient numerical algorithm for solving singular two-point linear and non-linear problems, which is based on the Modified Adomian Decomposition Method (MADM). Also, we proposed a new operator for solving singular boundary value problems (BVPs), which gives lesser error compared to MADM and other existing techniques given in the literature, at neighborhood of the right boundary. To illustrate its effectiveness, the algorithm is tested on two linear and two non-linear examples, and the results obtained using Mathematica 6.0 demonstrate the reliability and efficiency of the proposed algorithm.",
     "keywords": ["Singular boundary value problem", "Modified Adomian Decomposition Method", "Adomian polynomials"]},
    {"article name": "Homotopy parameter bounding in increasing the robustness of homotopy continuation methods in multiplicity studies",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.03.013",
     "publication date": "11-2010",
     "abstract": "Homotopy continuation methods are globally convergent methods, which can also be utilized in multiplicity studies. However, when the starting point and/or solution multiplicities lie on separate homotopy path branches, one or more of the solutions may be missed. This is due to the absence of real space connections between separate homotopy path branches, thus preventing multiple solutions being reached from a single starting point. In this paper, a concept is presented that enables a tracking starting point and solution multiplicities in cases where the standard problem-independent homotopy method fails. The concept is based on homotopy parameter bounding and enables the connection of separate homotopy path branches. The concept performance is examined using distillation column examples. In the examined cases the concept is found to improve robustness by establishing a path in real space such that solutions are approached that would be unattainable using the standard homotopy method.",
     "keywords": ["Homotopy continuation methods", "Bounded homotopies", "Path tracking", "Multiplicity studies", "Starting point multiplicity", "Distillation"]},
    {"article name": "Pervaporation of binary water\u2013alcohol and methanol\u2013alcohol mixtures through microporous methylated silica membranes: Maxwell\u2013Stefan modeling",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.03.014",
     "publication date": "11-2010",
     "abstract": "The transport of pure water and methanol and the selective pervaporation of binary mixtures of water or methanol in alcohols (ethanol, isopropanol, butanol) are studied for two methylated microporous silica membranes as a function of feed composition (0\u201315\u00a0vol% water or methanol) at 60\u00a0\u00b0C. Flux data show interaction effects between the polar components in the feed. Alcohol molecules block water and methanol transport, whereas water and methanol block or enhance the alcohol transport. A simplified Maxwell\u2013Stefan model is set up to predict fluxes from pure component pervaporation data and from literature adsorption coefficients. The Vignes equation is used to describe the concentration dependency of the mutual diffusion coefficient. The model allows a good quantitative prediction of the ethanol\u2013water, isopropanol\u2013water and ethanol\u2013methanol mixtures, but needs improvement for the isopropanol\u2013methanol and for the butanol mixtures either in water or in methanol.",
     "keywords": ["Pervaporation", "Microporous silica", "Maxwell\u2013Stefan", "Diffusion", "Vignes"]},
    {"article name": "Dimensional optimization of a tubular solid oxide fuel cell",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.02.030",
     "publication date": "11-2010",
     "abstract": "Solid oxide fuel cells (SOFCs) are very promising for their potential applications as power generators. However, the cost of these cells needs to be significantly reduced to make them a commercial success. Cost of the materials is a significant component of the overall cost. An improvement of the power density with respect to the weight of the cell, termed as gravimetric power density in this study, can help to achieve a lower material cost. On the other hand, a compact design is required for both man-portable and stationary powerhouse applications. The power density with respect to the overall volume of the cell is termed as volumetric power density in this study. A nonlinear constrained multiobjective optimization study using a lexicographic approach is performed to maximize the gravimetric and the volumetric power density of a tubular SOFC. The decision variables are the radius of the anode channel, the cell length, and the annulus size. To be used for optimization studies, a detailed steady state model is developed that can capture changes in the concentration, activation, and ohmic losses due to changes in the decision variables. The model is extensively validated with experimental data collected from an industrial cell spanning a wide range of temperatures, H2 flow rates, and DC polarizations. Although the model predictions are found to be satisfactory for most operating conditions, a significant mismatch between the simulation results and the experimental data is observed when the H2 flow rate is low. The validation study helps to identify the feasible region for the optimization study. The optimization study shows that significant improvements in both the power densities are possible for all the operating conditions considered in this study. The electrical efficiency of the cell also gets improved due to the optimization. In one of the operating conditions, about 30% improvement in the gravimetric power density and about 65% improvement in the volumetric power density are obtained due to the optimization. The percentage changes of the decision variables compared to their base case values are found to be similar for all the voltages other than the voltages close to the open circuit potential (OCP).",
     "keywords": ["Solid oxide fuel cell (SOFC)", "Steady state model", "Nonlinear", "Multiobjective", "Lexicographic", "Optimization", "Gravimetric power density", "Volumetric power density", "Momentum", "Adsorption"]},
    {"article name": "Oil production optimization\u2014A piecewise linear model, solved with two decomposition strategies",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.10.019",
     "publication date": "11-2010",
     "abstract": "This paper presents a new method for real-time optimization of process systems with a decentralized structure where the idea is to improve computational efficiency and transparency of a solution. The contribution lies in the application and assessment of the Lagrange relaxation and the Dantzig\u2013Wolfe methods, which allows us to efficiently decompose a real-time optimization problem. Furthermore, all nonlinearities are modeled by piecewise linear models, resulting in a mixed integer linear program, with the added benefit that error bounds on the solution can be computed.The merits of the method are studied by applying it to a semi-realistic model of the Troll west oil rim, a petroleum asset with severe production optimization challenges due to rate dependent gas-coning wells. This study indicates that both the Lagrange relaxation and in particular the Dantzig\u2013Wolfe approach offers an interesting option for complex production systems. Moreover, the method compares favorably with the non-decomposed method.",
     "keywords": ["Oil production planning", "Piecewise linearization", "Mixed integer linear programming", "Lagrangian decomposition", "Dantzig\u2013Wolfe decomposition"]},
    {"article name": "Multi-objective optimization of an industrial hydrogen plant consisting of a CO2 absorber using DGA and a methanator",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.01.001",
     "publication date": "11-2010",
     "abstract": "An existing hydrogen plant is simulated using a rigorous model for the steam reformer, shift converters, CO2 absorber using DGA (Diglycolamine) and the Methanator. The CO2 absorber using DGA along with the methanator modeled in this work replace the PSA (pressure swing adsorption) system used in other studies, making the current study unique in this respect. A close agreement is observed between the results of the modeling and industrial data from the hydrogen plant of the Tehran refinery. Thereafter, an adaptation of the non-dominated sorting genetic algorithm is employed to perform a multi-objective optimization. Simultaneous maximization of the hydrogen product and export steam flow rates are considered as two objective functions. For the design configuration considered in this study, sets of Pareto-optimal operating conditions are obtained.",
     "keywords": ["Hydrogen plant", "Optimization", "Genetic algorithm", "Modeling", "Process engineering"]},
    {"article name": "The sequential framework for heat exchanger network synthesis\u2014The minimum number of units sub-problem",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.12.002",
     "publication date": "11-2010",
     "abstract": "An overview of an iterative and sequential methodology, called the Sequential Framework for heat exchanger network synthesis (HENS), is presented in the paper. The main objective of the Sequential Framework is to solve industrial size problems. The subtasks of the design process are solved sequentially using mathematical programming. There are two main advantages of the methodology. First, the design procedure is, to a large extent, automated while keeping significant user interaction. Second, the subtasks of the framework (MILP and NLP problems) are much easier to solve numerically than the MINLP models that have been suggested for HENS.One of the limiting factors in the methodology is related to the two MILP models where significant improvements are required to prevent combinatorial explosion. To ease this problem for the minimum number of units MILP sub-problem, two different approaches to the issue were studied. The minimum number of units problem is modified to reduce the gap using physical insights and heuristics. Another novel approach tested was to reformulate some parts of the model by use of some ideas from set partitioning problems. Results show that though both methods succeed in tightening the LP relaxation, the model solution times remain too long to be of interest in the Sequential Framework.",
     "keywords": ["Heat exchanger network synthesis", "Sequential framework", "Discrete optimization", "Transhipment problem"]},
    {"article name": "Challenges in sustainable integrated process synthesis and the capabilities of an MINLP process synthesizer MipSyn",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.04.017",
     "publication date": "11-2010",
     "abstract": "The use of the mathematical programming approach to the synthesis of chemical processes has many valuable creative principles, i.e. optimality, feasibility and integrality of solutions, which are essential for obtaining \u201ctruly\u201d integrated sustainable solutions. However, our tools are still insufficient to perform the sustainable product-process synthesis integrally across the whole chemical supply chain. The ongoing development of concepts, methods and computer applications thus remains the most important role of the PSE community in order to provide engineers with powerful systems tools with which to shape sustainable development. The task, however, is very challenging even at the level of process synthesis since we are dealing with multi-criteria, an enormous amount of complex interactions, uncertainties, discrete and continuous decisions, giving rise to the use of the simultaneous, mixed-integer nonlinear programming (MINLP) approach. Although several efficient MINLP solvers have been developed in the last two decades, hardly any academic or professional MINLP synthesizer for solving such nontrivial synthesis problems has been developed so far. The present contribution wishes to shed light on some important issues relating to different challenges that had to and still have to be mastered as well as various capabilities which in turn were rewarded by mastering some of the challenges during the development of the advanced mixed-integer process synthesizer (MipSyn), the successor of the process synthesizer PROSYN-MINLP (Kravanja and Grossmann, 1990, Kravanja and Grossmann, 1994). The primary aim of future research is oriented towards the development of an even more advanced and robust synthesizer shell, capable of solving large-scale sustainable applications in different engineering domains.",
     "keywords": ["Sustainable synthesis", "Process synthesis", "Process synthesizer", "MipSyn", "MINLP"]},
    {"article name": "Development and validation of models for annealing furnace control from heat transfer fundamentals",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.01.012",
     "publication date": "11-2010",
     "abstract": "Temperature in a continuous annealing furnace is studied by furnace modelling using two methods. A 3D model is used to investigate the temperature distribution of the steel strip that is being annealed and the furnace thermocouple probes, and information from the 3D model enables the construction of a highly simplified 1D/2D model, which predicts furnace and strip temperatures with very good agreement to the 3D model. The simple model has a very short solution time and is suitable for rapid simulation of alternative furnace operating conditions in order to optimise heat treatment quality, plant throughput and energy consumption.",
     "keywords": ["Heat transfer", "Furnace", "Modelling", "Finite difference model problems", "Annealing", "Optimisation"]},
    {"article name": "Data-driven predictive control for blast furnace ironmaking process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.01.005",
     "publication date": "11-2010",
     "abstract": "High performance control of blast furnace (BF) ironmaking process is a difficult problem due to the high temperature and hostile measurement conditions for measuring devices in the process. Previous research focused on developing of accurate predictive models for silicon content in hot metal ([SI]) while control of the whole process is seldom discussed. In the present work, a data-driven predictive control method based on subspace method is presented for the blast furnace ironmaking process. The algorithm is based on input\u2013output data and easy to implement. Simulation results show the algorithm is effective for the control application. Finally, various practical issues concerning predictive control of blast furnace ironmaking process are also addressed, such as constraint handling, control objective and output set-point selection, adaptive strategy, etc.",
     "keywords": ["Blast furnace ironmaking", "Silicon content", "Data-driven", "Subspace method", "Predictive control"]},
    {"article name": "Real-time optimization using a jamming-free switching logic for gradient projection on active constraints",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.04.001",
     "publication date": "11-2010",
     "abstract": "This paper addresses the problem of extremum-seeking (real-time steady state optimization of dynamic systems via control of the gradient) under inequality constraints in continuous time. A gradient projection framework is adopted in this paper, where the main challenge lies in the identification of the set of active constraints. It is first shown that if the discrete-time switching logic widely used in the literature is directly used in continuous-time, it could cause jamming, i.e., getting stuck in a non-optimal solution. So, an appropriate normalization is proposed to get a jamming-free continuous-time switching logic. The absence of jamming is rigourously proved and is also illustrated on a numerical example. In addition, real-time optimization of the desired product of a reaction taking place in an isothermal continuous stirred-tank reactor is studied, where the gradient of the objective and the constraints are calculated simultaneously using a multi-unit framework.",
     "keywords": ["Real-time optimization", "Extremum-seeking control", "Multi-unit optimization", "Inequality constraints", "Gradient projection", "Barrier function"]},
    {"article name": "Dynamic optimization of the load change of a large-scale chemical plant by adaptive single shooting",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.02.036",
     "publication date": "11-2010",
     "abstract": "Dynamic optimization allows for the determination of the upper bound of achievable performance in the operation of continuous chemical processes in transient operation which often results from load or grade changes. Performance assessment can rely on the computation of optimal trajectories for selected scenarios which properly reflect the operational envelope. Realistic industrial problems, however, involve very large-scale dynamic process models and consequently require highly-efficient and robust optimization algorithms. In this work we demonstrate the feasibility of operability assessment by means of dynamic optimization in an industrial case study involving a large-scale process model comprising about 12,000 differential-algebraic model equations. The numerical strategy employed relies on a single shooting method combined with adaptive control grid refinement to minimize the complexity of the numerical problem to the extent possible. This algorithm proves to be the key to success and saves about 95% of computational complexity in comparison to a conventional equidistant discretization.",
     "keywords": ["Optimal control", "Switchability", "Operability", "Single shooting", "Control vector parameterization", "Grid adaptation", "Adaptive parameterization", "Chemical plant", "Distillation", "Reactor", "Recycle", "Turnpike effect"]},
    {"article name": "Stiction identification in nonlinear process control loops",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.02.040",
     "publication date": "11-2010",
     "abstract": "Nearly 20\u201330% of all process control loops oscillate due to stiction resulting in productivity losses. Thus, detection and quantification of stiction in control valves using routine operating data is an important component of any automated controller performance monitoring application. Many techniques have been proposed for the detection and quantification of stiction. However, most of the approaches assume that the underlying process is linear; very little work is available for nonlinear processes. In this paper, Volterra model-based technique is investigated for the detection of stiction in closed-loop nonlinear systems. The advantages of the proposed method are: (i) it can be used to detect stiction in nonlinear systems and (ii) requires no prior information on whether the loop is linear or nonlinear. Results obtained from simulation and industrial case studies demonstrate the utility of the proposed methodology.",
     "keywords": ["Nonlinearities", "Valve stiction", "Volterra series", "Root cause analysis"]},
    {"article name": "Electrical load tracking scheduling of a steel plant",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.01.011",
     "publication date": "11-2010",
     "abstract": "This paper presents a scheduling solution for electrical load tracking of a steel plant. In the scheduling problem the electricity consumption of the plant is pre-specified by an energy curve. During production all tasks must be scheduled such that the total electricity consumption of all machines tracks the scheduled consumption as closely as possible while respecting all production constraints. All energy over- and underconsumption is penalized since it results in fines from the electricity producer.The problem is formulated as a mixed integer programming problem. The scheduling model uses a continuous time formulation. The model is tested on production data from industry. The results show that the continuous time scheduling model can be successfully applied for the electrical load tracking scheduling of a steel plant. By using this type of scheduling the plant owner can potentially minimize over- and underconsumption and thereby reduce his costs.",
     "keywords": ["Steel plant", "Mixed integer programming problem", "Load tracking"]},
    {"article name": "Better reformulation of kinetic models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.06.007",
     "publication date": "11-2010",
     "abstract": "This short note proposes two model reformulations to improve the Hougen\u2013Watson mathematical form of kinetic models and validate them on a typical heterogeneous kinetic problem.",
     "keywords": ["Kinetics", "Nonlinear regressions", "Model formulation", "Parameter correlations", "Parameter estimation"]},
    {"article name": "Modeling Donnan dialysis separation for carboxylic anion recovery",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.03.003",
     "publication date": "10-2010",
     "abstract": "A dynamic model for transport of multiple ions through an anion exchange membrane is derived based on an irreversible thermodynamics approach. This model accounts for the convective transport of the dissociated and undissociated species in the channels with diffusion and migration across the boundary layers and membranes. Donnan equilibrium, flux continuity of the transported ions, the electroneutrality condition and Faraday\u2019s law are employed to describe the electrical potential and concentration discontinuities at the interfaces. The Nernst\u2013Planck equation is used to model the ion transport though boundary layers and membranes. The model consists of a system of partial differential equations that are solved numerically. The aim of this paper is to corroborate this general model for several monoprotic carboxylic acids reported in the literature. The model reproduces satisfactorily experimental fluxes for monoprotic ions. Additionally, previously qualitatively estimated concentration profiles within the boundary layers and membranes are predicted.",
     "keywords": ["Donnan dialysis", "Ion transport modeling", "Carboxylic anion recovery", "Lactic acid recovery"]},
    {"article name": "Experiment and CFD simulation of hybrid SNCR\u2013SCR using urea solution in a pilot-scale reactor",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.12.012",
     "publication date": "10-2010",
     "abstract": "The urea-based selective non-catalytic reduction (SNCR) experiment and modeling previously presented by Nguyen, Lim, et al. (2008) was extended in this study to the hybrid SNCR\u2013SCR process for nitrogen oxides (NOx) removal in a pilot-scale flow reactor. The 5\u00a0wt% urea\u2013water solution was sprayed into the SNCR zone and a commercial V2O5\u2013WO3/TiO2 catalyst in the form of monolith honeycomb was applied in the SCR zone. The NOx reduction efficiency of 91% was obtained from hybrid SNCR\u2013SCR experiments, while 81% of NOx was reduced from the SNCR zone at 940\u00a0\u00b0C and a normalized stoichiometric ratio (NSR) of 2.0. The turbulent reacting flow computational fluid dynamics (CFD) model with a nonuniform droplet size distribution was used, incorporating with the reduced seven-step reactions of SNCR and one Arrhenius-type SCR kinetics. The CFD simulation results showed a reasonable agreement with the experimental data in the temperature range between 900 and 980\u00a0\u00b0C.",
     "keywords": ["NOx reduction", "Selective non-catalytic reduction (SNCR)", "Selective catalytic reduction (SCR)", "Hybrid SNCR\u2013SCR", "Computational fluid dynamics (CFD)", "Pilot-scale flow reactor"]},
    {"article name": "A hybrid neural approach to model batch fermentation of \u201cricotta cheese whey\u201d to ethanol",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.11.010",
     "publication date": "10-2010",
     "abstract": "In this work, the fermentation of \u201cricotta cheese whey\u201d for the production of ethanol was simulated by means of a multiple hybrid neural model (HNM), obtained by coupling neural network approach to mass balance equations for lactose (substrate), ethanol (product) and biomass. A HNM represents an alternative method that may allow predicting the behaviour of complex systems, such as biotechnological processes, in a more efficient way. Some well-assessed phenomena, in fact, are described by a fundamental theoretical approach; some others, being very difficult to interpret, are analysed by means of rather simple \u201ccause\u2013effect\u201d models, based on artificial neural networks. The experimental data, necessary to develop the model, were collected during batch fermentation runs. For all the proposed networks, the inputs were chosen as the operating variables with the highest influence on reaction rate. Simulation results showed the ability of the developed model to represent the process dynamics. The HNM was capable of an accurate representation of the system behaviour by predicting biomass, lactose and ethanol concentration profiles with an average error percentage lower than 10%. Moreover, the hybrid approach showed the ability to limit error propagation into the models that can be caused by the purely black-box nature, typical of neural networks.",
     "keywords": ["Grey-box models", "Artificial neural networks", "Batch fermentation", "Modeling"]},
    {"article name": "Uncertainty propagation for effective reduced-order model generation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.02.034",
     "publication date": "10-2010",
     "abstract": "This work describes a procedure to quantify effective uncertainty propagation through the development of a reduced-order model. To accomplish this objective, the concept of a random fuzzy variable is applied to represent both random and systematic errors associated with uncertain variables. A procedure to obtain feasible combinations of multiple uncertain variables is described. To predict the output probabilistic measure accurately with a minimum number of sample, efficient sampling that combines the techniques of Latin hypercube sampling and Hammersley sequence pairing is used. Based on the output data a reduced-order model is generated using the well known Karhunen-Lo\u00e8ve expansion. The results show that the outputs of the reduced-order model track the outputs of the nonlinear physics-based model satisfactorily. A chemical reactor is used to demonstrate the concepts.",
     "keywords": ["PDEs partial differential equations", "partial differential equations", "KL Karhunen-Lo\u00e8ve", "Karhunen-Lo\u00e8ve", "MCS Monte Carlo sampling", "Monte Carlo sampling", "HSS Hammersley sequence sampling", "Hammersley sequence sampling", "EFFs empirical eigenfunctions", "empirical eigenfunctions", "ROM reduced-order model", "reduced-order model", "RFV random fuzzy variable", "random fuzzy variable", "LHS Latin hypercube sampling", "Latin hypercube sampling", "LHHS Latin hypercube-Hammersley sampling", "Latin hypercube-Hammersley sampling", "HDA hydrodealkylation", "hydrodealkylation", "Random fuzzy variable", "Latin hypercube-Hammersley sampling sequence", "Principal orthogonal decomposition"]},
    {"article name": "An optimization-simulation model for a simple LNG process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.10.018",
     "publication date": "10-2010",
     "abstract": "A gradient free optimization-simulation method for processes modelled with the simulator Aspen HYSYS is developed. The tool is based on a Tabu Search (TS) and the Nelder-Mead Downhill Simplex (NMDS) method. The local optima that result from the TS are fine-tuned with NMDS to reduce the required number of simulations. The tool has been applied to find the total refrigerant flow rate, composition and the refrigerant suction and condenser pressures that minimize the energy requirements of a Prico process. The main strength of this method is that it has a high probability of obtaining a better solution with significantly fewer simulation runs than other metaheuristic methods. Also, by changing the TS step size it is possible to influence the initial search pattern, thereby taking advantage of already gained process knowledge to decrease the optimization time. The method is general and can be applied to other processes modelled in Aspen HYSYS.",
     "keywords": ["Nelder-Mead", "Tabu Search", "Optimization", "Simulation", "Metaheuristics", "Liquefied Natural Gas"]},
    {"article name": "Orthogonal simulated annealing for multiobjective optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.11.015",
     "publication date": "10-2010",
     "abstract": "The paper proposes a new simulated annealing (SA) based multiobjective optimization algorithm, called orthogonal simulated annealing (OSA) algorithm in this work. The OSA algorithm incorporates an orthogonal experiment design (OED) with a simulated annealing based multiobjective algorithm aiming to provide an efficient multiobjective algorithm. OED involves several experiments based on an orthogonal table and a fractional factorial analysis to extract intelligently the best combination of decision vectors making the classical SA to explore search space effectively, to enhance convergence, and to improve quality of solutions in the Pareto set. These benefits have been tested by comparing the performance of OSA with one state-of-the-art multiobjective evolutionary algorithm (NSGA2) and one classical simulated annealing based multiobjective algorithm (CMOSA) considering multiobjective problems of varying degrees of complexity. The obtained Pareto sets by these three algorithms have been tested using standard methods like measure C, hypervolume comparison, etc. Simulation results show that the performance of and CPU time required by these algorithms are problem dependent, and with some problems, the OSA algorithm outperforms the other two algorithms. In particular, the comparison between OSA and CMOSA suggests that around 70% times OSA outperforms CMOSA and obtains a well diversified set of solutions. In addition, with some problems, OSA captures the Pareto fronts where CMOSA fails. Therefore, the development of OSA is noteworthy, and it provides an additional tool to solve multiobjective optimization problems.",
     "keywords": ["CMOSA classical multiobjective simulated annealing", "classical multiobjective simulated annealing", "OED orthogonal experiment design", "orthogonal experiment design", "MOEA multiobjective evolutionary algorithm", "multiobjective evolutionary algorithm", "MOSA multiobjective simulated annealing", "multiobjective simulated annealing", "OSA orthogonal simulated annealing", "orthogonal simulated annealing", "NSGA2 non-dominated sorting genetic algorithm 2", "non-dominated sorting genetic algorithm 2", "PDMOSA Pareto dominance based multiobjective simulated annealing", "Pareto dominance based multiobjective simulated annealing", "WMOSA weight based multiobjective simulated annealing", "weight based multiobjective simulated annealing", "PF Pareto front", "Pareto front", "SA simulated annealing", "simulated annealing", "Multiobjective optimization", "Fractional factorial analysis", "Orthogonal table", "OED", "OSA", "MOSA", "CMOSA", "SA"]},
    {"article name": "A randomized algorithm for the efficient synthesis of heat exchanger networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.12.003",
     "publication date": "10-2010",
     "abstract": "This article presents an algorithm for the synthesis of heat exchanger networks (HENs) using randomization as an effective tool. It optimizes the total cost of the network. The method proposed here is suitable for finding the optimal solution with stream-splitting and merging. The present approach provides significant advantage of randomization over other existing optimization techniques for obtaining the optimal solution. We have studied three benchmark problems already published in the literature to demonstrate how better solutions were undetected by the earlier approaches. The salient feature of the proposed algorithm is that it provides a variety of possible networks which are close to the optimal network. Another important aspect is that the algorithm is quite fast. For small- and medium-size problems, the technique proposed in this article will prove to be very effective for the design of heat exchanger networks.",
     "keywords": ["Cost optimization", "CPU time", "Heat exchanger network synthesis", "Optimum network", "Randomized algorithm", "Stream-splitting"]},
    {"article name": "Pressure swing batch distillation by double column systems in closed mode",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.02.037",
     "publication date": "10-2010",
     "abstract": "Two new double column systems operated in closed mode are suggested for pressure swing batch distillation. These configurations are investigated by feasibility studies based on several simplifying assumptions (e.g., infinite plate number, no stage hold-up) and are compared with double column batch rectifier and double column batch stripper in open mode. We study the configurations also by rigorous simulation based on less simplifying assumptions using a professional dynamic simulator. For the different configurations the influence of the most important operational parameters is studied. The results obtained for open and close modes are compared. The calculations and the simulations are performed for a binary minimum (acetone\u2013n-pentane) and for a maximum (ethylene-diamine\u2013water) azeotrope mixture.",
     "keywords": ["Feasibility study", "Pressure swing distillation", "Batch distillation", "Separation of azeotropes", "Dynamic simulation"]},
    {"article name": "Boundary model predictive control of Kuramoto\u2013Sivashinsky equation with input and state constraints",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.10.011",
     "publication date": "10-2010",
     "abstract": "In this work an asymptotic stabilization of highly dissipative Kuramoto\u2013Sivashinsky equation (KSE) by means of boundary model modal predictive control (MMPC) in the presence of input and state constraints is demonstrated. The KS equation is initially defined in an appropriate functional space setting and an exact transformation is used to reformulate the original boundary control problem as an abstract boundary control problem of the KSE partial differential equation (PDE). An appropriate discrete infinite-dimensional representation of the abstract boundary control problem is used for synthesis of low dimensional model modal predictive controller (MMPC) incorporating both the pointwise enforced KSE state constraints and input constraints. The proposed control problem formulation and the performance of the closed-loop system in the full state feedback controller realization have been evaluated through simulations.",
     "keywords": ["Distributed-parameter systems", "Kuramoto\u2013Sivashinsky equation", "Boundary control", "Model predictive control (MPC)", "Input/state constraints"]},
    {"article name": "Stability analysis of an approximate scheme for moving horizon estimation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.02.033",
     "publication date": "10-2010",
     "abstract": "We analyze the stability properties of an approximate algorithm for moving horizon estimation (MHE). The strategy provides instantaneous state estimates and is thus suitable for large-scale feedback control. In particular, we study the interplay between numerical approximation errors and the convergence of the estimator error. In addition, we establish connections between the numerical properties of the Hessian of the MHE problem and traditional observability definitions. We demonstrate the developments through a simulation case study.",
     "keywords": ["Estimation", "Stability", "Large scale", "Observability", "Nonlinear programming"]},
    {"article name": "Refinery scheduling of crude oil unloading, storage and processing using a model predictive control strategy",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.01.009",
     "publication date": "10-2010",
     "abstract": "A model predictive control (MPC) strategy is presented to determine the optimal control decisions for the short-term refinery scheduling problem. For cases where process disturbances occur or new plans need to be implemented during the scheduling period, the moving horizon strategy allows control decisions to be updated effectively to maintain an optimal operation. Furthermore, this strategy takes advantage of information regarding the system and disturbance prediction over the moving horizon to be used in obtaining the control decisions for the given time interval. To demonstrate the performance of the MPC strategy, especially for various moving horizon lengths, three different case studies concerning scheduling problem in a crude oil refinery were used. The refinery includes the shipping vessels, the storage and charging tanks, and the crude distillation units. Several disturbance scenarios regarding mixed oil demands were constructed to illustrate the performance of the proposed strategy.",
     "keywords": ["Scheduling", "Refinery", "Crude oil", "Moving horizon", "Model predictive control"]},
    {"article name": "Operational scheduling of refined products pipeline networks with simultaneous batch injections",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.03.005",
     "publication date": "10-2010",
     "abstract": "Petroleum refined products are mostly sent from oil refineries to distribution depots by trunk pipelines. Pipeline networks usually involve multiple input and exit terminals, and even dual-purpose stations. Several pumping operations can be simultaneously performed at different sources. Most of the computational burden on the scheduling of multi-source pipeline networks comes from three operational tasks: pump sequencing, batch sizing, and batch allocation. Previous contributions applied discrete decomposition approaches performing such tasks through heuristic-based decisions. This paper introduces an MILP continuous formulation for the operational scheduling of unidirectional pipeline networks that allows simultaneous batch injections. The problem goal is to satisfy depot requirements at minimum total cost. The optimal schedule of pumping and delivery operations is established all at once. Results show that simultaneous batch injections lead to a better use of the pipeline transport capacity and a substantial reduction on the overall time needed to meet depot demands.",
     "keywords": ["Pipeline network", "Operational planning", "Continuous approach", "Simultaneous injections", "Multiple sources"]},
    {"article name": "Inventory and delivery optimization under seasonal demand in the supply chain",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.02.009",
     "publication date": "10-2010",
     "abstract": "This work deals with the inventory, purchase and delivery optimization problem in the supply chain. The formulation of two problems is presented involving several decision levels. The first one optimizes the company inventory and purchase tasks in a medium-term horizon planning, assuming that the total amount purchased is delivered at the beginning of each period. Then, in a more detailed formulation, the purchased amount is distributed among several deliveries giving rise to a non-linear non-convex problem. Some transformation techniques are evaluated to overcome the non-convexities in order to find a global solution in a reasonable execution time. Finally, the results obtained considering some possible scenarios are analyzed and compared.",
     "keywords": ["Inventory optimization", "Purchase decisions", "Delivery optimization", "Supply chain management"]},
    {"article name": "Outer approximation-based algorithm for biotechnology studies in systems biology",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.03.001",
     "publication date": "10-2010",
     "abstract": "Optimization methods play a central role in systems biology studies as they can help in identifying key processes that can be experimentally changed so that specific biological goals can be attained. Standard optimization methods used in this field rely on simplified linear models that may fail in capturing the underlying complexity of the target metabolic network. Within this general context, we present a novel approach to globally optimize metabolic networks. The approach presented relies on (1) adopting a general modeling framework for metabolic networks: the Generalized Mass Action (GMA) representation; (2) posing the optimization task as a non-convex nonlinear programming (NLP) problem; and (3) devising an efficient solution method for globally optimizing the resulting NLP that embeds a GMA model of the metabolic network. The capabilities of our method are illustrated through two case studies: the anaerobic fermentation pathway in Saccharomyces cerevisiae and the citric acid production using Aspergillus niger. Numerical results show that the method presented provides near optimal solutions in low CPU times even in cases where the commercial global optimization package BARON fails to close the optimality gap.",
     "keywords": ["Global optimization", "Generalized Mass Action (GMA)", "Metabolic engineering"]},
    {"article name": "A new method for prediction of absorption/stripping factors",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.12.013",
     "publication date": "10-2010",
     "abstract": "Absorption and stripping are the unit operations which are widely used in the chemical processing industries. Many attempts have been made to define an average absorption factor method to short-cut the time consuming rigorous calculation procedures. The sole restriction of such a method is how well the average factor, as it is defined, will represent the absorption that actually occurs. The stripping operation is essentially the reverse of absorption and can be handled in a similar fashion. In this work, a simple predictive tool which is easier than existing approaches, less complicated with fewer computations is formulated to accurately predict the absorption efficiency as a function of absorption factor and number of absorber stages. The proposed predictive tool also can be used to determine the number of trays required for a given lean oil rate or to calculate recoveries with a given oil rate and tray count. The proposed method showed consistently accurate results for number of stages and absorption factors up to 20. Predictions showed an excellent agreement with the reported data with an average absolute deviation being less than 1%. The proposed method is superior owing to its accuracy and clear numerical background, wherein the relevant coefficients can be retuned quickly for various cases. This proposed simple-to-use approach can be of immense practical value for the engineers and scientists to have a quick check on absorption efficiency and estimating the trays required for a given lean oil rate or to calculate recoveries from a known oil rate and tray count at wide range of operating conditions without the necessity of any pilot plant set up and experimental runs. In particular process engineers would find the proposed approach to be user friendly involving transparent calculations with no complex expressions.",
     "keywords": ["Correlation", "Absorption", "Absorption efficiency", "Absorption stages", "Stripping"]},
    {"article name": "Design and modeling of optical modules for use in the \u201cEmerald Forest\u201d algae photobioreactors",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.02.019",
     "publication date": "09-2010",
     "abstract": "As part of the \u201cEmerald Forest\u201d concept proposed in previous work by the authors (Teymour, Al-Hallaj, Eltayeb, & Khalil, 2009), photobioreactors for large-scale algae production are developed. Optical waveguides are used to overcome light limitations, and to maximize light collection and optimize light distribution. The finite volume discrete ordinates (FVDO) method is used to model light transport efficiency by different designs of waveguides that can be used to transport light, or change its direction and intensity, as a first step in the design of an efficient waveguide-based algae photobioreactor.",
     "keywords": ["Algae", "Bioreactor", "Biofuels", "Light modeling", "Sustainable community"]},
    {"article name": "Dynamic optimization and robust explicit model predictive control of hydrogen storage tank",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.02.018",
     "publication date": "09-2010",
     "abstract": "We present a general framework for the optimal design and control of a metal-hydride bed under hydrogen desorption operation. The framework features: (i) a detailed two-dimension dynamic process model, (ii) a design and operational dynamic optimization step, and (iii) an explicit/multi-parametric model predictive controller design step. For the controller design, a reduced order approximate model is obtained, based on which nominal and robust multi-parametric controllers are designed.",
     "keywords": ["Hydrogen storage", "Metal-hydride reactor", "Dynamic optimization", "Explicit/multi-parametric model predictive control", "Robust model predictive control"]},
    {"article name": "Green process design, green energy, and sustainability: A systems analysis perspective",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.02.010",
     "publication date": "09-2010",
     "abstract": "This paper presents a systems analysis perspective that extends the traditional process design framework to green process design, green energy and industrial ecology leading to sustainability. For green process design this involves starting the design decisions as early as chemical and material selection stages on one end, and managing and planning decisions at the other end. However, uncertainties and multiple and conflicting objectives are inherent in such a design process. Uncertainties increase further in industrial ecology. The concept of overall sustainability goes beyond industrial ecology and brings in time dependent nature of the ecosystem and multi-disciplinary decision making. Optimal control methods and theories from financial literature can be useful in handling the time dependent uncertainties in this problem. Decision making at various stages starting from green process design, green energy, to industrial ecology, and sustainability is illustrated for the mercury cycling. Power plant sector is a major source of mercury pollution. In order to circumvent the persistent, bioaccumulative effect of mercury, one has to take decisions at various levels of the cycle starting with greener power systems, industrial symbiosis through trading, and controlling the toxic methyl mercury formation in water bodies and accumulation in aquatic biota.",
     "keywords": ["Green process design", "Green energy", "Sustainability"]},
    {"article name": "Point-based standard optimization with life cycle assessment for product design",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.02.013",
     "publication date": "09-2010",
     "abstract": "There has been a proliferation of building and product sustainability standards based on the success of the LEED\u2122 program from the green building council for new commercial building construction. A feature of these standards is the accumulation of points for undertaking various activities of different types, such as improving energy efficiency. The combined points total leads to certification of the product at different performance levels. The question is whether the standards actually promote products that are better from a life cycle perspective, or whether the standards are biased towards certain activities based on a perception that some activities are inherently better than others (recycling or bio-based materials for example). In this work, we used optimization methods coupled with life cycle inventory information to explore this question. A carpet standard is used to compare the life cycle optimization against an optimization to earn the maximum number of points in the standard.",
     "keywords": ["Process design", "Life cycle assessment (LCA)", "Point-based standard"]},
    {"article name": "Scope for the application of mathematical programming techniques in the synthesis and planning of sustainable processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.11.012",
     "publication date": "09-2010",
     "abstract": "Sustainability has recently emerged as a key issue in process systems engineering (PSE). Mathematical programming techniques offer a general modeling framework for including environmental concerns in the synthesis and planning of chemical processes. In this paper, we review major contributions in process synthesis and supply chain management, highlighting the main optimization approaches that are available, including the handling of uncertainty and the multi-objective optimization of economic and environmental objectives. Finally, we discuss challenges and opportunities identified in the area.",
     "keywords": ["Sustainability", "Uncertainty", "Process synthesis", "Supply chain management"]},
    {"article name": "Optimal design of cryogenic air separation columns under uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.02.007",
     "publication date": "09-2010",
     "abstract": "Cryogenic air separation, while widely used in industry, is an energy intensive process. Effective design can improve efficiency and reduce energy consumption, however, uncertainties can make determination of the optimal design difficult. This paper addresses the conceptual design of cryogenic air separation process under uncertainty. A rigorous, highly nonlinear model of three integrated columns is developed to capture the coupled nature of the process. The multi-scenario approach is used to incorporate the uncertainty, giving rise to a nonlinear programming problem with over half a million variables. Nevertheless, this problem is solved efficiently using Ipopt, demonstrating the effectiveness of interior-point methods on complex, large-scale nonlinear programming problems. The optimal design from the multi-scenario approach is compared against the optimal design using nominal parameter values. As expected, the results using the multi-scenario approach are more conservative than the nominal case; however, they may be less conservative than traditional overdesign factors.",
     "keywords": ["Cryogenic air separation", "Nonlinear programming", "Conceptual design", "Design under uncertainty", "Multi-scenario optimization", "Large-scale optimization"]},
    {"article name": "Assessing reliability of cellulose hydrolysis models to support biofuel process design\u2014Identifiability and uncertainty analysis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.02.012",
     "publication date": "09-2010",
     "abstract": "The reliability of cellulose hydrolysis models is studied using the NREL model. An identifiability analysis revealed that only 6 out of 26 parameters are identifiable from the available data (typical hydrolysis experiments). Attempting to identify a higher number of parameters (as done in the original NREL model publication) results in significant errors on the parameter estimates. The reasons for this poor identifiability are related to (i) model structure complexity, inherently containing correlated parameters due to Michaelis\u2013Menten type kinetics, and (ii) the available data, which are not informative enough (sensitivities of 16 parameters were insignificant). This indicates that the NREL model has severe parameter uncertainty, likely to be the case for other hydrolysis models as well since similar kinetic expressions are used. To overcome this impasse, we have used the Monte Carlo procedure to analyze the uncertainty of model predictions. This allows judging the fitness of the model to the purpose under uncertainty. Hence we recommend uncertainty analysis as a proactive solution when faced with model uncertainty, which is the case for biofuel process development research.",
     "keywords": ["Cellulose hydrolysis", "Design", "Modeling", "Identifiability", "Uncertainty", "Biofuels"]},
    {"article name": "An NSF perspective on next generation hydrocarbon biorefineries",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.02.025",
     "publication date": "09-2010",
     "abstract": "Next generation hydrocarbon biofuels have the potential to fulfill not only the biofuels production mandate of the Energy Independence and Security Act (EISA) of 2007, but also would not have to be exempted from the EISA-mandated increase in the corporate average fuel economy. Producing \u201cgreen gasoline\u201d, \u201cgreen diesel\u201d and \u201cgreen jet fuel\u201d from non-food lignocellulosic feedstocks such as forest waste and agricultural residue, and energy crops such as short rotation trees and perennial grasses grown on marginal or abandoned cropland will substantially lower greenhouse gas emissions and will not affect food prices. In this paper the development of the new paradigm of green gasoline from lignocellulose will be reviewed, the potential pathways to these infrastructure-compatible fuels will be discussed, and critical process design needs will be outlined.",
     "keywords": ["Hydrocarbon biofuels", "Biomass conversion", "Green gasoline"]},
    {"article name": "On the use of systems technologies and a systematic approach for the synthesis and the design of future biorefineries",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.02.021",
     "publication date": "09-2010",
     "abstract": "Systems technologies emerge with a powerful potential to support the deployment and design of future biorefineries. The chemical industry experiences a steady growth in the use of renewables induced by the gradual depletion of oil, uncertainties in energy supplies and a commanding requirement to reduce GHG emissions and save the planet. Renewables introduce an impressive range of options with biorefining at the center of attention as an emerging industrial concept, uniquely attached to chemical engineering and aiming to transform plant-derived biomass into a variety of products including transport fuels, platform chemicals, polymers, and specialty chemicals. In competing with conventional processes, biorefineries should match maximum efficiencies with better design and process integration. The paper highlights the pivotal role of systems technology to foster innovation, preview options, and support high-throughput computational experimentation, arguing that systems tools are largely under-deployed. Systems-enabled platforms could instead function as powerful environments to generate ideas for integrated designs and offer tremendous services to the complex and large problems produced by the numerous portfolios of feedstocks, unknown portfolios of products, multiple chemistries, and multiple processing paths. Complexities certainly exceed capabilities of previous methodologies but established achievements and experience with similar problems are excellent starting points for future contributions. Besides a general discussion, the paper outlines opportunities for innovation in design, concept-level synthesis, process integration, and the development of supply chains.",
     "keywords": ["Renewables", "Biorefineries", "Process synthesis", "Process integration", "Optimization"]},
    {"article name": "Extraction of biofuels and biofeedstocks from aqueous solutions using ionic liquids",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.02.020",
     "publication date": "09-2010",
     "abstract": "The production from biomass of chemicals and fuels by fermentation, biocatalysis, and related techniques implies energy-intensive separations of organics from relatively dilute aqueous solutions, and may require use of hazardous materials as entrainers to break azeotropes. We consider the design feasibility of using ionic liquids as solvents in liquid\u2013liquid extractions for separating organic compounds from dilute aqueous solutions. As an example, we focus on the extraction of 1-butanol from a dilute aqueous solution. We have recently shown (Chapeaux et al., 2008) that 1-hexyl-3-methylimidazolium bis(trifluoromethylsulfonyl)imide shows significant promise as a solvent for extracting 1-butanol from water. We will consider here two additional ionic liquids, 1-(6-hydroxyhexyl)-3-methylimidazolium bis-(trifluoromethylsulfonyl)imide and 1-hexyl-3-methylimidazolium tris(pentafluoroethyl)trifluorophosphate, as extraction solvents for 1-butanol. Preliminary design feasibility calculations will be used to compare the three ionic liquid extraction solvents considered. The ability to predict the observed ternary liquid\u2013liquid equilibrium behavior using selected excess Gibbs energy models, with parameters estimated solely using binary data and pure component properties, will also be explored.",
     "keywords": ["Ionic liquids", "1-Butanol", "Extraction", "Liquid\u2013liquid equilibrium", "Excess Gibbs energy models", "Biofuels"]},
    {"article name": "Towards sustainability of engineered processes: Designing self-reliant networks of technological\u2013ecological systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.02.026",
     "publication date": "09-2010",
     "abstract": "Sustainability of human activities is entirely dependent on the availability of ecosystem goods and services such as carbon sequestration, mineral and fossil resources, sunlight, biogeochemical cycles, soil formation, pollination, etc. However, existing methods in most disciplines, including sustainable engineering and industrial ecology ignore this crucial role played by nature. The use of such a narrow boundary can lead to misleading results and perverse decisions. This paper introduces the idea of designing networks of technological systems along with their supporting ecological systems. Such networks of technological and ecological systems exploit the synergy between them and can help in closing material loops and minimize exergy loss, leading to truly self-sustaining systems. Methods for designing such technological\u2013ecological synergy (TES) networks could be developed by extending existing process synthesis and design approaches to include ecological models. Such an approach would integrate industrial ecology with ecological engineering and require collaboration between engineers and ecologists. It presents many new challenges and opportunities for process systems engineering to contribute to the sustainability of engineered systems. The idea of TES networks is illustrated via several practical case studies, with focus on the life cycle of corn ethanol and a typical American residential system.",
     "keywords": ["Sustainability", "Design", "Ecosystems", "Life cycle", "Networks"]},
    {"article name": "Teaching \u201coperability\u201d in undergraduate chemical engineering design education",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.02.003",
     "publication date": "09-2010",
     "abstract": "This paper presents a proposal for increased emphasis on operability in the Chemical Engineering capstone design courses. Operability becomes a natural aspect of the process design courses for a project that is properly defined with variation in operations and model uncertainty. Key topics in operability are operating window, flexibility, reliability, safety, efficiency, operation during transitions, dynamic performance, and monitoring and diagnosis. The key barrier to improved teaching and learning of operability is identified as easily accessed and low-cost educational materials, and a proposal is offered to establish a portal open to all educators.",
     "keywords": ["Engineering education", "Process design", "Operability", "Trouble shooting"]},
    {"article name": "Mathematical modeling and global optimization of large-scale extended pooling problems with the (EPA) complex emissions constraints",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.02.014",
     "publication date": "09-2010",
     "abstract": "Environmental Protection Agency (EPA) Title 40 Code of Federal Regulations Part 80.45: Complex Emissions Model [40CFR80.45, 2007] codifies a mathematical model of gasoline emissions for reformulated gasoline (RFG) as a function of eleven fuel properties. In this paper we propose an extended pooling problem to maximize the profit of blending reformulated gasoline on a predetermined network structure of feed stocks, intermediate storage tanks, and gasoline products subject to applicable environmental standards. A mixed-integer nonlinear programming (MINLP) model is introduced which is nonconvex due to the presence of bilinear, polynomial, and fractional power terms. A mixed-integer linear programming (MILP) relaxation of the extended pooling problem is proposed and we introduce several test cases from small to medium to large scale and solve them to global optimality. The large-scale test case involves 14 feed stocks, 5 pools, and 10 products and consists of 1104 continuous variables, 150 binary variables, and 640 nonlinear terms. The nonconvexities in the large-scale case study include 410 bilinear terms, 40 polynomial terms, and 10 terms raised to a fractional power.",
     "keywords": ["Large-scale optimization", "Global optimization", "MINLP", "Bilinear", "Polynomial", "Pooling problem"]},
    {"article name": "Tight energy integration: Dynamic impact and control advantages",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.02.005",
     "publication date": "09-2010",
     "abstract": "Process integration is a key enabler to increasing efficiency in the process and energy generation industries. Efficiency improvements are obtained, however, at the cost of an increasingly complex dynamic behavior. As a result, tightly integrated designs continue to be regarded with caution owing to the dynamics and control difficulties that they pose. The present work introduces a generic class of integrated networks where significant energy flows (either arising from energy recycling or of external origin) result in dynamic models with a multi-time-scale structure. Such networks feature a clear distinction between the fast dynamics of individual units and the slow dynamics of the entire network. We draw a connection between specific (steady-state) design features and structural properties that afford the development of a framework for the derivation of low-order, non-stiff, nonlinear models of the core network dynamics. Furthermore, we demonstrate that tight energy integration and the presence of significant energy flows can facilitate, rather than hinder, control structure design and performance, and propose a cadre for hierarchical control predicated on the use of fast, distributed control for the individual units and nonlinear supervisory control for the entire network. The developed concepts are illustrated with examples.",
     "keywords": ["Energy integration", "Multi-time scale dynamics", "Hierarchical control"]},
    {"article name": "Multi-scale methods and complex processes: A survey and look ahead",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.10.004",
     "publication date": "09-2010",
     "abstract": "A comprehensive overview of multi-scale methods is presented with a focus on the ways in which information is communicated between scales. Liquid density computations, which are important in phase equilibrium of carbon storage, are used to illustrate multi-scale process engineering ideas. It is shown that using the Gibbs\u2013Helmholtz equation to constrain the energy parameter in cubic equations of state leads to (1) a natural bridge between bulk and molecular length scales, (2) a new Gibbs\u2013Helmholtz constrained equation of state, and (3) a novel mixing rule, and that the GHC equation can be coupled to NTP molecular simulations to provide more accurate predictions of density for use in phase equilibrium computations and reservoir simulations. Improvements for the next generation of multi-scale tools for analysis, visualization, simulation, and optimization of complex processes including the need to proceed with partial knowledge, multiplicity at various length scales, problems with many scales, and methods for predicting behavior over very long time and length scales are also discussed.",
     "keywords": ["Multi-scale methods", "Complex processes", "Communication between scales", "Deep ocean sedimentary CO2 storage", "Gibbs\u2013Helmholtz constrained equation of state"]},
    {"article name": "Design of ionic liquids via computational molecular design",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.02.017",
     "publication date": "09-2010",
     "abstract": "Computational molecular design (CMD) is a methodology which applies optimization techniques to develop novel lead compounds for a variety of applications. In this work, a CMD method is applied to the design of ionic liquids (ILs), which are being considered for use as environmentally benign solvents. The molecularly tunable nature of ILs yields an extraordinary number of possible cation and anion combinations, the majority of which have never been synthesized. The product design framework developed in this work seeks to accelerate the commonly used experimental trial-and-error approach by searching through this large molecular space and providing a set of chemical structures likely to match a set of desired property targets. To predict the physical and chemical properties of an ionic liquid in a specific system, quantitative structure\u2013property relations (QSPRs) have been developed. In this work, correlations were created for solubility, diffusivity, and melting temperature. The electronic structure of ionic liquids is quantified using molecular connectivity indices, which describe bonding environments, charge distribution, orbital hybridization and other interactions within and between ions. The resulting property prediction model is then integrated within a computational molecular design framework, which combines the QSPRs with structural feasibility constraints in a combinatorial optimization problem. The problem is reformulated as an MILP after exact linearization of structural constraints. An example is provided to test the formulation for the design of ionic liquids for use within a hydrofluorocarbon (refrigerant) gas separation system. A second example compares a stochastic optimization algorithm, Tabu Search, to a standard deterministic solver for the solution of a larger-scale refrigeration design problem. The computational efficiency and practical implementation of this product design methodology is also discussed.",
     "keywords": ["Molecular product design", "Ionic liquids", "Optimization"]},
    {"article name": "Simultaneous solution of process and molecular design problems using an algebraic approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.02.015",
     "publication date": "09-2010",
     "abstract": "Traditionally process design and molecular design problems have been treated as two separate problems, with little or no feedback between them. Introduction of the property integration framework has allowed for simultaneous representation of processes and products from a property perspective and hence established a link between molecular and process design. The simultaneous approach involves solving two reverse problems. The first reverse problem identifies the input molecules\u2019 property targets corresponding to the desired process performance. The second reverse problem is the reverse of a property prediction problem, which identifies the molecular structures that match the targets identified in the first problem. Group contribution methods (GCM) are used to form molecular property operators and these help in tracking properties. Earlier contributions in this area have tried to include higher order estimation of GCM for solving the molecular design problem. In this work, the accuracy of property prediction is enhanced by improving the techniques to enumerate higher order groups. Incorporation of these higher order enumeration techniques increases the efficiency of property prediction and thus the range of applicability of group contribution methods to molecular design problems. This method of generation enables the identification of structural isomers to some extent as it puts a check on the possibility of nonexistence of each higher order group in each combination. Property operator based techniques are used to track properties in both process and molecular design problems. The developed algorithm solves the set of inequality expressions of process and molecular design problems simultaneously to identify the molecules that meet the process performance and environmental restrictions defined in terms of properties. Since the algorithm should be able to solve for any number of properties, an algebraic approach is used to generate possible molecules within the required property range. This contribution will use a case study to highlight the principles of the developed methodology.",
     "keywords": ["Property operators", "Reverse problem formulation", "Molecular design"]},
    {"article name": "Column profile maps as a tool for synthesizing complex column configurations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.02.006",
     "publication date": "09-2010",
     "abstract": "There has recently been a renewed interest in the design of distillation processes due to rising energy cost and growing environmental concerns. Column profile maps (CPMs) have been developed as a graphical tool to simplify the design procedure of distillation schemes as well as a method for analyzing existing processes. Using CPMs one is able to change topology within the composition space to suit the requirements of the separation and hence many separations that have been thought of as difficult or unviable can now be better understood and consequently new designs may be devised. The CPM technique has also been proven to be extremely useful as a design tool for complex columns, as configurations irrespective of complexity can be modeled and graphically understood. This paper aims to summarize the most important and interesting results and applications obtained using the CPM technique. It shows how CPMs may be used to synthesize complex columns like a Petlyuk or Kaibel column, as well as showing how new sharp split separations can be devised.",
     "keywords": ["Column profile maps", "Distillation design", "Sharp splits", "Complex columns"]},
    {"article name": "Integrating product portfolio design and supply chain design for the forest biorefinery",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.02.004",
     "publication date": "09-2010",
     "abstract": "Supply chain (SC) design involves making strategic long-term decisions for a company, e.g. number, location and capacity of facilities, production rates, flow of material between SC nodes, as well as choosing suppliers and markets. The forest biorefinery is emerging as a promising opportunity for improving the business model of forest product companies; however it introduces significant challenges in terms of mitigating technology, economic and financial risks\u2014each of which must be systematically addressed in the SC design. In this regard, product portfolio definition and technology selection are two important decisions that have rarely been considered in a systematic SC evaluation. This paper presents a methodology, in which product/process portfolio design and SC design are linked in order to build a design decision making framework. According to this methodology, design of \u201cmanufacturing flexibility\u201d links product/process portfolio design to SC design, through a margins-based SC operating policy. Techno-economic studies along with scenario generation for price and demand changes representing market volatility are employed in the methodology.",
     "keywords": ["Forest biorefinery", "Supply chain design", "Product design"]},
    {"article name": "Targeted QSPR for the prediction of the laminar burning velocity of biofuels",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.02.022",
     "publication date": "09-2010",
     "abstract": "The upcoming change from fossil to biorenewable feedstock requires, among others, the identification of new liquid fuels to be used in mobile transportation. Although several molecules similar in structure to current petroleum-based fuels have already been proposed as biofuel candidates, a targeted search should identify molecules which are tailored to the numerous requirements posed by modern combustion engines. Such molecular design approaches strongly depend on reliable predictive models which describe the relation between the fuel's molecular structure and its combustion properties. To this end, we present here a targeted quantitative structure\u2013property relation (TQSPR) for the laminar burning velocity, which is a fundamental indicator for the quality of fuels for spark ignition engines. We present the methodology and demonstrate its feasibility by the correct prediction of the laminar burning velocity of ethanol. We show that the experimental value is predicted well within measurement accuracy for various conditions.",
     "keywords": ["Targeted QSPR", "Laminar burning velocity", "Property prediction", "Biofuels", "Product design", "CAMD"]},
    {"article name": "Computationally efficient black-box modeling for feasibility analysis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.02.016",
     "publication date": "09-2010",
     "abstract": "Computational cost is a major issue in modern large-scale simulations used across different disciplines of science and engineering. Computationally efficient surrogate models that can represent the original model with desired accuracy have been explored in the recent past. However, with the exception of few efforts, most of these techniques rely on a reduced order representation of the original complex model, resulting in a loss of information. In this paper we demonstrate the applicability of high dimensional model representation (HDMR) technique in addressing this issue while preserving the original model dimension. We will discuss the applicability of this surrogate modeling technique in the field of feasibility analysis drawing examples from process systems and materials design. It will be shown that the original physical models can be essentially considered as a black box, and same methodology can be applied across all the examples studied. It is found that the accuracy of the surrogate models depends on the order of the approximation and number of sampling points employed. While first-order approximation is largely inadequate, second-order approximation is sufficient for the model systems studied. Sampling requirement is also dramatically low for the construction of these surrogate models.",
     "keywords": ["Surrogate modeling", "High dimensional model representation (HDMR)", "Feasibility analysis", "Materials design", "Failure analysis"]},
    {"article name": "Design of inter-plant water network with central and decentralized water mains",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.02.024",
     "publication date": "09-2010",
     "abstract": "This paper presents a mathematical model for the inter-plant water integration of an industrial complex that consists of a number of process plants. Opportunities for water reuse/recycle across plants are exploited. In particular, a novel integration scheme is proposed, where central and decentralized water mains are placed to interconnect the water-using units of the individual plants. The principal advantage of using water mains is to improve the overall network practicability. The model formulation is based on a series of superstructures, and the design problem is optimized according to two objectives that include the minimization of fresh water consumption and total annualized cost. An example is solved to illustrate the proposed integration scheme and approach.",
     "keywords": ["Inter-plant", "Mathematical optimization", "Process integration", "Superstructure", "Water main", "Water network"]},
    {"article name": "Process/equipment co-simulation for design and analysis of advanced energy systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.02.011",
     "publication date": "09-2010",
     "abstract": "The grand challenge facing the power and energy industries is the development of efficient, environmentally friendly, and affordable technologies for next-generation energy systems. To provide solutions for energy and the environment, the U.S. Department of Energy's (DOE) National Energy Technology Laboratory (NETL) and its research partners in industry and academia are relying increasingly on the use of sophisticated computer-aided process design and optimization tools. In this paper, we describe recent progress toward developing an Advanced Process Engineering Co-Simulator (APECS) for the high-fidelity design, analysis, and optimization of energy plants. The APECS software system combines steady-state process simulation with multiphysics-based equipment simulations, such as those based on computational fluid dynamics (CFD). These co-simulation capabilities enable design engineers to optimize overall process performance with respect to complex thermal and fluid flow phenomena arising in key plant equipment items, such as combustors, gasifiers, turbines, and carbon capture devices. In this paper we review several applications of the APECS co-simulation technology to advanced energy systems, including coal-fired energy plants with carbon capture. This paper also discusses ongoing co-simulation R&D activities and challenges in areas such as CFD-based reduced-order modeling, knowledge management, advanced analysis and optimization, and virtual plant co-simulation. Continued progress in co-simulation technology \u2013 through improved integration, solution, and deployment \u2013 will have profound positive impacts on the design and optimization of high-efficiency, near-zero emission fossil energy systems.",
     "keywords": ["Process simulation", "Computational fluid dynamics", "Co-simulation", "Virtual engineering", "Fossil energy"]},
    {"article name": "Constructing and maintaining proper process models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.02.023",
     "publication date": "09-2010",
     "abstract": "Constructing and maintaining process models for any process systems engineering activity represents a major bottleneck in today's work flow. Industry has a need for a multitude of models often for the same plant. These models must be proper with respect to the underlying concepts, in our case mainly physics. They must be internally consistent and closed. Standard model simplification should be readily available so as to derive simplified models automatically. Generating code should be automated so as to eliminate transcript errors, which leads to significant cost savings.The approach is based on network modelling here constraint to physical\u2013chemical\u2013biological systems in which the nodes in the network represent capacities and the arcs the transport of extensive quantities. The network is extended by adding control, the nodes representing information processing systems.The framework and the key ideas are being discussed. An analysis of the process of generating a coded model serves as the backbone on which we shape the new modelling software environment, that we call ProcessModeller. It consists of a part for the user using it as a modelling tool and a set of component factories. The modelling tool, to which the common user is being exposed, consists of three main components: a multi-graph editor, a semantic plug-in module for the multi-graph definition, a semantic plug-in module for the selection of the node and arc descriptions. There are four factories: the first of the specialised component factories generates an ontology, which captures the behaviour description of the nodes and the arcs. The second one generates the graphical objects and their properties. The third constructs the coded paradigm for interface control. The fourth generates stand-alone thermodynamic models, which are structured such as to enable the ontologies to use basic theoretical representation of the thermodynamic functions. Special attention is paid to the indexing problem. The framework demonstrates the power of constructing ontologies that can be coupled and integrated with a set of paradigms.",
     "keywords": ["Computer-aided modelling", "Process systems engineering"]},
    {"article name": "Design and optimization of energy efficient complex separation networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.02.008",
     "publication date": "09-2010",
     "abstract": "Separation processes account for more than half of capital and operating costs in chemical manufacturing. Separations are the energy intensive operations in the chemical industry. Rising energy consumption combined with the environmental impact increases the need for energy saving separation processes. Fortunately complex column networks have the potential for major energy savings estimated to range between 30 and 70% over simple column configuration. In this paper, a computer-aided synthesis method is introduced to synthesize optimal complex column arrangements which encode the cost and states of global solutions with minimum user input. A robust feasibility criterion helps select design and operating conditions for the entire network that can be realized in practice. Our method builds on thermodynamic transformations entitled temperature collocation which provides crucial advantages to determine the operating conditions, structure, and size of the separation network for achieving the desired product cuts. The computational approach guarantees realizable column profiles, which can be validated with industrially accepted simulation software such as AspenHysys and AspenPlus.",
     "keywords": ["Difference point equations", "Aspen validation", "Complex network", "Temperature collocation"]},
    {"article name": "Review: Important contributions in development and improvement of the heat integration techniques",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.02.038",
     "publication date": "08-2010",
     "abstract": "The chemical processes and utility industries are central issues to modern living standards. The society evolution dictates that chemical processes will need continuous development and the advantages obtained of using process integration techniques consist in process improvement, increased productivity, energy conservation, pollution prevention, and capital and operating costs reductions of chemical plants.Therefore, the aim of this paper is to present in a comprehensive review the development through the years (1975\u20132008) of the heat integration and heat exchanger network synthesis (HENS) as a technique of process integration. From an impressive amount of studies related to this topic, a selection with the studies representing the turning points and the emerging trends in developing and improving of heat integration and HENS methods was made.The relationships between domains, authors, and journals, related with the field of research, are presented in an easy understanding visual format through the diagrams provided by CiteSpace II software.",
     "keywords": ["Heat integration", "Pinch analysis", "Mathematical programming", "Knowledge domain visualization"]},
    {"article name": "A fast and accurate solver for the general rate model of column liquid chromatography",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.03.008",
     "publication date": "08-2010",
     "abstract": "Practical application of the general rate model is often hindered by computational complexity and by unknown parameter values. Model simplifications and parameter correlations are not always applicable, and repeated model solutions are required for parameter estimation and process optimization. We have hence developed and implemented a fast and accurate solver for the general rate model with various adsorption isotherms.Several state of the art scientific computing techniques have been combined for maximal solver performance: (1) the model equations are spatially discretized with finite volumes and the weighted essentially non-oscillatory (WENO) method. (2) A non-commercial solver with variable stepwidth and order is applied for time integration. (3) The internal linear solver module is replaced by customized code that is based on domain decomposition and can be executed on parallel computers.We demonstrate the speed and accuracy of our solver with numerical examples. The WENO method significantly improves accuracy even on rather coarse grids, and the solver runtime scales linearly with relevant grid sizes and processor numbers. Code parallelization is essential for utilizing the full power of multiprocessor and multicore technology in modern personal computers. The general rate model of a strongly non-linear two component system is solved in few seconds.",
     "keywords": ["Column chromatography", "General rate model", "Weighted essentially non-oscillatory", "Domain decomposition", "Parallel computing"]},
    {"article name": "Systematic development of predictive mathematical models for animal cell cultures",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.03.012",
     "publication date": "08-2010",
     "abstract": "Fed-batch cultures are used in producing monoclonal antibodies industrially. Existing protocols are developed empirically. Model-based tools aiming to improve productivity are useful with model reliability and computational demand being important. Herein, a systematic framework for developing predictive models is presented comprising of model development, global sensitivity analysis, optimal experimental design for parameter estimation, and predictive capability checking. Its efficacy and validity are demonstrated using a fed-batch structured/unstructured model of antibody-secreting hybridoma cultures. Global sensitivity analysis is first used to identify sensitive model parameters (initial values estimated from batch cultures). Information-rich data from an optimally designed fed-batch experiment are then used to estimate these parameters, resulting in good agreement between simulation and experimental results. Finally, the model's predictive capability is confirmed by comparison with an independent set of fed-batch cultures. This approach systematises the process of developing predictive cell culture models at a minimum experimental cost, enabling model-based control and optimisation.",
     "keywords": ["Mammalian cell culture", "Mathematical modelling", "Global sensitivity analysis"]},
    {"article name": "A generalized TSK model with a novel rule antecedent structure: Structure identification and parameter estimation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.01.007",
     "publication date": "08-2010",
     "abstract": "TSK fuzzy models are convenient tools for describing complex nonlinear behavior. However, the existing combinatorial antecedent structure in TSK models makes them substantially suffer from the curse of dimensionality. In this work, a novel rule antecedent structure is proposed to design an efficient generalized TSK (GTSK) model by using fewer rules. The new rule antecedent only uses nonlinear variables. Additionally, one more degree of freedom is introduced to design antecedents to cover an antecedent space more efficiently, which further reduces the number of rules. The resultant GTSK model is identified in two stages. A novel recursive estimation based on spatially rearranged data is used to determine the consequent and antecedent variables. Model parameter values are obtained from partitioned antecedent space, which is the result of solving a series of splitting and regression problems.",
     "keywords": ["Dynamic modeling", "Fuzzy", "TSK models"]},
    {"article name": "Estimation of the temporal coefficients for an empirical approximator. New approach based on the Proper Orthogonal Decomposition modes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.02.001",
     "publication date": "08-2010",
     "abstract": "A new approach to identify the temporal coefficients of an empirical approximator of a process without any knowledge of the mathematical model of the system is introduced. This approach is based on the Proper Orthogonal Decomposition method, and only a few experimental values are required to determine the empirical eigenfunction, and the temporal coefficients. The scheme studied is verified by a numerical example regarding the chemical reaction in a tubular reactor. The approach consists of the minimisation of an objective function, which is based on the sum of the square errors between the original snapshots and the values predicted from the linear combination of the empirical eigenfunctions and the time coefficients. This method is easy and fast to implement, produces the lowest deviation from the original experimental data, and is very useful when the theoretical model of the system is unknown or difficult to determine.",
     "keywords": ["Proper Orthogonal Decomposition", "Empirical eigenfunction", "Temporal coefficient"]},
    {"article name": "Optimizing the design of complex energy conversion systems by Branch and Cut",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.03.007",
     "publication date": "08-2010",
     "abstract": "The paper examines the applicability of mathematical programming methods to the simultaneous optimization of the structure and the operational parameters of a combined-cycle-based cogeneration plant. The optimization problem is formulated as a nonconvex mixed-integer nonlinear problem (MINLP) and solved by the MINLP solver LaGO. The algorithm generates a convex relaxation of the MINLP and applies a Branch and Cut algorithm to the relaxation. Numerical results for different demands for electric power and process steam are discussed and a sensitivity analysis is performed.",
     "keywords": ["Cogeneration plant", "Energy conversion system", "Design optimization", "Global optimization", "Mixed-integer nonlinear programming", "Convex relaxation"]},
    {"article name": "Improving benders decomposition using maximum feasible subsystem (MFS) cut generation strategy",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.10.002",
     "publication date": "08-2010",
     "abstract": "A new multi-generation of cuts algorithm is presented in this paper to improve the efficiency of Benders decomposition approach for the cases that optimality cuts are difficult to be achieved within the iterations of the algorithm. This strategy is referred to as maximum feasible subsystem (MFS) cut generation strategy. In this approach in each iteration of the Benders algorithm an additional cut is generated that has the property to restrict the value of the objective function of the Benders master problem. To illustrate the efficiency of the proposed strategy, it is applied to a scheduling problem of multipurpose multiproduct batch plant. Two different partitioning alternatives are tested in order to show the importance of the way that a problem is decomposed upon the efficiency of the Benders algorithm. The application of the proposed acceleration procedure results in substantial reduction of CPU solution time and the total number of iterations in both decomposition alternatives.",
     "keywords": ["Benders decomposition", "Mixed integer programming", "Multi-generation of cuts", "Maximum feasible subsystem"]},
    {"article name": "A simple heuristic for reducing the number of scenarios in two-stage stochastic programming",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.10.009",
     "publication date": "08-2010",
     "abstract": "In this work we address the problem of solving multiscenario optimization models that are deterministic equivalents of two-stage stochastic programs. We present a heuristic approximation strategy where we reduce the number of scenarios and obtain an approximation of the original multiscenario optimization problem. In this strategy, a subset of the given set of scenarios is selected based on a proposed criterion, and probabilities are assigned to the occurrence of scenarios in the reduced set. The original stochastic programming model is converted into a deterministic equivalent using the reduced set of scenarios. A mixed-integer linear program (MILP) is proposed for the reduced scenario selection. We apply this practical heuristic strategy to four numerical examples and show that reformulating and solving the stochastic program with the reduced set of scenarios yields an objective value close to the optimum of the original multiscenario problem.",
     "keywords": ["Two-stage stochastic programming", "Multiscenario model", "Scenario reduction", "Stochastic approximation"]},
    {"article name": "A chance constrained approach for a gas processing plant with uncertain feed conditions",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.03.009",
     "publication date": "08-2010",
     "abstract": "In this work, we propose a new optimization model for gas processing plant under uncertain feed flow rate and composition. Deterministic model has initially being formulated; subsequently, linear steady state chance constrained models with time-dependent uncertainties under single and joint chance constraints have been developed. The chance constrained models have then been converted to their equivalent deterministic form so as to be solved. Case study has been taken for real plant to effectively implement the proposed model. The solution from the optimization provides prior-decision for the in-operation plant to produce the desired product at certain confidence level by satisfying the process constraints. Sensitivity analysis has been made to see the effect of each uncertain feed component flows. Final computational results ratify the need of taking optimal decisions using the chance constrained approach which comprises the profitability and reliability of the process compared to deterministic, \u201cworst case\u201d, and two-stage programming approaches.",
     "keywords": ["Chance constraints", "Optimization", "Sensitivity", "Uncertainty", "HYSYS", "GAMS"]},
    {"article name": "Nonlinear model-based control of highly nonlinear processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.03.011",
     "publication date": "08-2010",
     "abstract": "This paper examines some of the deficiencies of the pole-placement self-tuning controller, particularly for the control of nonlinear processes. A comparison is made with switched multiple-model control and a local controller network, based on the local model network. The difference between the realisations of the local networks is also examined. All these approaches are applied for the control of the continuous stirred tank reactor.",
     "keywords": ["Adaptive control", "Multiple-model control", "Local model networks", "Local controller networks"]},
    {"article name": "Plantwide control system design of the benchmark vinyl acetate monomer production plant",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.11.022",
     "publication date": "08-2010",
     "abstract": "This paper addresses control system design of the vinyl acetate monomer production plant that has been introduced as a plantwide control benchmark problem by Luyben et al. Nonlinear analysis of a simplified model, which includes detailed reactor behavior and ideal separation with gas and liquid recycle, is performed to identify the optimal operating condition and to develop intuition for control system design. Control system is hierarchically constructed to realize the optimal operating condition, which is verified through simulation on a rigorous nonlinear model.",
     "keywords": ["Process control", "Plantwide control", "Benchmark problem", "Vinyl acetate process", "Recycle system"]},
    {"article name": "Analysis and control of a partially heat integrated refinery debutanizer",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.03.002",
     "publication date": "08-2010",
     "abstract": "In this contribution, the internal heat integration concept has been applied on a commercial refinery debutanizer column for the separation of an eight-component hydrocarbon mixture. Here, the thermodynamic feasibility of this process has been identified. Then, an economically interesting partially heat integrated debutanizer column (HIDBC) configuration is explored. A sensitivity test has been conducted to select the compression ratio required to meet the product specification. This study deals with the parametric analysis to investigate the effect of important parameters on product purity and energy consumption. An economic comparison between the conventional debutanizer and the proposed thermally coupled debutanizer scheme is also performed. This paper proposes a control algorithm that considers the control of most sensitive tray temperatures. The singular-value decomposition (SVD) method is used for selecting the sensitive trays. Finally, the closed-loop control performance of the HIDBC has been examined. Due to the internal energy integration, better performance is achieved with up to 44% energy saving and more than 14% saving in total annual cost.",
     "keywords": ["Partial HIDBC", "Energy saving", "Sensitivity", "Economics", "Control"]},
    {"article name": "A novel multi-dimensional visualization technique for understanding the design parameters of drug formulations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.07.002",
     "publication date": "08-2010",
     "abstract": "The quality-by-design concept is a new regulatory paradigm for pharmaceutical development, while the response surface method (RSM) is a promising approach for understanding design parameters for drug formulations. RSM aims to provide a visual image to support statistical design and analysis of experiments. However, neither contour plots nor 3D surface plots that have commonly been used can completely visualize interactions between the parameters within the design space, due to their limited dimensionality. This article presents a visualization technique that can simultaneously display the responses to multi-dimensional factors by mapping N-dimensional data onto unique x\u2013y coordinates, re-defined by recursive slice-and-dice subdivision of the 2D plane. The applicability of the technique was confirmed using published data on the design of nasal drug formulations.",
     "keywords": ["Design of experiments", "Response surface method", "Data visualization", "Formulation design", "Design space", "Nasal drug formulations"]},
    {"article name": "Aggregate models based on improved group methods for simulation and optimization of distillation systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.02.029",
     "publication date": "08-2010",
     "abstract": "This note deals with aggregate models for complex distillation systems in large-scale flowsheets. Group methods were originally devised for simple absorber and stripper calculations with no major extensions for handling distillation. In this work, group methods are systematically analyzed and further improved by modifying some of the previously proposed approximations. As a result, the improved group method exhibits accurate predictions and this is demonstrated using simulation and optimization case studies for a variety of chemical systems and operating conditions. It is observed that the prediction of output variables is in close agreement with that of the rigorous equilibrium stage model. In case of optimization problems, the optimal number of trays and feed locations differ by only one or two trays. The aggregate model can be applied in a sequence of steps in order to improve the reliability and robustness of the solution procedure. A rounding heuristic is also proposed which can provide near-optimal solutions with a significant reduction in computational time.",
     "keywords": ["Optimal distillation design", "Aggregate design models", "Nonlinear programming", "Mixed integer nonlinear programming"]},
    {"article name": "Bioprocesses: Modeling needs for process evaluation and sustainability assessment",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.03.010",
     "publication date": "07-2010",
     "abstract": "The next generation of process engineers will face a new set of challenges, with the need to devise new bioprocesses, with high selectivity for pharmaceutical manufacture, and for lower value chemicals manufacture based on renewable feedstocks. In this paper the current and predicted future roles of process system engineering and life cycle inventory and assessment in the design, development and improvement of sustainable bioprocesses are explored. The existing process systems engineering software tools will prove essential to assist this work. However, the existing tools will also require further development such that they can also be used to evaluate processes against sustainability metrics, as well as economics as an integral part of assessments. Finally, property models will also be required based on compounds not currently present in existing databases. It is clear that many new opportunities for process systems engineering will be forthcoming in the area of integrated bioprocesses.",
     "keywords": ["Bioprocesses", "Biocatalysis", "Fermentation", "Modeling", "Simulation", "Life cycle assessment (LCA)", "Environmental footprint", "Sustainability"]},
    {"article name": "Computer-aided molecular design using the Signature molecular descriptor: Application to solvent selection",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.10.017",
     "publication date": "07-2010",
     "abstract": "There is a growing demand to develop more environmentally friendly solvents to reduce costs and comply with regulation. Researchers at GlaxoSmithKline (GSK) have developed a solvent selection guide that ranks 47 frequently used solvents from 1 to 10 in five areas related to environmental compatibility. In this work, we apply a computer-aided molecular design method known as inverse design with the Signature molecular descriptor to identify additional potentially green solvents outside of GSK's list. Applying this approach is much quicker, less expensive and allows for a more comprehensive search for the most suitable candidates than working with experimental data alone. We present results for solvents with optimal predicted properties that span the classes from the 47 compounds in the GSK solvent selection guide and include several which are hybrids that cross-cut amongst classes. Additionally, our technique \u201crediscovers\u201d the known green solvent ethyl lactate through this method by combining different solvent classes.",
     "keywords": ["Computer-aided molecular design", "CAMD", "Signature", "Inverse design", "Solvent selection", "Green solvents"]},
    {"article name": "Process modeling and optimization of batch fractional distillation to increase throughput and yield in manufacture of active pharmaceutical ingredient (API)",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.03.019",
     "publication date": "07-2010",
     "abstract": "An active pharmaceutical ingredient (API) is purified by fractional batch distillation to achieve a high purity drug substance. This work is to improve the process throughput and to increase the yield using batch distillation modeling and engineering principles to better understand the process so high quality product can be produced consistently. A batch of the drug substance was produced in about 7 days by batch distillation using a packed column with approximately 32 theoretical plates. Process improvements were required to decrease the cycle time. Based on process modeling and other engineering principles, four process improvements have been made to increase the manufacture throughput. After improvements the average cycle time has been reduced to 2.5 days from an average of 7 days process originally. Furthermore, the yield is also increased. This was achieved with the original equipment. Since the changes are within the regulatory filings for the process, they are implemented quickly in commercial production.",
     "keywords": ["Fractional batch distillation", "Process modeling", "Pharmaceutical API purification"]},
    {"article name": "The use of modeling in spray drying of emulsions and suspensions accelerates formulation and process development",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.02.031",
     "publication date": "07-2010",
     "abstract": "In respiratory drug delivery, porous, lipid-based microparticles have shown performance advantages over microcrystalline particles (Hirst et al., 2002). These engineered particles are manufactured by spray drying of emulsions or suspensions. The purpose of this work is to combine an understanding of underlying mechanisms with particle characterization data to develop models in support of a quality-by-design (QbD) approach to drug development. Powders were manufactured using a laboratory scale spray dryer. A thermodynamic model was developed for the spray drying process, allowing prediction of outlet temperature and powder residual water content. A stochastic model based on a Monte-Carlo simulator was developed to analyze the effects of relevant parameters on the aerodynamic size of the particles; modeled results were verified by comparison to measured aerodynamic size distributions of spray-dried powders. These modeling techniques allowed rank-ordering of the relative importance of formulation and process variables, and aided in developing an understanding of the drying unit operation.",
     "keywords": ["Pharmaceutical spray drying", "Particle engineering: Modeling", "Quality-by-design"]},
    {"article name": "Pharmaceutical process/equipment design methodology case study: Cyclone design to optimize spray-dried-particle collection efficiency",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.04.004",
     "publication date": "07-2010",
     "abstract": "This paper describes a case study using a cyclone-design methodology to increase cyclone collection efficiency for spray-dried dispersions (SDDs) for pharmaceutical applications.The six-step methodology combines the use of classical cyclone design (CCD) correlations and computational fluid dynamics (CFD) modeling techniques. By combining these techniques, the methodology avoids the limitations inherent with the use of either technique alone and represents an improved alternative to conventional trial-and-error methods. Specifically, the methodology increases the efficiency of the design process by reducing (1) computational time; (2) experimental time (i.e., the need for numerous development runs); and (3) the use of costly active pharmaceutical ingredient (API).The case study shows how the methodology was used to quickly and accurately design a cyclone with improved performance for a desired product characteristic (improved collection efficiency for small particles rather than broad particle-size distribution). Validation data are presented demonstrating the accuracy of the approach.",
     "keywords": ["Cyclone", "Spray-drying", "Computational fluid dynamics", "Particle collection"]},
    {"article name": "Practical application of roller compaction process modeling",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.03.004",
     "publication date": "07-2010",
     "abstract": "Very limited work has been reported on comparing the performance of the roller compaction process at different scales. The majority of the approaches highlighted in the literature discuss the applicability of using confined uniaxial compaction for predicting the performance of the roller compaction process. In this paper a method was developed that allows the rolling theory of granular solids developed by Johanson [Johanson, J. R. (1965). A rolling theory for granular solids. ASME, Journal of Applied Mechanics Series E, 32(4), 842\u2013848] to be used to infer the underlying material parameters from small-scale roller compaction experiments for both separation controlled or screw controlled configurations. Once these parameters are determined, the model can be used for predictive process design and scale-up in order to achieve target outputs such as ribbon density and throughput. The peak pressure, predicted by the model, can also be used to present roller compaction of a given formulation from a scale-independent perspective. This approach can be used to justify process parameter and equipment flexibility in the context of a pharmaceutical design space based on a proven acceptable range of peak pressures, or achieving target intermediate quality attributes, such as ribbon density.",
     "keywords": ["Roller compaction", "Dry granulation", "Pharmaceutical", "Modeling", "Ribbon density", "Scale-up"]},
    {"article name": "Understanding variation in roller compaction through finite element-based process modeling",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.04.008",
     "publication date": "07-2010",
     "abstract": "One of primary goals of the Quality by Design initiative in the pharmaceutical industry is to reduce variation in the product quality through increased understanding and control of the manufacturing process. In the case of roller compaction, in which mixtures of active and inert powders are fed via a screw to counter-rotating rolls, drawn into the nip region and compacted under hydrostatic and shear stresses, variation in density of the roller compacted material has been commonly observed. In the experimental part of this work we report measurements of pressure and shear under the rolls which show variation of the local stress conditions along the width of the roll which evolves with time. Also roll pressure and shear stress appear to persist past the minimum roll separation. To further investigate the potential causes of these variations, 2D and 3D explicit finite element-based models with adaptive meshing and arbitrary Eulerian\u2013Lagrangian capabilities were developed. A Drucker-Prager/cap constitutive model was used to describe the mechanical behavior of the powder. Microcrystalline cellulose was used as the model powder. The 2D model was used to evaluate the effects of feed stress, roll friction on roll force, profiles of roll pressure and roll shear stress, nip angle and relative density of the compacted powder. The results indicated increasing feed stress, and/or increasing roll friction lead to higher maximum roll surface pressure and attendant relative density at the exit. The results may be explained by the location of the nip angle and the amount of pre-densification in the feed zone. Simulations with pressure-dependent frictional coefficients indicated significant differences in densification. In addition, oscillating feed stress conditions revealed periodic variations in roll pressures and relative densities. The 3D model predicted lower roll pressure and densities near the edges due to presence of side seal friction. Variable inflow of material along the roll width was related to variation in roll pressure. Overall, the model predictions followed experimental trends. The process modeling provided greater insight into the potential causes of the variation in density of the roller compacted material and highlighted the significance of the design of the feed system, which may be used to evaluate potential design improvements.",
     "keywords": ["Roller compaction", "Process modeling", "Finite element", "Dry granulation"]},
    {"article name": "Optimizing the design of eccentric feed hoppers for tablet presses using DEM",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.04.016",
     "publication date": "07-2010",
     "abstract": "The discharge performance in eccentrically shaped, conical hoppers is investigated using the discrete element method (DEM). While powder discharge from hoppers has been well studied, most work has focused on axisymmetric, conical hoppers and plane flow, wedge-shaped hoppers. Yet, pharmaceutical tablet presses often use eccentric hopper designs that do not fall into either of these categories. The flow performance in these hoppers is compared to that in concentrically shaped, conical hoppers. Results show the discharge rate is up to 35% greater for eccentric hoppers and shows good agreement with a modified Beverloo correlation. The flow mode during discharge (mass-flow/funnel-flow) is similar for each hopper shape. Finally, the extent of segregation over time is comparable but a significant composition gradient exists across the outlet of eccentric hoppers. This has implications for processes where the discharge from an eccentric hopper is split downstream, such as the bifurcation of the powder feed to a double-sided tablet press.",
     "keywords": ["Hopper discharge", "Hopper shape", "Segregation", "Mass flow", "Flow rate", "Discrete element method"]},
    {"article name": "Temperature and density evolution during compaction of a capsule shaped tablet",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.04.012",
     "publication date": "07-2010",
     "abstract": "This paper continues the effort to validate the finite element based analysis of tableting. A three-dimensional capsule shaped tablet is studied using a Drucker\u2013Prager cap constitutive model and a coupled thermomechanical analysis. Predictions of the model for the internal distribution of porosity and temperature are found to be in agreement with X-ray microtomography measurements of relative density and infrared camera measurements of surface temperatures of the ejected tablet. The fact that the model is calibrated using data from a cylindrical flat-faced tablet which can predict the compaction of a completely different shape proves the capability offered by finite element analysis. In addition, this method demonstrates the role it can play in the optimization of tableting operations in the spirit of quality by design principles.",
     "keywords": ["Tableting", "Temperature", "Compaction", "Finite element simulation", "Density distribution"]},
    {"article name": "Drug product modeling predictions for scale-up of tablet film coating\u2014A quality by design approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.03.006",
     "publication date": "07-2010",
     "abstract": "This work describes the use of two fundamental tablet film coating models in a quality by design (QbD) approach to determine the operating parameters for scale-up of Varenicline IR (Chantix\u00ae/Champix\u00ae) tablet film coating. The models were used in this study to establish an acceptable range of process parameters in a previously unqualified film coater to match the proven acceptable range (PAR) of operating conditions in the existing coaters. The established process parameters were then used to prioritize the experimental design to minimize the number of required trial runs and shift the focus to optimization rather than validation. Based on the model predictions, the following equipment set points were determined: (1) upper and lower inlet air temperatures of 75\u00a0\u00b0C and 65\u00a0\u00b0C, respectively, to maintain environmental similarity during film coating, and (2) atomization air pressure of 3.0\u00a0bar at the lower spray rate condition and 3.5\u00a0bar at the upper spray rate condition to match the coating solution droplet diameter. The recommendations were provided to the commercial site to guide the design of their scale-up trials. The results from the actual trials were compared against the corresponding model predictions to assess the validity of the guidance provided based on the two models. The use of these theoretical models provides a scientific basis for establishing new operating parameters during scale-up of the film coating process.",
     "keywords": ["Quality by design", "Tablet film coating", "Scale-up", "Thermodynamics", "Atomization", "Process models"]},
    {"article name": "Handling uncertainty in the establishment of a design space for the manufacture of a pharmaceutical product",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.02.027",
     "publication date": "07-2010",
     "abstract": "Recent trends in the pharmaceutical sector are changing the way processes are designed and executed, moving from: allowing the process to operate in a fixed point, to: allowing a permissible region in the operating space (a.k.a. design-space). This trend is driving product development to design quality into the manufacturing process (Quality by Design) and not to rely solely on testing quality in the product. These changes address the presence of uncertainties entering the process and their negative effect on product quality if the operating conditions are not free to compensate for these disturbances. This work provides a review of methods to address the presence of these uncertainties, ranging from the establishment of multivariate specifications for incoming product, to feed-forward process control. It also presents the development of a feed-forward controller to compensate the process for the observed changes in the properties of the incoming material. The development and testing of this controller is illustrated with a wet-granulation process. The performance of the multiple control strategies is back-propagated to the changes in the acceptance regions for the raw materials. Ultimately, the controller and its model are used to define an integrated design-space that accounts for the network of complex relationships between materials, process conditions and product. This work proposes and demonstrates that the use of mathematics is the only way to reach the maximum potential of the Quality by Design by providing a tool to specify an integral Design Space.",
     "keywords": ["Design space", "Quality by Design", "Latent variable methods", "Uncertainty", "Pharmaceuticals", "Partial least squares"]},
    {"article name": "ICAS-PAT: A software for design, analysis and validation of PAT systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.06.021",
     "publication date": "07-2010",
     "abstract": "In chemicals based product manufacturing, as in pharmaceutical, food and agrochemical industries, efficient and consistent process monitoring and analysis systems (PAT systems) have a very important role. These PAT systems ensure that the chemicals based product is manufactured with the specified end product qualities. In an earlier article, Singh et al. [Singh, R., Gernaey, K. V., Gani, R. (2009). Model-based computer-aided framework for design of process monitoring and analysis systems. Computers & Chemical Engineering, 33, 22\u201342] proposed the use of a systematic model and data based methodology to design appropriate PAT systems. This methodology has now been implemented into a systematic computer-aided framework to develop a software (ICAS-PAT) for design, validation and analysis of PAT systems. Two supporting tools needed by ICAS-PAT have also been developed: a knowledge base (consisting of process knowledge as well as knowledge on measurement methods and tools) and a generic model library (consisting of process operational models). Through a tablet manufacturing process example, the application of ICAS-PAT is illustrated, highlighting as well, the main features of the software.",
     "keywords": ["DOF degree of freedom", "degree of freedom", "FDA U.S. Food and Drug Administration", "U.S. Food and Drug Administration", "ICAS integrated computer-aided system", "integrated computer-aided system", "MoT modeling testbed", "modeling testbed", "PAT process analytical technology", "process analytical technology", "PI proportional integral", "proportional integral", "rpm revolutions per minute", "revolutions per minute", "Process monitoring", "Quality control", "PAT", "Software", "Pharmaceutical tablet"]},
    {"article name": "An ontological knowledge-based system for the selection of process monitoring and analysis tools",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.04.011",
     "publication date": "07-2010",
     "abstract": "Efficient process monitoring and analysis tools provide the means for automated supervision and control of manufacturing plants and therefore play an important role in plant safety, process control and assurance of end product quality. The availability of a large number of different process monitoring and analysis tools for a wide range of operations has made their selection a difficult, time consuming and challenging task. Therefore, an efficient and systematic knowledge base coupled with an inference system is necessary to support the optimal selection of process monitoring and analysis tools, satisfying the process and user constraints. A knowledge base consisting of the process knowledge as well as knowledge on measurement methods and tools has been developed. An ontology has been designed for knowledge representation and management. The developed knowledge base has a dual feature. On the one hand, it facilitates the selection of proper monitoring and analysis tools for a given application or process. On the other hand, it permits the identification of potential applications for a given monitoring technique or tool. An efficient inference system based on forward as well as reverse search procedures has been developed to retrieve the data/information stored in the knowledge base.",
     "keywords": ["Knowledge base", "Ontology", "Process monitoring", "Sensor", "PAT", "Inference system"]},
    {"article name": "An ontological framework for automated regulatory compliance in pharmaceutical manufacturing",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.09.004",
     "publication date": "07-2010",
     "abstract": "Pharmaceutical manufacture is one of the most tightly legislated industries today. The industry is constantly challenged to meet the rising standards of manufacturing quality and safety through rigorous regulatory requirements. As a consequence, ensuring regulatory compliance and managing a myriad of validation documents constitutes a major informatics challenge. This study addresses this challenge by developing an ontological infrastructure to support decision making in regulatory compliance. The proposed ontological informatics system, called OntoReg, is integrated with a reasoner and a rule engine through a Java integrated development environment. The system is demonstrated through industrial case studies based on regulation examples taken from the Eudralex Guide 2007.",
     "keywords": ["Decision support systems", "Pharmaceutical manufacturing", "Regulatory compliance", "Ontologies", "Informatics"]},
    {"article name": "Integrative chemical product design strategies: Reflecting industry trends and challenges",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.02.039",
     "publication date": "06-2010",
     "abstract": "A review of integrative product design strategies is motivated by current trends and challenges faced by the chemical processing industry. The transition in the chemical process industry towards more complex formulated and structured products challenges existing approaches and scientific tools that are well suited for bulk chemical design and properties estimation (Charpentier and McKenna, 2004, Favre et al., 2002). Moreover, the ensuing market challenges, brought on by dominant global trends, demand efficient product design approaches that seek to balance technical specifications and market requirements with the business performance objectives. Integrative approaches to product design are therefore repositioned as useful strategies that simultaneously enhance technical performance and efficiency in design execution. The discussion of the integrative product design strategies is supported by findings of a recent industry benchmark study involving 15 chemical manufacturers.",
     "keywords": ["Chemical product design", "Integrative design strategies"]},
    {"article name": "Development of a thermodynamically consistent kinetic model for reactions in the solid oxide fuel cell",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.12.007",
     "publication date": "06-2010",
     "abstract": "The parameter estimation using the traditional kinetic modeling of complex reaction systems will give incorrect results if the reaction mechanism contains a loop. In this work, a thermodynamically consistent kinetic model of the anodic electrochemical hydrogen oxidation reaction mechanism of a solid oxide fuel cell (SOFC) is formulated. An iterative algorithm for estimating the reaction rate constants using the thermodynamically consistent model formulation is developed. The kinetic parameters estimated using the proposed method gives a better fit to the experimental data. Using the concept of \u2018Degree of rate control\u2019 it is found that the surface reactions may have a greater role in deciding the overall rate. The proposed iterative parameter estimation algorithm developed in this work can also be adapted to other complex chemical and biochemical reaction networks for which the reaction rate constants need to be estimated using the experimental data.",
     "keywords": ["Parameter estimation", "Thermodynamic consistency", "Reaction kinetics", "Solid oxide fuel cell"]},
    {"article name": "Computational analysis of transitional air flow through packed columns of spheres using the finite volume technique",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.10.013",
     "publication date": "06-2010",
     "abstract": "We compare computational simulations of the flow of air through a packed column containing spherical particles with experimental and theoretical results for equivalent beds. The column contained 160 spherical particles at an aspect ratio N = 7.14 , and the experiments and simulations were carried out at particle Reynolds numbers of ( R e d P = 700 \u2212 5000 ) . Experimental measurements were taken of the pressure drop across the column and compared with the correlation of Reichelt (1972) using the fitted coefficients of Eisfeld and Schnitzlein (2001). An equivalent computational domain was prepared using Monte Carlo packing, from which computational meshes were generated and analysed in detail. Computational fluid dynamics calculations of the air flow through the simulated bed was then performed using the finite volume technique. Results for pressure drop across the column were found to correlate strongly with the experimental data and the literature correlation. The flow structure through the bed was also analysed in detail.",
     "keywords": ["Packed bed", "Porous media", "Fluid mechanics", "Computational fluid dynamics", "Simulation"]},
    {"article name": "On the optimal numerical time integration for Lagrangian DEM within implicit flow solvers",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.10.003",
     "publication date": "06-2010",
     "abstract": "We investigate a selection of nominally first, second and fourth order time integration schemes with application to particle collision simulation using the discrete element method (DEM). The motivation being the typical requirement to efficiently two-way couple a continuum flow obtained with a finite volume solver employing an iterative implicit solution method to Lagrangian DEM. Using the linear force model to simulate particle repulsion, the actual order of accuracy with respect to initial separation (\u2018free motion\u2019), timestep, stiffness, damping and impact velocity is investigated. Due to the discontinuities of the inter-particle repulsive force upon contact, we find that without damping, the numerical schemes tested are generally limited to second order accuracy. The addition of damping can reduce actual order of accuracy further depending on the inter-particle free motion. This finding is compared against a continual interaction case (without free motion) where it is found that the expected higher order accuracy is recovered.",
     "keywords": ["Discrete element method", "Numerical integration"]},
    {"article name": "MPEC strategies for cost optimization of pipeline operations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.07.012",
     "publication date": "06-2010",
     "abstract": "This study develops a mathematical program with equilibrium constraints (MPECs) approach for efficient operation of gas pipelines. The resulting model handles time dependent operations in order to determine minimum energy consumption and operating cost over a given time horizon. The MPEC structure also allows flow reversals, flow transitions and other nonsmooth elements to be incorporated within the approach. Applied to industrial gas pipelines, this approach can also deal with customer demand satisfaction in the presence of compressor outages and minimize recovery time for systems that are unable to meet customer demands at all times. A large-scale oxygen pipeline case study is considered to demonstrate this approach and complex energy pricing schemes are also applied to this problem. These schemes include time of day electricity pricing, along with extensions to Real Time Pricing and Day Ahead Pricing. Compared to flat rate and minimum energy optimizations, respectively, we observe operating cost savings up to 5.13% for time of day electricity pricing and up to 12.85% for Real Time Pricing.",
     "keywords": ["Pipeline optimization", "Electricity pricing", "Dynamic optimization", "Complementarity", "MPEC", "Nonlinear programming"]},
    {"article name": "Strengthening of lower bounds in the global optimization of Bilinear and Concave Generalized Disjunctive Programs",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.10.016",
     "publication date": "06-2010",
     "abstract": "This paper is concerned with global optimization of Bilinear and Concave Generalized Disjunctive Programs. A major objective is to propose a procedure to find relaxations that yield strong lower bounds. We first present a general framework for obtaining a hierarchy of linear relaxations for nonconvex Generalized Disjunctive Programs (GDP). This framework combines linear relaxation strategies proposed in the literature for nonconvex MINLPs with the results of the work by Sawaya and Grossmann (2009) for Linear GDPs. We further exploit the theory behind Disjunctive Programming by proposing several rules to guide more efficiently the generation of relaxations by considering the particular structure of the problems. Finally, we show through a set of numerical examples that these new relaxations can substantially strengthen the lower bounds for the global optimum, often leading to a significant reduction of the number of nodes when used within a spatial branch and bound framework.",
     "keywords": ["Disjunctive Programming", "Mixed-integer nonlinear programming", "Global optimization", "Relaxations"]},
    {"article name": "Energy targeting and minimum energy distillation column sequences",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.10.006",
     "publication date": "06-2010",
     "abstract": "The determination of distillation column configurations that consume the least total energy is studied. The novel contributions of the proposed design methodology for finding global minimum energy column sequences presented in this article include: (1) the definition of a total stripping line distance function for any sequence, (2) a robust energy targeting strategy that provides a continuously differentiable description of column sequences, (3) the flexibility to use any phase equilibrium model, (4) the ability to find column sequences that contain non-pinched, minimum energy columns within a sequence, and (5) the ability to include heat integration. The proposed energy targeting approach, which is used in conjunction with the two-level design methodology of Amale & Lucia (2008b), is shown to be a reliable and effective tool for finding minimum energy distillation column sequences. A number of example problems are presented to show the efficacy of the proposed design methodology.",
     "keywords": ["Distillation", "Column sequences", "Energy targeting", "Two-level design method", "Total stripping line distance"]},
    {"article name": "An interactive multi-objective approach to heat exchanger network synthesis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.01.002",
     "publication date": "06-2010",
     "abstract": "In this work we present a multi-objective approach to heat exchanger network synthesis. The approach solves a modified version of the Synheat model using an interactive multi-objective optimisation method, NIMBUS, which is implemented in GAMS. The results obtained demonstrate the potential of interactive multi-objective optimisation.",
     "keywords": ["Pareto optimality", "Synheat model", "Multi-objective optimisation", "MINLP", "NIMBUS", "GAMS"]},
    {"article name": "An automatic initialization procedure in parameter estimation problems with parameter-affine dynamic models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.10.020",
     "publication date": "06-2010",
     "abstract": "This paper proposes an initialization approach for parameter estimation problems (PEPs) involving parameter-affine dynamic models. By using the state measurements, the nonconvex PEP is modified such that a convex approximation to the original PEP is obtained. The modified problem is solved by convex optimization methods yielding an approximate solution to the original PEP. The approximate solution can be further refined by linearizing the original problem around the obtained minimum. An assessment of the distance between the real solution and the one provided by the linearization of the problem around the convex approximation is presented. The optimum obtained by the convex approximation is used to subsequently initialize a simultaneous Gauss\u2013Newton (SGN) approach on the original nonconvex PEP. Comparative results for the SGN with arbitrary initialization and with the proposed approach are presented using three benchmark examples in the chemical and biological fields.",
     "keywords": ["Parameter estimation", "Multiple shooting", "Convex optimization"]},
    {"article name": "POD-based observer for estimation in Navier\u2013Stokes flow",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.12.001",
     "publication date": "06-2010",
     "abstract": "In this paper, we propose a POD-based technique that is suitable for the design of reliable observers for the estimation of velocity field and contaminant flow for Navier\u2013Stokes flow. POD modes are constructed using the method snapshot. Karhunen\u2013Loeve (Galerkin) projection to develop a reduced-order model obtained by projecting the velocity field onto the most important POD modes. The resulting finite-dimensional dynamical system is suitable for the design of nonlinear observers. The estimate of the velocity field is then used to estimate the concentration field of a contaminant from the 2D advection\u2013diffusion equation. The prime application considered is the estimation of airflow and contaminant flow in building systems. A 2D simulation example is provided to demonstrate the applicability of the technique.",
     "keywords": ["Model reduction", "Proper orthogonal decomposition", "Building systems"]},
    {"article name": "Modelling and simulation of a continuous process with feedback control and pulse feeding",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.09.002",
     "publication date": "06-2010",
     "abstract": "This paper deals with feedback control of a microorganism continuous culture process with pulse dosage supply of substrate and removal of products. By the analysis of the dynamic properties and numerical simulation of the continuous process, the conditions are obtained for the existence and stability of positive period-1 solution of the system. It is also pointed out that there does not exist positive period-2 solution. The results simplify the choice of suitable operating conditions for continuous culture systems. It also gives the complete expression of the period of the positive period-1 solution, which provides the precise feeding period for a regularly continuous culture system to achieve the same stable output as a continuous culture system with feedback control in the same production environment.",
     "keywords": ["Continuous culture", "Feedback control", "State impulsive", "Variable biomass yield"]},
    {"article name": "Robust planning of multisite refinery networks: Optimization under uncertainty",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.02.032",
     "publication date": "06-2010",
     "abstract": "This paper considers the problem of multisite integration and coordination strategies within a network of petroleum refineries under uncertainty and using robust optimization techniques. The framework of simultaneous analysis of process network integration, originally proposed by Al-Qahtani & Elkamel [Al-Qahtani, K., & Elkamel, A. (2008). Multisite facility network integration design and coordination: An application to the refining industry. Computers & Chemical Engineering, 32, 2198], is extended to account for uncertainty in model parameters. Robustness is analyzed based on both model robustness and solution robustness, where each measure is assigned a scaling factor to analyze the sensitivity of the refinery plan and integration network due to variations. Parameters uncertainty considered include coefficients of the objective function and right-hand-side parameters in the inequality constraints. The proposed method makes use of the sample average approximation (SAA) method with statistical bounding techniques. The proposed model was tested on two industrial-scale studies of a single refinery and a network of complex refineries. Modeling uncertainty in the process parameters provided a practical perspective of this type of problems in the chemical industry where benefits not only appear in terms of economic considerations, but also in terms of process flexibility.",
     "keywords": ["Planning under uncertainty", "Robust optimization", "Multisite coordination"]},
    {"article name": "Production planning and scheduling integration through augmented Lagrangian optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.11.016",
     "publication date": "06-2010",
     "abstract": "To improve the quality of decision making in the process operations, it is essential to implement integrated planning and scheduling optimization. Major challenge for the integration lies in that the corresponding optimization problem is generally hard to solve because of the intractable model size. In this paper, augmented Lagrangian method is applied to solve the full-space integration problem which takes a block angular structure. To resolve the non-separability issue in the augmented Lagrangian relaxation, we study the traditional method which approximates the cross-product term through linearization and also propose a new decomposition strategy based on two-level optimization. The results from case study show that the augmented Lagrangian method is effective in solving the large integration problem and generating a feasible solution. Furthermore, the proposed decomposition strategy based on two-level optimization can get better feasible solution than the traditional linearization method.",
     "keywords": ["Planning and scheduling integration", "Decomposition method", "Augmented Lagrangian relaxation"]},
    {"article name": "Combined property clustering and GC+ techniques for process and product design",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.12.005",
     "publication date": "05-2010",
     "abstract": "Recent developments in the area of process and product integration have enabled the systematic identification of suitable candidate molecules to meet certain process performance. In this approach, the property targets for the input molecules corresponding to the optimum process performance have been identified in the first step and the molecules that have the target properties have been designed in the next step. The focus of this work is to develop a combined property clustering and GC+ algorithm to identify molecules that meet the property targets identified during the process design stage. In our earlier works, a methodology was introduced for identifying molecules with a given set of properties by combining property clustering and group contribution methods. Yet, there are situations when the property contributions of some of the molecular groups of interest are not available in literature. To address this limitation, an algorithm has been developed to include the property contributions predicted by combined group contribution and connectivity indices methods into the cluster space. For the design of simple monofunctional molecules, a modified visual approach has been used, while for the design of more complicated structures an algebraic method has been developed. The applicability of the algebraic method has been increased by including the property contributions from second and third order groups.",
     "keywords": ["Molecular design", "Group contribution", "Connectivity indices", "Property clustering"]},
    {"article name": "Modelling and experimental validation of emulsification processes in continuous rotor\u2013stator units",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.01.017",
     "publication date": "05-2010",
     "abstract": "Despite the wide range of industrial applications of structured emulsions, current approaches toward process design and scale-up are commonly based on trial-and-error experimentation. As this design approach is foreseen to deliver most likely suboptimal process solutions, we propose in this contribution a model-based approach as the way forward to designing manufacturing processes of structured emulsions. In this context, process modelling and simulation techniques are applied to predict production rates and equipment sizing. Moreover, sensitivity analysis of the process model provides insight about potential bottlenecks in the process.",
     "keywords": ["Emulsification", "Dynamic", "Modelling", "Rotor\u2013stator"]},
    {"article name": "Medium-term planning of a multiproduct batch plant under evolving multi-period multi-uncertainty by means of a moving horizon strategy",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.01.013",
     "publication date": "05-2010",
     "abstract": "In this contribution, a moving horizon strategy (MHS) based on two-stage stochastic mixed-integer linear programming with recourse (2S-MILP) for medium-term planning under multi-period multi-uncertainty (MPMU) (demand, plant capacity and yields uncertainties) is proposed. A dynamic 2S-MILP formulation is introduced to consider the evolution of the uncertainties in medium-term planning. The moving horizon strategy with explicit uncertainty model is applied to the medium-term planning of a polymer production plant with batch and continuous production steps where different amounts of final products are delivered by each batch according to the chosen recipes and the products can be assigned to different demands. The 2S-MILP results are compared to the results obtained from planning using the expected values of the uncertain parameters (EEV).",
     "keywords": ["Medium-term planning", "Two-stage stochastic mixed-integer linear programming", "Moving horizon strategy"]},
    {"article name": "Planning and scheduling of salt harvest in solar evaporation ponds",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.09.008",
     "publication date": "05-2010",
     "abstract": "Several chemicals are produced from brines by solar crystallization using solar ponds. The process of salt harvest consists on mechanically retiring the salts precipitated in the solar evaporation ponds and to leave them in their respective stockpile. In an industrial operation several ponds are used for the salt crystallization, and hence the harvest planning can be a nontrivial task. Therefore, the objective of this work is to plan the feeding flow to each of the solar ponds, the manipulation of brine solution between ponds, brine and solids inventories in each pond that maximizes the production and the harvest periods. All this having as input data the evaporation rate, feed concentration, pond concentration range, and the operational initial conditions. The presented model corresponds to a MINLP, which includes the mass balances in each pond, equilibrium conditions, and planning and operational restrictions. The problem was solved in two steps: firstly, the maximization of the salt harvest was determined and then, using this maximum harvest, the maximum availability of the contractor was determined. Several cases have been studied, including: ternary (NaNO3\u2013KNO3\u2013H2O) and quaternary systems (KCl\u2013KNO3\u2013K2SO4\u2013H2O), pond systems with 3 and 4 ponds, and considering 12 and 26 operation periods per year.",
     "keywords": ["Solar evaporation ponds", "Crystallization", "Solar salt harvest"]},
    {"article name": "A semi-supervised approach to fault diagnosis for chemical processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.12.008",
     "publication date": "05-2010",
     "abstract": "A new methodology for improving the performance of fault diagnosis systems (FDS) has been proposed by combining both supervised and unsupervised learning methods. Within this framework, different techniques have been applied, such as Independent Component Analysis (ICA) as feature extraction method, Gaussian Mixture Models (GMM) with Bayesian Information Criterion (BIC) for unsupervised clustering and Support Vector Machines (SVM) for the classification steps. Since supervised learning may fail to fully discriminate some individual faults, the algorithm presented allows the unsupervised grouping of some critical faults (classes) having a diagnosis performance below a threshold defined by the user. Next, an additional classification step provides practical information for decision-making in terms of the quantitative confidence on the occurrence of one fault (or some) among a known reduced subset. The methodology presented was assessed on the Tennessee Eastman Process (TEP) benchmark. The whole set of the TEP faults was considered and an improved diagnosis performance was obtained for all of them, including those faults (3, 9 and 15) whose diagnosis had hardly been addressed previously. These results demonstrate the enhanced capability of this method and the promising potential for the diagnosis of industrial applications.",
     "keywords": ["FDS fault diagnosis system", "fault diagnosis system", "AEM abnormal event management", "abnormal event management", "SVM Support Vector Machines", "Support Vector Machines", "TEP Tennessee Eastman Process", "Tennessee Eastman Process", "mL monolabel", "monolabel", "ML multilabel", "multilabel", "PCA principal component analysis", "principal component analysis", "ICA Independent Component Analysis", "Independent Component Analysis", "GMM Gaussian Mixture Models", "Gaussian Mixture Models", "BIC Bayesian Information Criterion", "Bayesian Information Criterion", "Fault diagnosis", "Tennessee Eastman", "SVM", "ICA", "GMM", "BIC"]},
    {"article name": "Advanced control for fuel cells connected to a DC/DC converter and an electric motor",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.11.001",
     "publication date": "05-2010",
     "abstract": "The transient behavior of a polymer electrolyte membrane fuel cells (PEMFC) system under an improved adaptive predictive control with robust filter (APCWRF) is analyzed using a nonlinear dynamic, control-oriented model. Sudden changes in the stack current are associated with the abrupt changes in the power demanded by the electric motor of a vehicle, powered by the PEMFC. The APCWRF is designed for controlling the compressor motor voltage. Because of the wide working range the control algorithm is improved accounting three different zones supported by three nominal models. It is specially thought to achieve a better efficiency and to maintain the necessary level of the oxygen in the cathode to prevent short circuit and membrane damage. A DC/DC converter is connected to the electric motor. It is used as an actuator in a cascade control loop to regulate the torque output of a DC electric motor with a PI controller in the external loop. Several results are presented considering the PEMFC with the APCWRF showing its potentiality for a wide working range imposed by two types of DC motors.",
     "keywords": ["Fuel cell", "Predictive adaptive robust control", "DC/DC converter", "Electric motor"]},
    {"article name": "Dynamic conceptual design of industrial processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.01.004",
     "publication date": "05-2010",
     "abstract": "This paper focuses on chemical process design and layout optimization based on a theoretical approach that takes into account the dynamic feature of price/cost fluctuations. As originally supposed, conceptual and systematic process design consider fixed prices and costs as input data in their models. These models do not account for well-known price and cost fluctuations such as electric energy and crude oil prices. The conventional modeling approach to conceptual design can find a sub-optimal solution because of neglecting the dynamic changes of economic terms within a given time horizon. This manuscript modifies the perspective, and considers the fluctuations of prices and costs within the conceptual design activity, in order to maximize a so-called economic potential. The term fluctuation refers to the deterministic variations of prices and costs while it does not quantify the stochastic oscillations of the market. These oscillations are bound to the mood of investors and require a forecasting approach, which is typical of forms of investment such as the exchange-traded derivatives. A straightforward case study, based on an energy intensive chemical process, shows the benefits and the opportunities of this approach. The mathematical model is based on the implementation of dynamic superstructures, which call for a MINLP formulation. Finally, the manuscript presents and discusses some numerical results and the economic benefits coming from the dynamic approach to conceptual design.",
     "keywords": ["Dynamic conceptual design", "Dynamic superstructures", "Price and cost fluctuations", "Market uncertainties", "Economic potential", "Process design"]},
    {"article name": "Towards an ontological infrastructure for chemical batch process management",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.12.009",
     "publication date": "05-2010",
     "abstract": "A crucial step for batch process improvement and optimization is to develop information structures that streamline data gathering and, above all, are capable of integrating transactional data into a system using the analytical tools that are developed. Current trends in electronics, computer science, artificial intelligence and control system technology are providing technical capability that greatly facilitates the development of multilevel decision-making support. In this paper, we present the batch process ontology (BaPrOn), wherein different concepts regarding batch processes are categorized and the relationships between them are examined and structured in accordance with ANSI/ISA-88 standards, which provide a solid and transparent framework for integrating batch-related information. This paper also focuses on systematic integration of different actors within the control process. The proposed approach bases the conceptualization through the ANSI/ISA-88 representation, providing the advantage of establishing a more general conceptualization of the batch process domain. The capabilities of the envisaged ontological framework were assessed in a test bed PROCEL pilot plant: scheduling-monitoring and control-rescheduling was closed, information quality was accessed by knowledge description, and an optimum decision-making task was performed. The ontological structure can be extended in the future to incorporate other hierarchical levels and their respective modeling knowledge.",
     "keywords": ["Ontology", "Knowledge representation", "Knowledge sharing", "Batch process", "Decision-support systems"]},
    {"article name": "Integration of process design and controller design for chemical processes using model-based methodology",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.01.016",
     "publication date": "05-2010",
     "abstract": "In this paper, a novel systematic model-based methodology for performing integrated process design and controller design (IPDC) for chemical processes is presented. The methodology uses a decomposition method to solve the IPDC typically formulated as a mathematical programming (optimization with constraints) problem. Accordingly the optimization problem is decomposed into four sub-problems: (i) pre-analysis, (ii) design analysis, (iii) controller design analysis, and (iv) final selection and verification, which are relatively easier to solve. The methodology makes use of thermodynamic-process insights and the reverse design approach to arrive at the final process design\u2013controller design decisions. The developed methodology is illustrated through the design of: (a) a single reactor, (b) a single separator, and (c) a reactor\u2013separator-recycle system and shown to provide effective solutions that satisfy design, control and cost criteria. The advantage of the proposed methodology is that it is systematic, makes use of thermodynamic-process knowledge and provides valuable insights to the solution of IPDC problems in chemical engineering practice.",
     "keywords": ["Model-based methodology", "Process design", "Controller design", "Decomposition method", "Graphical method", "Integration"]},
    {"article name": "Solution of separation-network synthesis problems by the P-graph methodology",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.01.019",
     "publication date": "05-2010",
     "abstract": "The current work demonstrates that separation-network synthesis (SNS) problems can be transformed into process-network synthesis (PNS) problems: the SNS problems constitute a particular class of PNS problems. Thus, the transformed SNS problems are solvable by resorting to the P-graph methodology originally introduced for the PNS problems. The methodology has been unequivocally proven to be inordinately effective.",
     "keywords": ["Separation-network synthesis", "Optimization", "P-graphs", "Process-network synthesis", "Mathematical modeling"]},
    {"article name": "Recent advances in multiparametric nonlinear programming",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.10.012",
     "publication date": "05-2010",
     "abstract": "In this paper, we present recent developments in multiparametric nonlinear programming. For the case of convex problems, we highlight key issues regarding the full characterization of the parametric solution space and we discuss, through an illustrative example problem, four alternative state-of-the-art multiparametric nonlinear programming algorithms. We also identify a number of main challenges for the non-convex case and highlight future research directions.",
     "keywords": ["Multiparametric programming", "Multiparametric nonlinear programming", "Convex problems", "Non-convex problems"]},
    {"article name": "Tight, efficient bounds on the solutions of chemical kinetics models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.11.021",
     "publication date": "05-2010",
     "abstract": "An efficient method is presented for computing time-varying, component-wise bounds on the solutions of chemical kinetics models subject to an interval of permissible parameters. The model may be any system of ordinary differential equations whose right-hand side may be written as a stoichiometric matrix premultiplying a vector of potentially nonlinear rate functions, and model parameters may include initial conditions, kinetic rate coefficients, controls, disturbances, etc. The method presented differs from other bounding methods in that it takes advantage of the special structure of chemical kinetics models, including known physical bounds and the presence of affine reaction invariants. The method is demonstrated for several case studies, and it is shown that the resulting bounds are nearly always significantly tighter than those computed by a similar method which does not take reaction invariants into account. Finally, the additional computational cost of taking reaction invariants into consideration is negligible.",
     "keywords": ["34A40", "65G40", "80A30", "Differential inequalities", "Reaction invariants", "Reaction kinetics"]},
    {"article name": "Model simplification and time-scale assumptions applied to distillation modelling",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.11.002",
     "publication date": "05-2010",
     "abstract": "A generic industrial plant (or section of plant) can be abstracted as a set of capacities exchanging extensive quantities through connecting streams. This abstraction is applicable with different granularities, focusing on smaller or bigger control volumes according on how well the dynamics of the process must be studied. The studied system can be represented as a directed graph, where the capacities are the nodes and the streams are the arcs. The graph of the plant (or of the single operation unit) is a schematisation of the process; the model can then be written as a system of differential and algebraic equations to be solved with a numerical solver. Here we discuss how one can simplify a detailed distillation model. In the studied detailed distillation model, each distillation stage is a dynamic rate-based flash. A cascade of simplified models is accomplished by a systematic procedure that combines singular perturbation and lumping.",
     "keywords": ["Graph theory", "Network modelling", "First-principle models", "Model simplification", "Distillation"]},
    {"article name": "Investigation of combined DOC and NSRC diesel car exhaust catalysts",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.01.003",
     "publication date": "05-2010",
     "abstract": "The system of two monolithic catalytic reactors \u2013 diesel oxidation catalyst (DOC) and NOx storage and reduction catalyst (NSRC) \u2013 is studied. The catalysts are used in series for the conversion of exhaust gases from automobiles with Diesel engines (or other lean-burn engines). Several tens of catalytic reactions are considered in the description by a model based on a system of non-linear partial differential equations (mass and enthalpy ballances). A simulation study is performed to examine the effects of parameters determining the performance of the upstream-located DOC (size and effective heat capacity of the monolith, catalytic activity in individual reaction steps) on the performance of the NSRC in the European driving cycle (NEDC). Effective numerical methods used for the solution of the resulting non-linear PDEs are described. Results of the maximization of the overall NOx conversion over the NEDC for several DOC and NSRC reactor configurations and different regeneration strategies are presented. It is shown, that the DOC may have a positive effect on the overall NOx conversion over the NEDC. Regarding NOx conversion over the NEDC, the effect of the NOx storage capacity of the NSRC is significant.",
     "keywords": ["DOC diesel oxidation catalyst", "diesel oxidation catalyst", "GHSV gas hourly space velocity (h\u22121)", "gas hourly space velocity (h\u22121)", "HC hydrocarbons", "hydrocarbons", "NM noble metals", "noble metals", "NSRC NOx storage and reduction catalyst", "NOx storage and reduction catalyst", "SR steam reforming", "steam reforming", "WGS water gas shift", "water gas shift", "NSRC", "DOC", "Catalyst", "Optimization", "Fuel consumption"]},
    {"article name": "Axial dispersion/population balance model of heat transfer in gas\u2013solid turbulent fluidized beds",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.01.014",
     "publication date": "05-2010",
     "abstract": "An axial dispersion/population balance model is presented for describing heat transfer processes in gas\u2013solid turbulent fluidized beds. In the model, the gas and particle transport is described by the axial dispersion model, while the particle\u2013particle and particle\u2013wall heat transfers are modeled as collisional random events, characterized by the collision frequencies and random variables with probability density functions determined on interval [0,1]. An infinite hierarchy of moment equations is derived from the population balance equation, which can be closed at any order of moments. The properties of the model and the effects of process parameters are examined by numerical experimentation.",
     "keywords": ["Turbulent fluidization", "Heat transfer", "Axial dispersion/population balance model", "Moment equations hierarchy", "Simulation"]},
    {"article name": "The role of Computer Aided Process Engineering in physiology and clinical medicine",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.10.021",
     "publication date": "05-2010",
     "abstract": "This paper discusses the potential role for Computer Aided Process Engineering (CAPE) in developing engineering analysis and design approaches to biological systems across multiple levels\u2014cell signalling networks, gene, protein and metabolic networks, cellular systems, through to physiological systems. The 21st Century challenge in the Life Sciences is to bring together widely dispersed models and knowledge in order to enable a system-wide understanding of these complex systems. This systems level understanding should have broad clinical benefits. Computer Aided Process Engineering can bring systems approaches to (i) improving understanding of these complex chemical and physical (particularly molecular transport in complex flow regimes) interactions at multiple scales in living systems, (ii) analysis of these models to help to identify critical missing information and to explore the consequences on major output variables resulting from disturbances to the system, and (iii) \u2018design\u2019 potential interventions in in vivo systems which can have significant beneficial, or potentially harmful, effects which need to be understood. This paper develops these three themes drawing on recent projects at UCL. The first project has modeled the effects of blood flow on endothelial cells lining arteries, taking into account cell shape change resulting in changes in the cell skeleton which cause consequent chemical changes. A second is a project which is building an in silico model of the human liver, tieing together models from the molecular level to the liver. The composite model models glucose regulation in the liver and associated organs. Both projects involve molecular transport, chemical reactions, and complex multiscale systems, tackled by approaches from CAPE.Chemical Engineers solve multiple scale problems in manufacturing processes \u2013 from molecular scale through unit operations scale to plant-wide and enterprise wide systems \u2013 so have an appropriate skill set for tackling problems in physiology and clinical medicine, in collaboration with life and clinical scientists.",
     "keywords": ["Computer Aided Process Engineering", "Process Systems Engineering", "Systems Biology", "Systems Medicine", "Modelling", "Engineering design"]},
    {"article name": "Global sensitivity analysis in dynamic metabolic networks",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.01.006",
     "publication date": "05-2010",
     "abstract": "In this work, we have performed global sensitivity analysis on a large-scale dynamic metabolic network through variance-based techniques. Time profiles for sensitivity indices have been calculated for each parameter, based on Sobol\u2019 approach (2001). The global sensitivity analysis has been carried out on a dynamic model for the Embden\u2013Meyerhof\u2013Parnas pathway, the phosphotransferase system and the pentose-phosphate pathway of Escherichia coli K-12 strain W3110 (Chassagnole et al., 2002). The model comprises eighteen dynamic mass balance equations for extracellular glucose and intracellular metabolites, thirty kinetic rate expressions and seven additional algebraic equations that represent concentration profiles for co-metabolites. Each parameter has been considered to have a normal probability distribution centered on its nominal value and sample sizes of two thousand and five hundred scenarios have been considered. The preceding analysis has allowed identification of eleven parameters as the most influential ones on the complex metabolic network under study.",
     "keywords": ["Global sensitivity analysis", "Metabolic networks", "DAE systems"]},
    {"article name": "Optimisation of regional energy supply chains utilising renewables: P-graph approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.11.020",
     "publication date": "05-2010",
     "abstract": "This paper presents a new method for regional energy targeting and supply chain synthesis. The method is based on a novel approach to the optimisation of renewable energy supply. A new algorithm for revealing energy supply chain clusters is introduced, described and illustrated by a case study. It has been developed on two levels. The first is a top-level supply chain network with lowest Carbon Footprint generated. It consists of a number of zone clusters. The definition of zones is provided; it can be e.g. a village or a town. Each zone is considered as a unit. At the second level is a supply chain synthesis carried out by P-graph based optimisation within each cluster. It provides a more detailed analysis. The use of the P-graph framework as a synthesis toolset provides a strong mathematically proven fundament for handling the complexity of the synthesis problem. It contributes to the optimal network design with a high computational efficiency. This approach contributes to the cleaner generation of energy from biomass, approaching CO2 neutrality as much as possible. It is beneficial for extending the use of biomass as a renewable source of energy.",
     "keywords": ["ABB accelerated branch-and-bound (algorithm)", "accelerated branch-and-bound (algorithm)", "AD Anaerobic Digester", "Anaerobic Digester", "CFP Carbon Footprint", "Carbon Footprint", "CHP combined heat and power", "combined heat and power", "FP fermentation plant", "fermentation plant", "FC fuel cell", "fuel cell", "FCGT combined fuel cell and gas turbine", "combined fuel cell and gas turbine", "G gasifer", "gasifer", "GIS Geographic Information System", "Geographic Information System", "GT gas turbine", "gas turbine", "HRSG heat recovery steam generator", "heat recovery steam generator", "INC incinerator", "incinerator", "JEP juice extraction plant", "juice extraction plant", "LF landfill", "landfill", "NG natural gas", "natural gas", "MSG Maximal Structure Generation", "Maximal Structure Generation", "MSW municipal solid waste", "municipal solid waste", "PP pellet plant", "pellet plant", "REC Regional Energy Clustering", "Regional Energy Clustering", "RES renewable energy sources", "renewable energy sources", "RESDC Regional Energy Surplus-Deficit Curves", "Regional Energy Surplus-Deficit Curves", "SFP saccharification\u2013fermentation plant", "saccharification\u2013fermentation plant", "SSG Solution Structure Generation", "Solution Structure Generation", "ST steam turbine", "steam turbine", "T transport", "transport", "Regional Energy Clustering Algorithm", "Biomass supply chain synthesis", "Process graph (P-graph)", "Carbon Footprint minimisation", "Renewable energy sources"]},
    {"article name": "Performance analysis of a multi-plant specialty chemical manufacturing enterprise using an agent-based model",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.01.020",
     "publication date": "05-2010",
     "abstract": "Modern day manufacturing enterprises consist of networks of worldwide production sites, each of which has its own supply chain. There are complex interactions between the decisions at various levels of such enterprises that lead to intricate dynamics. To make holistic decisions, it is necessary to measure and analyze performance of the enterprise and its constituents under various conditions. Such performance analysis calls for appropriate modeling and simulation tools. Agent-based modeling has been demonstrated as a promising approach for modeling such complex networks of distributed actors. In this paper, we demonstrate how an agent-based model can be developed to explicitly capture the interactions among the various constituents including the plants, functional departments, and external entities. As an illustrative case, an agent-based model of a lube additive manufacturing supply chain is introduced and the performance of the system studied under a significant range of behaviors, business policies, and environmental events.",
     "keywords": ["Agent-based modeling", "Supply chain", "Performance analysis", "Simulation", "Abnormal situation management", "Decision support"]},
    {"article name": "Modelling and sensitivity analysis of ATAD",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.11.019",
     "publication date": "05-2010",
     "abstract": "Several authors have pointed out the need to identify the optimum operating conditions (OCs) of autothermal thermophilic aerobic digestion (ATAD). This study proves the hypothesis that the OCs have the potential to substantially improve the energy efficiency and plant capacity of established ATAD systems. As ATAD is a semi-batch process, its energy efficiency has to be optimized via dynamic optimization (DO). This methodology requires an adequate mathematical model, and appropriate selection of optimization variables. The paper presents an improved mathematical ATAD model based on previous models found in the literature. A global sensitivity analysis (GSA) was performed in order to identify variables with significant influence upon energy efficiency and plant capacity, thus paving the way for the DO of ATAD systems. The results of the GSA show that reactor volume, reactor temperature, and aeration flowrate are significant variables, which is consistent with reported literature. The results of the GSA also show that both energy efficiency and plant capacity of ATAD systems can be substantially improved by altering reactor volume and OCs.",
     "keywords": ["Wastewater treatment", "Autothermal thermophilic aerobic digestion", "Energy efficiency", "Plant capacity", "Modelling", "Sensitivity analysis"]},
    {"article name": "Separative reactors for integrated production of bioethanol and biodiesel",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.09.005",
     "publication date": "05-2010",
     "abstract": "Conventional integration of bioethanol and biodiesel plants employs the use of anhydrous ethanol in the biodiesel production process. The problem is that the production of anhydrous bioethanol is very energy-demanding, especially due to the azeotropic distillation required to producing high purity ethanol. The use of hydrous ethanol in the biodiesel production is preferable but unfeasible in conventional processes due to the equilibrium limitations and the economic penalties caused by the additional process steps. To solve this problem, this study proposes a novel energy-efficient integrated production of biodiesel from hydrous bioethanol. The key to success is a novel setup that combines the advantages of using solid catalysts with the integration of reaction and separation. This integrated process eliminates all typical catalyst-related operations, and efficiently uses the raw materials and the reactor volume in a separative reactor that allows significant savings in the capital and operating costs.Rigorous simulations embedding experimental results were performed using computer aided process engineering tools \u2013 such as AspenTech Aspen Plus \u2013 to design the separative reactor and evaluate the overall technical feasibility of the process. The RD column was simulated using the rigorous RADFRAC unit with RateSep (rate-based) model, and explicitly considering three phases balances. Sensitivity analysis was used to determine the optimal range of the operating parameters. The main results are given for a plant producing 10\u00a0ktpy biodiesel (>99.9%wt) from hydrous bioethanol (96%wt) and waste vegetable oil with high free fatty acids content (\u223c100%), using solid acids as green catalysts.",
     "keywords": ["Reactive distillation", "Green catalysts", "Solid acid/base", "Sustainable biofuels", "FFA", "FAEE"]},
    {"article name": "Graph-theoretic approach to the catalytic-pathway identification of methanol decomposition",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.12.004",
     "publication date": "05-2010",
     "abstract": "Catalytic decomposition of methanol (MD) plays a vital role in hydrogen production, which is the desirable fuel for both proton exchange membrane and direct methanol fuel cell systems. Thus, the catalytic mechanisms, or pathways, of MD have lately been the focus of research interest. Recently, the feasible independent pathways (IPis) have been reported on the basis of a set of highly plausible elementary reactions. Nevertheless, no feasible acyclic combined pathways (APis) comprising IPis have been reported. Such APis cannot be ignored in identifying dominant pathways.",
     "keywords": ["Methanol decomposition", "Graph theory", "Reaction pathways", "Independent pathways", "Acyclic combined pathways"]},
    {"article name": "Automated targeting technique for concentration- and property-based total resource conservation network",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.01.018",
     "publication date": "05-2010",
     "abstract": "Resource conservation networks (RCNs) are among the most effective systems for reducing the consumption of fresh materials and the discharge of waste streams. A typical RCN involves multiple elements of resource pre-treatment, material reuse/recycle, regeneration/interception, and waste treatment for final discharge. Due to the close interactions among these individual elements, simultaneous synthesis of a total RCN is necessary. This paper presents an optimisation-based procedure known as automated targeting technique to locate the minimum resource usage or total cost of a concentration- or property-based total RCNs. This optimisation-based approach provides the same benefits as conventional pinch analysis techniques in yielding various network targets prior to detailed design. Additionally, this approach offers more advantages than the conventional pinch-based techniques through its flexibility in setting an objective function and the ability to handle different impurities/properties for reuse/recycle and waste treatment networks. Furthermore, the concentration-based RCN is treated as the special case of property integration, and solved by the same model. Literature examples are solved to illustrate the proposed approach.",
     "keywords": ["Process integration", "Resource conservation", "Automated targeting", "Waste minimisation", "Optimisation", "Property integration"]},
    {"article name": "Accident prevention by control system reconfiguration",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.01.015",
     "publication date": "05-2010",
     "abstract": "The control system of a process plant, as other physical systems, can break up or fail to function satisfactorily, which means that control objectives are not achieved. In order to avoid the shut downs or accidents related to the control objectives unachieved, an analysis of both the process and control system must be performed. This paper introduces the use of functional modeling for control system reconfiguration. An extension of the Multilevel Flow Modeling (MFM) methodology is presented; this extension provides the functionality that analyzes the control system status as well as possible reconfigurations in a semiautomatic and consistent way, which means to enhance systems\u2019 autonomy and robustness. In order to be able to have an integrated model an open, neutral, domain and platform independent language is needed. The Systems Modeling Language (SysML) complies with these requirements and can be applied to develop MFM models.",
     "keywords": ["Functional modeling", "Autonomous systems", "Control reconfiguration"]},
    {"article name": "CFD analysis of mixing in thermal polymerization of styrene",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.11.009",
     "publication date": "04-2010",
     "abstract": "Thermal polymerization of styrene in a lab-scale CSTR equipped with a pitched blade turbine impeller was simulated using a computational fluid dynamics (CFD) software package. The rotation of the reactor impeller was modeled using the multiple reference frames (MRF) technique. The path lines of the particles, released at the reactor inlet, were also generated to analyze the reaction progress throughout the reactor vessel. The effects of the impeller speed, the input\u2013output stream locations and the residence time were investigated. The simulation showed the formation of a well-mixed region around the impeller and stagnant or slow moving fluids elsewhere in the reactor due to high viscosity of the polymer mass. The monomer conversion computed using the CFD model was in good agreement with that obtained from the CSTR model at low residence time. The input\u2013output locations have a significant impact on the monomer conversion and the system homogeneity in the CSTR.",
     "keywords": ["Computational fluid dynamics", "Thermal polymerization", "Polystyrene", "Mixing"]},
    {"article name": "CFD analysis of single-phase flows inside helically coiled tubes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.11.008",
     "publication date": "04-2010",
     "abstract": "It has been well established that heat transfer in a helical coil is higher than that in a corresponding straight pipe. However, the detailed characteristics of fluid flow and heat transfer inside helical coil is not available from the present literature. This paper brings out clearly the variation of local Nusselt number along the length and circumference at the wall of a helical pipe. Movement of fluid particles in a helical pipe has been traced. CFD simulations are carried out for vertically oriented helical coils by varying coil parameters such as (i) pitch circle diameter, (ii) tube pitch and (iii) pipe diameter and their influence on heat transfer has been studied. After establishing influence of these parameters, correlations for prediction of Nusselt number has been developed. A correlation to predict the local values of Nusselt number as a function of angular location of the point is also presented.",
     "keywords": ["Computational fluid dynamics (CFD)", "Helical coil", "Fluid mechanics", "Heat transfer", "Numerical analysis", "Mathematical modelling"]},
    {"article name": "A theoretical study of continuous counter-current chromatography for adsorption isotherms with inflection points",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.10.001",
     "publication date": "04-2010",
     "abstract": "Continuous counter-current chromatographic processes have been increasingly applied in the last years. For chromatographic systems characterized by linear or Langmuir adsorption equilibria, there are nowadays reliable design rules available to decide whether a separation is feasible and which operating parameters should be used. For more complex equilibrium functions, theoretical methods are less developed. More realistic adsorption isotherms are frequently characterized by inflection points in their courses. A flexible model capable to quantify single solute and competitive adsorption isotherms is provided by statistical thermodynamics. In this work, second-order truncations of a statistical model involving inflection points are investigated. The classical scanning technique is used to identify the region of applicable operating parameters. Then an alternative approach is suggested, which verifies the existence and shape of the suitable parameter region by infeasibility certificates. Using the True Moving Bed model, both approaches are compared for different feed concentrations, purity requirements and column efficiencies.",
     "keywords": ["Continuous counter-current chromatography", "Inflection points of adsorption isotherms", "Feasibility analysis", "Global optimization", "Convex relaxations"]},
    {"article name": "Multiple steady states detection in a packed-bed reactive distillation column using bifurcation analysis",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.09.001",
     "publication date": "04-2010",
     "abstract": "A packed reactive distillation column producing ethyl tert-butyl ether from tert-butyl alcohol and ethanol was simulated for detection of multiple steady states using Aspen Plus\u00ae. A rate-based approach was used to make the simulation model more realistic. A base-case was first developed and fine-tuned to fit experimental data. Sensitivity analyses were then performed for reboiler duty and distillate molar flow as continuation parameters to trace the respective bifurcation curves in the region of multiplicity. The results show output multiplicity at three distinct steady states at high reboiler duties. Input multiplicities were detected at high reflux ratios. Temperature and composition profiles of the solution branches were analyzed to identify the stable and desirable steady state. The optimum operating point was determined to be at a reboiler duty of 0.38\u00a0kW and a reflux ratio of 5\u20137. These results closely match the experimental results.",
     "keywords": ["Reactive distillation", "Rate-based simulation", "Multiple steady states", "ETBE synthesis"]},
    {"article name": "Data smoothing and numerical differentiation by a regularization method",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.10.007",
     "publication date": "04-2010",
     "abstract": "While data smoothing by regularization is not new, the method has been little used by scientists and engineers to analyze noisy data. In this tutorial survey, the general concepts of the method and mathematical development necessary for implementation for a variety of data types are presented. The method can easily accommodate unequally spaced and even non-monotonic scattered data. Methods for scaling the regularization parameter and determining its optimal value are also presented. The method is shown to be especially useful for determining numerical derivatives of the data trend, where the usual finite-difference approach amplifies the noise. Additionally, the method is shown to be helpful for interpolation and extrapolation. Two examples data sets were used to demonstrate the use of smoothing by regularization: a model data set constructed by adding random errors to a sine curve and global mean temperature data from the NASA Goddard Institute for Space Studies.",
     "keywords": ["Data smoothing", "Regularization", "Numerical differentiation", "Inverse problems"]},
    {"article name": "Determination of optimum gas holdup conditions in a three-phase fluidized bed by genetic algorithm",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.07.003",
     "publication date": "04-2010",
     "abstract": "Experiments have been carried out in a 0.1\u00a0m internal diameter, 1.88\u00a0m height vertical Plexiglas column with an antenna type modified air sparger in the gas\u2013liquid distributor section. This arrangement provides uniform mixing of the fluids, ensures the gas entering the fluidizing section as fine bubbles and reduces the pressure drop encountered through a conventional distributor used for the purpose. The important dimensionless groups which have influence on the gas holdup have been developed by the use of Buckingham Pi theorem. The relation of gas holdup with these dimensionless groups has been expressed in the form of a power law equation developed with very high correlation coefficient. Residual analysis has been carried out to validate the regression model. The optimum operating conditions for maximum gas holdup in a co-current three-phase fluidized bed have been predicted using genetic algorithm.",
     "keywords": ["Gas\u2013liquid\u2013solid fluidization", "Multiphase flow", "Gas holdup", "Genetic algorithm", "Optimization"]},
    {"article name": "Fitting of kinetic parameters of NO reduction by CO in fibrous media using a genetic algorithm",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.07.013",
     "publication date": "04-2010",
     "abstract": "An empirical model is fitted to the performance of NO reduction by CO gas on an alumina fibrous catalyst support media. The model is highly non-linear and contains 13 fitted parameters. To fit the reaction kinetic parameters in the model to the experimental results, a genetic algorithm was applied to search for the best fit parameter values. The fitted model and experimental results for Pd, Pt and Rh catalysts are in good agreement. The effects of NO decomposition by varying fiber diameter, media depth, and catalyst particle size on rhodium catalyst can be calculated from the model. The model results show the temperature of complete NO decomposition decreases with greater media depth, higher fractional area of coverage by the catalyst particles, and with smaller catalyst particle size.",
     "keywords": ["Genetic algorithm", "Electrospinning", "Nanofiber", "Catalysis"]},
    {"article name": "Performance of a tubular electrochemical reactor, operated with different inlets, to remove Cr(VI) from wastewater",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.05.016",
     "publication date": "04-2010",
     "abstract": "In the present work, the performance of a tubular electrochemical reactor operated with three different inlets (central, lateral and tangential) was evaluated. Fluid flow simulations through the electrochemical reactor were performed using computational fluid dynamics (CFD). The steady state fluid flow was simulated to visualize the velocity field behavior along the reactor for each different inlet. The type of reactor inlet affects clearly the performance of the reactor. It was found that axial velocity distribution is more homogeneous when the reactor is operated with the tangential inlet than with the other inlets. The dispersion number (Nd) and the residence time in the reactor were obtained for each type of inlet. These parameters were used to evaluate the performance of the electrochemical reactor to reduce the Cr(VI) concentration at values lower than 0.5\u00a0mg/L. The reactor operated with the tangential inlet had the best performance.",
     "keywords": ["Axial velocity", "Electrochemical", "Tubular reactor", "Hexavalent chromium", "CFD simulations"]},
    {"article name": "On-line multivariate statistical monitoring of batch processes using Gaussian mixture model",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.08.007",
     "publication date": "04-2010",
     "abstract": "This paper considers multivariate statistical monitoring of batch manufacturing processes. It is known that conventional monitoring approaches, e.g. principal component analysis (PCA), are not applicable when the normal operating conditions of the process cannot be sufficiently represented by a multivariate Gaussian distribution. To address this issue, Gaussian mixture model (GMM) has been proposed to estimate the probability density function (pdf) of the process nominal data, with improved monitoring results having been reported for continuous processes. This paper extends the application of GMM to on-line monitoring of batch processes. Furthermore, a method of contribution analysis is presented to identify the variables that are responsible for the onset of process fault. The proposed method is demonstrated through its application to a batch semiconductor etch process.",
     "keywords": ["Batch processes", "Fault detection and diagnosis", "Mixture model", "Principal component analysis", "Probability density estimation", "Multivariate statistical process monitoring"]},
    {"article name": "Repetitive control and online optimization of Catofin propane process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.12.011",
     "publication date": "04-2010",
     "abstract": "The Catofin propane process is an emerging industrial process for propylene production through dehydrogenation of propane. It is composed of multiple adiabatic fixed-bed reactors which undergo cyclic operations where propane dehydrogenation and catalyst regeneration alternate over roughly 10-min period for each. One of the major concerns in the operation of the Catofin process is maintaining the reactor at an optimum condition while overcoming gradual catalyst deactivation. Addressing this issue, an online optimization of the Catofin process combined with a repetitive control has been investigated. The optimizer computes optimum initial bed temperatures for dehydrogenation and optimum air flow rate for regeneration, and the repetitive controller performs cycle-wise feedback action during regeneration to attain the target bed temperatures at the terminal time of the regeneration period. Numerical studies have shown that the proposed online optimizing control system performs satisfactorily coping with the catalyst deactivation and other disturbances.",
     "keywords": ["Catofin process", "Online optimization", "Repetitive control", "Adiabatic fixed-bed reactor"]},
    {"article name": "An effective hybrid particle swarm optimization for batch scheduling of polypropylene processes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.12.010",
     "publication date": "04-2010",
     "abstract": "Short-term scheduling for batch processes which allocates a set of limited resources over time to manufacture one or more products plays a key role in batch processing systems of the enterprise for maintaining competitive position in fast changing market. This paper proposes an effective hybrid particle swarm optimization (HPSO) algorithm for polypropylene (PP) batch industries to minimize the maximum completion time, which is modeled as a complex generalized multi-stage flow shop scheduling problem with parallel units at each stage and different inventory storage policies. In HPSO, a novel encoding scheme based on random key representation, a new assignment scheme STPT (smallest starting processing time) by taking the different intermediate storage strategies into account, an effective local search based on the Nawaz\u2013Enscore\u2013Ham (NEH) heuristic, as well as a local search based on simulated annealing with an adaptive meta-Lamarckian learning strategy are proposed. Simulation results based on a set of random instances and comparisons with several adaptations of constructive methods and meta-heuristics demonstrate the effectiveness of the proposed HPSO.",
     "keywords": ["Particle swarm optimization", "Polypropylene batch", "Batch scheduling", "Multi-stage flow shop", "Hybrid flow shop", "Simulated annealing", "Zero-wait", "No intermediate storage"]},
    {"article name": "Tank cycling and scheduling analysis of high fusion point oil transportation for crude oil operations in refinery",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.11.007",
     "publication date": "04-2010",
     "abstract": "For short-term scheduling of crude oil operations, oil residency time and high fusion crude oil transportation constraints are difficult to model. With high setup cost for high fusion point oil transportation, it is desired that the volume of such oil should be transported as much as possible by a single setup. To do so and obtain a feasible schedule, charging tank cycling is an effective strategy. With a hybrid Petri net model, scheduling analysis is carried out and schedulability conditions under a charging tank cycling strategy are presented. They can be used for determining a refining schedule and checking if a target refining schedule is realizable. If so, a detailed feasible schedule can be easily obtained by creating the operation decisions one by one and put into immediate industrial use.",
     "keywords": ["Crude oil operations", "Scheduling", "Schedulability", "Petri net"]},
    {"article name": "Cyclic scheduling for best profitability of industrial cracking furnace system",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.10.008",
     "publication date": "04-2010",
     "abstract": "An ethylene plant employs multiple cracking furnaces in parallel to convert various hydrocarbon feedstocks to smaller hydrocarbon molecules, mostly ethylene and propylene. The continuous operational performance of cracking furnaces gradually decays because of coke formation in the reaction coils, which requires each furnace to be periodically shut down for decoking. Given multiple feeds and different cracking furnaces as well as various product prices and manufacturing costs, the operational scheduling for the entire furnace system should be optimized to achieve the best economic performance. In this paper, a new MINLP (mixed-integer nonlinear programming) model has been developed to obtain cyclic scheduling strategies for cracking furnace systems. Compared to previous studies, the new model has more capabilities to address operation profitability of multiple feeds cracked in multiple furnaces. Meanwhile, it inherently avoids unpractical conditions such as simultaneous shutdown of multiple furnaces. Case studies demonstrate the efficacy of the developed methodology.",
     "keywords": ["Cyclic scheduling", "Ethylene cracking furnace", "MINLP", "Optimization"]},
    {"article name": "Multiobjective scheduling for semiconductor manufacturing plants",
     "doi": "https://doi.org/10.1016/j.compchemeng.2010.01.010",
     "publication date": "04-2010",
     "abstract": "Scheduling of semiconductor wafer manufacturing system is identified as a complex problem, involving multiple and conflicting objectives (minimization of facility average utilization, minimization of waiting time and storage, for instance) to simultaneously satisfy. In this study, we propose an efficient approach based on an artificial neural network technique embedded into a multiobjective genetic algorithm for multi-decision scheduling problems in a semiconductor wafer fabrication environment.",
     "keywords": ["Discrete event simulation", "Artificial neural networks", "Multiobjective genetic algorithm", "Semiconductor manufacturing"]},
    {"article name": "Probabilistic modeling and dynamic optimization for performance improvement and risk management of plant-wide operation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.12.006",
     "publication date": "04-2010",
     "abstract": "This study presents a novel algorithm for constructing a probabilistic model based on historical operation data and performing dynamic optimization for plant-wide control applications. The proposed approach consists of applying a self-organizing map (SOM) for identifying representative plant operation modes based on a discounted infinite horizon cost and approximate dynamic programming techniques for learning an optimal policy. A quantitative measure for risk is defined in terms of transition probability, and a systematic guideline for striking balance between risk and profit in decision making is provided with a mathematical proof. The efficacy of the proposed approach is illustrated on an integrated plant consisting of a reactor, a storage tank, and a separator with a recycle loop and Tennessee Eastman challenge problem. The algorithm is useful for learning an improved policy and reducing risk in plant operation when a plant-wide model is difficult to obtain and uncertainties affect operation performance significantly.",
     "keywords": ["Approximate dynamic programming", "Real-time optimization", "Multi-stage stochastic optimization", "Plant-wide control"]},
    {"article name": "Perspectives on the potential roles of engineers in the formulation, implementation and enforcement of policies",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.10.010",
     "publication date": "03-2010",
     "abstract": "This article discusses and aims to motivate a potential future role of engineers as developers of methods and tools to support policy development, thus extending the reach of the engineering profession further into a normative role. Policies and policy development are first analysed from the systems theory viewpoint and the ideas behind some proof-of-concept software systems supporting policy development are then introduced. Three specific stages from the policy cycle are addressed in particular: policy formulation, implementation and enforcement. The discussion of these issues is done in the context of the similarities and differences between two strands of human endeavour: process design and policy development.",
     "keywords": ["Decision support systems", "Policy formulation", "Process design", "Ontologies", "Decision rationale and management"]},
    {"article name": "A simplified numerical method for the General Rate model",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.10.014",
     "publication date": "03-2010",
     "abstract": "The General Rate (GR) model has been considered as the most comprehensive mathematical model which describes the mass transfer processes of the solute in the chromatographic column. But its numerical calculation is complicated and time consuming. In this work, the modified Equilibrium-Dispersive (MED) model which provides a simplified method to obtain the numerical solution of GR model is deduced according to the moment analysis result of the GR model. And the equivalence of the numerical results of the GR models and the MED model is also investigated. The results reflect that the numerical calculation of the GR model can be substituted by that of the MED model.",
     "keywords": ["GR model", "Numerical calculation", "Simplification", "Moment analysis", "MED model"]},
    {"article name": "Comparison of the accuracy and performance of quadrature-based methods for population balance problems with simultaneous breakage and aggregation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.11.005",
     "publication date": "03-2010",
     "abstract": "Simulations of polydisperse multiphase flows must include the effects of particle breakage and aggregation, which requires the solution of the population balance equation (PBE). Therefore, the analysis of the existing numerical techniques to solve the PBE regarding their efficiency and accuracy is paramount to their implementation in CFD codes. This work focused on analyzing the three quadrature-based methods available in the literature (QMOM, DQMOM and PPDC) in terms of efficiency and accuracy and against the classical method of classes. Analytical solutions were used to derive test cases from dominant breakage to dominant aggregation. The methods were evaluated in terms of moment accuracy and convergence. The computational costs were evaluated for all cases. It was verified that PPDC has poor convergence and is not adequate. For all cases, the QMOM and DQMOM solutions presented similar accuracy, but the DQMOM was the most efficient method.",
     "keywords": ["Population balance", "Aggregation", "Breakage", "QMOM", "DQMOM", "PPDC"]},
    {"article name": "Confidence interval estimation under the presence of non-Gaussian random errors: Applications to uncertainty analysis of chemical processes and simulation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.11.004",
     "publication date": "03-2010",
     "abstract": "Confidence intervals (CIs) are common methods to characterize the uncertain output of experimental measurements, process design calculations and simulations. Usually, probability distributions (pdfs) such as Gaussian and t-Student are used to quantify them. There are situations where the pdfs have anomalous behavior such as heavy tails, which can arise in uncertainty analysis of nonlinear computer models with input parameters subject to different sources of errors. We present a method for the estimation of CIs by analyzing the tails of the pdfs regardless of their nature. We present case studies in which heavy tail behavior appears due to the systematic errors in the input variables of the model. Taking into account the probability distributions behavior to estimate appropriate CIs is a more realistic approach to characterize and analyze the effect of random and systematic errors for uncertainty analysis of computer models.",
     "keywords": ["Confidence intervals", "Uncertainty analysis", "Heavy tails", "Error propagation", "Monte Carlo"]},
    {"article name": "Economic, environmental and mixed objective functions in non-linear process optimization using simulated annealing and tabu search",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.10.015",
     "publication date": "03-2010",
     "abstract": "Screening of topologies developed by hierarchical heuristic procedures can be carried out by comparing their optimal performance. In this work we will be exploiting mono-objective process optimization using two algorithms, simulated annealing and tabu search, and four different objective functions: two of the net present value type, one of them including environmental costs and two of the global potential impact type. The hydrodealkylation of toluene to produce benzene was used as case study, considering five topologies with different complexities mainly obtained by including or not liquid recycling and heat integration.The performance of the algorithms together with the objective functions was observed, analyzed and discussed from various perspectives: average deviation of results for each algorithm, capacity for producing high purity product, screening of topologies, objective functions robustness in screening of topologies, trade-offs between economic and environmental type objective functions and variability of optimum solutions.",
     "keywords": ["Simulated annealing", "Tabu search", "Economic objective function", "Environmental objective function"]},
    {"article name": "Global optimization for the synthesis of property-based recycle and reuse networks including environmental constraints",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.10.005",
     "publication date": "03-2010",
     "abstract": "This paper presents a new formulation and a mathematical programming model for the direct recycle and reuse of mass exchange networks considering simultaneously process and environmental constraints. The model is based on mass and property integration. The properties constrained by the sinks include composition, density, viscosity, pH, and reflectivity, whereas the environmental constraints include the composition for hazardous materials, toxicity, chemical oxygen demand, color, and odor. The model eliminates most of the nonlinearities of the system, and the bilinear terms that remain are handled with a relaxation approach that yields a global optimal solution. The model minimizes the total annual cost that includes the cost of fresh sources and the annualized cost for property interceptors. Two examples are presented to show the effectiveness of the proposed model. The results show that even for a large size problem, the computation effort is relatively small as a result of the linearization procedure.",
     "keywords": ["Property interception networks", "Mass exchange networks", "Interceptors", "Environmental constraints", "Optimization", "Recycle and reuse"]},
    {"article name": "Integrated gasification combined cycle (IGCC) process simulation and optimization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.04.007",
     "publication date": "03-2010",
     "abstract": "The integrated gasification combined cycle (IGCC) is an electrical power generation system which offers efficient generation from coal with lower effect on the environment than conventional coal power plants. However, further improvement of its efficiency and thereby lowering emissions are important tasks to achieve a more sustainable energy production. In this paper, a process simulation tool is proposed for simulation of IGCC. This tool is used to improve IGCC's efficiency and the environmental performance through an analysis of the operating conditions, together with process integration studies. Pinch analysis principles and process integration insights are then employed to make topological changes to the flowsheet to improve the energy efficiency and minimize the operation costs. Process data of the Texaco gasifier and the associated plants (coal preparation, air separation unit, gas cleaning, sulfur recovery, gas turbine, steam turbine and the heat recovery steam generator) are considered as a base case, and simulated using Aspen Plus\u00ae. The results of parameter analysis and heat integration studies indicate that thermal efficiency of 45% can be reached, while a significant decrease in CO2 and SOx emissions is observed. The CO2 and SOx emission levels reached are 698\u00a0kg/MWh and 0.15\u00a0kg/MWh, respectively. Application of pinch analysis determines energy targets, and also identifies potential modifications for further improvement to overall energy efficiency. Benefits of energy integration and steam production possibilities can further be quantified. Overall benefits can be translated to minimum operation costs and atmospheric emissions.",
     "keywords": ["IGCC", "Process simulation", "Process optimization", "Process integration"]},
    {"article name": "Simulation of the transient behavior of fuel cells by using operator splitting techniques for real-time applications",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.11.006",
     "publication date": "03-2010",
     "abstract": "The functioning of fuel cells, in which simultaneous processes having different kinetics and different time constants occur, can be simulated by applying rather complex models. For the sake of better modeling larger numbers of sub-processes and their couplings have to be considered, which leads to complex and multi-step simulation frameworks. In this work new methods are introduced for the simulation of the behavior of fuel cells, which are based on operator splitting techniques. These methods can be applied for the simulation of rather complex problems, consequently they open up new vistas in respect to the real-time simulation. The errors of the schemes are analyzed while applying different kinetic approaches. The effects of constant current, current sweep and pulsed current are calculated. The qualitative and quantitative errors are analyzed and compared with measured data. It is proven that the method developed is suitable for describing the fast transient behavior, therefore it makes the real-time monitoring and controlling of the functioning of fuel cells possible.",
     "keywords": ["Fuel cell", "Real-time simulation", "Operator splitting", "Pulsed load", "Peak power", "FPGA"]},
    {"article name": "Condition monitoring of centrifuge vibrations using kernel PLS",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.11.003",
     "publication date": "03-2010",
     "abstract": "A warning system for detection of excessive vibration in a centrifuge system of a product treatment plant is built using a database of past faults and an equivalent amount of normal operating null data. A logistic Partial Least Squares (PLS) model is derived using wavelet coefficients to approximately decorrelate the time series data. This model provides a baseline to evaluate any improvement through kernel methods. The kernel paradigm is introduced from a Bayesian perspective and used to develop a detector with significantly less false positives and missed detections.",
     "keywords": ["Partial Least Squares", "Kernel methods", "Condition monitoring"]},
    {"article name": "Impact of plant dynamics on the performance of steady-state data reconciliation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.11.018",
     "publication date": "03-2010",
     "abstract": "This paper studies the impact of plant dynamics on the performance of steady-state data reconciliation for processes operating in stationary mode. Data reconciliation can effectively reduce effects of measurement errors on process variables estimation. This has greatly motivated the use of steady-state observers in the process industry. However, processes are operating under unsteady-state conditions and the fundamental hypothesis of steady-state is not respected. The objective of the paper is to develop criteria for assessing the impact of stationary disturbances on estimates quality and therefore to propose applicability indices for steady-state data reconciliation methods in disturbed plants. A generic criterion based on generalized variance is given and criteria are proposed for plant performance indicators using variance reduction in specific state-space directions. The various applicability indices are illustrated through a material balance example for a simulated mineral separation circuit and the steady-state approach is compared to an optimal stationary data reconciliation method.",
     "keywords": ["Data reconciliation", "Observer", "Mass balancing", "Covariance matrix", "Performance evaluation", "Generalized variance"]},
    {"article name": "Plant-wide design and control of DMC synthesis process via reactive distillation and thermally coupled extractive distillation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.05.002",
     "publication date": "03-2010",
     "abstract": "In the study, reactive distillation is designed to produce dimethyl carbonate and ethylene glycol by the transesterification of ethylene carbonate and methanol. The products of the reactive distillation column include ethylene glycol and a dimethyl carbonate/methanol mixture, close to azeotropic composition. The azeotrope is separated in the study by pressure-swing distillation and extractive distillation using phenol as an extractive agent. The extractive distillation is found to be much more economical than the pressure-swing distillation. Thermal coupling technology is also implemented between the two columns of the extractive distillation. Thermally coupled extractive distillation provides the best energy efficiency. Proper selection and pairing of controlled and manipulated variables are determined by using steady-state analysis. A simple temperature control scheme is sufficient to maintain stoichiometric balance between the reactant feeds and product purities at or around their designed values for the plant-wide process of reactive distillation\u00a0+\u00a0thermally coupled extractive distillation.",
     "keywords": ["Reactive distillation", "Pressure-swing distillation", "Extractive distillation", "Thermal coupling", "Design", "Control"]},
    {"article name": "Evaluation of plant alarm systems by behavior simulation using a virtual subject",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.11.017",
     "publication date": "03-2010",
     "abstract": "To design an effective plant alarm system, it is essential to evaluate various performances of fault detection and identification (FDI) in emergencies. In this study, an operator model, which mimics the FDI behavior of a human operator with primary cognitive and executive capabilities, is developed as a virtual subject for supervising a chemical plant system. Analyzing the FDI tracks generated by the behavior simulation using the virtual subject under an alarm system makes it possible to evaluate the performance of the alarm system. In a case study, the evaluation method using the virtual subject is applied for designing an appropriate alarm system in a boiler-plant simulator. FDI performances of the original alarm system for all assumed malfunctions are evaluated and alarm signal selection and limit settings are reconsidered according to the evaluation results. The results of the case study demonstrate the usefulness of the model-based evaluation method.",
     "keywords": ["Operator model", "Virtual subject", "Plant alarm system", "Fault detection and identification", "Behavior simulation"]},
    {"article name": "Production scheduling of a multi-grade PET resin plant",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.05.017",
     "publication date": "03-2010",
     "abstract": "We present a discrete-time, Mixed Integer Linear Programming (MILP) model for the production scheduling of a continuous-process multi-grade PET resin plant. The objective is to minimize the cost associated with grade changeovers in order to avoid undesirable variations in base resin properties and process conditions that occur during such changes. The constraints of the model include requirements related to sequence-dependent changeovers, sequential processing with production and space capacity, mixed and flexible finite intermediate storage, and intermediate demand due-dates. We present a case study that illustrates the application of the model on a real problem scenario and provides insight into its behavior. The numerical experience demonstrates that the computational requirements of the model are quite reasonable for problem sizes that typically arise in practical applications.",
     "keywords": ["Polymer industry", "PET resin", "Continuous multi-grade process", "Production scheduling", "Mixed Integer Linear Programming"]},
    {"article name": "A mathematical model for planning transportation of multiple petroleum products in a multi-pipeline system",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.11.014",
     "publication date": "03-2010",
     "abstract": "A multiproduct pipeline provides an economic way to transport large volumes of refined petroleum products over long distances. In such a pipeline, different products are pumped back-to-back without any separation device between them. Sometimes, multiproduct pipelines can be connected together, resulting in a more complex system commonly named multi-pipeline system. This paper proposes a new discrete mathematical approach to solve short-term operational planning of multi-pipeline systems for refined products. This model is based on a discrete approach that divides both the planning horizon into time intervals of equal duration and the individual polyducts into packages of equal volume each containing a single product. Numerical examples are solved in order to show the performance of the proposed model. All the instances are implemented with the OPL modeling language running CPLEX as solver.",
     "keywords": ["Multiproduct pipeline", "Multi-pipeline system", "Transportation", "Planning and scheduling", "Discrete approach", "Mixed-integer linear program"]},
    {"article name": "Simulation and optimization of milk pasteurization processes using a general process simulator (ProSimPlus)",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.11.013",
     "publication date": "03-2010",
     "abstract": "In order to increase product quality and to minimize energy consumption and impact on the environment, it is necessary to optimize the food process design and operation. Therefore, the aim of this work was to study the optimization feasibility of the milk pasteurization process using a general process simulator (ProSimPlus). To simulate the pasteurization process, a simulation flowsheet was developed using the simulator, and a database was built in the simulator with the values and/or functions to estimate the necessary physical properties of the whole milk. Using the ProSimPlus utilities (control units) an objective function was set up in order to optimize the process design and the process operation of the milk pasteurization process. For that purpose a ProSimPlus module (optimization: OPTI) was used. By introducing physical properties related to other products and redefining the objective function the application of this general simulator could be extended in the Food Industry.",
     "keywords": ["General simulator", "Pasteurization process", "Process optimization", "ProSimPlus"]},
    {"article name": "Toward simulation of full-scale monolithic catalytic converters with complex heterogeneous chemistry",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.05.018",
     "publication date": "02-2010",
     "abstract": "Computational fluid dynamic (CFD) modeling of full-scale catalytic converters with realistic chemistry has remained elusive primarily due to the extreme computational requirements. In this work, a new low-memory coupled implicit solver, based on the conservative unstructured finite-volume method, was utilized to simulate laboratory-scale catalytic converters with implicit coupling between fluid flow, heat transfer (including conjugate heat transfer), mass transfer, and heterogeneous chemical reactions. Steady-state calculations were performed for a catalytic methane\u2013air combustion process with 24 reaction steps and 19 species (8 gas-phase species, 11 surface-adsorbed species), and for a three-way catalytic conversion process with 61 reaction steps and 31 species (8 gas-phase species, 23 surface-adsorbed species). Both calculations were conducted on a single processor for a monolith with 57 channels discretized using 354,300 control volumes. The catalytic combustion simulation was completed in 19\u00a0h and required 900\u00a0MB of memory, while the three-way conversion simulation required 6 days and 1\u00a0GB of memory, indicating that the complexity of the surface reaction mechanism dominates the overall CPU time requirements. Subsequently, the solver was parallelized, and the same catalytic combustion case was simulated for a monolith with 293 channels discretized using 1.27\u00a0million control volumes. A 4-node cluster was utilized for the parallel computations, and the parallelization efficiency was found to be about 80%.",
     "keywords": ["CFD", "Reacting flow", "Full-scale", "Catalytic converter", "Methane combustion", "Three-way conversion"]},
    {"article name": "A simple numerical solution to the Ward\u2013Tordai equation for the adsorption of non-ionic surfactants",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.08.004",
     "publication date": "02-2010",
     "abstract": "A simple numerical scheme for solving the equation of Ward and Tordai (1946) for the diffusion-controlled adsorption of non-ionic surfactants to interfaces is proposed and pseudo-code, as well as C++ source code, is provided. The scheme utilises the trapezium rule of numerical integration and the accuracy and robustness of the method is enhanced by the bisection method of root-finding. The scheme is efficient and flexible in that it can be used with any adsorption isotherm and is readily modified for solving the problem of adsorption onto a convex interface. This scheme is not suggested for the adsorption onto a concave interface and the confusions that have previously arisen in relation to this problem are discussed.",
     "keywords": ["Diffusion-controlled adsorption", "Non-ionic surfactants", "Dynamic surface tension"]},
    {"article name": "Quasi-weighted least squares estimator for data reconciliation",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.09.007",
     "publication date": "02-2010",
     "abstract": "Data reconciliation is important in chemical industries. Because of random and possibly gross errors in measurements, data reconciliation is needed to minimize the measurement errors. The most common estimator for data reconciliation is the weighted least squares, which is not robust. A robust estimator, quasi-weighted least squares, is proposed for data reconciliation. The properties of the estimator are analyzed, and the influence function is used to show that the estimator is robust. Two estimators, weighted least squares and quasi-weighted least squares, are used in atmospheric tower, ethylene separation and air separation process systems. Comparisons with other approaches are made on the steam metering process. The effectiveness of the robust estimator is demonstrated.",
     "keywords": ["Data reconciliation", "Gross error detection", "Weighted least squares", "Robust estimator", "Quasi-weighted least squares"]},
    {"article name": "A structured approach to optimizing offshore oil and gas production with uncertain models",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.07.011",
     "publication date": "02-2010",
     "abstract": "Optimizing offshore production of oil and gas has received comparatively little attention despite the large scale of revenues involved. The complexity of multiphase flow means that any model for use in production optimization must be fitted to production data for accuracy, but the low information content of production data means that the uncertainty in the fitted parameters of any such model will be significant. Due to costs and risk the information content in production data cannot be increased through excitation unless the benefits are documented.A structured approach is suggested which iteratively updates setpoints while documenting the benefits of each proposed setpoint change through excitation planning and result analysis. In simulations on an analog which mimics a real-world oil field and its typical low information content data the approach is able to realize a significant portion of the available profit potential while ensuring feasibility despite large initial model uncertainty.",
     "keywords": ["Uncertainty", "Production optimization", "Parameter estimation", "Oil and gas production", "Excitation planning", "Result analysis"]},
    {"article name": "Optimization model for re-circulating cooling water systems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.07.006",
     "publication date": "02-2010",
     "abstract": "This paper presents an optimization model for the simultaneous synthesis and detailed design of re-circulating cooling water systems. A cooler network superstructure that embeds all network configurations of practical interest is used as part of the integrated model. Pressure drops in each cooler are treated as optimization variables, and design guidelines and constraints are included in order to provide practical and feasible units. The model is based on a generalized disjunctive programming formulation, which gives rise to a mixed-integer nonlinear programming problem. The objective is to find cooling water systems that minimize the total annual cost. Solution of this mathematical formulation provides the optimal system configuration as well as the optimal operating conditions and design parameters required for each cooler unit in the network and in the cooling tower. Three example problems are presented to show the application of the proposed approach.",
     "keywords": ["Cooling water system", "Cooling tower", "Cooler network", "MINLP", "Optimization", "Pressure drops"]},
    {"article name": "A reactive distillation process for deep hydrodesulfurization of diesel: Multiplicity and operation aspects",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.07.014",
     "publication date": "02-2010",
     "abstract": "A systematic study of the operating conditions and parameter sensibility under which multiple steady states may occur in a reactive distillation column (RDC) for diesel deep hydrodesulfurization is presented. The multiplicity analysis is performed through bifurcation diagrams for two feed case scenarios. The main variables that affect the steady state behavior are the reflux ratio, the hydrocarbon feed flowrate, the reboiler heat duty, and the catalyst load (liquid holdup). For the first case only a single steady state was found, while in the second case input and output multiplicities were determined. It is shown that the introduction in the feed of the recalcitrant 4,6-DMDBT, plays a crucial role in the RDC design and operation. A set of values of the bifurcation parameters for an appropriate operation of the RDC are recommended to avoid the multiplicity region and to reach the required output targets.",
     "keywords": ["Diesel deep HDS", "Multiple steady states", "Reactive distillation"]},
    {"article name": "On the dynamic modeling of mammalian cell metabolism and mAb production",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.06.019",
     "publication date": "02-2010",
     "abstract": "A general optimization-based technique for the estimation of kinetic parameter values in dynamic models of cell metabolism is presented. A discretization strategy is used to transform continuous differential equations given in the model into an approximating set of algebraic equations. The discretized equation set is then used to constrain a non-linear optimization problem, whose solution is an optimal set of model parameter values. As a case study, we examine a simplified dynamic model of mammalian cell culture [Gao, J., Gorenflo, V. M., Scharer, J. M., & Budman, H. M. (2007). Dynamic metabolic modeling for a mab bioprocess. Biotechnology Progress, 23 (1), 168\u2013181]. Our parameter estimation technique is shown to solve the unsimplified variant of this model, and to provide a more accurate simulation of the experimentally observed system behavior than originally reported. We are additionally able to predict physically reasonable system data even in the absence of corresponding experimental measurements, and, finally, to provide a quantitative basis for challenging certain underlying assumptions in the original model solution.",
     "keywords": ["Optimization", "Bioprocess development", "Monoclonal antibodies", "Metabolism", "Pharmaceuticals", "Kinetics"]},
    {"article name": "A robust clustering method for detection of abnormal situations in a process with multiple steady-state operation modes",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.05.012",
     "publication date": "02-2010",
     "abstract": "Many classical multivariate statistical process monitoring (MSPM) techniques assume normal distribution of the data and independence of the samples. Very often, these assumptions do not hold for real industrial chemical processes, where multiple plant operating modes lead to multiple nominal operation regions. MSPM techniques that do not take account of this fact show increased false alarm and missing alarm rates. In this work, a simple fault detection tool based on a robust clustering technique is implemented to detect abnormal situations in an industrial installation with multiple operation modes. The tool is applied to three case studies: (i) a two-dimensional toy example, (ii) a realistic simulation usually used as a benchmark example, known as the Tennessee\u2013Eastman Process, and (iii) real data from a methanol plant. The clustering technique on which the tool relies assumes that the observations come from multiple populations with a common covariance matrix (i.e., the same underlying physical relations). The clustering technique is also capable of coping with a certain percentage of outliers, thus avoiding the need of extensive preprocessing of the data. Moreover, improvements in detection capacity are found when comparing the results to those obtained with standard methodologies. Hence, the feasibility of implementing fault detection tools based on this technique in the field of chemical industrial processes is discussed.",
     "keywords": ["Fault detection", "Multiple operating modes", "Multivariate statistical process monitoring"]},
    {"article name": "Indirect neural control for plant-wide systems: Application to the Tennessee Eastman Challenge Process",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.08.003",
     "publication date": "02-2010",
     "abstract": "An autonomous indirect scheme is proposed for multivariable process control and is extended to unstable open-loop plant-wide processes. Our principal objective in this work is to prove the feasibility to control an industrial plant by a small size neural system without any a priori training. The control scheme is made of an adaptive instantaneous neural model, a Neural Controller based on fully connected \u201cReal-Time Recurrent Learning\u201d networks and an on-line parameters updating law. This control scheme is applied to the Tennessee Eastman Challenge Process. Performances such as set point stabilisation, mode switching and disturbances rejection are pointed out. Results are discussed according to the Down and Vogel control objectives.",
     "keywords": ["Adaptive control", "Neural networks", "Multivariable systems", "Stability", "Robustness", "Lyapunov function", "Tennessee Eastman Challenge Process", "Plant-wide control"]},
    {"article name": "A functional HAZOP methodology",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.06.028",
     "publication date": "02-2010",
     "abstract": "A HAZOP methodology is presented where a functional plant model assists in a goal oriented decomposition of the plant purpose into the means of achieving the purpose. This approach leads to nodes with simple functions from which the selection of process and deviation variables follow directly. The functional HAZOP methodology lends itself directly for implementation into a computer aided reasoning tool to perform root cause and consequence analysis. Such a tool can facilitate finding causes and/or consequences far away from the site of the deviation. A functional HAZOP assistant is proposed and investigated in a HAZOP study of an industrial scale Indirect Vapor Recompression Distillation pilot Plant (IVaRDiP) at DTU-Chemical and Biochemical Engineering. The study shows that the functional HAZOP methodology provides a very efficient paradigm for facilitating HAZOP studies and for enabling reasoning to reveal potential hazards in safety critical operations.",
     "keywords": ["Risk assessment", "Systems engineered HAZOP analysis"]},
    {"article name": "Optimizing the design of global supply chains at Dow AgroSciences",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.08.002",
     "publication date": "02-2010",
     "abstract": "The design of the underlying supply chain network can have a tremendous impact on the profitability, manageability, and level of risk of a global supply chain. Taxes, duties, and tariffs vary from country to country as well as trading bloc to trading bloc and can consume as much as 10% of the revenues of certain products. In the highly regulated business environment of agricultural chemicals, the country of origin of an active ingredient can determine where the final product can be marketed and the amount of taxes and duties applied to the product, making it necessary to trace all batches of product through many layers of the supply chain to their sources. This article presents a mixed integer linear programming model in use at Dow AgroSciences LLC that simultaneously optimizes the network design underlying global supply chains and the monthly production and shipping schedules for maximum profitability. This work contributes to the supply chain design literature by demonstrating a novel method of tracing products to their source for inventory valuation, taxation, and duty computation in a production environment where the products change into other products as they pass through nodes in the network. It also demonstrates an iterative scheme for determining unit fixed costs for fixed cost allocation for the same purposes. Finally, it provides a case study of a supply chain design initiative in a global enterprise.",
     "keywords": ["Supply chain", "Optimization", "Strategic planning", "Industrial application"]},
    {"article name": "CFD and experimental studies on the effect of valve weight on performance of a valve tray column",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.07.001",
     "publication date": "01-2010",
     "abstract": "In the present study, the volume of fluid (VOF) method implemented in the commercial CFD package, FLUENT6.2 has been used to model the gas\u2013liquid flow in a valve tray column. The effect of valve weight has been investigated using three valves having different weights. An experimental Perspex column equipped with a single valve tray, a weir and two downcomers has been used. A fluctuating plate has been utilized for measuring the quality of gas distribution inside the liquid phase. In order to prove the repeatability and consistency of the measurements, the results were analyzed using two-stage nested designs. Bubble size distributions obtained from photographs confirm that more bubble dispersions can be obtained using heavier valves with cost of higher pressure drops, which is quantified by interface\u2013pressure drop performance. The CFD predictions, using upward momentum integral (UMI) parameter, also show that the produced gas\u2013liquid interface increases by employing heavier valves.",
     "keywords": ["Valve", "Tray", "CFD", "Modeling", "Nested design"]},
    {"article name": "Mathematical modeling of brown stock washing problems and their numerical solution using MATLAB",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.08.005",
     "publication date": "01-2010",
     "abstract": "The mechanism of the displacement washing of the bed of pulp fibers is mathematically modeled by the basic material balance equation. Non-linear Langmuir type adsorption isotherm is used to describe the relationship between the concentration of the solute in the liquor and concentration of the solute on the fibers. In the present study, the numerical solutions are obtained of the displacement washing model for multistage in counter current manner. For the numerical solution \u201cpdepe\u201d solver in MATLAB is applied on the axial domain of the system of governing partial differential equations. Numerical solutions thus obtained are in good agreement with the results of earlier workers. The technique used in the present investigation is simple, elegant and convenient for solving two point boundary value problems with varying range of parameters.",
     "keywords": ["MATLAB, \u201cpdepe\u201d solver", "Pulp washing model", "Peclet number", "Adsorption isotherm", "Multistage", "Counter current"]},
    {"article name": "Integrated model-centric framework for support of manufacturing operations. Part I: The framework",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.08.006",
     "publication date": "01-2010",
     "abstract": "Model-centric technologies must overcome a number of challenges limiting their ability to meet the needs of the Process Industries. First, a software component enabling the formulation of hybrid data-driven/model-based problems must be developed so that a number of realistic computer-aided process-engineering (CAPE) activities can be performed in an industrial environment. Second, these software components must be integrated into a single software platform so that the synergy between complementary model-based applications is unlocked and fully exploited by end-users. In this work we present a model-centric framework for integrated simulation, parameter estimation, data reconciliation and optimisation of industrial process systems. This framework is based on a new software architecture for CAPE that promotes the formulation of process-operations problems based on first-principles process models. This is achieved via the innovative Problem Definition Component (PDC) and the underlying mechanism of domain-specific data models, given in the form of templates (Data-Model Templates, or DMTs) and definitions (Data-Model Definitions, or DMDs). An industrial case-study is presented to illustrate the framework.",
     "keywords": ["Custom-modelling tools", "Model-centric technologies", "First-principles models", "Model-based engineering", "Integrated environment", "Operations support", "Optimisation"]},
    {"article name": "A hybrid genetic algorithm for twice continuously differentiable NLP problems",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.09.006",
     "publication date": "01-2010",
     "abstract": "This paper introduces a new hybrid genetic algorithm to solve twice continuously differentiable non-linear programming (NLP) problems. The algorithm combines the genetic algorithm with local solver differently from some hybrid genetic algorithms. Numerical experiments show the new hybrid genetic algorithm can obtain results better than the known ones reported by the literatures.",
     "keywords": ["Hybrid genetic algorithm", "Twice continuously differentiable NLP", "Local search", "Global optimization"]},
    {"article name": "A global optimization strategy for the environmentally conscious design of chemical supply chains under uncertainty in the damage assessment model",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.09.003",
     "publication date": "01-2010",
     "abstract": "This paper addresses the optimal design and planning of sustainable chemical supply chains (SCs) in the presence of uncertainty in the damage model used to evaluate their environmental performance. The environmental damage is assessed through the Eco-indicator 99, which includes recent advances made in life cycle assessment (LCA). The overall problem is formulated as a bi-criterion stochastic non-convex mixed-integer nonlinear program (MINLP). The deterministic equivalent of such a model is obtained by reformulating the joint chance-constraint employed to calculate the environmental performance of the SC in the space of uncertain parameters. The resulting bi-criterion non-convex MINLP is solved by applying the epsilon constraint method. To guarantee the global optimality of the Pareto solutions found, we propose a novel spatial branch and bound method that exploits the specific structure of the problem. The capabilities of our modeling framework and the performance of the proposed solution strategy are illustrated through a case study.",
     "keywords": ["Multi-objective optimization", "Supply chain management", "Life cycle assessment", "Uncertainty", "Global optimization"]},
    {"article name": "Introducing a new operational policy: The PIS operational policy",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.07.008",
     "publication date": "01-2010",
     "abstract": "A novel operational policy, the Process Intermediate Storage operational policy, is introduced and used to synthesize, schedule and design multipurpose batch plants. The model is based on the State Sequence Network and non-uniform discretization of the time horizon of interest model developed by Majozi and Zhu [Majozi, T., Zhu, X. (2001). A novel continuous-time MILP formulation for multipurpose batch plants. 1. Short-term scheduling. Industrial and Engineering Chemistry Research, 40(23), 5935\u20135949]. Two cases are studied to determine the effectiveness of this operational policy. In the first case, which excludes any dedicated storage, the use of this operational policy results in 50% improvement in throughput. The second case is used to determine the minimum amount of intermediate storage while maintaining the throughput achieved with infinite intermediate storage. This results in 20% reduction in the amount of dedicated intermediate storage. The models developed for both cases are MILP models. An MINLP design model is then developed to exploit the attributes of the PIS operational policy.",
     "keywords": ["Batch process", "Scheduling", "PIS policy", "Design", "Latent storage", "MILP"]},
    {"article name": "Synthesis of distillation configurations: I. Characteristics of a good search space",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.05.003",
     "publication date": "01-2010",
     "abstract": "The exhaustive enumeration of all possible distillation configurations for zeotropic n-component feed mixtures into n nearly pure product streams leads to an exponentially large number of configurations, making the search for an optimal configuration intractable for even a modest number of components. To reduce the search space to a manageable level, we divide the column configurations into two categories: (i) Basic, configurations having (n\u00a0\u2212\u00a01) distillation columns; and (ii) Non-basic, configurations having more columns. On exhaustively simulating multiple four-component feed conditions using Underwood's method for pinched columns, we find that for none of the simulated feed conditions does a non-basic configuration have a lower heat duty than the lowest heat duty basic configuration. Based on this observation, we assert that a much smaller search space containing only basic configurations should be sufficient to find an optimal n-component configuration. We also find basic configurations allowing non-sharp splits (ABC\u00a0\u2192\u00a0AB/BC) have lower heat duty more often than those with only sharp splits (ABC\u00a0\u2192\u00a0A/BC or ABC\u00a0\u2192\u00a0AB/C). Finally, the heat duty reduction due to thermal coupling is found to be less when compared to non-sharp basic configurations as against sharp-split basic configurations.",
     "keywords": ["Multicomponent distillation", "Synthesis", "Search space", "Basic", "Non-basic"]},
    {"article name": "Synthesis of distillation configurations. II: A search formulation for basic configurations",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.05.004",
     "publication date": "01-2010",
     "abstract": "It is known that there exist a combinatorially large number of multicomponent distillation column configurations for a given feed and given product specifications. In the accompanying part I of the paper, we classified configurations into basic configurations (those that use (n\u00a0\u2212\u00a01) columns for an n-component feed mixture) and non-basic configurations (those that use more than (n\u00a0\u2212\u00a01) columns for the same feed). As n exceeds four, the number of non-basic configurations greatly exceeds the number of basic configurations and have a huge impact on the size of the search space. However, through extensive calculations for a four-component feed, we have found that a non-basic configuration never has a lower heat duty than the lowest heat duty basic configuration. While prior researchers have proposed a mathematical formulation that could be used to generate only the set of basic configurations, we present an alternative supernetwork model to achieve this goal. We show how the supernetwork can be reduced to a binary integer program (BIP), and how all feasible configurations can be drawn for a given application using a suitable solver. We present different solution techniques, including parallel algorithms based on the fact that the evaluation of the performance for each configuration is independent of any other configuration. Through such a procedure, all applicable basic configurations can be rank listed for a given application according to a chosen criterion. We also extend the supernetwork formulation to a unified non-convex MINLP that can be used in a flexible way to find the best candidate configurations for a given application. We show the power of our search formulation in accommodating general separation techniques other than distillation, such as membranes, adsorption, etc.",
     "keywords": ["Multicomponent distillation", "Synthesis", "Supernetwork", "Optimization"]},
    {"article name": "Estimation and control of solid oxide fuel cell system",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.06.018",
     "publication date": "01-2010",
     "abstract": "Fuel cell is an electro-chemical device that converts chemical energy into electrical energy. Optimal operation of solid oxide fuel cells require different components built around them. The physical structure of the fuel cell and associated components impose certain restrictions on the operation of the system. Thus control of a fuel cell system is performed by nonlinear model predictive control which is able to handle multivariate objective and constraints. This work describes the application of nonlinear model predictive control applied on the fuel cell system by utilizing estimated states from the unscented Kalman filter. A brief description of the orthogonal collocation method as a means to discretize the nonlinear continuous model has also been provided in this work.",
     "keywords": ["Solid oxide fuel cell", "SOFC", "MPC", "UKF", "Orthogonal collocation method"]},
    {"article name": "Plantwide dynamics and control of processes with crystallization",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.07.004",
     "publication date": "01-2010",
     "abstract": "Recently Ward et al. [Ward, J. D., Doherty, M. F., & Yu, C. C. (2007). Plantwide operation and control of processes with crystallization. AIChE Journal, 53, 2885\u20132896] presented new shortcut expressions for the steady-state operation of plantwide processes with crystallization. The method allows the engineer to predict the steady-state gain between the production rate and certain other key process variables for various choices of controlled variables. The result is a simple analytical model that shows how the process operates for various choices of control structure. The previous work of Ward et al. considered only steady-state process operation. In this contribution we apply the method and results from our previous work to develop plantwide control structures, and test the structures using a rigorous non-linear dynamic process model. We demonstrate that the method produces workable plantwide control structures, and that the steady-state behavior of the dynamic process is consistent with the predictions of our previous work.",
     "keywords": ["Plantwide control", "Crystallization"]},
    {"article name": "Optimal facility layout under toxic release in process facilities: A stochastic approach",
     "doi": "https://doi.org/10.1016/j.compchemeng.2009.08.001",
     "publication date": "01-2010",
     "abstract": "This paper presents a new approach for the optimal facility siting considering the uncertainty of toxic release in one of the installed facilities. The proposed formulation incorporates the effect of wind speed, wind direction and atmospheric stability to calculate the risk of death via probit functions and Monte Carlo simulation. The overall problem is initially modeled as a disjunctive program where the Cartesian coordinates of each new facility for siting and cost-related variables are the main unknowns to determine. Then, the convex hull approach is used to reformulate the problem as a mixed integer nonlinear program (MINLP). The numerical difficulties are shown in a case study where multiple optimal layouts have been found. In general, the numerical results demonstrate the potential of this approach to improve the process layout design activity.",
     "keywords": ["MINLP applications", "Disjunctive programming", "Plant layout", "Toxic release uncertainty", "Convex hull"]}
    ]
}